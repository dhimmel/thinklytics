{
  "comments": [
    {
      "body_html": "<p>We just submitted this proposal in response to an NIH UH2 funding opportunity announcement for <a href=\"http://grants.nih.gov/grants/guide/rfa-files/RFA-CA-15-006.html\">Advancing Biomedical Science Using Crowdsourcing and Interactive Digital Media</a>. We love to get general feedback that we could use to improve upon the proposal if/when we need to resubmit or even if accepted. In particular, we are interested in resources and technologies we could use in this work, as well as collaborators who are independently working in areas of science game design and platform development.</p>",
      "body_md": "We just submitted this proposal in response to an NIH UH2 funding opportunity announcement for [Advancing Biomedical Science Using Crowdsourcing and Interactive Digital Media](http://grants.nih.gov/grants/guide/rfa-files/RFA-CA-15-006.html). We love to get general feedback that we could use to improve upon the proposal if/when we need to resubmit or even if accepted. In particular, we are interested in resources and technologies we could use in this work, as well as collaborators who are independently working in areas of science game design and platform development.\r\n",
      "comment_id": 243,
      "profile_id": 104,
      "published": "2015-06-09T02:28:14.517031Z",
      "thread_id": 73,
      "url": "/discussion/open-for-feedback/73"
    },
    {
      "body_html": "<p>The proposal primarily measures the novelty of the 3,985 pilot pathway images based on unique genes — genes not currently in any WikiPathway pathways. While this is the simplest measure and therefore a good starting point, I think unique edges are a better assessment of novelty than unique genes. For example, a pathway could be composed of genes that are all already present elsewhere but are not connected anywhere else. This distinction is mentioned in the proposal:</p>\r\n\r\n<blockquote><p>Thus, even the 28% of symbols that overlap with current human pathways (Fig. 2, blue) may provide new interaction content.</p></blockquote>\r\n\r\n<p>While the initial OCR implementation does not appear to extract edges, each pathway could be represented as a gene set. Then novelty could be measured as:</p>\r\n\r\n<ul><li>whether any other pathways are a superset or subset of the query pathway</li><li>the max <a href=\"https://en.wikipedia.org/wiki/Jaccard_index\">Jaccard index</a> of the query pathway with all other pathways</li></ul>\r\n\r\n<p>An edge-based conception of novelty will provide greater recall of novel pathways. At the current stage, where plentiful novel information exists, node-based novelty will definitely work. I agree that expanding the number of genes in at least one pathway should be a primary goal but think that your current node-based metrics may undersell the extent of novel pathways.</p>",
      "body_md": "The proposal primarily measures the novelty of the 3,985 pilot pathway images based on unique genes -- genes not currently in any WikiPathway pathways. While this is the simplest measure and therefore a good starting point, I think unique edges are a better assessment of novelty than unique genes. For example, a pathway could be composed of genes that are all already present elsewhere but are not connected anywhere else. This distinction is mentioned in the proposal:\r\n\r\n> Thus, even the 28% of symbols that overlap with current human pathways (Fig. 2, blue) may provide new interaction content.\r\n\r\nWhile the initial OCR implementation does not appear to extract edges, each pathway could be represented as a gene set. Then novelty could be measured as:\r\n\r\n+ whether any other pathways are a superset or subset of the query pathway\r\n+ the max [Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index) of the query pathway with all other pathways\r\n\r\nAn edge-based conception of novelty will provide greater recall of novel pathways. At the current stage, where plentiful novel information exists, node-based novelty will definitely work. I agree that expanding the number of genes in at least one pathway should be a primary goal but think that your current node-based metrics may undersell the extent of novel pathways.",
      "comment_id": 248,
      "profile_id": 17,
      "published": "2015-06-10T23:17:15.345426Z",
      "thread_id": 75,
      "url": "/discussion/pathway-novelty-based-on-unique-relationships-rather-than-genes/75"
    },
    {
      "body_html": "<p>How do you plan to handle when the same pathway is represented in multiple images? For example, one publication may add an additional gene to a pathway first put forward by a previous publication. Is merging duplicated pathways outside the scope of this proposal?</p>\r\n\r\n<p>I don't think that consolidating duplicates needs to be a focus of this project, but am intrigued by the difficulty of this problem and curious as to your insights. Does WikiPathways currently rely on users to merge duplicates?</p>",
      "body_md": "How do you plan to handle when the same pathway is represented in multiple images? For example, one publication may add an additional gene to a pathway first put forward by a previous publication. Is merging duplicated pathways outside the scope of this proposal?\r\n\r\nI don't think that consolidating duplicates needs to be a focus of this project, but am intrigued by the difficulty of this problem and curious as to your insights. Does WikiPathways currently rely on users to merge duplicates?",
      "comment_id": 249,
      "profile_id": 17,
      "published": "2015-06-10T23:23:57.888448Z",
      "thread_id": 76,
      "url": "/discussion/duplicate-pathway-detection-and-resolution/76"
    },
    {
      "body_html": "<p>I will provide my comments related to formatting and visual improvements here — feedback that is unrelated to any substantial aspect of the proposal.</p>\r\n\r\n<ol><li><p>Misspelling of most:</p><blockquote><p><strong>mosst</strong> pathway information is still published solely as static</p></blockquote></li><li><p>I would use the markdown list feature for the paragraph beginning with</p><blockquote><p>Several caveats to this survey ...</p></blockquote></li><li><p>Figure 3 is too low resolution — specific gene names are not visible. The human/machine difficultly aspect of the image is discernible, but I would like to see the actual images, especially to gain familiarity with examples of pathway figures.</p></li><li><p>Thinklab citations would improve online readability. Minimal extra work would be needed if you use the <a href=\"http://thinklab.com/help/writing-in-markdown\">Citation Key method</a>. I would use <a href=\"http://crossref.org/\">crossref</a> for doi lookup when needed. The intelligent citations will help with readability and hopefully spur discussions on referenced works.</p></li></ol>",
      "body_md": "I will provide my comments related to formatting and visual improvements here -- feedback that is unrelated to any substantial aspect of the proposal.\r\n\r\n1. Misspelling of most:\r\n> **mosst** pathway information is still published solely as static\r\n\r\n+ I would use the markdown list feature for the paragraph beginning with\r\n> Several caveats to this survey ...\r\n\r\n+ Figure 3 is too low resolution -- specific gene names are not visible. The human/machine difficultly aspect of the image is discernible, but I would like to see the actual images, especially to gain familiarity with examples of pathway figures.\r\n  \r\n+ Thinklab citations would improve online readability. Minimal extra work would be needed if you use the [Citation Key method](http://thinklab.com/help/writing-in-markdown). I would use [crossref](http://crossref.org/) for doi lookup when needed. The intelligent citations will help with readability and hopefully spur discussions on referenced works.",
      "comment_id": 250,
      "profile_id": 17,
      "published": "2015-06-10T23:34:36.687297Z",
      "thread_id": 77,
      "url": "/discussion/thinklab-specific-comments/77"
    },
    {
      "body_html": "<p>What a fantastic proposal! The contribution is immense — unlocking decades of knowledge residing in raster images. The proposal combines many state of the art tools and methodologies. It builds off of important open science resources, such as PubMed Central and wikis. Not only is the idea exceptional, the technical details of the implementation appear sound and current. The team has proven their technical expertise through their creation of <a href=\"http://www.wikipathways.org/index.php/WikiPathways\">WikiPathways</a>.</p>\r\n\r\n<p>As a scientist who relies on open data as the input to my research <span class=\"citation\">[<a href=\"/p/rephetio\" class=\"citation\" data-key=\"10.15363/thinklab.4\">1</a>]</span>, I can attest to the importance of literature-derived informatics resources. The compound-target databases such as BindingDB, ChEMBL, PubChem Assay, and DrugBank have been exceptionally helpful. MEDLINE topic annotations and curated protein interaction databases have also been crucial. I wholeheartedly agree that pathway images are a fruitful information hive in need of a skilled investigator.</p>",
      "body_md": "What a fantastic proposal! The contribution is immense -- unlocking decades of knowledge residing in raster images. The proposal combines many state of the art tools and methodologies. It builds off of important open science resources, such as PubMed Central and wikis. Not only is the idea exceptional, the technical details of the implementation appear sound and current. The team has proven their technical expertise through their creation of [WikiPathways](http://www.wikipathways.org/index.php/WikiPathways).\r\n\r\nAs a scientist who relies on open data as the input to my research [@10.15363/thinklab.4], I can attest to the importance of literature-derived informatics resources. The compound-target databases such as BindingDB, ChEMBL, PubChem Assay, and DrugBank have been exceptionally helpful. MEDLINE topic annotations and curated protein interaction databases have also been crucial. I wholeheartedly agree that pathway images are a fruitful information hive in need of a skilled investigator.",
      "comment_id": 251,
      "profile_id": 17,
      "published": "2015-06-10T23:48:36.579195Z",
      "thread_id": 73,
      "url": "/discussion/open-for-feedback/73#2"
    },
    {
      "body_html": "<blockquote><p>Figure 3 is too low resolution</p></blockquote>\r\n\r\n<p>Just FYI, people will soon be able to insert figures and have them appear somewhat like they do on PLOS and other websites. There will be an option to insert figures at a smaller size that can be clicked to zoom, and an option to have the figure full width. There will also be a button to download the original. This should help things.</p>\r\n\r\n<blockquote><p>Thinklab citations would improve online readability</p></blockquote>\r\n\r\n<p><a href=\"/u/alexanderpico\" class=\"username\">@alexanderpico</a> we'd be happy to take care of this for you.</p>",
      "body_md": "> Figure 3 is too low resolution\r\n\r\nJust FYI, people will soon be able to insert figures and have them appear somewhat like they do on PLOS and other websites. There will be an option to insert figures at a smaller size that can be clicked to zoom, and an option to have the figure full width. There will also be a button to download the original. This should help things.\r\n\r\n> Thinklab citations would improve online readability\r\n\r\n@alexanderpico we'd be happy to take care of this for you.\r\n",
      "comment_id": 253,
      "profile_id": 2,
      "published": "2015-06-11T04:08:54.782714Z",
      "thread_id": 77,
      "url": "/discussion/thinklab-specific-comments/77#2"
    },
    {
      "body_html": "<p>I'm most excited about the novel edges as well, but this is simply impossible to estimate from the current OCR results on our sample set. Realize that on average one gene is recognized per image... The distribution of recognized genes is heavily skewed, however (Fig 2). But even if we consider the 1104 pathways were 5 or more genes were recognized, how could we honestly estimate edges. We can't assume that the detected nodes are connected to each other.</p>\r\n\r\n<p>Likewise, the other novelty measures you suggest would be great to apply to complete gene sets from a sample set of pathways. Unfortunately, we just aren't even close to getting that from the current OCR results.</p>\r\n\r\n<p>Of course, after we actually model a few hundred of these images, we will be able to start estimating novel nodes, novel sets and novel edges more reliably. Between now and the NIH reviews, perhaps we'll try to brute force a set of these to get at these numbers.</p>",
      "body_md": "I'm most excited about the novel edges as well, but this is simply impossible to estimate from the current OCR results on our sample set. Realize that on average one gene is recognized per image... The distribution of recognized genes is heavily skewed, however (Fig 2). But even if we consider the 1104 pathways were 5 or more genes were recognized, how could we honestly estimate edges. We can't assume that the detected nodes are connected to each other.\r\n\r\nLikewise, the other novelty measures you suggest would be great to apply to complete gene sets from a sample set of pathways. Unfortunately, we just aren't even close to getting that from the current OCR results.\r\n\r\nOf course, after we actually model a few hundred of these images, we will be able to start estimating novel nodes, novel sets and novel edges more reliably. Between now and the NIH reviews, perhaps we'll try to brute force a set of these to get at these numbers.",
      "comment_id": 255,
      "profile_id": 104,
      "published": "2015-06-12T01:49:50.154857Z",
      "thread_id": 75,
      "url": "/discussion/pathway-novelty-based-on-unique-relationships-rather-than-genes/75#2"
    },
    {
      "body_html": "<p>Contrary to the strategy of most pathway archive that strive for a single set of canonical pathways, WikiPathways loves redundancy... because, well, that's how biology actually works :)  </p>\r\n\r\n<p>For example, instead of 1 Apoptosis pathway, we'd like to see 50+ Apoptosis pathways, depending on cell type, tissue type, conditions, developmental stages, disease states, etc. These might have a lot of redundancy, but the typical pathway analysis methods can already handle that. They typically provide a rank order of overrepresented pathways, for example, and being able to see exactly <em>which</em> Apoptosis version is most like your dataset would be very useful.</p>\r\n\r\n<p>So, this is predicated on having ontology-based tags for the sources of variation. We've started with 3 standard ontologies, but this can expand by demand.  And, if two pathways are 100% identical in all ways, well then, yes, we should merge them. This will be rare (hasn't happened yet) and the bulk of the work will be on providing good search, browse and grouping tools in our UI.</p>\r\n\r\n<p>For this project, I left out these details because I'm just focusing on nodes and edges (for simplicity), but you can easily imagine taking the same products from this first round and doing another cycle where the focus is capturing biological context.  The issue of redundancy is also address by our simple approach of prioritizing (and providing point bonuses for) novel genes. This alone will drive focus to less redundant pathway first... though we still want all variants in the end.</p>",
      "body_md": "Contrary to the strategy of most pathway archive that strive for a single set of canonical pathways, WikiPathways loves redundancy... because, well, that's how biology actually works :)  \r\n\r\nFor example, instead of 1 Apoptosis pathway, we'd like to see 50+ Apoptosis pathways, depending on cell type, tissue type, conditions, developmental stages, disease states, etc. These might have a lot of redundancy, but the typical pathway analysis methods can already handle that. They typically provide a rank order of overrepresented pathways, for example, and being able to see exactly *which* Apoptosis version is most like your dataset would be very useful.\r\n\r\nSo, this is predicated on having ontology-based tags for the sources of variation. We've started with 3 standard ontologies, but this can expand by demand.  And, if two pathways are 100% identical in all ways, well then, yes, we should merge them. This will be rare (hasn't happened yet) and the bulk of the work will be on providing good search, browse and grouping tools in our UI.\r\n\r\nFor this project, I left out these details because I'm just focusing on nodes and edges (for simplicity), but you can easily imagine taking the same products from this first round and doing another cycle where the focus is capturing biological context.  The issue of redundancy is also address by our simple approach of prioritizing (and providing point bonuses for) novel genes. This alone will drive focus to less redundant pathway first... though we still want all variants in the end.",
      "comment_id": 256,
      "profile_id": 104,
      "published": "2015-06-12T02:00:25.184992Z",
      "thread_id": 76,
      "url": "/discussion/duplicate-pathway-detection-and-resolution/76#2"
    },
    {
      "body_html": "<ol><li>Fixed</li><li>I think I prefer the paragraph form here.</li><li>Looking forward to the PLoS-like feature. The goal of this figure, though, isn't really to inspect the details of the image. Honestly, I'd simply recommend searching pubmed central for \"signaling pathway\" and you'll see exactly what we're working with.</li><li>Thanks Jesse. The help with formatting would be appreciated. I'm going to stick with Endnote in Word for the final version, so I'm not too motivated to markdown all the refs...</li></ol>",
      "body_md": "1. Fixed\r\n2. I think I prefer the paragraph form here.\r\n3. Looking forward to the PLoS-like feature. The goal of this figure, though, isn't really to inspect the details of the image. Honestly, I'd simply recommend searching pubmed central for \"signaling pathway\" and you'll see exactly what we're working with.\r\n4. Thanks Jesse. The help with formatting would be appreciated. I'm going to stick with Endnote in Word for the final version, so I'm not too motivated to markdown all the refs...",
      "comment_id": 258,
      "profile_id": 104,
      "published": "2015-06-12T02:10:38.420194Z",
      "thread_id": 77,
      "url": "/discussion/thinklab-specific-comments/77#3"
    },
    {
      "body_html": "<p>I've gone ahead and converted the references in your proposal to a native ThinkLab format.  In the future we may actually be able to automate this process.</p>\r\n\r\n<p>I also took the liberty to convert your figures to use <a href=\"http://thinklab.com/discussion/we-now-have-a-figures-feature/79\">our new figures feature</a>. If you'd like to edit the figures, you can find them by clicking \"edit figures\" from the project manager page. If you'd like to go ahead and upload a higher resolution version of the figure that <a href=\"/u/dhimmel\" class=\"username\">@dhimmel</a> commented on I think that would be a good idea. We currently don't have a click to zoom or download feature but those will be added soon.</p>",
      "body_md": "I've gone ahead and converted the references in your proposal to a native ThinkLab format.  In the future we may actually be able to automate this process.\r\n\r\nI also took the liberty to convert your figures to use [our new figures feature](http://thinklab.com/discussion/we-now-have-a-figures-feature/79). If you'd like to edit the figures, you can find them by clicking \"edit figures\" from the project manager page. If you'd like to go ahead and upload a higher resolution version of the figure that @dhimmel commented on I think that would be a good idea. We currently don't have a click to zoom or download feature but those will be added soon.",
      "comment_id": 261,
      "profile_id": 2,
      "published": "2015-06-15T05:27:53.605230Z",
      "thread_id": 77,
      "url": "/discussion/thinklab-specific-comments/77#4"
    },
    {
      "body_html": "<p>This is great. I am currently trying to integrate Pathway data into my analysis. So I will try to help as much as I can. </p>",
      "body_md": "This is great. I am currently trying to integrate Pathway data into my analysis. So I will try to help as much as I can. ",
      "comment_id": 355,
      "profile_id": 35,
      "published": "2015-08-06T15:03:09.615079Z",
      "thread_id": 73,
      "url": "/discussion/open-for-feedback/73#3"
    },
    {
      "body_html": "<p>At this point it was unclear to me what an internal curation team was</p>",
      "body_md": "At this point it was unclear to me what an internal curation team was",
      "comment_id": 516,
      "profile_id": 2,
      "published": "2015-10-30T22:07:19.360877Z",
      "thread_id": 117,
      "url": "/doc/3/review#1"
    },
    {
      "body_html": "<p>I like that you are highlighting your efforts to correct the problem at its source</p>",
      "body_md": "I like that you are highlighting your efforts to correct the problem at its source",
      "comment_id": 517,
      "profile_id": 2,
      "published": "2015-10-30T22:23:55.572675Z",
      "thread_id": 117,
      "url": "/doc/3/review#2"
    },
    {
      "body_html": "<p>It is unclear to me what I am supposed to learn from this graph. Does it aide in understanding something? </p>",
      "body_md": "It is unclear to me what I am supposed to learn from this graph. Does it aide in understanding something? ",
      "comment_id": 518,
      "profile_id": 2,
      "published": "2015-10-30T22:24:33.963013Z",
      "thread_id": 117,
      "url": "/doc/3/review#3"
    },
    {
      "body_html": "<p>Is it safe to assume that people already know what tunable game mechanics are?</p>",
      "body_md": "Is it safe to assume that people already know what tunable game mechanics are?",
      "comment_id": 519,
      "profile_id": 2,
      "published": "2015-10-30T22:28:02.164257Z",
      "thread_id": 117,
      "url": "/doc/3/review#4"
    },
    {
      "body_html": "<p>It's unclear to me what aggregation and valuation mean here.</p>",
      "body_md": "It's unclear to me what aggregation and valuation mean here.",
      "comment_id": 520,
      "profile_id": 2,
      "published": "2015-10-30T22:28:31.521058Z",
      "thread_id": 117,
      "url": "/doc/3/review#5"
    },
    {
      "body_html": "<p>Great, but this seems weak without explanation. Perhaps mention you'll get into the detail later?</p>",
      "body_md": "Great, but this seems weak without explanation. Perhaps mention you'll get into the detail later?",
      "comment_id": 521,
      "profile_id": 2,
      "published": "2015-10-30T22:29:40.919670Z",
      "thread_id": 117,
      "url": "/doc/3/review#6"
    },
    {
      "body_html": "<p>I find drawing this analogy just makes things harder to read. I also don't think it adds much value. </p>",
      "body_md": "I find drawing this analogy just makes things harder to read. I also don't think it adds much value. ",
      "comment_id": 522,
      "profile_id": 2,
      "published": "2015-10-30T22:30:19.549193Z",
      "thread_id": 117,
      "url": "/doc/3/review#7"
    },
    {
      "body_html": "<p>Why not just say something like \"Engaging the general public will accelerate pathway modeling\"? Are you trying to match the language of the FOA?</p>",
      "body_md": "Why not just say something like \"Engaging the general public will accelerate pathway modeling\"? Are you trying to match the language of the FOA?",
      "comment_id": 523,
      "profile_id": 2,
      "published": "2015-10-30T22:35:41.703015Z",
      "thread_id": 117,
      "url": "/doc/3/review#8"
    },
    {
      "body_html": "<p>Is it worth saying this here if you're not going to explain how it does this?</p>",
      "body_md": "Is it worth saying this here if you're not going to explain how it does this?",
      "comment_id": 524,
      "profile_id": 2,
      "published": "2015-10-30T22:37:59.785570Z",
      "thread_id": 117,
      "url": "/doc/3/review#9"
    },
    {
      "body_html": "<p>If points calculations are done in the browser it seems like a user could cheat and artificially inflate their score. If we are talking about simple calculations it is hard to imagine there is a real need to have these done client side.</p>",
      "body_md": "If points calculations are done in the browser it seems like a user could cheat and artificially inflate their score. If we are talking about simple calculations it is hard to imagine there is a real need to have these done client side.",
      "comment_id": 525,
      "profile_id": 2,
      "published": "2015-10-30T22:43:42.562382Z",
      "thread_id": 117,
      "url": "/doc/3/review#10"
    },
    {
      "body_html": "<p>Looks good </p>",
      "body_md": "Looks good ",
      "comment_id": 526,
      "profile_id": 2,
      "published": "2015-10-30T22:44:03.539623Z",
      "thread_id": 117,
      "url": "/doc/3/review#11"
    },
    {
      "body_html": "<p>Perhaps you should say that you are in fact <em>designing</em> it for others to extend — not just anticipating. You could also mention other open source projects that were successfully extended in the manner you imagine for Pathways4Life. </p>",
      "body_md": "Perhaps you should say that you are in fact *designing* it for others to extend -- not just anticipating. You could also mention other open source projects that were successfully extended in the manner you imagine for Pathways4Life. ",
      "comment_id": 527,
      "profile_id": 2,
      "published": "2015-10-30T22:44:58.727823Z",
      "thread_id": 117,
      "url": "/doc/3/review#12"
    },
    {
      "body_html": "<p>I suggest a points system and leaderboard specific to each disease. I suspect there are people motivated to help out and be acknowledged within the scope of a particular disease.</p>",
      "body_md": "I suggest a points system and leaderboard specific to each disease. I suspect there are people motivated to help out and be acknowledged within the scope of a particular disease.",
      "comment_id": 528,
      "profile_id": 2,
      "published": "2015-10-30T22:45:59.353661Z",
      "thread_id": 117,
      "url": "/doc/3/review#13"
    },
    {
      "body_html": "<blockquote><p>The equation for calculating confidence scores will be the sum of observations (+1 for confirmation, –1 for rejection, o), weighted by the relative skill level of the participant (0–1, w). We can include a multiplier for negative observations to convey the extra effort in making a negative call (e.g., 2, m), and assess this sum against a threshold (e.g., 10, T) to ultimately mark a snippet as confirmed (e.g., S≥1).</p></blockquote>\r\n\r\n<p>Once you have a decent amount of data accumulated, and assuming you've marked a decent number of pathways as officially confirmed, I expect that you could use machine learning to optimize confidence score equations.</p>\r\n\r\n<p>You could start by optimizing the equation for calculating how much trust the system should have in any given user. You might look at how new the user is, how many of their edits turned out to be correct, and what was the difficulty level associated with those edits. You would then probably want to optimize the equation for confidence in a particular snippet, as well as confidence in an entire pathway being correct.</p>",
      "body_md": "> The equation for calculating confidence scores will be the sum of observations (+1 for confirmation, –1 for rejection, o), weighted by the relative skill level of the participant (0–1, w). We can include a multiplier for negative observations to convey the extra effort in making a negative call (e.g., 2, m), and assess this sum against a threshold (e.g., 10, T) to ultimately mark a snippet as confirmed (e.g., S≥1).\r\n\r\nOnce you have a decent amount of data accumulated, and assuming you've marked a decent number of pathways as officially confirmed, I expect that you could use machine learning to optimize confidence score equations.\r\n\r\nYou could start by optimizing the equation for calculating how much trust the system should have in any given user. You might look at how new the user is, how many of their edits turned out to be correct, and what was the difficulty level associated with those edits. You would then probably want to optimize the equation for confidence in a particular snippet, as well as confidence in an entire pathway being correct.",
      "comment_id": 529,
      "profile_id": 2,
      "published": "2015-10-30T23:15:04.371754Z",
      "thread_id": 119,
      "url": "/discussion/use-machine-learning-to-calculate-confidence-scores-for-users-snippets-and-pathways/119"
    },
    {
      "body_html": "<p>The word 'curate' is used a lot. I personally find this word vague. It may be helpful to be more specific about what you're doing.</p>",
      "body_md": "The word 'curate' is used a lot. I personally find this word vague. It may be helpful to be more specific about what you're doing.",
      "comment_id": 530,
      "profile_id": 2,
      "published": "2015-10-30T23:20:37.694173Z",
      "thread_id": 117,
      "url": "/doc/3/review#14"
    },
    {
      "body_html": "<p>A not insignificant part of the proposal seems to be gamification based on users leveling up to tackle more challenging pathways, while scoring more points.</p>\r\n\r\n<p>This is great but it seems to me this presumes that there are in fact noticeable differences in difficulty between tasks. As someone who is not familiar with modeling pathways, I wonder what makes one harder than another? Aren't users essentially just copying the nodes, arrows, and relationships they see in the image? Couldn't most users do all of them with relative ease? It might be worthwhile addressing this.</p>\r\n\r\n<p>The <a href=\"http://thinklab.com/p/pathways4life/proposal/1/review#section-19\">difficulty matrix figure</a> only specifies a set of easy pathways for humans (90% of them) and a set of hard ones (9%).</p>",
      "body_md": "A not insignificant part of the proposal seems to be gamification based on users leveling up to tackle more challenging pathways, while scoring more points.\r\n\r\nThis is great but it seems to me this presumes that there are in fact noticeable differences in difficulty between tasks. As someone who is not familiar with modeling pathways, I wonder what makes one harder than another? Aren't users essentially just copying the nodes, arrows, and relationships they see in the image? Couldn't most users do all of them with relative ease? It might be worthwhile addressing this.\r\n\r\nThe [difficulty matrix figure](http://thinklab.com/p/pathways4life/proposal/1/review#section-19) only specifies a set of easy pathways for humans (90% of them) and a set of hard ones (9%).",
      "comment_id": 531,
      "profile_id": 2,
      "published": "2015-10-30T23:49:10.464266Z",
      "thread_id": 120,
      "url": "/discussion/are-there-significant-differences-in-difficulty-between-pathway-modeling-tasks/120"
    },
    {
      "body_html": "<blockquote><p>Pathway information is immensely useful for analyzing and interpreting large-scale omics data</p></blockquote>\r\n\r\n<p>Does this proposal assume that the reader is already aware of how valuable it would be to have pathways modeled and machine readable? As someone who is not familiar with this, I didn't come away with a great appreciation of the value of this proposal.</p>\r\n\r\n<p>How about listing all the awesome science that your project will enable? I think we want readers thinking, \"Damn, we need all pathways modeled ASAP — how can we build upon WikiPathways and accelerate this progress?\"</p>",
      "body_md": "> Pathway information is immensely useful for analyzing and interpreting large-scale omics data\r\n\r\nDoes this proposal assume that the reader is already aware of how valuable it would be to have pathways modeled and machine readable? As someone who is not familiar with this, I didn't come away with a great appreciation of the value of this proposal.\r\n\r\nHow about listing all the awesome science that your project will enable? I think we want readers thinking, \"Damn, we need all pathways modeled ASAP -- how can we build upon WikiPathways and accelerate this progress?\"",
      "comment_id": 532,
      "profile_id": 2,
      "published": "2015-10-31T00:11:28.660534Z",
      "thread_id": 121,
      "url": "/discussion/how-about-examples-of-the-awesome-science-that-pathway-modeling-enables/121"
    },
    {
      "body_html": "<p>While there is <a href=\"http://thinklab.com/p/pathways4life/proposal/1/review#section-7\">a section</a> of the  proposal addressing this, I feel the scope of the challenge could be clarified. </p>\r\n\r\n<p>Here's what I'd like to know:</p>\r\n\r\n<ul><li>How many total pathways do you expect users to model throughout the lifetime of the platform? (Presuming the project is successful and users do in fact use it)</li><li>How many man-hours do you expect it to take, on average, to model each pathway?</li></ul>\r\n\r\n<p>With these two pieces of data we can think clearly about whether it's worth the time investment of creating a crowdsourcing platform and gamifying the experience. As the <a href=\"http://thinklab.com/p/pathways4life/proposal/1/review#section-42\">proposal mentioned</a>, you could just pay people to do this through Amazon Mechanical Turk.</p>\r\n\r\n<p>Now I think there's likely a lot of value to getting real science do-gooders involved, creating a platform that may be reusable, etc. However, I think there should be more clarity about the scope of what you're proposing.</p>",
      "body_md": "While there is [a section](http://thinklab.com/p/pathways4life/proposal/1/review#section-7) of the  proposal addressing this, I feel the scope of the challenge could be clarified. \r\n\r\nHere's what I'd like to know:\r\n\r\n- How many total pathways do you expect users to model throughout the lifetime of the platform? (Presuming the project is successful and users do in fact use it)\r\n- How many man-hours do you expect it to take, on average, to model each pathway?\r\n\r\nWith these two pieces of data we can think clearly about whether it's worth the time investment of creating a crowdsourcing platform and gamifying the experience. As the [proposal mentioned](http://thinklab.com/p/pathways4life/proposal/1/review#section-42), you could just pay people to do this through Amazon Mechanical Turk.\r\n\r\nNow I think there's likely a lot of value to getting real science do-gooders involved, creating a platform that may be reusable, etc. However, I think there should be more clarity about the scope of what you're proposing.",
      "comment_id": 533,
      "profile_id": 2,
      "published": "2015-10-31T00:29:30.955580Z",
      "thread_id": 122,
      "url": "/discussion/clarifying-the-scope-of-the-challenge/122"
    },
    {
      "body_html": "<p>If I was considering funding this proposal one of the big questions I would be asking myself is: will users use the platform? Sometimes you build it and they don't come.</p>\r\n\r\n<p>As you'll see from my annotations I think you've got a great <a href=\"http://thinklab.com/p/pathways4life/proposal/1/review#section-36\">plan for gamifying the experience</a> — and it sounds like you've got good <a href=\"http://thinklab.com/p/pathways4life/proposal/1/review#section-41\">networks for reaching out to people</a> as well. However, one of the things I'd really like to know is do you know exactly what made other platforms successful? I think it would be useful to do an analysis and have the summary in the proposal. What can you learn from those that have succeeded, and what can you learn from those that failed? Have you learned those lessons?</p>\r\n\r\n<p>It would also be great to get those that have been involved in previous efforts commenting here on Thinklab as well!</p>",
      "body_md": "If I was considering funding this proposal one of the big questions I would be asking myself is: will users use the platform? Sometimes you build it and they don't come.\r\n\r\nAs you'll see from my annotations I think you've got a great [plan for gamifying the experience](http://thinklab.com/p/pathways4life/proposal/1/review#section-36) -- and it sounds like you've got good [networks for reaching out to people](http://thinklab.com/p/pathways4life/proposal/1/review#section-41) as well. However, one of the things I'd really like to know is do you know exactly what made other platforms successful? I think it would be useful to do an analysis and have the summary in the proposal. What can you learn from those that have succeeded, and what can you learn from those that failed? Have you learned those lessons?\r\n\r\nIt would also be great to get those that have been involved in previous efforts commenting here on Thinklab as well!",
      "comment_id": 534,
      "profile_id": 2,
      "published": "2015-10-31T00:48:03.323554Z",
      "thread_id": 123,
      "url": "/discussion/what-made-previous-crowdsourced-science-games-succeed-or-not/123"
    },
    {
      "body_html": "<p>How about a link?</p>",
      "body_md": "How about a link?",
      "comment_id": 535,
      "profile_id": 2,
      "published": "2015-10-31T01:50:26.212983Z",
      "thread_id": 117,
      "url": "/doc/3/review#15"
    },
    {
      "body_html": "<p>This is a word that is used frequently and pretty well understood within the community of folks that build and maintain biological databases.  e.g. an important annual conference is run by the 'biocuration' society.  <a href=\"http://www.biocurator.org\">http://www.biocurator.org</a>  From my viewpoint the use is pretty clear.  Perhaps a definition early on would clarify this for folks a little farther outside this community though.</p>",
      "body_md": "This is a word that is used frequently and pretty well understood within the community of folks that build and maintain biological databases.  e.g. an important annual conference is run by the 'biocuration' society.  http://www.biocurator.org  From my viewpoint the use is pretty clear.  Perhaps a definition early on would clarify this for folks a little farther outside this community though.",
      "comment_id": 542,
      "profile_id": 48,
      "published": "2015-11-09T19:59:52.874185Z",
      "thread_id": 117,
      "url": "/doc/3/review#16"
    },
    {
      "body_html": "<p>Scanning the proposal without reading deeply, I would concur with your confusion.  The point of the figure and its relevance could be more clear.</p>",
      "body_md": "Scanning the proposal without reading deeply, I would concur with your confusion.  The point of the figure and its relevance could be more clear.",
      "comment_id": 543,
      "profile_id": 48,
      "published": "2015-11-09T20:01:48.772705Z",
      "thread_id": 117,
      "url": "/doc/3/review#17"
    },
    {
      "body_html": "<p>Some thoughts on this.  The biggest successes so far (Foldit, EteRNA, Eyewire, MalariaSpot, Galaxy Zoo) in this genre have focused on problems that are almost entirely visual.  The work that has so far been done on non-visual problems (our lab: Dizeez, The Cure, mark2cure, VU Amsterdam/IBM: Dr. Detective, CMU: Verbosity, MIT/ISI: Learner) have met with less success - though haven't been total flops either.  However, far less money and time have gone into the development of games for the non-visual problems which creates a chicken and egg problem.  Have people not invested in other problem classes because its really just a bad a idea or have we just not seen the success because we haven't made the investment yet?  </p>\r\n\r\n<p>The wikipathways proposal is interesting in this sense because, though it does have a strong visual appeal, the tasks involved (e.g. making sure a node is correctly marked with the right gene identifier) are mainly linguistic in nature.   </p>",
      "body_md": "Some thoughts on this.  The biggest successes so far (Foldit, EteRNA, Eyewire, MalariaSpot, Galaxy Zoo) in this genre have focused on problems that are almost entirely visual.  The work that has so far been done on non-visual problems (our lab: Dizeez, The Cure, mark2cure, VU Amsterdam/IBM: Dr. Detective, CMU: Verbosity, MIT/ISI: Learner) have met with less success - though haven't been total flops either.  However, far less money and time have gone into the development of games for the non-visual problems which creates a chicken and egg problem.  Have people not invested in other problem classes because its really just a bad a idea or have we just not seen the success because we haven't made the investment yet?  \r\n\r\nThe wikipathways proposal is interesting in this sense because, though it does have a strong visual appeal, the tasks involved (e.g. making sure a node is correctly marked with the right gene identifier) are mainly linguistic in nature.   ",
      "comment_id": 544,
      "profile_id": 48,
      "published": "2015-11-09T20:42:12.361109Z",
      "thread_id": 123,
      "url": "/discussion/what-made-previous-crowdsourced-science-games-succeed-or-not/123#2"
    },
    {
      "body_html": "<p>Possibly. Given that this was for a 2-year grant, however, I think dedicating machine learning to the tuning of a simple scoring mechanism would be outside the scope of the project.  </p>",
      "body_md": "Possibly. Given that this was for a 2-year grant, however, I think dedicating machine learning to the tuning of a simple scoring mechanism would be outside the scope of the project.  ",
      "comment_id": 576,
      "profile_id": 104,
      "published": "2015-12-16T02:14:56.590794Z",
      "thread_id": 119,
      "url": "/discussion/use-machine-learning-to-calculate-confidence-scores-for-users-snippets-and-pathways/119#2"
    },
    {
      "body_html": "<p>Thanks!  You can edit and add pathways of interest today at WikiPathways.org. This grant proposal was rejected, but was for <em>additional</em> mechanisms of curation. The basics are already in place and ready for your participation.</p>",
      "body_md": "Thanks!  You can edit and add pathways of interest today at WikiPathways.org. This grant proposal was rejected, but was for *additional* mechanisms of curation. The basics are already in place and ready for your participation.",
      "comment_id": 577,
      "profile_id": 104,
      "published": "2015-12-16T02:16:35.842191Z",
      "thread_id": 73,
      "url": "/discussion/open-for-feedback/73#4"
    },
    {
      "body_html": "<p>I was surprised by this as well. In our preliminary look at 40k images, we found the difficulty range to be huge. I tried to express this in the matrix figure, but really didn't have the space to include a sampling of images at sufficient resolution. There's wide range and a spectrum of levels between.</p>\r\n\r\n<p>The easy ones are really easy. The harder ones would actually be hard for an expert biologist. In fact, I suspect there are many that are NOT possible to model. We would filter out as many of these as we could ahead of time, but we could also detect these by \"skip\" event and lack of consensus.  It comes down to the fact that researchers literally makeup their own unique formats and conventions almost every time a pathway drawing is made outside of a dedicated modeling tool.  It's a big problem.</p>",
      "body_md": "I was surprised by this as well. In our preliminary look at 40k images, we found the difficulty range to be huge. I tried to express this in the matrix figure, but really didn't have the space to include a sampling of images at sufficient resolution. There's wide range and a spectrum of levels between.\r\n\r\nThe easy ones are really easy. The harder ones would actually be hard for an expert biologist. In fact, I suspect there are many that are NOT possible to model. We would filter out as many of these as we could ahead of time, but we could also detect these by \"skip\" event and lack of consensus.  It comes down to the fact that researchers literally makeup their own unique formats and conventions almost every time a pathway drawing is made outside of a dedicated modeling tool.  It's a big problem.",
      "comment_id": 579,
      "profile_id": 104,
      "published": "2015-12-16T02:23:21.634439Z",
      "thread_id": 120,
      "url": "/discussion/are-there-significant-differences-in-difficulty-between-pathway-modeling-tasks/120#2"
    },
    {
      "body_html": "<p>Right. This is written to a fairly specific target audience of reviewers. The assumption is safe that with the initial paragraph they'd connect the value of the modeling to major questions being asked across almost all fields of biomedical research.</p>\r\n\r\n<p>In fact, now that I've got the Summary Statement back from the review committee, I can confirm that our \"significance\" score was one of the best overall. The only critique in this category was from one reviewer regarding the lack of \"advancing the science of crowdsourcing\"...  That indeed was not an aim of this proposal, but it was important to this reviewer :)</p>",
      "body_md": "Right. This is written to a fairly specific target audience of reviewers. The assumption is safe that with the initial paragraph they'd connect the value of the modeling to major questions being asked across almost all fields of biomedical research.\r\n\r\nIn fact, now that I've got the Summary Statement back from the review committee, I can confirm that our \"significance\" score was one of the best overall. The only critique in this category was from one reviewer regarding the lack of \"advancing the science of crowdsourcing\"...  That indeed was not an aim of this proposal, but it was important to this reviewer :)",
      "comment_id": 580,
      "profile_id": 104,
      "published": "2015-12-16T02:32:04.037108Z",
      "thread_id": 121,
      "url": "/discussion/how-about-examples-of-the-awesome-science-that-pathway-modeling-enables/121#2"
    },
    {
      "body_html": "<p>I'd like to know that too! :)</p>\r\n\r\n<p>This funding mechanism is explicitly billed as \"exploratory.\" It's two years of initialization funding to see if there is something worth <em>really</em> investing in, especially in terms of gamification.  So, I don't have real answers to those questions because it's never been done before... the platform doesn't exist in any form currently. </p>\r\n\r\n<p>My goal with this project was to build and organize biomedical knowledge. So, I focused on how there is definitely knowledge out there waiting to be curated and a few ways to go about curating it, the primary way being gamification with some backup plans.</p>\r\n\r\n<p>Based on the NIH reviewer statements, however, there was no critique of the biomedical aims and value, nor on the approach towards those aims, but rather on the fact that the approach itself wasn't a new way to crowdsource and the aims didn't advance crowdsourcing science itself.</p>",
      "body_md": "I'd like to know that too! :)\r\n\r\nThis funding mechanism is explicitly billed as \"exploratory.\" It's two years of initialization funding to see if there is something worth *really* investing in, especially in terms of gamification.  So, I don't have real answers to those questions because it's never been done before... the platform doesn't exist in any form currently. \r\n\r\nMy goal with this project was to build and organize biomedical knowledge. So, I focused on how there is definitely knowledge out there waiting to be curated and a few ways to go about curating it, the primary way being gamification with some backup plans.\r\n\r\nBased on the NIH reviewer statements, however, there was no critique of the biomedical aims and value, nor on the approach towards those aims, but rather on the fact that the approach itself wasn't a new way to crowdsource and the aims didn't advance crowdsourcing science itself.",
      "comment_id": 581,
      "profile_id": 104,
      "published": "2015-12-16T02:42:32.907557Z",
      "thread_id": 122,
      "url": "/discussion/clarifying-the-scope-of-the-challenge/122#2"
    }
  ],
  "documents": [
    {
      "body_html": "<h1 id=\"abstract\">Abstract</h1>\r\n\r\n<p>A wealth of novel pathway information is trapped in published figures. This information, if properly modeled would be immensely useful for analyzing and interpreting large-scale omics datasets. Aligned with the broader BD2K initiative, this proposal sets out to transform the wealth of information currently embedded in countless figure images (big data) into pathway models amenable to analysis and research (knowledge). Computational approaches alone have failed to fully automate the extraction of this knowledge, which is no surprise given the wide diversity among images. Likewise, human efforts have fallen short, both at the level of internal curation teams (it&#8217;s a massively distributed problem, after all) and at the level of individual researchers who choose PowerPoint and Illustrator over the freely available pathway modeling standards and tools. This challenge is particularly well suited for a computer-assisted, human-crowdsourced solution. We propose to develop the Pathways4Life platform, which combines image processing, text recognition, and cutting-edge pathway modeling software together with scalable infrastructure for content management and tunable game mechanics to facilitate the rapid modeling of pathway images through human crowdsourced tasks.</p>\r\n\r\n<h1 id=\"research-strategy\">RESEARCH STRATEGY</h1>\r\n\r\n<h2 id=\"significance\">Significance</h2>\r\n\r\n<p>Pathway information is immensely useful for analyzing and interpreting large-scale omics data <span class=\"citation\">[<a href=\"https://doi.org/10.1038/ng1109\" class=\"citation\" data-key=\"10.1038/ng1109\">1</a>, <a href=\"https://doi.org/10.1371/journal.pbio.1000472\" class=\"citation\" data-key=\"10.1371/journal.pbio.1000472\">2</a>]</span>. Pathway analysis software for general use was developed to meet the challenge of analyzing and interpreting high-throughput transcriptomics data. After the commercialization of microarrays, pathway analysis tools such as GenMAPP <span class=\"citation\">[<a href=\"https://doi.org/10.1038/ng0502-19\" class=\"citation\" data-key=\"10.1038/ng0502-19\">3</a>]</span>, Pathway Tools <span class=\"citation\">[<a href=\"https://doi.org/10.1093/bioinformatics/18.suppl_1.s225\" class=\"citation\" data-key=\"10.1093/bioinformatics/18.suppl_1.s225\">4</a>]</span>, and Ingenuity (www.qiagen.com/ingenuity) proliferated. These tools ultimately rely on knowledge bases of pathway models. In this grant, we stress the distinction between pathway figures, which are drawn purely for illustration purposes in a graphical file format (e.g., jpg, gif, png) and pathway models, which contain standard identifiers and semantics that can be mapped to external resources in a structured file format (e.g., xml, owl, json). Aligned with the broader BD2K initiative, this proposal sets out to transform the wealth of information currently trapped in countless figure images (big data) into properly modeled pathways amenable to analysis and research (knowledge).</p>\r\n\r\n<p>Eight years ago, we conceived of WikiPathways to bring a new approach to the task of collecting, curating and distributing pathway models <span class=\"citation\">[<a href=\"https://doi.org/10.1371/journal.pbio.0060184\" class=\"citation\" data-key=\"10.1371/journal.pbio.0060184\">5</a>, <a href=\"https://doi.org/10.1093/nar/gkr1074\" class=\"citation\" data-key=\"10.1093/nar/gkr1074\">6</a>]</span>. Like other pathway knowledgebases, such as KEGG <span class=\"citation\">[<a href=\"https://doi.org/10.1093/nar/28.1.27\" class=\"citation\" data-key=\"10.1093/nar/28.1.27\">7</a>]</span> and Reactome <span class=\"citation\">[<a href=\"https://doi.org/10.1093/nar/gkq1018\" class=\"citation\" data-key=\"10.1093/nar/gkq1018\">8</a>]</span>, WikiPathways manages a wide range of canonical metabolic, regulatory, and signaling pathways (<a href=\"#VennHuman\">Figure 1</a>). The pathway creation and editing tools we use to centrally curate content, however, are the same ones we embedded into the MediaWiki platform and make available to anyone at wikipathways.org. Crowdsourcing the tasks involved in curation enables WikiPathways to manage more updates, tap into more diverse domain experts, and service specialized research communities (<a href=\"#CurationActivity\">Table 1</a>). WikiPathways hosts any pathway model that is of interest to any individual researcher. </p>\r\n\r\n<a name=\"VennHuman\"></a><div class=\"figure\" figure-id=\"VennHuman\">\n                <table><tr><td style=\"width:50%\">\n                    <div class=\"figure-content\"><img src=\"http://think-lab.s3.amazonaws.com/m/figures/4.png\"></div>\n                </td><td style=\"width:50%\">\n                    \n            <div class=\"signature-2\">\n                <div class=\"figure-title\">Figure 1. Proportional Venn Diagram of Human Pathway Resources</div>\n                <div class=\"figure-description\"><p>The overlap of unique human genes (numbers) of the pathway models at WikiPathways (red), Reactome (green) and KEGG (yellow). While each resource maintains some exclusive content, the bulk is represented in a common set of canonical pathways (center, blue). Identifiers from each resource were collected in May 2015 and converted to Ensembl in order to make this comparison.</p></div>\n            </div>\n        \n                </td></tr></table>\n            </div>\r\n\r\n<a name=\"CurationActivity\"></a><div class=\"figure\" figure-id=\"CurationActivity\">\n            <div class=\"signature-3\">\n                <div class=\"figure-title\">Table 1. Curation Activity of Pathway Resources</div>\n                <div class=\"figure-description\"><p>Estimates of curation activity for WikiPathways, Reactome and KEGG, including examples of specialized domains captured by crowdsourcing at WikiPathways. Reactome statistics are based on their <a href=\"http://wiki.reactome.org/index.php/Reactome_Calendar\">archive of editorial calendars</a>. KEGG statistics are based on <a href=\"http://www.kegg.jp/kegg/docs/upd_map.html\">their update history</a> and <a href=\"http://www.kanehisa.jp/en/people.html\">Kanehisa lab membership</a>.</p></div>\n            </div>\n        <div class=\"figure-content\"><table class=\"table markdown-table\"><thead><tr><th>Curation Activity</th><th>WikiPathways</th><th>Reactome</th><th>KEGG</th></tr></thead><tbody><tr><td>Updated pathways in past year</td><td>1048 (&gt;3,200 edits)</td><td>62</td><td>16 (peak year was 2009 at 71)</td></tr><tr><td>Unique contributors in past year</td><td>208</td><td>77</td><td>est. ~11 (i.e., half of Kanehisa lab)</td></tr><tr><td>Canonical metabolic, regulatory, signaling, and disease pathways</td><td>&#10004;</td><td>&#10004;</td><td>&#10004;</td></tr><tr><td>Stem cell and tissue differentiation pathways</td><td>&#10004;</td><td></td></tr><tr><td>Extracellular RNA pathways</td><td>&#10004;</td><td></td></tr><tr><td>Micronutrient pathways</td><td>&#10004;</td><td></td></tr><tr><td>Curated (not inferred) pathways for fly, chicken, and Arabidopsis</td><td>&#10004;</td><td>&#10004; third-party sites</td></tr><tr><td>Curated (not inferred) pathways for mouse, cow, zebrafish, yeast, plants, E. coli, tuberculosis, etc.</td><td>&#10004;</td><td></td></tr></tbody></table></div></div>\r\n\r\n<p>We are collaborating with Reactome to convert and host their human content for crowdsourced curation and distribution. Thus, despite the yellow portion in <a href=\"#VennHuman\">Figure 1</a> (the KEGG-only content that is not programmatically accessible without a license), WikiPathways is an ideal resource from which to launch a new, ambitious crowdsourcing initiative. Despite our efforts and the tremendous effort by all pathway knowledge bases over the past decade, most pathway information is still published solely as static, arbitrarily drawn images&#8212;isolated, inert representations of knowledge that cannot readily be reused or remixed in future studies.</p>\r\n\r\n<p>A survey of ~4000 published pathway figures highlights the challenges we propose to address. A PubMed Central (PMC) image search using the keyword &#8220;signaling pathway&#8221; generates over 40,000 results. Visual inspection of the first 5000 results, from publications spanning 2000&#8211;2015, revealed that 3985 (79.7%) contain a pathway image; the remainder contained only the word &#8220;pathway&#8221; in their captions. We then performed optical character recognition (OCR) using two parallel approaches: Adobe Acrobat Text Recognition (www.adobe.com) and Google&#8217;s Tesseract (code.google.com/p/tesseract-ocr). We cross-referenced the extracted text results against all known HGNC human gene symbols <span class=\"citation\">[<a href=\"https://doi.org/10.1093/nar/gku1071\" class=\"citation\" data-key=\"10.1093/nar/gku1071\">9</a>]</span>, including aliases and prior symbols, to assess the potential of these images to inform the curation of human and orthologous pathways. Acrobat and Tesseract each extracted over ~2300 HGNC symbols; ~730 (~32%) contained new information, human genes, and orthologs not captured in any pathway for any species at WikiPathways. Further, these approaches found significantly different sets of symbols&#8212;each with its own uniquely trained OCR method&#8212;such that the combined results provide greater extraction counts across all categories: 3187 HGNC symbols in total and 1087 (34%) new to WikiPathways (<a href=\"#HGNC\">Figure 2, green</a>).</p>\r\n\r\n<a name=\"HGNC\"></a><div class=\"figure\" figure-id=\"HGNC\"><div class=\"figure-content\"><img src=\"http://think-lab.s3.amazonaws.com/m/figures/6.png\"></div>\n            <div class=\"signature-1\">\n                <div class=\"figure-title\">Figure 2. Recognized HGNC Symbols versus WikiPathways Content</div>\n                <div class=\"figure-description\"><p>(A) Of 3187 recognized HGNC symbols extracted from 3985 pathway images, 878 (28%) matched symbols on human pathways at WikiPathways (blue), 1222 (38%) matched symbols on orthologous pathways at WikiPathways (red); 1087 (34%) are completely novel with respect to WikiPathways (green). (B) Individual counts per pathway using Google&#8217;s Tesseract OCR method (Adobe Acrobat results are very similar, not shown). The stacked bar charts are sorted by total and novel counts for all pathways where at least one HGNC symbol was recognized, using the same color coding as panel A. </p></div>\n            </div>\n        </div>\r\n\r\n<p>Several caveats to this survey suggest that even more new pathway information can be extracted from published images. (1) Another 38% of extracted symbols (<a href=\"#HGNC\">Figure 2, red</a>) are found only on nonhuman pathways and thus may still represent novel human pathway content. (2) The OCR methods were used &#8220;out-of-the-box&#8221; and not trained on pathway images, and the images were not pre-processed. Thus there is a significant opportunity to increase total extraction counts. (3) Since we considered only 5000 of 40,000 results from only a single pathway-related search term, the search result space is much greater. (4) This survey ignored the interactions shown in the images. Thus, even the 28% of symbols that overlap with current human pathways (<a href=\"#HGNC\">Figure 2, blue</a>) may provide new interaction content. These caveats far outweigh the potential for false positives in this survey. (1) About 5.5% of extracted symbols are dictionary words that might not represent human genes in a pathway (e.g., BIG, CELL, HOOK, MASS). (2) Some occurrences of extracted symbols might be peripheral to the image and not part of the pathway. (3) Each OCR method has an inherent false-positive rate of &lt;5% [@ 10.1109/icdar.2007.4376991], which is only partially mitigated by the narrow focus on HGNC cross-referenced hits.</p>\r\n\r\n<p>It is difficult to estimate the total number of new human gene symbols in the entire corpus of published pathway images. The proportion of new genes must plateau&#8212;new results giving diminishing returns&#8212;as the total number of known genes in pathways is approached. But for perspective, even if we were to only consider the first 3985 images and conservatively estimate the average number of genes per pathway image to be 7, and generously estimate the false positive rate to be as high as 20%, then at a proportion of 34% we would expect ~6100 new genes. <strong>This would almost double the count of unique human genes at WikiPathways today&#8212;the equivalent of 6 years of work at current crowdsourcing rates.</strong> The effort would also greatly expand upon the interactions and biological context for practically all the genes in WikiPathways. As detailed in our data sharing plan, all of this new knowledge will be available in multiple formats, including BioPAX and RDF, and distributed to a wide range of independent resources through channels already established by the WikiPathways project, including Pathway Commons <span class=\"citation\">[<a href=\"https://doi.org/10.1093/nar/gkq1039\" class=\"citation\" data-key=\"10.1093/nar/gkq1039\">10</a>]</span>, <a href=\"www.ncbi.nlm.nih.gov/biosystems\">NCBI</a> and Network2Canvas <span class=\"citation\">[<a href=\"https://doi.org/10.1093/bioinformatics/btt319\" class=\"citation\" data-key=\"10.1093/bioinformatics/btt319\">11</a>]</span>, which is a LINCS-BD2K project.</p>\r\n\r\n<p>Can the state of current pathway knowledge bases really be so poor that this minor fraction of published pathways images could have such a large effect? Here, again, the survey set of PMC pathway figures is illustrative. Despite the combined efforts of KEGG, Reactome, SBML, SBGN, WikiPathways, and even Ingenuity to provide pathway models as publication-quality images over the past decade, we counted only 230 images from any of these sources in our sample set of 3985 pathway figures. No other modeled formats were seen in appreciable number. Evidently, <strong>the vast majority of pathway images (over 94%) are arbitrarily drawn with illustration software</strong>, with little to no consistency in visual lexicon or layout and without reference to standard identifiers, interaction types, or contextual semantics.</p>\r\n\r\n<h2 id=\"innovation\">Innovation</h2>\r\n\r\n<p>Clearly, a wealth of novel pathway information is trapped in published figures. The next challenge: how to efficiently extract this information and model it as biological knowledge. Computational approaches alone have failed to fully automate this process, which is no surprise given the wide diversity among the images. Likewise, human efforts have fallen short, both at the level of central curation teams (it&#8217;s a massively distributed problem, after all) and at the level of individual researchers who choose PowerPoint and Illustrator over freely available pathway modeling standards and tools. This challenge is particularly well suited for a computer-assisted, human-crowdsourced solution. We propose to develop the Pathways4Life platform as such an innovative solution. <strong>The Pathways4Life platform combines image processing, text recognition, and cutting-edge pathway modeling software together with scalable infrastructure for content management and tunable game mechanics to facilitate the rapid modeling of pathway images through human crowdsourced tasks.</strong> Each component derives from existing research projects and technologies&#8212;the innovation lies in bringing them together to address an ongoing biomedical challenge at an unprecedented rate.</p>\r\n\r\n<p>In Aim 1, we are following in the footsteps of other groups who tackled the problem of parsing and indexing figures from the scientific literature. Yale Image Finder <span class=\"citation\">[<a href=\"https://doi.org/10.1093/bioinformatics/btn340\" class=\"citation\" data-key=\"10.1093/bioinformatics/btn340\">12</a>]</span> indexed over 1.5 million open-access images, but they parsed only the captions and not the text embedded in the images. Michael Baitaluk, et al. fine tuned an OCR method to parse entire pathways to generate BiologicalNetworks.org <span class=\"citation\">[<a href=\"https://doi.org/10.1093/bioinformatics/bts018\" class=\"citation\" data-key=\"10.1093/bioinformatics/bts018\">13</a>]</span>. This work will certainly inform our optimization work in Aim 1. Unfortunately, the models from this project were never released in a community standard format, and are no longer publically available. Regardless, even with optimization, the results were limited to 1012 pathways from ~25,000 images, of which 87% were considered to be high quality. The only human input in this process was a &#8220;like/don&#8217;t like&#8221; button and comment form. So, <strong>the real innovation we propose is to not rely solely on computational OCR, but rather to couple it from the start with a human crowdsourcing component.</strong> The OCR results are intended to lower the barrier to entry for participants, giving them a handful of recognized nodes to build upon and in the process learn how to add the remaining nodes.</p>\r\n\r\n<p>The technical innovation in Aims 2, 3, and 4 has less to do with individual technologies (relational databases, Python/Django, JavaScript, etc.) and more to do with the modular, extensible architecture, which will support iterative, agile development. <strong>This approach allows us to roll out early iterations of the platform and enables others to leverage the same framework for a wide range of crowdsourcing challenges.</strong> For example, swap in an indexed set of articles and a text highlighting tool and you would have a platform for crowdsourcing text annotation or semantic knowledge extraction, with tunable game mechanics built-in. The goal of the technology is to streamline the display, aggregation, and valuation of tasks. Embedded in this platform will be a customized version of our pathway modeling software (described in Aim 2) that will simplify the human tasks associated with pathway curation down to the barest minimum. Combined with the assistance of pre-parsed nodes from Aim 1, the curation experience will be made effortless&#8212;even enjoyable&#8212;and will far exceed the capabilities of the current WikiPathways toolset.</p>\r\n\r\n<p>The WikiPathways project is already contributing to a paradigm shift in modeling biomedical knowledge. Rather than relying solely on a centralized group of curators, we have engaged a relatively broad spectrum of researchers to contribute pathway information in their areas of expertise. This proposal aims to shift the paradigm even further. By analogy to the limitations of a centralized group that the WikiPathways project overcame by enabling all active researchers, we propose to overcome the limitations of active researchers (e.g., their number and available time) by enabling the general public. The scope and timescale of this approach will have a dramatic impact on the rate and limits of new knowledge in the form of pathway models. By scaling up the pathway image collection to 16,000 and extrapolating the sample set results (4 x 6100 = 24,400), we predict that we will be inside the region of diminishing returns (w.r.t. unique genes because there are only so many), allowing us to prioritize our selection of pathways to crowdsource based on organism, density of novel genes and biological contexts (Aim 1).  <strong>This collection will contain more unique human genes than all current pathway archives combined. The modeling of this collection would thus approach the goal of having at least one representation of every human gene that is in a known pathway context.</strong> This will lead to entirely new metrics for pathway resources, as we go on to target the full diversity of interactions and contexts for genes, as well as splice variants, miRNA, metabolites, drugs, etc. The impact on the number of interactions modeled from this collection will be even greater, as novel interactions are captured even for non-novel nodes in pathway images. More broadly, the platform has the potential to make a lasting impact on how pathway modeling as well as other knowledge extraction is performed, shifting the focus closer and closer to the source, to capture this information in sync with the act of publication.</p>\r\n\r\n<h2 id=\"approach\">Approach</h2>\r\n\r\n<p>We propose four aims. First, we will collect, process, and classify pathway images from the open-access literature. The classification step will allow us to prioritize images based on curation goals (e.g., novel genes and disease contexts) and difficulty (both human and computational). The second aim will be to develop an engaging interactive digital media platform for presenting these images to human participants with pre-annotated nodes (from OCR results) and simple tasks, such as connecting existing nodes and adding new nodes. The third aim will focus on the crowdsourcing effort, including recruiting participants and &#8220;tuning&#8221; their experience by adjusting for difficulty level, regulating point systems and visual feedback (e.g., animations), as well as by communicating scientific accomplishments. The fourth aim will entail the transformation of data from the crowdsourced tasks into confirmed pathway models. Statistical analyses will automatically feedback to the prior aim of prioritization and experience tuning to drive activity toward completion and accuracy. Community review and curation of the results will lead to their dissemination via multiple open-standard formats and communication channels, including WikiPathways, Pathway Commons (BioPAX), and linked data (RDF). </p>\r\n\r\n<h3>Aim 1: Collect, Process, and Classify Pathway Images</h3>\r\n\r\n<p>We implemented an efficient process to maximize collection of pathway images from PMC publication figures as part of the sample analysis described above. The process starts with a query into the PMC image search feature. Results are returned as HTML, which is parsed to download the full-size figure image files and generate an annotation file for each image. The annotation file, containing figure caption, author names, article title, and article hyperlink, will allow us to present critical contextual information during the crowdsourcing stage. It will also be used to index images by author and to support focused keyword searches (e.g., for images relating to particular diseases). This process will scale to encompass a broad range of queries to collect diverse and high-value pathway image file sets. We plan to collect a set of 16,000 pathway images in the first iteration of Aim 1, approximately 4x the size of the sample set described above.</p>\r\n\r\n<p>Also as part of the sample analysis, we began to explore text-extraction software. We have already scripted the application of Adobe Acrobat and Tesseract to generate extracted text file sets. These are useful for assessing the results in terms of recognizable gene symbol counts. But these programs also provide positional information for each block of extracted text. We will use this information to generate JSON models of nodes, preserving the annotation and position from the original image. This will allow us to overlay an interactive layer of modeled nodes onto the original pathway figures (see Aim 2). We will also improve the out-of-the-box performance of these programs. As an actively developed, open-source effort, Tesseract, in particular has considerable potential for improvement. To both programs, we will add a common image pre-processing step that uses ImageMagick (www.imagemagick.org) to increase contrast, adjust orientation, and remove noise (e.g., other graphics) from a copy of the image. We will also use OpenCV <span class=\"citation\">[<a href=\"https://doi.org/10.1145/2184319.2184337\" class=\"citation\" data-key=\"10.1145/2184319.2184337\">14</a>]</span> to identify and optimize regions of text in the image prior to OCR. Other methods are available to isolate, orient, and filter &#8220;objects&#8221; that may contain recognizable text <span class=\"citation\">[<a href=\"https://doi.org/10.1093/bioinformatics/bts018\" class=\"citation\" data-key=\"10.1093/bioinformatics/bts018\">13</a>, <a href=\"https://doi.org/10.1093/bioinformatics/btp318\" class=\"citation\" data-key=\"10.1093/bioinformatics/btp318\">15</a>, <a href=\"https://doi.org/10.1109/bsec.2011.5872319\" class=\"citation\" data-key=\"10.1109/bsec.2011.5872319\">16</a>, <a href=\"https://doi.org/10.1186/2041-1480-5-10\" class=\"citation\" data-key=\"10.1186/2041-1480-5-10\">17</a>]</span>. This is a worthwhile area to explore, considering the Difficulty Matrix on our sample set of 3985 images (<a href=\"#DifficultyMatrix\">Figure 3</a>). Fortunately, the Easy-Easy corner of the matrix (top left) contains a large proportion of pathway figure images, which can be targeted in early rounds of our crowdsourcing effort. However, half of the images are in the Easy-Hard quadrant (easy for human, hard for computer; top right), meaning that any improvements in automatic text extraction will result in having more pathways that are ready and amenable for human processing. The lower half of the matrix includes such small percentages that we can simply ignore it for the purposes of this proposal. Of course, the difficulty level is not really binary, so we can leverage the gradient of difficulty in the human scale, for example, to rank the pathways for display to a gradient of participants from novice to expert.</p>\r\n\r\n<a name=\"DifficultyMatrix\"></a><div class=\"figure\" figure-id=\"DifficultyMatrix\">\n                <table><tr><td style=\"width:50%\">\n                    <div class=\"figure-content\"><img src=\"http://think-lab.s3.amazonaws.com/m/figures/7.png\"></div>\n                </td><td style=\"width:50%\">\n                    \n            <div class=\"signature-2\">\n                <div class=\"figure-title\">Figure 3. Difficulty Matrix for Human and Computer Parsing of Pathway Images</div>\n                <div class=\"figure-description\"><p>Examples of images that are easy for humans (top row), hard for humans (bottom row), easy for computers (left column), and hard for computers (right column). The percentages of pathways in each quadrant were estimated by inspecting the 3985 sample images described in the text. Red highlights the &#8220;Easy-Easy&#8221; corner.</p></div>\n            </div>\n        \n                </td></tr></table>\n            </div>\r\n\r\n<p>The independent difficulty levels for humans and computers are just one example of how images will be classified. We will also assess the potential gene content per image based on the automatically extracted text. As we did with the sample set, we will contrast these gene sets with those already captured in properly modeled pathways. Given the set of novel genes, we will classify images based on the proportion and absolute number each pathway represents (<a href=\"#HGNC\">Figure 2B, green</a>). This will allow us to define high-value targets for the crowdsourcing effort&#8212;those images that will add the most new unique genes at the highest rate to pathway knowledge bases. We will also classify pathway images by the overrepresented GO terms and disease associations in their gene sets. Classification by disease, for example, will allow us to prioritize not only generally but also specifically for crowdsourcing efforts that target a single disease or research area. Even in cases where most of the genes are not novel, the interactions and contextual information that will be modeled are just as likely to impact subsequent research.</p>\r\n\r\n<p>The product of this aim is an ever-growing database of pathway images, annotated not only with source information but also with extracted gene symbols, multiple dimensions of functional classifications, and a JSON data overlay ready to be rendered and made interactive in Aim 2.</p>\r\n\r\n<h4>Strategic Vision</h4>\r\n\r\n<p>The FOA points out that this work is expected to be iterative. The work we propose covers the first iteration from conceptual design to established demonstration. Each section of the plan will include a Strategic Vision subsection like this to describe a plan for subsequent iterations on this work.</p>\r\n\r\n<p>Beyond scouring html results for published pathway images, the longer-term strategy should be to get closer and closer to the source. The Pathways4Life platform could be extended to allow journal editors to manage inputs. For example, they could directly populate the figures to be processed by the crowd in sync with the publishing process, even as a way of promoting a new article. In this way, they would learn to appreciate the need for providing editorial feedback to authors concerning the legibility of pathway figures, making them more amenable to computational processing. In the final iteration, the authors themselves should find these modeling tools so easy and useful that they will model pathways from the start.</p>\r\n\r\n<h3>Aim 2: Develop an Interactive Digital Media Platform</h3>\r\n\r\n<p>We began to explore this approach in 2007 with the WikiPathways project <span class=\"citation\">[<a href=\"https://doi.org/10.1371/journal.pbio.0060184\" class=\"citation\" data-key=\"10.1371/journal.pbio.0060184\">5</a>, <a href=\"https://doi.org/10.1126/science.320.5881.1289b\" class=\"citation\" data-key=\"10.1126/science.320.5881.1289b\">18</a>, <a href=\"https://doi.org/10.1038/455022a\" class=\"citation\" data-key=\"10.1038/455022a\">19</a>]</span>. Today, my group is in Year 3 of 5 of the first R01 for WikiPathways development (GM100039). We are currently rolling out a JavaScript replacement of the Java Applet, called pvjs (pathvisiojs), which converts our xml pathway models into a JSON model and renders it as SVG. The user can interact with the rendered view by clicking on nodes (e.g., genes, proteins, metabolites) and interactions to pull up information panels driven by the use of standard identifiers. In edit mode, users can add new identifiers, reposition nodes, and draw new interactions. Beyond these immediate functions, we designed pvjs with a view toward specialization for educational, publishing, and even game possibilities. The first step in our development process for pvjs was to synthesize a set of best practices by reviewing the architectures of 40 relevant libraries and frameworks, including d3.js, Cytoscape.js, AngularJS, SVG-edit, VISIBIOweb, and biographer. The insights we gained led us to a modularized, model-view-controller (MVC) architectural pattern that integrates virtual DOM capabilities for fast performance and easy extensibility. We use an agile development process in which new features are broken into their smallest independently useful components and released frequently to ensure a tight coupling of user and developer goals and expectations. This proposal will be the first realization of this potential for extensibility.</p>\r\n\r\n<p>A specialized version of pvjs will be a critical component of the Pathways4Life platform. For this proposal, we will describe the unique requirements, refactoring, and new development that will make up the overall implementation plan for the platform.</p>\r\n\r\n<p><strong>Backend database and control logic</strong>&#8212;We will design and host a database to contain image, annotation, and JSON file references, with indexed classification values and various progress-tracking metrics. These entries will map to participant, node, and interaction tables in the database. The schema will support queries to cache specific subsets of content for targeted events (e.g., with a disease focus). Indexed classification values will also be used in dynamic queries to determine the next pathway image to show a given user. The skill level of participants and the point value of nodes and interactions will be updated in their respective tables in rounds of activity (e.g., per pathway, set of pathways, or even per day), depending on performance profiling.</p>\r\n\r\n<p>A basic Python/Django web framework will be implemented to form template-based queries and views to serve content to the customized pvjs tool and to update the database with new contributions and calculated activity metrics. For example, as a participant adds nodes and interactions, they will accumulate corresponding points and attain a higher skill level. The next pathway (or set of pathways) shown to this participant will be based on their skill level and the difficulty class of the pathway. Simple calculations based on the participant&#8217;s actions with a given pathway (e.g., accumulation of points) will be done in the client browser and returned to server after each session. On the server side, we will process aggregated data across all sessions to assign confidence scores for each node and interaction, updating their database records. This strategy will allow us to distribute low-CPU, frequent computation at scale with the number of participants while also restricting server-side, moderate-CPU computation to fixed periods that we can adjust according to demand and resources.</p>\r\n\r\n<p>We have sufficient infrastructure in place to develop and test the platform. As a modular set of virtualized services, we will deploy them using Amazon Web Services (AWS) to host large-scale beta and production crowdsourcing events toward the end of the funding period. By then we will have demonstrated the viability, scalability, and initial popularity of our approach, which will inform the strategy plan in future iterations.</p>\r\n\r\n<p><strong>Customized Pvjs</strong>&#8212;Pvjs will require customization to work as a component of the Pathways4Life platform. The modular architecture of pvjs will readily accommodate customization. The new modules will add support for attribute-value accessory data and an SVG visual feedback layer (<a href=\"#Interface\">Figure 4</a>).</p>\r\n\r\n<a name=\"Interface\"></a><div class=\"figure\" figure-id=\"Interface\"><div class=\"figure-content\"><img src=\"http://think-lab.s3.amazonaws.com/m/figures/8.png\"></div>\n            <div class=\"signature-1\">\n                <div class=\"figure-title\">Figure 4. Pathways4Life Interactive Digital Media Interface</div>\n                <div class=\"figure-description\"><p>A series of views during a crowdsourced task. (A) a pathway image and SVG-based modeling layer with OCR-identified nodes (boxed) and indications of available tasks and their point values, (B) the interface mid-task, (C) a completed task with client-side calculated points and visual feedback.</p></div>\n            </div>\n        </div>\r\n\r\n<p>The first module will leverage the extensibility of our existing JSON format for pathway information by defining attributes to handle pre-calculated point values and confidence scores for each pathway, node, and interaction. These values will be retrieved from the backend database described above and combined with standard pathway information from the XML model. As part of the JSON model, this information will be available to the SVG layer and the browser. The module will be designed to work with any third-party database and any set of arbitrary attribute-value pairs. Thus, the same module could also be used to represent any accessory data, such as public or user-provided omics datasets, linkouts to custom resources, or metadata from other modeling standards, such as SBML.</p>\r\n\r\n<p>The next module will build upon the current SVG rendering and interaction capabilities of pvjs. Currently, it supports only basic representations of nodes and interactions. To make pvjs more engaging and interactive, we will design and implement more visually engaging objects and activity feedback. For example, color and animation effects can be used to indicate the point value of a particular node or interaction, and the act of forming a new or confirmed interaction could be accompanied by a visually rewarding glow, pulse, or burst effect. The module will define the mapping between available JSON attributes and SVG elements. The mapping pattern can be reused to provide custom graphics and animations for any defined set of accessory data, such as gradient-fill colors for omics datasets, hover effects for custom linkouts, and support for other graphical standards, such as SBGN.</p>\r\n\r\n<p>A potential challenge that could arise is the performance of SVG for highly complex diagrams. SVG supports dynamic diagrams with up to about 5000 to 10,000 elements, depending on the browser. This limitation is unlikely to present a problem, because the vast majority of pathways are more focused than the average raw network visualization. But if the goals of the project shift such that support is required for additional elements, pvjs is designed so that some or all of the SVG rendering can be replaced with technologies suited for rendering extremely large numbers of elements, such as webGL or canvas. For example, it would be possible to render completed portions of a pathway using SVG, on top of which additional elements, such as a large number of candidate pathway elements generated by automated techniques, could be rendered as a webGL layer. This could be done using an open-source library such as Pixi.js, a performance-focused HTML5 rendering engine that defaults to webGL but falls back to canvas to support older browsers.</p>\r\n\r\n<h4>Strategic Vision</h4>\r\n\r\n<p>The backend database and control logic elements will be designed to scale with demand and hardware resources. Thus, future iterations will require minimal refactoring as computer, storage, and bandwidth resources are increased. During the initial funding period, we will coordinate with other science crowdsourcing efforts to leverage any common platforms we might contribute to in order to reach these goals faster and more sustainably. For example, during an NIH-hosted informational webinar on this FOA, the potential grantees formed a Google Group for Crowdsourced Science Games that will also be a source of collaborative idea sharing, development, and outreach strategies. In particular, for many years, we have collaborated on open science and crowdsourcing strategies with Drs. Su and Good at Scripps. They are exploring a crowdsourcing platform built around their Mark2Cure effort (mark2cure.org). We will share our requirements and feature ideas with them and other groups to work together wherever possible on a platform that could support our independently developed tools. In the iterations that follow this funding period, we will be in a position to consider longer-term strategies for hosting Pathways4Life. There are already enthusiastic hosting services for science-related games and crowdsourcing efforts, such as Purpose Games, Games for Change, and Zooniverse. Such hosting opportunities will continue to diversify and grow in number.</p>\r\n\r\n<p>In terms of pvjs customization, we have outlined a strategy that will meet the immediate goals of this proposal, to produce an engaging interactive digital media experience, while also enabling a wide range of future project ideas. We routinely accept patches and extensions to our open-source projects, especially in areas where we have established a framework for extensions and clear programing patterns. Thus, in addition to our own further customization of JSON attributes and mapped SVG graphics, we anticipate that Pathways4Life will be a popular framework for other groups to extend for their own custom use cases. In particular, we would work with colleagues in the SBGN community to support their standard visual lexicon for pathways through an iteration of this module strategy.</p>\r\n\r\n<h3>Aim 3: Crowdsource Tasks and Engage Participation</h3>\r\n\r\n<p>Participants will define nodes and interactions. To define an interaction, the participant clicks on an existing node to anchor the source (an active &#8220;rubber band&#8221; line will now track with the mouse position) and then clicks on a second node or another interaction to indicate the target (an interaction arrow will now be drawn). A list of interaction types will appear from which the participant must select to complete the task and move on. To define a node, the participant right-clicks on the image where the node should be added (e.g., on the name or symbol for a gene, protein, or metabolite that OCR failed to recognize), types the name or symbol (which triggers an autocomplete pvjs database lookup), and then selects the correct identity (a new node will then be drawn). Subsequent interactions to and from the newly added nodes can then be drawn. In this way, complete pathway images can be traced and effectively modeled by a series of these two easy-to-learn tasks.</p>\r\n\r\n<p>Each task will be associated with an adjustable point value. Tunable point values support the basic game mechanic of balancing the economy of player&#8217;s attention and time investment. For example, rare nodes and interactions will be worth more points than common ones already captured in the current archives of pathway models. This will allow us to tune the prioritization of novel information. The overall difficulty of a given pathway (i.e., per human difficulty scale in <a href=\"#DifficultyMatrix\">Figure 3</a>) can also be a variable in calculating a task&#8217;s value, both to balance challenge and reward and to encourage skill building and return participation. Even the ordinality (1st...Nth) could be used to value nodes and interactions to encourage completion of a given pathway.</p>\r\n\r\n<p>To assess quality and confidence (see Aim 4), we will need to collect redundant information from multiple participants on any given task. We will do this in two ways: (1) by showing the same version of a pathway to multiple participants, excluding newly added nodes and interactions that have yet to be confirmed and (2) by allowing participants to right-click on existing nodes and interactions to contest the information, which would contribute to a confirmed rejection and removal of that information in future rounds. This strategy will also help address false-positive OCR results that generate inaccurate nodes. Again, tunable point values will be used to balance confirmation versus pioneering activity (e.g., by increasing the values for successive confirmations). And participants will gain/lose points post hoc based on the long-term confirmation/rejection status of their tasks. This tunable value will balance accuracy against speed. <a href=\"#TunableVariables\">Table 2</a> summarizes the tunable economy of the platform via task point values, as well as when the calculation occurs.</p>\r\n\r\n<a name=\"TunableVariables\"></a><div class=\"figure\" figure-id=\"TunableVariables\">\n            <div class=\"signature-3\">\n                <div class=\"figure-title\">Table 2. Tunable Variables</div>\n                <div class=\"figure-description\"><p>Five examples of variables that can be tuned to shift activity with respect to various outcomes. The last column specifies when and where each variable would be evaluated with respect to tasks performed by participants.</p></div>\n            </div>\n        <div class=\"figure-content\"><table class=\"table markdown-table\"><thead><tr><th>Variable</th><th>Outcome</th><th>Computed</th></tr></thead><tbody><tr><td>Pathway difficulty</td><td>Skill building and return play</td><td>Server-side, before task</td></tr><tr><td>Rarity vis-&#224;-vis current models</td><td>Novel information capture</td><td>Server-side, before task</td></tr><tr><td>Ordinality per pathway</td><td>Completed pathways</td><td>Client-side, during task</td></tr><tr><td>Confirmation level</td><td>Confirmed pathways</td><td>Client-side, during task</td></tr><tr><td>Confirmed/Rejected status</td><td>Accurate pathways</td><td>Server-side, after task</td></tr></tbody></table></div></div>\r\n\r\n<p>The precise values assigned to each variable will be determined during initial &#8220;play testing&#8221; and periodically adjusted to match our evolving goals and crowd of participants. The tuning and balancing of game economies is standard practice in simulation and massively multiplayer online games, which share these same evolving properties. By building in these mechanisms from the start, we can seamlessly redirect attention to tasks we deem a priority, even as our priorities change. Changes to values will be determined by three mechanisms, each optimized to bring about a specific outcome: statistical analysis, direct feedback, and manual override. The first two are described in Aim 4. The third mechanism can simply be described as us intervening from time to time based on our observations of game play and outcome. Regardless of mechanism, the final key property of our task management strategy will be transparency. At any given time, participants will know the value of each task they perform by means of immediate visual feedback (e.g., a brief animation of the point value). They will also see their current total score and progress toward successive skill levels. At the end of each round (e.g., 10 tasks), we can apply bonus points (or deduct points) according to the ongoing server-side calculation of confirmed/rejected status of prior tasks and then display their current level. This highlighting of bonus points, and anticipation of leveling, will thus be directly associated with the importance of accuracy. Leveling will unlock more difficult pathways with greater point potential, etc. Each participant will progress through training levels as well, where the point systems are highlighted and pre-selected pathway images are used to demonstrate the tasks to be performed on the elements they will encounter. These simple game mechanics will provide sufficient incentive for a large, yet to be fully engaged population of people interested in purposeful games and science and disease-related crowdsourced tasks.</p>\r\n\r\n<p>The initial pool of participants and alpha testers will come from the open-source and open-science connections we have established and cultivated over the past decade. These include pathway modeling groups (e.g., BioPAX, SBML/SBGN) and our own WikiPathways, as well as our general network biology communities: NRNB, Cytoscape, and the NetBio COSI. Through WikiPathways, for example, we have designated over 40 external teams, each representing communities of pathway enthusiasts, including funding and advocacy agencies (California Institute for Regenerative Medicine, National Brain Tumor Society), research organizations and consortiums (Progenitor Cell Biology Consortium, Luxembourg Centre for Systems Biomedicine, Extracellular RNA Communication, The Cancer Genome Atlas), and research resources (SGD, WormBase, Science Commons, Sage Bionetworks). Relevant connections also include DREAM Challenges (dreamchallenges.org) and Crowdsourced Science Games (groups.google.com). Each of these efforts has existing communication channels (e.g., Google Groups, Twitter, Facebook, Google+, LinkedIn Groups, blogs, mailing lists, and conferences) that we can leverage to immediately reach tens of thousands of individuals. In the beta testing phase, we will leverage our contacts within educational institutions (high school, university, and post-graduate) to reach a broader audience than we normally would, given our current, specialist-focused toolset. In fact, based on the experience of contemporary science game efforts, we anticipate our audience will largely consist of nonspecialists. So, we will direct most of our attention to the communication channels that readily extend outside the walls of academia, such as popular social networks and general interest blogs. The outreach will focus on disease-focused messaging. The classification of pathways in Aim 1 will allow us to define sets of disease-related pathways and run focused campaigns.</p>\r\n\r\n<p>If we cannot engage a sufficiently large volunteer base in this fashion, we will use Amazon Mechanical Turk (AMT) to finish assessing the viability of our platform by the end of the funding period. The basic idea of AMT is to facilitate the distribution of tasks to a ready &#8220;army&#8221; of workers who receive micropayments per completed task. Since the Pathways4Life platform is designed to be deployed on AWS already (as described in Aim 2), we would only need to add a few calls to AMT&#8217;s application programming interface to send and retrieve task data as sets of name-value pairs <span class=\"citation\">[<a href=\"http://docs.aws.amazon.com/AWSMechTurk/latest/AWSMturkAPI/ApiReference_ExternalQuestionArticle.html\" class=\"citation\" data-key=\"amazon\">20</a>]</span>. The budget for AWS time and bandwidth would thus be shifted to AMT workers, giving us fewer months of hosted time, but guaranteed returns if outreach efforts fall short in this compressed time period.</p>\r\n\r\n<h4>Strategic Vision</h4>\r\n\r\n<p>In future iterations, we could add support for tasks to draw and identify subcellular compartments based on the provided pathway image, to indicate complexes and paralogs, and to tag pathways with ontology terms based on provided captions, titles, and links to the original paper. These data would greatly increase knowledge extracted from otherwise inert images. Our underlying JSON pathway model already accommodates subcellular compartment information and any ontology terms supported by NCBO&#8217;s BioPortal (bioportal.bioontology.org), including Pathway, Cell Type, and Disease ontologies. We would start with tags for high-priority information (e.g., Organism and Disease) that almost anyone could recognize. The tasks that require more than the most basic understanding of biology would be added later for advanced participants. In this proposal, for simplicity and space limitations, we focus on the two most essential tasks&#8212;nodes and interactions&#8212;but these additional tasks are relatively straightforward to implement and will likely make it into a version of the platform late in Year 2.</p>\r\n\r\n<p>We outline a minimal set of game mechanics for this initial iteration of the project, but we envision the potential for rapid iterations in this direction without any changes to the infrastructure or architecture. The visual nature of the source material&#8212;the pathway images that authors, graphic designers, and editors have already taken care in producing&#8212;can be woven together with creative storytelling to make a more engaging and broadly appealing experience. For example, these pathway images can be framed as navigable maps discovered from ancient alien civilizations. The act of tracing and interpreting thus becomes one of exploration and risk/reward adventure. In additional to valuing individual nodes and interactions, we can also calculate points and generate animations based on the extent of connectivity across an entire pathway, thus encouraging activity along extended paths. A progression of simple animations depicting flowing water, marching ants, migrating animals and colonizing humans, for example, could play out over the graph as it grows and gains confirmation status. We can assign landmark names to sets of gene symbols to carry along the story, reserving special categories of landmarks for the rare genes we value most. The confirmation/rejection then translates directly into reward/risk in the adventure of accurately navigating these maps. And the discovery of more advanced landmarks leads to the progression to more difficult pathway maps. Social features, such as teams competing in tournaments or in conquering of new territory, could also be layered onto the tasks, together with compelling storytelling.</p>\r\n\r\n<h3>Aim 4: Assemble Results: Transforming Big Data into Knowledge</h3>\r\n\r\n<p>The output of the components described so far will be a stream of JSON snippets that represent the individual changes (or &#8220;diffs&#8221;) made per task. Each snippet will be associated with a particular pathway image and participant. As structured data in a predefined JSON format, they can readily and reliably be compared. With the collection periodically indexed by pathway image identifier, for example, we can quickly confirm that a particular snippet is novel or an Nth confirmation of a prior result. The comparison of snippets representing new nodes will require a tolerance factor to account for minor deviations in positioning. But numerical interval comparison is still a trivially fast calculation. These data then feed into a calculated confidence score for the snippet as well as a potential bonus score for the participant. The equation for calculating confidence scores will be the sum of observations (+1 for confirmation, &#8211;1 for rejection, o), weighted by the relative skill level of the participant (0&#8211;1, w). We can include a multiplier for negative observations to convey the extra effort in making a negative call (e.g., 2, m), and assess this sum against a threshold (e.g., 10, T) to ultimately mark a snippet as confirmed (e.g., S&#8805;1).</p>\r\n\r\n<p><span class=\"math\">$$S =&#8721;(owm)_i /T $$</span></p>\r\n\r\n<p>We will periodically reassess the modifier and threshold values based on manual assessment of confirmed results. When a snippet is confirmed, it will be excluded from the pool of confirmable entities on that particular pathway, unless a rejection observation produces a subthreshold score or the content is reset (e.g., due to a change in thresholding). When all the snippets on a particular pathway are confirmed, the model will be queued for manual review before being added to the WikiPathways archive for distribution. A model might be rejected, for example, if a portion of the image has not been modeled (i.e., missed by both OCR and crowdsourcing). In these cases, we can simply add to back to the pool pending additional confirmed snippets or even initiate a few new nodes ourselves before re-releasing it. We expect each resulting pathway will require some level of editing and final touching-up. The progression from computational OCR, to crowdsourcing of simple tasks, to final assembly will ultimately pass through WikiPathways review stage. However, this type of curation activity is routinely performed by WikiPathways staff and volunteers, and does not pose a significant burden on this grant. Community review and curation of the results will lead to their dissemination via multiple open-standard formats and communication channels, including but not limited to WikiPathways, Pathway Commons (BioPAX), and linked data (RDF).</p>\r\n\r\n<p>Aggregate statistics on snippet confidence scores per pathway will also be used to statistically assess the tuning variables described in Aim 3. Ideally, we want to see an average score near 0.5 during the bulk of the crowdsourcing activity for a given pathway, indicating a balance of initiating and confirming activity. While it is less than 0.5, we can increase the point values associated with Confirmation level to encourage confirmation activity; and while it is greater than 0.5, including when it is near completion, we can decrease the value to encourage finding new content in the image. If average alone is not sufficiently sensitive, we can also determine the slope of a sigmoidal fit to a plot of sorted scores per pathway and similarly use it in a function to adjust Confirmation level. This tuning can be completely automated by using confidence scores to calculate point values for each node and interaction in the model before being served to pvjs, where the points will be displayed to the next participant. In the same way, the values associated with Ordinality per pathway can be adjusted by direct feedback to the model and display to the participant based on a simple count of snippets detected thus far. And if the rejection rates are deemed to be too high (another number we can readily count per pathway or across the entire collection), we can increase the penalty assessed per round and use these intermissions to point out mistakes, make suggestions, and even direct participants to repeat training levels. </p>\r\n\r\n<p>During this funding period, we plan to complete at least one disease-focused crowdsourcing event using the Pathways4Life platform. Following the precedent set by science competitions and other crowdsourcing events, we will spearhead a publication together with all participants as co-authors, focusing on the characterization of the extracted data and the resource of new knowledge that has been generated. Where possible, we will coordinate with journal editors before these events to incentivize involvement and stress both the attribution and responsibility that comes with Pathways4Life participation.</p>\r\n\r\n<h4>Strategic Vision</h4>\r\n\r\n<p>Following this first iteration, we will continue to organize events around specific diseases and research areas. We will also continue to feed in new pathway images from more extensive searches and new publications. We will work with publishers to submit pathway images themselves or provide clear author instructions. In this manner, we envision a two-fold solution to the pathway modeling problem: (1) we will get closer and closer to the source of published pathway images while simultaneously capturing prior published work and (2) we will be putting easy-to-use pathway modeling tools in the hands for more and more people. The post hoc modeling tool proposed here is based on the same technology we provide for de novo modeling of original pathways. Thus, our larger strategic goal is for researchers to draw their pathways in modeling tools in the first place and deliver the stylized versions as a byproduct for publication figures.</p>\r\n\r\n<p>We also envision an evolution of the digital media platform and tools as the community of participants evolves, both technically (e.g., tablet and mobile support) and interactively (e.g., more layers of gamification and story-based abstraction). These iterations will continue to lower the barrier for broader participation in the curation of biomedically relevant pathway knowledge. </p>\r\n\r\n<h2 id=\"milestones-metrics-and-benchmarks\">Milestones, Metrics, and Benchmarks</h2>\r\n\r\n<p>The following timeline outlines a set of milestones, including a few key metrics and benchmarks along the way to measure progress on our aims and longer-term goals.  </p>\r\n\r\n<ul><li>Feb-Apr 2016:<ul><li>Refine image preprocessing and optimize OCR results</li><li>Amass collection of 16,000 pathway image</li></ul></li><li>May-Jul 2016: <ul><li>Process, OCR and classify 16,000 pathway images. </li><li>Identify novel genes</li><li>Identify at least 3 disease-related subsets</li></ul></li><li>May-Oct 2016: <ul><li>Initial development of database, control logic and web framework to host pathway images and their metadata</li><li>Initial customization of pvjs to work with expanded JSON model, tasks and API development</li></ul></li><li>Nov-Jan 2017:<ul><li>Completed participant registration and account system; added to database schema</li><li>Prototype of Pathways4Life platform hosted on local server for early alpha testing</li></ul></li><li>Feb-Mar 2017:<ul><li>Completed SVG and style designs for tasks, points, animations and round summaries</li><li>Completed client- and server-side calculations for assessing snippet diffs and points; added to database schema</li></ul></li><li>Apr-June 2017:<ul><li>Feature complete beta version hosted on local server for live testing</li><li>Launch official campaign to engage participants for beta testing and upcoming First Event</li></ul></li><li>July-Aug 2017: <ul><li>Testing, debugging, user feedback, initial round of variable tuning</li><li>Deploy to Amazon Web Services</li></ul></li><li>Sept-Nov 2017:<ul><li>Host First Event on carefully selected disease-relevant set of pathways</li><li>Tune variables and collect feedback</li><li>Disseminate new pathway knowledge in multiple formats via WikiPathways</li><li>Publish results with participants as co-authors</li></ul></li><li>Dec-Jan 2018:<ul><li>Host Second Event; or run continuously; or employ Amazon Mechanical Turk</li><li>Continue to tune, collect feedback and disseminate new pathway knowledge</li><li>Assess platform; publish on technology, initial impact, future events and future developments&#8195;</li></ul></li></ul>\r\n\r\n",
      "body_md": "# Abstract\r\n\r\nA wealth of novel pathway information is trapped in published figures. This information, if properly modeled would be immensely useful for analyzing and interpreting large-scale omics datasets. Aligned with the broader BD2K initiative, this proposal sets out to transform the wealth of information currently embedded in countless figure images (big data) into pathway models amenable to analysis and research (knowledge). Computational approaches alone have failed to fully automate the extraction of this knowledge, which is no surprise given the wide diversity among images. Likewise, human efforts have fallen short, both at the level of internal curation teams (it’s a massively distributed problem, after all) and at the level of individual researchers who choose PowerPoint and Illustrator over the freely available pathway modeling standards and tools. This challenge is particularly well suited for a computer-assisted, human-crowdsourced solution. We propose to develop the Pathways4Life platform, which combines image processing, text recognition, and cutting-edge pathway modeling software together with scalable infrastructure for content management and tunable game mechanics to facilitate the rapid modeling of pathway images through human crowdsourced tasks.\r\n\r\n# RESEARCH STRATEGY\r\n## Significance\r\nPathway information is immensely useful for analyzing and interpreting large-scale omics data [@10.1038/ng1109 @10.1371/journal.pbio.1000472]. Pathway analysis software for general use was developed to meet the challenge of analyzing and interpreting high-throughput transcriptomics data. After the commercialization of microarrays, pathway analysis tools such as GenMAPP [@10.1038/ng0502-19], Pathway Tools [@10.1093/bioinformatics/18.suppl_1.s225], and Ingenuity (www.qiagen.com/ingenuity) proliferated. These tools ultimately rely on knowledge bases of pathway models. In this grant, we stress the distinction between pathway figures, which are drawn purely for illustration purposes in a graphical file format (e.g., jpg, gif, png) and pathway models, which contain standard identifiers and semantics that can be mapped to external resources in a structured file format (e.g., xml, owl, json). Aligned with the broader BD2K initiative, this proposal sets out to transform the wealth of information currently trapped in countless figure images (big data) into properly modeled pathways amenable to analysis and research (knowledge).\r\n\r\nEight years ago, we conceived of WikiPathways to bring a new approach to the task of collecting, curating and distributing pathway models [@10.1371/journal.pbio.0060184 @10.1093/nar/gkr1074]. Like other pathway knowledgebases, such as KEGG [@10.1093/nar/28.1.27] and Reactome [@10.1093/nar/gkq1018], WikiPathways manages a wide range of canonical metabolic, regulatory, and signaling pathways ([Figure {n}](#VennHuman)). The pathway creation and editing tools we use to centrally curate content, however, are the same ones we embedded into the MediaWiki platform and make available to anyone at wikipathways.org. Crowdsourcing the tasks involved in curation enables WikiPathways to manage more updates, tap into more diverse domain experts, and service specialized research communities ([Table {n}](#CurationActivity)). WikiPathways hosts any pathway model that is of interest to any individual researcher. \r\n\r\n[:figure](VennHuman)\r\n\r\n[:table](CurationActivity)\r\n\r\nWe are collaborating with Reactome to convert and host their human content for crowdsourced curation and distribution. Thus, despite the yellow portion in [Figure {n}](#VennHuman) (the KEGG-only content that is not programmatically accessible without a license), WikiPathways is an ideal resource from which to launch a new, ambitious crowdsourcing initiative. Despite our efforts and the tremendous effort by all pathway knowledge bases over the past decade, most pathway information is still published solely as static, arbitrarily drawn images—isolated, inert representations of knowledge that cannot readily be reused or remixed in future studies.\r\n\r\nA survey of ~4000 published pathway figures highlights the challenges we propose to address. A PubMed Central (PMC) image search using the keyword “signaling pathway” generates over 40,000 results. Visual inspection of the first 5000 results, from publications spanning 2000–2015, revealed that 3985 (79.7%) contain a pathway image; the remainder contained only the word “pathway” in their captions. We then performed optical character recognition (OCR) using two parallel approaches: Adobe Acrobat Text Recognition (www.adobe.com) and Google’s Tesseract (code.google.com/p/tesseract-ocr). We cross-referenced the extracted text results against all known HGNC human gene symbols [@10.1093/nar/gku1071], including aliases and prior symbols, to assess the potential of these images to inform the curation of human and orthologous pathways. Acrobat and Tesseract each extracted over ~2300 HGNC symbols; ~730 (~32%) contained new information, human genes, and orthologs not captured in any pathway for any species at WikiPathways. Further, these approaches found significantly different sets of symbols—each with its own uniquely trained OCR method—such that the combined results provide greater extraction counts across all categories: 3187 HGNC symbols in total and 1087 (34%) new to WikiPathways ([Figure {n}, green](#HGNC)).\r\n\r\n[:figure](HGNC)\r\n\r\nSeveral caveats to this survey suggest that even more new pathway information can be extracted from published images. (1) Another 38% of extracted symbols ([Figure {n}, red](#HGNC)) are found only on nonhuman pathways and thus may still represent novel human pathway content. (2) The OCR methods were used “out-of-the-box” and not trained on pathway images, and the images were not pre-processed. Thus there is a significant opportunity to increase total extraction counts. (3) Since we considered only 5000 of 40,000 results from only a single pathway-related search term, the search result space is much greater. (4) This survey ignored the interactions shown in the images. Thus, even the 28% of symbols that overlap with current human pathways ([Figure {n}, blue](#HGNC)) may provide new interaction content. These caveats far outweigh the potential for false positives in this survey. (1) About 5.5% of extracted symbols are dictionary words that might not represent human genes in a pathway (e.g., BIG, CELL, HOOK, MASS). (2) Some occurrences of extracted symbols might be peripheral to the image and not part of the pathway. (3) Each OCR method has an inherent false-positive rate of <5% [@ 10.1109/icdar.2007.4376991], which is only partially mitigated by the narrow focus on HGNC cross-referenced hits.\r\n\r\nIt is difficult to estimate the total number of new human gene symbols in the entire corpus of published pathway images. The proportion of new genes must plateau—new results giving diminishing returns—as the total number of known genes in pathways is approached. But for perspective, even if we were to only consider the first 3985 images and conservatively estimate the average number of genes per pathway image to be 7, and generously estimate the false positive rate to be as high as 20%, then at a proportion of 34% we would expect ~6100 new genes. **This would almost double the count of unique human genes at WikiPathways today—the equivalent of 6 years of work at current crowdsourcing rates.** The effort would also greatly expand upon the interactions and biological context for practically all the genes in WikiPathways. As detailed in our data sharing plan, all of this new knowledge will be available in multiple formats, including BioPAX and RDF, and distributed to a wide range of independent resources through channels already established by the WikiPathways project, including Pathway Commons [@10.1093/nar/gkq1039], [NCBI](www.ncbi.nlm.nih.gov/biosystems) and Network2Canvas [@10.1093/bioinformatics/btt319], which is a LINCS-BD2K project.\r\n\r\nCan the state of current pathway knowledge bases really be so poor that this minor fraction of published pathways images could have such a large effect? Here, again, the survey set of PMC pathway figures is illustrative. Despite the combined efforts of KEGG, Reactome, SBML, SBGN, WikiPathways, and even Ingenuity to provide pathway models as publication-quality images over the past decade, we counted only 230 images from any of these sources in our sample set of 3985 pathway figures. No other modeled formats were seen in appreciable number. Evidently, **the vast majority of pathway images (over 94%) are arbitrarily drawn with illustration software**, with little to no consistency in visual lexicon or layout and without reference to standard identifiers, interaction types, or contextual semantics.\r\n\r\n## Innovation\r\nClearly, a wealth of novel pathway information is trapped in published figures. The next challenge: how to efficiently extract this information and model it as biological knowledge. Computational approaches alone have failed to fully automate this process, which is no surprise given the wide diversity among the images. Likewise, human efforts have fallen short, both at the level of central curation teams (it’s a massively distributed problem, after all) and at the level of individual researchers who choose PowerPoint and Illustrator over freely available pathway modeling standards and tools. This challenge is particularly well suited for a computer-assisted, human-crowdsourced solution. We propose to develop the Pathways4Life platform as such an innovative solution. **The Pathways4Life platform combines image processing, text recognition, and cutting-edge pathway modeling software together with scalable infrastructure for content management and tunable game mechanics to facilitate the rapid modeling of pathway images through human crowdsourced tasks.** Each component derives from existing research projects and technologies—the innovation lies in bringing them together to address an ongoing biomedical challenge at an unprecedented rate.\r\n\r\nIn Aim 1, we are following in the footsteps of other groups who tackled the problem of parsing and indexing figures from the scientific literature. Yale Image Finder [@10.1093/bioinformatics/btn340] indexed over 1.5 million open-access images, but they parsed only the captions and not the text embedded in the images. Michael Baitaluk, et al. fine tuned an OCR method to parse entire pathways to generate BiologicalNetworks.org [@10.1093/bioinformatics/bts018]. This work will certainly inform our optimization work in Aim 1. Unfortunately, the models from this project were never released in a community standard format, and are no longer publically available. Regardless, even with optimization, the results were limited to 1012 pathways from ~25,000 images, of which 87% were considered to be high quality. The only human input in this process was a “like/don’t like” button and comment form. So, **the real innovation we propose is to not rely solely on computational OCR, but rather to couple it from the start with a human crowdsourcing component.** The OCR results are intended to lower the barrier to entry for participants, giving them a handful of recognized nodes to build upon and in the process learn how to add the remaining nodes.\r\n\r\nThe technical innovation in Aims 2, 3, and 4 has less to do with individual technologies (relational databases, Python/Django, JavaScript, etc.) and more to do with the modular, extensible architecture, which will support iterative, agile development. **This approach allows us to roll out early iterations of the platform and enables others to leverage the same framework for a wide range of crowdsourcing challenges.** For example, swap in an indexed set of articles and a text highlighting tool and you would have a platform for crowdsourcing text annotation or semantic knowledge extraction, with tunable game mechanics built-in. The goal of the technology is to streamline the display, aggregation, and valuation of tasks. Embedded in this platform will be a customized version of our pathway modeling software (described in Aim 2) that will simplify the human tasks associated with pathway curation down to the barest minimum. Combined with the assistance of pre-parsed nodes from Aim 1, the curation experience will be made effortless—even enjoyable—and will far exceed the capabilities of the current WikiPathways toolset.\r\n\r\nThe WikiPathways project is already contributing to a paradigm shift in modeling biomedical knowledge. Rather than relying solely on a centralized group of curators, we have engaged a relatively broad spectrum of researchers to contribute pathway information in their areas of expertise. This proposal aims to shift the paradigm even further. By analogy to the limitations of a centralized group that the WikiPathways project overcame by enabling all active researchers, we propose to overcome the limitations of active researchers (e.g., their number and available time) by enabling the general public. The scope and timescale of this approach will have a dramatic impact on the rate and limits of new knowledge in the form of pathway models. By scaling up the pathway image collection to 16,000 and extrapolating the sample set results (4 x 6100 = 24,400), we predict that we will be inside the region of diminishing returns (w.r.t. unique genes because there are only so many), allowing us to prioritize our selection of pathways to crowdsource based on organism, density of novel genes and biological contexts (Aim 1).  **This collection will contain more unique human genes than all current pathway archives combined. The modeling of this collection would thus approach the goal of having at least one representation of every human gene that is in a known pathway context.** This will lead to entirely new metrics for pathway resources, as we go on to target the full diversity of interactions and contexts for genes, as well as splice variants, miRNA, metabolites, drugs, etc. The impact on the number of interactions modeled from this collection will be even greater, as novel interactions are captured even for non-novel nodes in pathway images. More broadly, the platform has the potential to make a lasting impact on how pathway modeling as well as other knowledge extraction is performed, shifting the focus closer and closer to the source, to capture this information in sync with the act of publication.\r\n\r\n## Approach\r\nWe propose four aims. First, we will collect, process, and classify pathway images from the open-access literature. The classification step will allow us to prioritize images based on curation goals (e.g., novel genes and disease contexts) and difficulty (both human and computational). The second aim will be to develop an engaging interactive digital media platform for presenting these images to human participants with pre-annotated nodes (from OCR results) and simple tasks, such as connecting existing nodes and adding new nodes. The third aim will focus on the crowdsourcing effort, including recruiting participants and “tuning” their experience by adjusting for difficulty level, regulating point systems and visual feedback (e.g., animations), as well as by communicating scientific accomplishments. The fourth aim will entail the transformation of data from the crowdsourced tasks into confirmed pathway models. Statistical analyses will automatically feedback to the prior aim of prioritization and experience tuning to drive activity toward completion and accuracy. Community review and curation of the results will lead to their dissemination via multiple open-standard formats and communication channels, including WikiPathways, Pathway Commons (BioPAX), and linked data (RDF). \r\n\r\n### Aim 1: Collect, Process, and Classify Pathway Images\r\nWe implemented an efficient process to maximize collection of pathway images from PMC publication figures as part of the sample analysis described above. The process starts with a query into the PMC image search feature. Results are returned as HTML, which is parsed to download the full-size figure image files and generate an annotation file for each image. The annotation file, containing figure caption, author names, article title, and article hyperlink, will allow us to present critical contextual information during the crowdsourcing stage. It will also be used to index images by author and to support focused keyword searches (e.g., for images relating to particular diseases). This process will scale to encompass a broad range of queries to collect diverse and high-value pathway image file sets. We plan to collect a set of 16,000 pathway images in the first iteration of Aim 1, approximately 4x the size of the sample set described above.\r\n\r\nAlso as part of the sample analysis, we began to explore text-extraction software. We have already scripted the application of Adobe Acrobat and Tesseract to generate extracted text file sets. These are useful for assessing the results in terms of recognizable gene symbol counts. But these programs also provide positional information for each block of extracted text. We will use this information to generate JSON models of nodes, preserving the annotation and position from the original image. This will allow us to overlay an interactive layer of modeled nodes onto the original pathway figures (see Aim 2). We will also improve the out-of-the-box performance of these programs. As an actively developed, open-source effort, Tesseract, in particular has considerable potential for improvement. To both programs, we will add a common image pre-processing step that uses ImageMagick (www.imagemagick.org) to increase contrast, adjust orientation, and remove noise (e.g., other graphics) from a copy of the image. We will also use OpenCV [@10.1145/2184319.2184337] to identify and optimize regions of text in the image prior to OCR. Other methods are available to isolate, orient, and filter “objects” that may contain recognizable text [@10.1093/bioinformatics/bts018 @10.1093/bioinformatics/btp318 @10.1109/bsec.2011.5872319 @10.1186/2041-1480-5-10]. This is a worthwhile area to explore, considering the Difficulty Matrix on our sample set of 3985 images ([Figure {n}](#DifficultyMatrix)). Fortunately, the Easy-Easy corner of the matrix (top left) contains a large proportion of pathway figure images, which can be targeted in early rounds of our crowdsourcing effort. However, half of the images are in the Easy-Hard quadrant (easy for human, hard for computer; top right), meaning that any improvements in automatic text extraction will result in having more pathways that are ready and amenable for human processing. The lower half of the matrix includes such small percentages that we can simply ignore it for the purposes of this proposal. Of course, the difficulty level is not really binary, so we can leverage the gradient of difficulty in the human scale, for example, to rank the pathways for display to a gradient of participants from novice to expert.\r\n\r\n[:figure](DifficultyMatrix)\r\n\r\nThe independent difficulty levels for humans and computers are just one example of how images will be classified. We will also assess the potential gene content per image based on the automatically extracted text. As we did with the sample set, we will contrast these gene sets with those already captured in properly modeled pathways. Given the set of novel genes, we will classify images based on the proportion and absolute number each pathway represents ([Figure {n}B, green](#HGNC)). This will allow us to define high-value targets for the crowdsourcing effort—those images that will add the most new unique genes at the highest rate to pathway knowledge bases. We will also classify pathway images by the overrepresented GO terms and disease associations in their gene sets. Classification by disease, for example, will allow us to prioritize not only generally but also specifically for crowdsourcing efforts that target a single disease or research area. Even in cases where most of the genes are not novel, the interactions and contextual information that will be modeled are just as likely to impact subsequent research.\r\n\r\nThe product of this aim is an ever-growing database of pathway images, annotated not only with source information but also with extracted gene symbols, multiple dimensions of functional classifications, and a JSON data overlay ready to be rendered and made interactive in Aim 2.\r\n\r\n#### Strategic Vision\r\nThe FOA points out that this work is expected to be iterative. The work we propose covers the first iteration from conceptual design to established demonstration. Each section of the plan will include a Strategic Vision subsection like this to describe a plan for subsequent iterations on this work.\r\n\r\nBeyond scouring html results for published pathway images, the longer-term strategy should be to get closer and closer to the source. The Pathways4Life platform could be extended to allow journal editors to manage inputs. For example, they could directly populate the figures to be processed by the crowd in sync with the publishing process, even as a way of promoting a new article. In this way, they would learn to appreciate the need for providing editorial feedback to authors concerning the legibility of pathway figures, making them more amenable to computational processing. In the final iteration, the authors themselves should find these modeling tools so easy and useful that they will model pathways from the start.\r\n\r\n### Aim 2: Develop an Interactive Digital Media Platform\r\nWe began to explore this approach in 2007 with the WikiPathways project [@10.1371/journal.pbio.0060184 @10.1126/science.320.5881.1289b @10.1038/455022a]. Today, my group is in Year 3 of 5 of the first R01 for WikiPathways development (GM100039). We are currently rolling out a JavaScript replacement of the Java Applet, called pvjs (pathvisiojs), which converts our xml pathway models into a JSON model and renders it as SVG. The user can interact with the rendered view by clicking on nodes (e.g., genes, proteins, metabolites) and interactions to pull up information panels driven by the use of standard identifiers. In edit mode, users can add new identifiers, reposition nodes, and draw new interactions. Beyond these immediate functions, we designed pvjs with a view toward specialization for educational, publishing, and even game possibilities. The first step in our development process for pvjs was to synthesize a set of best practices by reviewing the architectures of 40 relevant libraries and frameworks, including d3.js, Cytoscape.js, AngularJS, SVG-edit, VISIBIOweb, and biographer. The insights we gained led us to a modularized, model-view-controller (MVC) architectural pattern that integrates virtual DOM capabilities for fast performance and easy extensibility. We use an agile development process in which new features are broken into their smallest independently useful components and released frequently to ensure a tight coupling of user and developer goals and expectations. This proposal will be the first realization of this potential for extensibility.\r\n\r\nA specialized version of pvjs will be a critical component of the Pathways4Life platform. For this proposal, we will describe the unique requirements, refactoring, and new development that will make up the overall implementation plan for the platform.\r\n\r\n**Backend database and control logic**—We will design and host a database to contain image, annotation, and JSON file references, with indexed classification values and various progress-tracking metrics. These entries will map to participant, node, and interaction tables in the database. The schema will support queries to cache specific subsets of content for targeted events (e.g., with a disease focus). Indexed classification values will also be used in dynamic queries to determine the next pathway image to show a given user. The skill level of participants and the point value of nodes and interactions will be updated in their respective tables in rounds of activity (e.g., per pathway, set of pathways, or even per day), depending on performance profiling.\r\n\r\nA basic Python/Django web framework will be implemented to form template-based queries and views to serve content to the customized pvjs tool and to update the database with new contributions and calculated activity metrics. For example, as a participant adds nodes and interactions, they will accumulate corresponding points and attain a higher skill level. The next pathway (or set of pathways) shown to this participant will be based on their skill level and the difficulty class of the pathway. Simple calculations based on the participant’s actions with a given pathway (e.g., accumulation of points) will be done in the client browser and returned to server after each session. On the server side, we will process aggregated data across all sessions to assign confidence scores for each node and interaction, updating their database records. This strategy will allow us to distribute low-CPU, frequent computation at scale with the number of participants while also restricting server-side, moderate-CPU computation to fixed periods that we can adjust according to demand and resources.\r\n\r\nWe have sufficient infrastructure in place to develop and test the platform. As a modular set of virtualized services, we will deploy them using Amazon Web Services (AWS) to host large-scale beta and production crowdsourcing events toward the end of the funding period. By then we will have demonstrated the viability, scalability, and initial popularity of our approach, which will inform the strategy plan in future iterations.\r\n\r\n**Customized Pvjs**—Pvjs will require customization to work as a component of the Pathways4Life platform. The modular architecture of pvjs will readily accommodate customization. The new modules will add support for attribute-value accessory data and an SVG visual feedback layer ([Figure {n}](#Interface)).\r\n \r\n[:figure](Interface)\r\n\r\nThe first module will leverage the extensibility of our existing JSON format for pathway information by defining attributes to handle pre-calculated point values and confidence scores for each pathway, node, and interaction. These values will be retrieved from the backend database described above and combined with standard pathway information from the XML model. As part of the JSON model, this information will be available to the SVG layer and the browser. The module will be designed to work with any third-party database and any set of arbitrary attribute-value pairs. Thus, the same module could also be used to represent any accessory data, such as public or user-provided omics datasets, linkouts to custom resources, or metadata from other modeling standards, such as SBML.\r\n\r\nThe next module will build upon the current SVG rendering and interaction capabilities of pvjs. Currently, it supports only basic representations of nodes and interactions. To make pvjs more engaging and interactive, we will design and implement more visually engaging objects and activity feedback. For example, color and animation effects can be used to indicate the point value of a particular node or interaction, and the act of forming a new or confirmed interaction could be accompanied by a visually rewarding glow, pulse, or burst effect. The module will define the mapping between available JSON attributes and SVG elements. The mapping pattern can be reused to provide custom graphics and animations for any defined set of accessory data, such as gradient-fill colors for omics datasets, hover effects for custom linkouts, and support for other graphical standards, such as SBGN.\r\n\r\nA potential challenge that could arise is the performance of SVG for highly complex diagrams. SVG supports dynamic diagrams with up to about 5000 to 10,000 elements, depending on the browser. This limitation is unlikely to present a problem, because the vast majority of pathways are more focused than the average raw network visualization. But if the goals of the project shift such that support is required for additional elements, pvjs is designed so that some or all of the SVG rendering can be replaced with technologies suited for rendering extremely large numbers of elements, such as webGL or canvas. For example, it would be possible to render completed portions of a pathway using SVG, on top of which additional elements, such as a large number of candidate pathway elements generated by automated techniques, could be rendered as a webGL layer. This could be done using an open-source library such as Pixi.js, a performance-focused HTML5 rendering engine that defaults to webGL but falls back to canvas to support older browsers.\r\n\r\n#### Strategic Vision\r\nThe backend database and control logic elements will be designed to scale with demand and hardware resources. Thus, future iterations will require minimal refactoring as computer, storage, and bandwidth resources are increased. During the initial funding period, we will coordinate with other science crowdsourcing efforts to leverage any common platforms we might contribute to in order to reach these goals faster and more sustainably. For example, during an NIH-hosted informational webinar on this FOA, the potential grantees formed a Google Group for Crowdsourced Science Games that will also be a source of collaborative idea sharing, development, and outreach strategies. In particular, for many years, we have collaborated on open science and crowdsourcing strategies with Drs. Su and Good at Scripps. They are exploring a crowdsourcing platform built around their Mark2Cure effort (mark2cure.org). We will share our requirements and feature ideas with them and other groups to work together wherever possible on a platform that could support our independently developed tools. In the iterations that follow this funding period, we will be in a position to consider longer-term strategies for hosting Pathways4Life. There are already enthusiastic hosting services for science-related games and crowdsourcing efforts, such as Purpose Games, Games for Change, and Zooniverse. Such hosting opportunities will continue to diversify and grow in number.\r\n\r\nIn terms of pvjs customization, we have outlined a strategy that will meet the immediate goals of this proposal, to produce an engaging interactive digital media experience, while also enabling a wide range of future project ideas. We routinely accept patches and extensions to our open-source projects, especially in areas where we have established a framework for extensions and clear programing patterns. Thus, in addition to our own further customization of JSON attributes and mapped SVG graphics, we anticipate that Pathways4Life will be a popular framework for other groups to extend for their own custom use cases. In particular, we would work with colleagues in the SBGN community to support their standard visual lexicon for pathways through an iteration of this module strategy.\r\n\r\n### Aim 3: Crowdsource Tasks and Engage Participation\r\nParticipants will define nodes and interactions. To define an interaction, the participant clicks on an existing node to anchor the source (an active “rubber band” line will now track with the mouse position) and then clicks on a second node or another interaction to indicate the target (an interaction arrow will now be drawn). A list of interaction types will appear from which the participant must select to complete the task and move on. To define a node, the participant right-clicks on the image where the node should be added (e.g., on the name or symbol for a gene, protein, or metabolite that OCR failed to recognize), types the name or symbol (which triggers an autocomplete pvjs database lookup), and then selects the correct identity (a new node will then be drawn). Subsequent interactions to and from the newly added nodes can then be drawn. In this way, complete pathway images can be traced and effectively modeled by a series of these two easy-to-learn tasks.\r\n\r\nEach task will be associated with an adjustable point value. Tunable point values support the basic game mechanic of balancing the economy of player’s attention and time investment. For example, rare nodes and interactions will be worth more points than common ones already captured in the current archives of pathway models. This will allow us to tune the prioritization of novel information. The overall difficulty of a given pathway (i.e., per human difficulty scale in [Figure {n}](#DifficultyMatrix)) can also be a variable in calculating a task’s value, both to balance challenge and reward and to encourage skill building and return participation. Even the ordinality (1st...Nth) could be used to value nodes and interactions to encourage completion of a given pathway.\r\n\r\nTo assess quality and confidence (see Aim 4), we will need to collect redundant information from multiple participants on any given task. We will do this in two ways: (1) by showing the same version of a pathway to multiple participants, excluding newly added nodes and interactions that have yet to be confirmed and (2) by allowing participants to right-click on existing nodes and interactions to contest the information, which would contribute to a confirmed rejection and removal of that information in future rounds. This strategy will also help address false-positive OCR results that generate inaccurate nodes. Again, tunable point values will be used to balance confirmation versus pioneering activity (e.g., by increasing the values for successive confirmations). And participants will gain/lose points post hoc based on the long-term confirmation/rejection status of their tasks. This tunable value will balance accuracy against speed. [Table {n}](#TunableVariables) summarizes the tunable economy of the platform via task point values, as well as when the calculation occurs.\r\n\r\n[:table](TunableVariables)\r\n\r\nThe precise values assigned to each variable will be determined during initial “play testing” and periodically adjusted to match our evolving goals and crowd of participants. The tuning and balancing of game economies is standard practice in simulation and massively multiplayer online games, which share these same evolving properties. By building in these mechanisms from the start, we can seamlessly redirect attention to tasks we deem a priority, even as our priorities change. Changes to values will be determined by three mechanisms, each optimized to bring about a specific outcome: statistical analysis, direct feedback, and manual override. The first two are described in Aim 4. The third mechanism can simply be described as us intervening from time to time based on our observations of game play and outcome. Regardless of mechanism, the final key property of our task management strategy will be transparency. At any given time, participants will know the value of each task they perform by means of immediate visual feedback (e.g., a brief animation of the point value). They will also see their current total score and progress toward successive skill levels. At the end of each round (e.g., 10 tasks), we can apply bonus points (or deduct points) according to the ongoing server-side calculation of confirmed/rejected status of prior tasks and then display their current level. This highlighting of bonus points, and anticipation of leveling, will thus be directly associated with the importance of accuracy. Leveling will unlock more difficult pathways with greater point potential, etc. Each participant will progress through training levels as well, where the point systems are highlighted and pre-selected pathway images are used to demonstrate the tasks to be performed on the elements they will encounter. These simple game mechanics will provide sufficient incentive for a large, yet to be fully engaged population of people interested in purposeful games and science and disease-related crowdsourced tasks.\r\n\r\nThe initial pool of participants and alpha testers will come from the open-source and open-science connections we have established and cultivated over the past decade. These include pathway modeling groups (e.g., BioPAX, SBML/SBGN) and our own WikiPathways, as well as our general network biology communities: NRNB, Cytoscape, and the NetBio COSI. Through WikiPathways, for example, we have designated over 40 external teams, each representing communities of pathway enthusiasts, including funding and advocacy agencies (California Institute for Regenerative Medicine, National Brain Tumor Society), research organizations and consortiums (Progenitor Cell Biology Consortium, Luxembourg Centre for Systems Biomedicine, Extracellular RNA Communication, The Cancer Genome Atlas), and research resources (SGD, WormBase, Science Commons, Sage Bionetworks). Relevant connections also include DREAM Challenges (dreamchallenges.org) and Crowdsourced Science Games (groups.google.com). Each of these efforts has existing communication channels (e.g., Google Groups, Twitter, Facebook, Google+, LinkedIn Groups, blogs, mailing lists, and conferences) that we can leverage to immediately reach tens of thousands of individuals. In the beta testing phase, we will leverage our contacts within educational institutions (high school, university, and post-graduate) to reach a broader audience than we normally would, given our current, specialist-focused toolset. In fact, based on the experience of contemporary science game efforts, we anticipate our audience will largely consist of nonspecialists. So, we will direct most of our attention to the communication channels that readily extend outside the walls of academia, such as popular social networks and general interest blogs. The outreach will focus on disease-focused messaging. The classification of pathways in Aim 1 will allow us to define sets of disease-related pathways and run focused campaigns.\r\n\r\nIf we cannot engage a sufficiently large volunteer base in this fashion, we will use Amazon Mechanical Turk (AMT) to finish assessing the viability of our platform by the end of the funding period. The basic idea of AMT is to facilitate the distribution of tasks to a ready “army” of workers who receive micropayments per completed task. Since the Pathways4Life platform is designed to be deployed on AWS already (as described in Aim 2), we would only need to add a few calls to AMT’s application programming interface to send and retrieve task data as sets of name-value pairs [@amazon]. The budget for AWS time and bandwidth would thus be shifted to AMT workers, giving us fewer months of hosted time, but guaranteed returns if outreach efforts fall short in this compressed time period.\r\n\r\n#### Strategic Vision\r\nIn future iterations, we could add support for tasks to draw and identify subcellular compartments based on the provided pathway image, to indicate complexes and paralogs, and to tag pathways with ontology terms based on provided captions, titles, and links to the original paper. These data would greatly increase knowledge extracted from otherwise inert images. Our underlying JSON pathway model already accommodates subcellular compartment information and any ontology terms supported by NCBO’s BioPortal (bioportal.bioontology.org), including Pathway, Cell Type, and Disease ontologies. We would start with tags for high-priority information (e.g., Organism and Disease) that almost anyone could recognize. The tasks that require more than the most basic understanding of biology would be added later for advanced participants. In this proposal, for simplicity and space limitations, we focus on the two most essential tasks—nodes and interactions—but these additional tasks are relatively straightforward to implement and will likely make it into a version of the platform late in Year 2.\r\n\r\nWe outline a minimal set of game mechanics for this initial iteration of the project, but we envision the potential for rapid iterations in this direction without any changes to the infrastructure or architecture. The visual nature of the source material—the pathway images that authors, graphic designers, and editors have already taken care in producing—can be woven together with creative storytelling to make a more engaging and broadly appealing experience. For example, these pathway images can be framed as navigable maps discovered from ancient alien civilizations. The act of tracing and interpreting thus becomes one of exploration and risk/reward adventure. In additional to valuing individual nodes and interactions, we can also calculate points and generate animations based on the extent of connectivity across an entire pathway, thus encouraging activity along extended paths. A progression of simple animations depicting flowing water, marching ants, migrating animals and colonizing humans, for example, could play out over the graph as it grows and gains confirmation status. We can assign landmark names to sets of gene symbols to carry along the story, reserving special categories of landmarks for the rare genes we value most. The confirmation/rejection then translates directly into reward/risk in the adventure of accurately navigating these maps. And the discovery of more advanced landmarks leads to the progression to more difficult pathway maps. Social features, such as teams competing in tournaments or in conquering of new territory, could also be layered onto the tasks, together with compelling storytelling.\r\n\r\n### Aim 4: Assemble Results: Transforming Big Data into Knowledge\r\nThe output of the components described so far will be a stream of JSON snippets that represent the individual changes (or “diffs”) made per task. Each snippet will be associated with a particular pathway image and participant. As structured data in a predefined JSON format, they can readily and reliably be compared. With the collection periodically indexed by pathway image identifier, for example, we can quickly confirm that a particular snippet is novel or an Nth confirmation of a prior result. The comparison of snippets representing new nodes will require a tolerance factor to account for minor deviations in positioning. But numerical interval comparison is still a trivially fast calculation. These data then feed into a calculated confidence score for the snippet as well as a potential bonus score for the participant. The equation for calculating confidence scores will be the sum of observations (+1 for confirmation, –1 for rejection, o), weighted by the relative skill level of the participant (0–1, w). We can include a multiplier for negative observations to convey the extra effort in making a negative call (e.g., 2, m), and assess this sum against a threshold (e.g., 10, T) to ultimately mark a snippet as confirmed (e.g., S≥1).\r\n\r\n$$S =∑(owm)_i /T $$\r\n\r\nWe will periodically reassess the modifier and threshold values based on manual assessment of confirmed results. When a snippet is confirmed, it will be excluded from the pool of confirmable entities on that particular pathway, unless a rejection observation produces a subthreshold score or the content is reset (e.g., due to a change in thresholding). When all the snippets on a particular pathway are confirmed, the model will be queued for manual review before being added to the WikiPathways archive for distribution. A model might be rejected, for example, if a portion of the image has not been modeled (i.e., missed by both OCR and crowdsourcing). In these cases, we can simply add to back to the pool pending additional confirmed snippets or even initiate a few new nodes ourselves before re-releasing it. We expect each resulting pathway will require some level of editing and final touching-up. The progression from computational OCR, to crowdsourcing of simple tasks, to final assembly will ultimately pass through WikiPathways review stage. However, this type of curation activity is routinely performed by WikiPathways staff and volunteers, and does not pose a significant burden on this grant. Community review and curation of the results will lead to their dissemination via multiple open-standard formats and communication channels, including but not limited to WikiPathways, Pathway Commons (BioPAX), and linked data (RDF).\r\n\r\nAggregate statistics on snippet confidence scores per pathway will also be used to statistically assess the tuning variables described in Aim 3. Ideally, we want to see an average score near 0.5 during the bulk of the crowdsourcing activity for a given pathway, indicating a balance of initiating and confirming activity. While it is less than 0.5, we can increase the point values associated with Confirmation level to encourage confirmation activity; and while it is greater than 0.5, including when it is near completion, we can decrease the value to encourage finding new content in the image. If average alone is not sufficiently sensitive, we can also determine the slope of a sigmoidal fit to a plot of sorted scores per pathway and similarly use it in a function to adjust Confirmation level. This tuning can be completely automated by using confidence scores to calculate point values for each node and interaction in the model before being served to pvjs, where the points will be displayed to the next participant. In the same way, the values associated with Ordinality per pathway can be adjusted by direct feedback to the model and display to the participant based on a simple count of snippets detected thus far. And if the rejection rates are deemed to be too high (another number we can readily count per pathway or across the entire collection), we can increase the penalty assessed per round and use these intermissions to point out mistakes, make suggestions, and even direct participants to repeat training levels. \r\n\r\nDuring this funding period, we plan to complete at least one disease-focused crowdsourcing event using the Pathways4Life platform. Following the precedent set by science competitions and other crowdsourcing events, we will spearhead a publication together with all participants as co-authors, focusing on the characterization of the extracted data and the resource of new knowledge that has been generated. Where possible, we will coordinate with journal editors before these events to incentivize involvement and stress both the attribution and responsibility that comes with Pathways4Life participation.\r\n\r\n#### Strategic Vision\r\nFollowing this first iteration, we will continue to organize events around specific diseases and research areas. We will also continue to feed in new pathway images from more extensive searches and new publications. We will work with publishers to submit pathway images themselves or provide clear author instructions. In this manner, we envision a two-fold solution to the pathway modeling problem: (1) we will get closer and closer to the source of published pathway images while simultaneously capturing prior published work and (2) we will be putting easy-to-use pathway modeling tools in the hands for more and more people. The post hoc modeling tool proposed here is based on the same technology we provide for de novo modeling of original pathways. Thus, our larger strategic goal is for researchers to draw their pathways in modeling tools in the first place and deliver the stylized versions as a byproduct for publication figures.\r\n\r\nWe also envision an evolution of the digital media platform and tools as the community of participants evolves, both technically (e.g., tablet and mobile support) and interactively (e.g., more layers of gamification and story-based abstraction). These iterations will continue to lower the barrier for broader participation in the curation of biomedically relevant pathway knowledge. \r\n\r\n## Milestones, Metrics, and Benchmarks\r\nThe following timeline outlines a set of milestones, including a few key metrics and benchmarks along the way to measure progress on our aims and longer-term goals.  \r\n\r\n * Feb-Apr 2016:\r\n  * Refine image preprocessing and optimize OCR results\r\n  * Amass collection of 16,000 pathway image\r\n * May-Jul 2016: \r\n  * Process, OCR and classify 16,000 pathway images. \r\n   * Identify novel genes\r\n   * Identify at least 3 disease-related subsets\r\n * May-Oct 2016: \r\n  * Initial development of database, control logic and web framework to host pathway images and their metadata\r\n  * Initial customization of pvjs to work with expanded JSON model, tasks and API development\r\n * Nov-Jan 2017:\r\n  * Completed participant registration and account system; added to database schema\r\n  * Prototype of Pathways4Life platform hosted on local server for early alpha testing\r\n * Feb-Mar 2017:\r\n  * Completed SVG and style designs for tasks, points, animations and round summaries\r\n  * Completed client- and server-side calculations for assessing snippet diffs and points; added to database schema\r\n * Apr-June 2017:\r\n  * Feature complete beta version hosted on local server for live testing\r\n  * Launch official campaign to engage participants for beta testing and upcoming First Event\r\n * July-Aug 2017: \r\n  * Testing, debugging, user feedback, initial round of variable tuning\r\n  * Deploy to Amazon Web Services\r\n * Sept-Nov 2017:\r\n  * Host First Event on carefully selected disease-relevant set of pathways\r\n  * Tune variables and collect feedback\r\n  * Disseminate new pathway knowledge in multiple formats via WikiPathways\r\n  * Publish results with participants as co-authors\r\n * Dec-Jan 2018:\r\n  * Host Second Event; or run continuously; or employ Amazon Mechanical Turk\r\n  * Continue to tune, collect feedback and disseminate new pathway knowledge\r\n  * Assess platform; publish on technology, initial impact, future events and future developments \r\n\r\n[@amazon]: http://docs.aws.amazon.com/AWSMechTurk/latest/AWSMturkAPI/ApiReference_ExternalQuestionArticle.html \"Amazon Mechanical Turk, External Questions documentation.\"",
      "doc_published": "2015-06-09T18:10:17.765156Z",
      "document_id": 3,
      "doi": "10.15363/thinklab.a3",
      "intro_html": "",
      "intro_md": "",
      "title": "",
      "topic_field": "",
      "url": "/p/pathways4life/proposal",
      "views": 468
    }
  ],
  "notes": [
    {
      "added": "2015-06-15T16:20:21.984055Z",
      "body_html": "<span><a href=\"/u/jspauld\" class=\"username\">@jspauld</a>, most of the non-DOI references have DOIs. These should be converted, right?</span>",
      "body_md": "@jspauld, most of the non-DOI references have DOIs. These should be converted, right?",
      "comment_id": 261,
      "note_id": 126,
      "profile_id": 17,
      "url": "/discussion/thinklab-specific-comments/77#note-126"
    },
    {
      "added": "2015-12-16T02:19:11.348174Z",
      "body_html": "<p>Yes, you're probably right ;)</p>",
      "body_md": "Yes, you're probably right ;)",
      "comment_id": 576,
      "note_id": 199,
      "profile_id": 2,
      "url": "/discussion/use-machine-learning-to-calculate-confidence-scores-for-users-snippets-and-pathways/119#note-199"
    },
    {
      "added": "2015-12-16T02:29:11.162305Z",
      "body_html": "<p>Better get them on <a href=\"http://www.wikipathways.org/\">WikiPathways</a> to start with then!</p>",
      "body_md": "Better get them on [WikiPathways](http://www.wikipathways.org/) to start with then!",
      "comment_id": 579,
      "note_id": 200,
      "profile_id": 2,
      "url": "/discussion/are-there-significant-differences-in-difficulty-between-pathway-modeling-tasks/120#note-200"
    }
  ],
  "profiles": [
    {
      "first_name": "Jesse",
      "last_name": "Spaulding",
      "profile_id": 2,
      "url": "/u/jspauld",
      "username": "jspauld"
    },
    {
      "first_name": "Daniel",
      "last_name": "Himmelstein",
      "profile_id": 17,
      "url": "/u/dhimmel",
      "username": "dhimmel"
    },
    {
      "first_name": "Venkat",
      "last_name": "Malladi",
      "profile_id": 35,
      "url": "/u/vsmalladi",
      "username": "vsmalladi"
    },
    {
      "first_name": "Benjamin",
      "last_name": "Good",
      "profile_id": 48,
      "url": "/u/b_good",
      "username": "b_good"
    },
    {
      "first_name": "Alexander",
      "last_name": "Pico",
      "profile_id": 104,
      "url": "/u/alexanderpico",
      "username": "alexanderpico"
    }
  ],
  "retrieved": "2017-05-29T22:38:48.833691Z",
  "threads": [
    {
      "document_id": null,
      "doi": "10.15363/thinklab.d73",
      "doi_field": null,
      "profile_id": 104,
      "published": "2015-06-09T02:28:14.495546Z",
      "subject": "Open for Feedback",
      "thread_id": 73,
      "topic_field": "Citizen Science,Crowdsourcing,Open Sourse,Game Design,Pathways,Image Analysis",
      "url": "/discussion/open-for-feedback/73",
      "views": 50
    },
    {
      "document_id": null,
      "doi": "10.15363/thinklab.d75",
      "doi_field": null,
      "profile_id": 17,
      "published": "2015-06-10T23:17:15.323621Z",
      "subject": "Pathway novelty based on unique relationships rather than genes",
      "thread_id": 75,
      "topic_field": "Gene Relationships,Gene Interactions,Interactions,Pathways,Gene Sets",
      "url": "/discussion/pathway-novelty-based-on-unique-relationships-rather-than-genes/75",
      "views": 55
    },
    {
      "document_id": null,
      "doi": "10.15363/thinklab.d76",
      "doi_field": null,
      "profile_id": 17,
      "published": "2015-06-10T23:23:57.865673Z",
      "subject": "Duplicate pathway detection and resolution",
      "thread_id": 76,
      "topic_field": "Duplicates,Pathways,Curation",
      "url": "/discussion/duplicate-pathway-detection-and-resolution/76",
      "views": 38
    },
    {
      "document_id": null,
      "doi": "10.15363/thinklab.d77",
      "doi_field": null,
      "profile_id": 17,
      "published": "2015-06-10T23:34:36.664789Z",
      "subject": "Thinklab specific comments",
      "thread_id": 77,
      "topic_field": "Markdown,ThinkLab,Spelling,Formatting",
      "url": "/discussion/thinklab-specific-comments/77",
      "views": 54
    },
    {
      "document_id": null,
      "doi": "10.15363/thinklab.d117",
      "doi_field": null,
      "profile_id": null,
      "published": "2015-10-09T21:05:26.485680Z",
      "subject": "Pathways4Life: Crowdsourcing Pathway Modeling from Published Figures",
      "thread_id": 117,
      "topic_field": "Crowdsourcing,Game Design,Pathways,Image Analysis",
      "url": "/doc/3/review",
      "views": 56
    },
    {
      "document_id": null,
      "doi": "10.15363/thinklab.d119",
      "doi_field": null,
      "profile_id": 2,
      "published": "2015-10-30T23:15:04.302204Z",
      "subject": "Use machine learning to calculate confidence scores for users, snippets, and pathways",
      "thread_id": 119,
      "topic_field": "Machine learning",
      "url": "/discussion/use-machine-learning-to-calculate-confidence-scores-for-users-snippets-and-pathways/119",
      "views": 174
    },
    {
      "document_id": null,
      "doi": "10.15363/thinklab.d120",
      "doi_field": null,
      "profile_id": 2,
      "published": "2015-10-30T23:49:10.382918Z",
      "subject": "Are there significant differences in difficulty between pathway modeling tasks?",
      "thread_id": 120,
      "topic_field": "Gamification",
      "url": "/discussion/are-there-significant-differences-in-difficulty-between-pathway-modeling-tasks/120",
      "views": 36
    },
    {
      "document_id": null,
      "doi": "10.15363/thinklab.d121",
      "doi_field": null,
      "profile_id": 2,
      "published": "2015-10-31T00:11:28.596713Z",
      "subject": "How about examples of the awesome science that pathway modeling enables?",
      "thread_id": 121,
      "topic_field": "",
      "url": "/discussion/how-about-examples-of-the-awesome-science-that-pathway-modeling-enables/121",
      "views": 54
    },
    {
      "document_id": null,
      "doi": "10.15363/thinklab.d122",
      "doi_field": null,
      "profile_id": 2,
      "published": "2015-10-31T00:29:30.890698Z",
      "subject": "Clarifying the scope of the challenge",
      "thread_id": 122,
      "topic_field": "",
      "url": "/discussion/clarifying-the-scope-of-the-challenge/122",
      "views": 57
    },
    {
      "document_id": null,
      "doi": "10.15363/thinklab.d123",
      "doi_field": null,
      "profile_id": 2,
      "published": "2015-10-31T00:48:03.240373Z",
      "subject": "What made previous crowdsourced science games succeed or not?",
      "thread_id": 123,
      "topic_field": "Crowdsourcing,Gamification,Science Games,Serious Games",
      "url": "/discussion/what-made-previous-crowdsourced-science-games-succeed-or-not/123",
      "views": 55
    }
  ]
}