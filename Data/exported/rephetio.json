{
  "comments": [
    {
      "body_html": "<p>We are looking to construct a catalog of <a href=\"https://en.wikipedia.org/wiki/Indication_%28medicine%29\">indications</a> (efficacious drug-disease pairs) with the following attributes (ordered by importance):</p>\r\n\r\n<ol><li>automated and high-throughput construction</li><li>high-quality, or varying levels of quality as long as quality level is annotated</li><li>comprehensive</li><li>disease modifying rather than symptomatic</li><li>compounds which map to pubchem</li><li><a href=\"https://en.wikipedia.org/wiki/Contraindication\">contraindications</a> and adverse effects are excluded and cataloged separately</li><li>diseases which map to the disease ontology</li><li>source is retrievable</li></ol>\r\n\r\n<p>A few options we can consider:</p>\r\n\r\n<ul><li><a href=\"http://ftp.ncbi.nlm.nih.gov/pub/lu/LabeledIn/\"><strong>LabeledIn</strong></a> — Curators manually identified indications from drug labels for 250 human prescription ingredients (drugs) <span class=\"citation\">[<a href=\"/doi/10.1016/j.jbi.2014.08.004\" class=\"citation\" data-key=\"10.1016/j.jbi.2014.08.004\">1</a>]</span>.</li><li><a href=\"http://knowledgemap.mc.vanderbilt.edu/research/content/MEDI\"><strong>MEDI</strong></a> — Indications extracted from RxNorm, SIDER 2, MedlinePlus, and Wikipedia were integrated into a single resource. The high-precision subset (indications in RxNorm or two other resources) includes 13,304 unique indications for 2,136 medications <span class=\"citation\">[<a href=\"/doi/10.1136/amiajnl-2012-001431\" class=\"citation\" data-key=\"10.1136/amiajnl-2012-001431\">2</a>]</span>. Further work added indication prevalence information <span class=\"citation\">[<a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3900157/\" class=\"citation\" data-key=\"MediPrev\">3</a>]</span>. MEDI compares favorably to SemRep for extracting indications from clinical text <span class=\"citation\">[<a href=\"/doi/10.1136/amiajnl-2014-002954\" class=\"citation\" data-key=\"10.1136/amiajnl-2014-002954\">4</a>]</span>.</li><li><strong>SemRep</strong> — \"<a href=\"http://semrep.nlm.nih.gov/\">SemRep</a> is a program that extracts semantic predications (subject-relation-object triples) from biomedical free text\" <span class=\"citation\">[<a href=\"/doi/10.1016/j.jbi.2003.11.003\" class=\"citation\" data-key=\"10.1016/j.jbi.2003.11.003\">5</a>]</span>. SemRep has been used to extract <em>TREAT</em> relations from MeSH scope notes, Daily Med, DrugBank, and <a href=\"http://www.ahfsdruginformation.com/product-consumer-info.aspx\">AHFS Consumer Medication Information</a> <span class=\"citation\">[<a href=\"/doi/10.1145/1882992.1883096\" class=\"citation\" data-key=\"10.1145/1882992.1883096\">6</a>]</span>. SemRep has also been used to identify <em>TREAT</em> relations from Medline abstracts <span class=\"citation\">[<a href=\"http://www.d.umn.edu/~tpederse/Pubs/amia2012-liu.pdf\" class=\"citation\" data-key=\"LiuSemRep\">7</a>]</span>. A project called <a href=\"//skr3.nlm.nih.gov/SemMedDB/\">SemMedDB</a> provides the SemRep results from mining PubMed <span class=\"citation\">[<a href=\"/doi/10.1093/bioinformatics/bts591\" class=\"citation\" data-key=\"10.1093/bioinformatics/bts591\">8</a>]</span>. </li><li><strong>SPL-X</strong> — <strong>S</strong>tructured <strong>P</strong>roduct <strong>L</strong>abels e<strong>X</strong>tractor — Using MetaMap, this project extracted indications from DailyMed drug labels that were available as XML <span class=\"citation\">[<a href=\"/doi/10.1136/amiajnl-2012-001291\" class=\"citation\" data-key=\"10.1136/amiajnl-2012-001291\">9</a>]</span>. Data does not appear to be available.</li><li><strong><a href=\"http://ctdbase.org/\">Comparative Toxicogenomics Database</a></strong> <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gku935\" class=\"citation\" data-key=\"10.1093/nar/gku935\">10</a>]</span> — Manual literature curators annotated drug-disease pairs as 'therapeutic'. The resource is extensive (the 'therapeutic' threshold was low) but incomplete.</li><li><strong><a href=\"http://sideeffects.embl.de/\">SIDER 2</a></strong> — In addition to extracting side effects from drug labels, SIDER also extracts indications <span class=\"citation\">[<a href=\"/doi/10.1038/msb.2009.98\" class=\"citation\" data-key=\"10.1038/msb.2009.98\">11</a>]</span>. Since the approach is automated, some side effects may be extracted as indications and <em>vice versa</em>. This approach would only provide information for drugs with labels from the US FDA or Canada.</li></ul>\r\n\r\n<p>Any additional resources or suggestions?</p>\r\n\r\n<p></p>\r\n\r\n<p></p>",
      "body_md": "We are looking to construct a catalog of [indications](https://en.wikipedia.org/wiki/Indication_%28medicine%29) (efficacious drug-disease pairs) with the following attributes (ordered by importance):\r\n\r\n1. automated and high-throughput construction\r\n+ high-quality, or varying levels of quality as long as quality level is annotated\r\n+ comprehensive\r\n+ disease modifying rather than symptomatic\r\n+ compounds which map to pubchem\r\n+ [contraindications](https://en.wikipedia.org/wiki/Contraindication) and adverse effects are excluded and cataloged separately\r\n+ diseases which map to the disease ontology\r\n+ source is retrievable\r\n\r\nA few options we can consider:\r\n\r\n+ [**LabeledIn**](http://ftp.ncbi.nlm.nih.gov/pub/lu/LabeledIn/) -- Curators manually identified indications from drug labels for 250 human prescription ingredients (drugs) [@10.1016/j.jbi.2014.08.004].\r\n+ [**MEDI**](http://knowledgemap.mc.vanderbilt.edu/research/content/MEDI) -- Indications extracted from RxNorm, SIDER 2, MedlinePlus, and Wikipedia were integrated into a single resource. The high-precision subset (indications in RxNorm or two other resources) includes 13,304 unique indications for 2,136 medications [@10.1136/amiajnl-2012-001431]. Further work added indication prevalence information [@MediPrev]. MEDI compares favorably to SemRep for extracting indications from clinical text [@10.1136/amiajnl-2014-002954].\r\n+ **SemRep** -- \"[SemRep](http://semrep.nlm.nih.gov/) is a program that extracts semantic predications (subject-relation-object triples) from biomedical free text\" [@10.1016/j.jbi.2003.11.003]. SemRep has been used to extract *TREAT* relations from MeSH scope notes, Daily Med, DrugBank, and [AHFS Consumer Medication Information](http://www.ahfsdruginformation.com/product-consumer-info.aspx) [@10.1145/1882992.1883096]. SemRep has also been used to identify *TREAT* relations from Medline abstracts [@LiuSemRep]. A project called [SemMedDB](//skr3.nlm.nih.gov/SemMedDB/) provides the SemRep results from mining PubMed [@10.1093/bioinformatics/bts591]. \r\n+ **SPL-X** -- **S**tructured **P**roduct **L**abels e**X**tractor -- Using MetaMap, this project extracted indications from DailyMed drug labels that were available as XML [@10.1136/amiajnl-2012-001291]. Data does not appear to be available.\r\n+ **[Comparative Toxicogenomics Database](http://ctdbase.org/)** [@10.1093/nar/gku935] -- Manual literature curators annotated drug-disease pairs as 'therapeutic'. The resource is extensive (the 'therapeutic' threshold was low) but incomplete.\r\n+ **[SIDER 2](http://sideeffects.embl.de/)** -- In addition to extracting side effects from drug labels, SIDER also extracts indications [@10.1038/msb.2009.98]. Since the approach is automated, some side effects may be extracted as indications and *vice versa*. This approach would only provide information for drugs with labels from the US FDA or Canada.\r\n\r\nAny additional resources or suggestions?\r\n\r\n[@LiuSemRep]: http://www.d.umn.edu/~tpederse/Pubs/amia2012-liu.pdf \"Liu, Ying, et al. 'Using SemRep to label semantic relations extracted from clinical text.' AMIA annual symposium proceedings. Vol. 2012. American Medical Informatics Association, 2012.\"\r\n\r\n[@MediPrev]: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3900157/ \"Wei W-Q, Mosley JD, Bastarache L, Denny JC (2013) Validation and enhancement of a Computable Medication Indication Resource (MEDI) using a large practice-based dataset. AMIA Annual Symposium Proceedings.\"",
      "profile": 17,
      "published": "2015-01-14T05:55:24.832895Z",
      "thread": 21,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21"
    },
    {
      "body_html": "<p>Are there any types of nodes or edges that you think we should include? Or do you think there is a superior resource for an information type than the one proposed?</p>\r\n\r\n<p>When constructing edges, we prefer data that is:</p>\r\n\r\n<ul><li>systematic, without knowledge bias</li><li>extensive in coverage</li><li>easy to process</li><li>without prohibitive reuse restrictions</li></ul><p>For node suggestions, we prefer controlled vocabularies so annotating the nodes with new edge types in the future is possible.</p>",
      "body_md": "Are there any types of nodes or edges that you think we should include? Or do you think there is a superior resource for an information type than the one proposed?\r\n\r\nWhen constructing edges, we prefer data that is:\r\n\r\n+ systematic, without knowledge bias\r\n+ extensive in coverage\r\n+ easy to process\r\n+ without prohibitive reuse restrictions\r\n\r\nFor node suggestions, we prefer controlled vocabularies so annotating the nodes with new edge types in the future is possible.",
      "profile": 17,
      "published": "2015-01-16T00:46:28.796323Z",
      "thread": 22,
      "url": "/discussion/suggestions-for-additional-information-types/22"
    },
    {
      "body_html": "<p>Daniel — we talked a bit about how you'll be constructing this resource in a manner that is automated and reproducible. I think it would be good to put these details in the research plan — it will give people more opportunities to make suggestions. And if you'll be using techniques that you've used before it's probably a good idea to link people to that work.</p>\r\n\r\n<p>I want people to be able to share their opinion on how you can create the most value from this project. I'm sure you'll agree that enabling reproducibility and reuse is a big part of that.</p>\r\n\r\n<p>What do you think?</p>",
      "body_md": "Daniel -- we talked a bit about how you'll be constructing this resource in a manner that is automated and reproducible. I think it would be good to put these details in the research plan -- it will give people more opportunities to make suggestions. And if you'll be using techniques that you've used before it's probably a good idea to link people to that work.\r\n\r\nI want people to be able to share their opinion on how you can create the most value from this project. I'm sure you'll agree that enabling reproducibility and reuse is a big part of that.\r\n\r\nWhat do you think?",
      "profile": 2,
      "published": "2015-01-16T10:18:57.266583Z",
      "thread": 23,
      "url": "/discussion/enabling-reproducibility-and-reuse/23"
    },
    {
      "body_html": "<blockquote><p>I think it would be good to put these details in the research plan — it will give people more opportunities to make suggestions.</p></blockquote>\r\n\r\n<p>Perhaps, we should discuss here what formats of data and types of services would be most valuable to the community. Then once we reach some consensus, I will update the proposal.</p>\r\n\r\n<p>Currently, I was planning on releasing the network in a few file formats:</p>\r\n\r\n<ul><li>as a single JSON text file based on a <a href=\"https://github.com/dhimmel/hetio/blob/master/hetio/readwrite.py\">specification we've developed</a>.</li><li>as a <a href=\"http://wiki.cytoscape.org/Cytoscape_User_Manual/Network_Formats\">SIF file</a> which is a text file of all edges. We will also provide an accompanying node table, with node attributes. This format is ideal for <a href=\"http://www.cytoscape.org/\">Cytoscape</a> users.</li><li>as a <a href=\"https://en.wikipedia.org/wiki/Graph_Modelling_Language\">GML file</a> — a poorly documented and varyingly implemented format for storing graphs. Despite it's problems, this format is widely supported.</li><li>as separate files for each metanode and metaedge. This will help users who are only interested in a single part of the network, bypass having to process the rest of the network.</li><li>as matrices for edge types that were constructed from continuous pairwise scores.</li></ul><p>An online tool for browsing the network would also be nice. I will look into options here.</p>",
      "body_md": "> I think it would be good to put these details in the research plan -- it will give people more opportunities to make suggestions.\r\n\r\nPerhaps, we should discuss here what formats of data and types of services would be most valuable to the community. Then once we reach some consensus, I will update the proposal.\r\n\r\nCurrently, I was planning on releasing the network in a few file formats:\r\n\r\n+ as a single JSON text file based on a [specification we've developed](https://github.com/dhimmel/hetio/blob/master/hetio/readwrite.py).\r\n+ as a [SIF file](http://wiki.cytoscape.org/Cytoscape_User_Manual/Network_Formats) which is a text file of all edges. We will also provide an accompanying node table, with node attributes. This format is ideal for [Cytoscape](http://www.cytoscape.org/) users.\r\n+ as a [GML file](https://en.wikipedia.org/wiki/Graph_Modelling_Language) -- a poorly documented and varyingly implemented format for storing graphs. Despite it's problems, this format is widely supported.\r\n+ as separate files for each metanode and metaedge. This will help users who are only interested in a single part of the network, bypass having to process the rest of the network.\r\n+ as matrices for edge types that were constructed from continuous pairwise scores.\r\n\r\nAn online tool for browsing the network would also be nice. I will look into options here.",
      "profile": 17,
      "published": "2015-01-16T19:49:29.082635Z",
      "thread": 23,
      "url": "/discussion/enabling-reproducibility-and-reuse/23#2"
    },
    {
      "body_html": "<p>I am looking for a method to weight features based on their uniqueness: I want to downweight redundant features and upweight distinct features.</p>\r\n\r\n<p>The specific problem is weighting side effects when calculating side effect similarity between two drugs. The initial implementation of this analysis <span class=\"citation\">[<a href=\"/doi/10.1126/science.1158140\" class=\"citation\" data-key=\"10.1126/science.1158140\">1</a>]</span> used the Gerstein-Sonnhammer-Chothia Algorithm <span class=\"citation\">[<a href=\"/doi/10.1016/0022-2836%2894%2990012-4\" class=\"citation\" data-key=\"10.1016/0022-2836(94)90012-4\">2</a>]</span>. The initial implementation <a href=\"http://www.sciencemag.org/content/suppl/2008/07/10/321.5886.263.DC1/Campillos.SOM.pdf\">describes their method</a>:</p>\r\n\r\n<blockquote><p>Not all side effects are independent of each other; for example, 90% of drugs that cause nausea also cause vomiting. We correct for this redundancy by weighting side effects in a manner analogous to the down-weighting of similar protein sequences within multiple alignments <span class=\"citation\">[<a href=\"/doi/10.1016/0022-2836%2894%2990012-4\" class=\"citation\" data-key=\"10.1016/0022-2836(94)90012-4\">2</a>]</span> (Fig. S1C). In order to determine the correlation weight, the correlation of side effects was determined by clustering all side effects according to their assigned drugs using a Tanimoto/Jacquard score to compute a distance matrix: The distance between two drugs was calculated by dividing the number of drugs that feature both side effects by the number of drugs that have either side effect associated. <strong>The Gerstein–Sonnhammer–Chothia algorithm <span class=\"citation\">[<a href=\"/doi/10.1016/0022-2836%2894%2990012-4\" class=\"citation\" data-key=\"10.1016/0022-2836(94)90012-4\">2</a>]</span> was used to compute weights based on a hierarchal clustering with the aforementioned distance matrix <span class=\"citation\">[<a href=\"/doi/10.1073/pnas.95.25.14863\" class=\"citation\" data-key=\"10.1073/pnas.95.25.14863\">3</a>]</span>.</strong></p></blockquote>\r\n\r\n<p>I would like to perform an identical analysis, while making the code and results public. The analysis follows the following steps:</p>\r\n\r\n<ol><li>Calculating pairwise side effect correlations.</li><li>Performing hierarchical clustering of side effects to produce a dendrogram</li><li>Inputting the denrogram into the GSC algorithm to calculate side effect weights.</li></ol>\r\n\r\n<p>I am looking for an implementation of step 3, that it easy to integrate with implementations of steps 1 and 2. My preferred languages are <em>R</em>, <em>python</em>, and <em>julia</em> (in that order). The author should be willing to post the code publicly under an open source license.</p>\r\n\r\n<p>An in detail description of the GSC algorithm can be found in the paper appendix titled, <em><a href=\"https://pdf.yt/d/Sx3jMbr8vANgxAej/download\">A Method to Weight Protein Sequences to Correct for Unequal Representation</a></em>.</p>\r\n\r\n<p>Also, if you know of a simpler or superior method for accomplishing this weighting task, please suggest.</p>",
      "body_md": "I am looking for a method to weight features based on their uniqueness: I want to downweight redundant features and upweight distinct features.\r\n\r\nThe specific problem is weighting side effects when calculating side effect similarity between two drugs. The initial implementation of this analysis [@10.1126/science.1158140] used the Gerstein-Sonnhammer-Chothia Algorithm [@10.1016/0022-2836(94)90012-4]. The initial implementation [describes their method](http://www.sciencemag.org/content/suppl/2008/07/10/321.5886.263.DC1/Campillos.SOM.pdf):\r\n\r\n> Not all side effects are independent of each other; for example, 90% of drugs that cause nausea also cause vomiting. We correct for this redundancy by weighting side effects in a manner analogous to the down-weighting of similar protein sequences within multiple alignments [@10.1016/0022-2836(94)90012-4] (Fig. S1C). In order to determine the correlation weight, the correlation of side effects was determined by clustering all side effects according to their assigned drugs using a Tanimoto/Jacquard score to compute a distance matrix: The distance between two drugs was calculated by dividing the number of drugs that feature both side effects by the number of drugs that have either side effect associated. **The Gerstein–Sonnhammer–Chothia algorithm [@10.1016/0022-2836(94)90012-4] was used to compute weights based on a hierarchal clustering with the aforementioned distance matrix [@10.1073/pnas.95.25.14863].**\r\n\r\nI would like to perform an identical analysis, while making the code and results public. The analysis follows the following steps:\r\n\r\n1. Calculating pairwise side effect correlations.\r\n2. Performing hierarchical clustering of side effects to produce a dendrogram\r\n3. Inputting the denrogram into the GSC algorithm to calculate side effect weights.\r\n\r\nI am looking for an implementation of step 3, that it easy to integrate with implementations of steps 1 and 2. My preferred languages are *R*, *python*, and *julia* (in that order). The author should be willing to post the code publicly under an open source license.\r\n\r\nAn in detail description of the GSC algorithm can be found in the paper appendix titled, *[A Method to Weight Protein Sequences to Correct for Unequal Representation](https://pdf.yt/d/Sx3jMbr8vANgxAej/download)*.\r\n\r\nAlso, if you know of a simpler or superior method for accomplishing this weighting task, please suggest.",
      "profile": 17,
      "published": "2015-01-22T18:14:09.286193Z",
      "thread": 25,
      "url": "/discussion/seeking-an-open-source-implementation-of-the-gerstein-sonnhammer-chothia-algorithm/25"
    },
    {
      "body_html": "<p>As a follow-up to this, are you planning to make only the network available, or are you planning to release all of the code necessary to construct the network? If you release that code, will you use some testing and continuous integration process to evaluate regressions? Releasing the code would allow others in the future to assess the role that new experiments or new sources of information play in prediction quality. I agree with Jesse that more details on the overall development process would be great.</p>",
      "body_md": "As a follow-up to this, are you planning to make only the network available, or are you planning to release all of the code necessary to construct the network? If you release that code, will you use some testing and continuous integration process to evaluate regressions? Releasing the code would allow others in the future to assess the role that new experiments or new sources of information play in prediction quality. I agree with Jesse that more details on the overall development process would be great.",
      "profile": 22,
      "published": "2015-01-22T20:43:07.278879Z",
      "thread": 23,
      "url": "/discussion/enabling-reproducibility-and-reuse/23#3"
    },
    {
      "body_html": "<p>Hey Daniel,</p>\r\n\r\n<p>This should do it:<br><a href=\"https://github.com/antoine-lizee/R-GSC/blob/master/GSC.R\" target=\"_blank\">https://github.com/antoine-lizee/R-GSC/blob/master/GSC.R</a></p>\r\n\r\n<p>It uses dendrogram objects in R.</p>\r\n\r\n<p>Once you get it, the implementation is quite straightforward and short, so it doesn't really justify creating a package. Everything in one file, you can just source it.<br>Have a go at it and tell me what you think.</p>\r\n\r\n<p>Best,<br>Antoine</p>\r\n\r\n<p>PS: the attached reference that explains the algorithm has its main example wrong because of the low (and surprisingly inconsistent) precision they use to do the maths.</p>",
      "body_md": "Hey Daniel,\r\n\r\nThis should do it:\r\nhttps://github.com/antoine-lizee/R-GSC/blob/master/GSC.R\r\n\r\nIt uses dendrogram objects in R.\r\n\r\nOnce you get it, the implementation is quite straightforward and short, so it doesn't really justify creating a package. Everything in one file, you can just source it.\r\nHave a go at it and tell me what you think.\r\n\r\nBest,\r\nAntoine\r\n\r\nPS: the attached reference that explains the algorithm has its main example wrong because of the low (and surprisingly inconsistent) precision they use to do the maths.",
      "profile": 23,
      "published": "2015-01-23T04:11:25.882145Z",
      "thread": 25,
      "url": "/discussion/seeking-an-open-source-implementation-of-the-gerstein-sonnhammer-chothia-algorithm/25#2"
    },
    {
      "body_html": "<p><a href=\"/u/alizee\" class=\"username\">@alizee</a>, thanks for the <a href=\"https://github.com/antoine-lizee/R-GSC/\">open source implementation</a> — you just spared scientists all over the pain of unnecessary reimplementation.</p>\r\n\r\n<p>Here is the code I wrote, which calls your <code>GSC</code> function</p>\r\n\r\n<p></p><pre><code class=\"r\">GitHubScript &lt;- function(...) {\r\n  # Source a script from GitHub\r\n  library(RCurl)\r\n  github.url &lt;- file.path('https://raw.githubusercontent.com', ...)\r\n  script &lt;- RCurl::getURL(github.url)\r\n  eval(parse(text = script), envir = .GlobalEnv)\r\n}\r\n\r\nUnderrepresentationWeight &lt;- function(mat) {\r\n  # Returns underrepresentation weight for each column\r\n  col.dist &lt;- stats::dist(t(mat), method = 'binary')\r\n  col.clust &lt;- hclust(col.dist, method = 'ward.D2')\r\n  col.dendro &lt;- as.dendrogram(col.clust)\r\n  GitHubScript('antoine-lizee', 'R-GSC', 'master', 'GSC.R')\r\n  GSC(col.dendro)\r\n}</code></pre>\r\n\r\n<p>I have come accross two errors for my use cases</p>\r\n\r\n<p>The following code (which computes only the first 100 side effects for time) returns a vector of only <code>NaN</code>:<br></p>\r\n\r\n<p></p><pre><code class=\"r\">underrep.ind.vec &lt;- UnderrepresentationWeight(se.mat[, 1:100])</code></pre>\r\n\r\n<p>On a slightly larger matrix, I also got a maximum recursion depth error.</p>\r\n\r\n<p><code>se.mat</code> is a matrix where rows are compounds and columns are side effects. An element is <code>0</code> for no relationship and <code>1</code> for side effect. You can <a href=\"http://stat.ethz.ch/R-manual/R-devel/library/base/html/load.html\">load</a> <code>se.mat</code> from <a href=\"https://www.dropbox.com/s/8e12q0gkmxs54b6/se.mat.RData?raw=1\">this file</a>.</p>",
      "body_md": "@alizee, thanks for the [open source implementation](https://github.com/antoine-lizee/R-GSC/) -- you just spared scientists all over the pain of unnecessary reimplementation.\r\n\r\nHere is the code I wrote, which calls your `GSC` function\r\n\r\n```r\r\nGitHubScript <- function(...) {\r\n  # Source a script from GitHub\r\n  library(RCurl)\r\n  github.url <- file.path('https://raw.githubusercontent.com', ...)\r\n  script <- RCurl::getURL(github.url)\r\n  eval(parse(text = script), envir = .GlobalEnv)\r\n}\r\n\r\nUnderrepresentationWeight <- function(mat) {\r\n  # Returns underrepresentation weight for each column\r\n  col.dist <- stats::dist(t(mat), method = 'binary')\r\n  col.clust <- hclust(col.dist, method = 'ward.D2')\r\n  col.dendro <- as.dendrogram(col.clust)\r\n  GitHubScript('antoine-lizee', 'R-GSC', 'master', 'GSC.R')\r\n  GSC(col.dendro)\r\n}\r\n```\r\nI have come accross two errors for my use cases\r\n\r\nThe following code (which computes only the first 100 side effects for time) returns a vector of only `NaN`:\r\n```r\r\nunderrep.ind.vec <- UnderrepresentationWeight(se.mat[, 1:100])\r\n```\r\nOn a slightly larger matrix, I also got a maximum recursion depth error.\r\n\r\n`se.mat` is a matrix where rows are compounds and columns are side effects. An element is `0` for no relationship and `1` for side effect. You can [load](http://stat.ethz.ch/R-manual/R-devel/library/base/html/load.html) `se.mat` from [this file](https://www.dropbox.com/s/8e12q0gkmxs54b6/se.mat.RData?raw=1).",
      "profile": 17,
      "published": "2015-01-23T19:58:27.683271Z",
      "thread": 25,
      "url": "/discussion/seeking-an-open-source-implementation-of-the-gerstein-sonnhammer-chothia-algorithm/25#3"
    },
    {
      "body_html": "<p>Hi Daniel,</p>\r\n\r\n<p>Thank you for your feedback. <br>I solved your first \"Nan\" problem. Below is a short explanation, and on the repository I added a <a href=\"https://github.com/antoine-lizee/R-GSC/blob/master/dhimmel_bugs_test.R\">file</a> in which I investigate the problem.</p>\r\n\r\n<p>The problem came from the fact that some of your vectors of features in your original matrix were <strong>exactly the same</strong>, i.e. some of your compounds have the exact same side effects. This resulted in a computed distance of zero between these two objects, and the clustering is putting them at the bottom of the dendrogram, at height 0. The algorithm wasn't designed for such an edge case and a division by 0 was giving you the NaN. I slightly changed the algorithm to work even with this edge case.<br>I quickly tested that the coefficients at this edge case were roughly the limit of the coefficients when the two vectors are getting similar, which shows that the handling of this edge case is appropriate. </p>\r\n\r\n<p>I am looking into some profiling and making it work for bigger matrices, so I'll update you regarding your other problem soon.</p>\r\n\r\n<p>Best,<br>Antoine</p>",
      "body_md": "Hi Daniel,\r\n\r\nThank you for your feedback. \r\nI solved your first \"Nan\" problem. Below is a short explanation, and on the repository I added a [file](https://github.com/antoine-lizee/R-GSC/blob/master/dhimmel_bugs_test.R) in which I investigate the problem.\r\n\r\nThe problem came from the fact that some of your vectors of features in your original matrix were **exactly the same**, i.e. some of your compounds have the exact same side effects. This resulted in a computed distance of zero between these two objects, and the clustering is putting them at the bottom of the dendrogram, at height 0. The algorithm wasn't designed for such an edge case and a division by 0 was giving you the NaN. I slightly changed the algorithm to work even with this edge case.\r\nI quickly tested that the coefficients at this edge case were roughly the limit of the coefficients when the two vectors are getting similar, which shows that the handling of this edge case is appropriate. \r\n\r\nI am looking into some profiling and making it work for bigger matrices, so I'll update you regarding your other problem soon.\r\n\r\nBest,\r\nAntoine",
      "profile": 23,
      "published": "2015-01-25T03:22:18.788239Z",
      "thread": 25,
      "url": "/discussion/seeking-an-open-source-implementation-of-the-gerstein-sonnhammer-chothia-algorithm/25#4"
    },
    {
      "body_html": "<p>I plan to follow the 10 rules of reproducible computational research <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1003285\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1003285\">1</a>]</span>.</p>\r\n\r\n<p><a href=\"/u/caseygreene\" class=\"username\">@caseygreene</a>, I would like to release all of the code. I plan on doing a lot of the work in notebooks (of the <a href=\"http://rmarkdown.rstudio.com/\">R</a> and <a href=\"http://ipython.org/notebook.html\">python</a> varieties). However, my current plan lacks the level of automation that you are suggesting.</p>\r\n\r\n<p>Testing whether a new source of information has improved prediction should be straightforward. I am not sure exactly what you mean by \"continuous integration process\".</p>\r\n\r\n<p>I would like to provide a single script that performs the entire analysis, but this may be difficult because the computation will be performed in different venues and locations.</p>",
      "body_md": "I plan to follow the 10 rules of reproducible computational research [@10.1371/journal.pcbi.1003285].\r\n\r\n@caseygreene, I would like to release all of the code. I plan on doing a lot of the work in notebooks (of the [R](http://rmarkdown.rstudio.com/) and [python](http://ipython.org/notebook.html) varieties). However, my current plan lacks the level of automation that you are suggesting.\r\n\r\nTesting whether a new source of information has improved prediction should be straightforward. I am not sure exactly what you mean by \"continuous integration process\".\r\n\r\nI would like to provide a single script that performs the entire analysis, but this may be difficult because the computation will be performed in different venues and locations.",
      "profile": 17,
      "published": "2015-01-26T15:00:59.175127Z",
      "thread": 23,
      "url": "/discussion/enabling-reproducibility-and-reuse/23#4"
    },
    {
      "body_html": "<p>If you get the entire analysis boiled down a one-step build process, there are services that can monitor your github repository, look for commits, and then kick off a build. You'd need to define test cases that determine that something looks different than you expect. Maybe you have a set of positive controls (known multi-use drugs?) and you evaluate the extent to which they are accurately predicted. Depending on how long the entire process takes, these services might be enough to monitor and identify any commits that produce large changes in your overall results. You could also use some sort of correlation measure to previous runs to look for any commits that produce abnormally large changes in the output.</p>",
      "body_md": "If you get the entire analysis boiled down a one-step build process, there are services that can monitor your github repository, look for commits, and then kick off a build. You'd need to define test cases that determine that something looks different than you expect. Maybe you have a set of positive controls (known multi-use drugs?) and you evaluate the extent to which they are accurately predicted. Depending on how long the entire process takes, these services might be enough to monitor and identify any commits that produce large changes in your overall results. You could also use some sort of correlation measure to previous runs to look for any commits that produce abnormally large changes in the output.",
      "profile": 22,
      "published": "2015-01-26T15:14:48.758534Z",
      "thread": 23,
      "url": "/discussion/enabling-reproducibility-and-reuse/23#5"
    },
    {
      "body_html": "<p>Hey,</p>\r\n\r\n<p>I think the code is ready now. </p>\r\n\r\n<p>In addition to the fix above that let the algorithm run on datasets with objects that have the same representation, I added some routine check to increase the maximum recursive depth of R when calling the function. I used a robust adaptive approach to increase this recursion limit based on the estimated number of elements in the dendrogram, see the <a href=\"https://github.com/antoine-lizee/R-GSC/blob/master/GSC.R#L47\">code</a>.</p>\r\n\r\n<p>I also did some extensive testing and profiling in the <a href=\"https://github.com/antoine-lizee/R-GSC/blob/master/dhimmel_bugs_test.R\">dhimmel file</a> mentioned earlier. As expected, the algorithm is very cheap: execution time is linear in the number of elements of the dendrogram, with roughly <a href=\"https://github.com/antoine-lizee/R-GSC/blob/master/test/profiling.pdf\">10ms / kElements</a>. I tried to half the stack depth by writing a <a href=\"https://github.com/antoine-lizee/R-GSC/blob/master/test/GSC2.R\">more verbose</a> version of the GSC algorithm, but it didn't change performance, so I kept the first more elegant version.</p>\r\n\r\n<p>Enjoy!</p>",
      "body_md": "Hey,\r\n\r\nI think the code is ready now. \r\n\r\nIn addition to the fix above that let the algorithm run on datasets with objects that have the same representation, I added some routine check to increase the maximum recursive depth of R when calling the function. I used a robust adaptive approach to increase this recursion limit based on the estimated number of elements in the dendrogram, see the [code](https://github.com/antoine-lizee/R-GSC/blob/master/GSC.R#L47).\r\n\r\nI also did some extensive testing and profiling in the [dhimmel file] (https://github.com/antoine-lizee/R-GSC/blob/master/dhimmel_bugs_test.R) mentioned earlier. As expected, the algorithm is very cheap: execution time is linear in the number of elements of the dendrogram, with roughly [10ms / kElements] (https://github.com/antoine-lizee/R-GSC/blob/master/test/profiling.pdf). I tried to half the stack depth by writing a [more verbose](https://github.com/antoine-lizee/R-GSC/blob/master/test/GSC2.R) version of the GSC algorithm, but it didn't change performance, so I kept the first more elegant version.\r\n\r\nEnjoy!",
      "profile": 23,
      "published": "2015-01-26T23:25:48.859352Z",
      "thread": 25,
      "url": "/discussion/seeking-an-open-source-implementation-of-the-gerstein-sonnhammer-chothia-algorithm/25#5"
    },
    {
      "body_html": "<p>SIDER is a <a href=\"http://sideeffects.embl.de/\">resource</a> which automatically parsed labels for approved drugs and annotated side effects and indications <span class=\"citation\">[<a href=\"/doi/10.1038/msb.2009.98\" class=\"citation\" data-key=\"10.1038/msb.2009.98\">1</a>]</span>.</p>\r\n\r\n<p>We <a href=\"http://git.dhimmel.com/SIDER2/\">performed an analysis</a> using the <a href=\"http://sideeffects.embl.de/download/\">raw SIDER data</a>, to evaluate the accuracy, quality, and usefulness of this resource. In addition to using the indications, we are interested in adding side effects as a separate node and edge type in our network.</p>\r\n\r\n<p>After quality control to resolve conflicts (cases where a concept was annotated to a compound as both a side effect and indication), we found indications for 1005 drugs. <a href=\"/u/leobrueggeman\" class=\"username\">@leobrueggeman</a>, manually classified 101 random indications and found a precision of 63.4% [95% CI: 53.1–72.6%]. We browsed the indications for multiple sclerosis and found that many symptomatic treatments were included while many of the disease-modifying small molecules were absent. Overall the SIDER indications alone will be a rather <em>poor</em> resource. One possibility is combining <a href=\"http://git.dhimmel.com/SIDER2/data/sider2-processed.txt\">SIDER indications</a> with orthogonal methods.</p>\r\n\r\n<p>The precision of side effects was considerably better at 92.0% [95% CI: 84.4–96.2%]. However, despite being largely accurate, not all side effects are of the same relevance (frequencies vary and placebo levels of occurrence are frequently lacking). <a href=\"/u/leobrueggeman\" class=\"username\">@leobrueggeman</a>, could you provide some additional information on the deficiencies or strengths of the SIDER approach?</p>",
      "body_md": "SIDER is a [resource](http://sideeffects.embl.de/) which automatically parsed labels for approved drugs and annotated side effects and indications [@10.1038/msb.2009.98].\r\n\r\nWe [performed an analysis](http://git.dhimmel.com/SIDER2/) using the [raw SIDER data](http://sideeffects.embl.de/download/), to evaluate the accuracy, quality, and usefulness of this resource. In addition to using the indications, we are interested in adding side effects as a separate node and edge type in our network.\r\n\r\nAfter quality control to resolve conflicts (cases where a concept was annotated to a compound as both a side effect and indication), we found indications for 1005 drugs. @leobrueggeman, manually classified 101 random indications and found a precision of 63.4% [95% CI: 53.1--72.6%]. We browsed the indications for multiple sclerosis and found that many symptomatic treatments were included while many of the disease-modifying small molecules were absent. Overall the SIDER indications alone will be a rather *poor* resource. One possibility is combining [SIDER indications](http://git.dhimmel.com/SIDER2/data/sider2-processed.txt) with orthogonal methods.\r\n\r\nThe precision of side effects was considerably better at 92.0% [95% CI: 84.4--96.2%]. However, despite being largely accurate, not all side effects are of the same relevance (frequencies vary and placebo levels of occurrence are frequently lacking). @leobrueggeman, could you provide some additional information on the deficiencies or strengths of the SIDER approach?",
      "profile": 17,
      "published": "2015-02-13T02:25:05.540731Z",
      "thread": 30,
      "url": "/discussion/assessing-the-quality-and-applicability-of-the-sider-2-resource/30"
    },
    {
      "body_html": "<p><a href=\"/u/caseygreene\" class=\"username\">@caseygreene</a> and <a href=\"/u/jspauld\" class=\"username\">@jspauld</a>, thanks for the suggestions. I've <a href=\"http://thinklab.com/p/rephetio/plan/compare/0ed4f4242ca776e6460be3f24e9e7e4d3a5c6ad3/2f775f436f7c99413a195f729ae9a20bfc25e6c5\">added</a> an <a href=\"http://thinklab.com/p/rephetio/plan#open-science\">open science section</a> to the proposal where I make commitments to:</p>\r\n\r\n<ul><li>the 10 simple rules for reproducible research in computational biology <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1003285\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1003285\">1</a>]</span></li><li>releasing all code on <a href=\"https://github.com/dhimmel\">GitHub</a></li><li>releasing all data</li><li>CC-BY or CC-0 licensing</li></ul><p>I also mention R Markdown and IPython notebooks, which I've been using extensively thus far — for example, when <a href=\"http://git.dhimmel.com/SIDER2/\">analyzing SIDER 2 data</a>.</p>\r\n\r\n<hr><p><a href=\"/u/caseygreene\" class=\"username\">@caseygreene</a>, I think your suggestion of implementing a continuous integration process is fantastic. I am hesitant to commit on this issue for a few reasons:</p>\r\n\r\n<ul><li>In the past, the computations have been extreme (requiring cluster usage) and this may be unwieldy to execute after every commit. I hope to optimize the algorithm, which may help in this regard.</li><li>I need to investigate these services more.</li><li>Our heterogeneous network framework and analyses are still rapidly evolving, so I don't want to invest lot's of time in code or methods that will soon be replaced.</li><li>I need to start writing unit tests for my programs. This seems like a more immediate issue that I will prioritize. Once I create proper unit tests, I will enable them as <a href=\"http://git-scm.com/book/en/v2/Customizing-Git-Git-Hooks#Client-Side-Hooks\">pre-commit hooks</a>.</li></ul><p>That being said, your suggestion is in line with the philosophy of ThinkLab and I will keep it in mind.</p>",
      "body_md": "@caseygreene and @jspauld, thanks for the suggestions. I've [added](http://thinklab.com/p/rephetio/plan/compare/0ed4f4242ca776e6460be3f24e9e7e4d3a5c6ad3/2f775f436f7c99413a195f729ae9a20bfc25e6c5) an [open science section](http://thinklab.com/p/rephetio/plan#open-science) to the proposal where I make commitments to:\r\n\r\n+ the 10 simple rules for reproducible research in computational biology [@10.1371/journal.pcbi.1003285]\r\n+ releasing all code on [GitHub](https://github.com/dhimmel)\r\n+ releasing all data\r\n+ CC-BY or CC-0 licensing\r\n\r\nI also mention R Markdown and IPython notebooks, which I've been using extensively thus far -- for example, when [analyzing SIDER 2 data](http://git.dhimmel.com/SIDER2/).\r\n\r\n***\r\n\r\n@caseygreene, I think your suggestion of implementing a continuous integration process is fantastic. I am hesitant to commit on this issue for a few reasons:\r\n\r\n+ In the past, the computations have been extreme (requiring cluster usage) and this may be unwieldy to execute after every commit. I hope to optimize the algorithm, which may help in this regard.\r\n+ I need to investigate these services more.\r\n+ Our heterogeneous network framework and analyses are still rapidly evolving, so I don't want to invest lot's of time in code or methods that will soon be replaced.\r\n+ I need to start writing unit tests for my programs. This seems like a more immediate issue that I will prioritize. Once I create proper unit tests, I will enable them as [pre-commit hooks](http://git-scm.com/book/en/v2/Customizing-Git-Git-Hooks#Client-Side-Hooks).\r\n\r\nThat being said, your suggestion is in line with the philosophy of ThinkLab and I will keep it in mind.",
      "profile": 17,
      "published": "2015-02-16T22:41:28.927019Z",
      "thread": 23,
      "url": "/discussion/enabling-reproducibility-and-reuse/23#6"
    },
    {
      "body_html": "<p><strong>Update: See reply — this issue has been resolved. Links to our analyses and code have been changed to archived versions in this post.</strong></p>\r\n\r\n<hr>\r\n\r\n<p>MEDI is a <a href=\"http://knowledgemap.mc.vanderbilt.edu/research/content/MEDI\">publicly-available indication resource</a> standardized to ICD9/UMLS concepts for diseases and RxNorm ingredients for drugs. The accompanying publication clearly and concisely presents the analysis, which follows a rational, resourceful, and thorough methodology <span class=\"citation\">[<a href=\"/doi/10.1136/amiajnl-2012-001431\" class=\"citation\" data-key=\"10.1136/amiajnl-2012-001431\">1</a>]</span>.</p>\r\n\r\n<p>The data is also already online, which <a href=\"http://thinklab.com/p/rephetio/how-should-we-construct-a-catalog-of-drug-indications/21\">is not the case</a> with some other indication resources we've evaluated. However, when <a href=\"http://cdn.rawgit.com/dhimmel/indications/65dbe8ec52c1c882cecb4451f5bcf433d5b189c7/medi/index.html\">processing the data</a> (<a href=\"https://github.com/dhimmel/indications/tree/65dbe8ec52c1c882cecb4451f5bcf433d5b189c7/medi\">source on github</a>), we came across a potential discrepancy between <a href=\"http://dx.doi.org/10.1136/amiajnl-2012-001431#T1\">Table 2 of the manuscript</a> and the <a href=\"http://cdn.rawgit.com/dhimmel/indications/65dbe8ec52c1c882cecb4451f5bcf433d5b189c7/medi/index.html#resource-counts\">statistics we generated</a>. The problem could have arisen from a mistake in our data processing or in MEDI's data export.</p>\r\n\r\n<p>Specifically, from the <a href=\"http://dx.doi.org/10.1136/amiajnl-2012-001431#T1\">manuscript</a> <span class=\"citation\">[<a href=\"/doi/10.1136/amiajnl-2012-001431\" class=\"citation\" data-key=\"10.1136/amiajnl-2012-001431\">1</a>]</span>:</p>\r\n\r\n<blockquote><p><strong>Table 2</strong>: Number of unique medications, ICD9 codes, and indication pairs extracted from each resource</p><table class=\"table markdown-table\"><thead><tr><th>Resource</th><th>Medications (% of total)</th><th>ICD9 codes (% of total)</th><th>Indication pairs (% of total)</th></tr></thead><tbody><tr><td>RxNorm</td><td>1,726 (56)</td><td>999 (33)</td><td>8,040 (13)</td></tr><tr><td>SIDER 2</td><td>1,554 (50)</td><td>1,703 (57)</td><td>17,702 (28)</td></tr><tr><td>MedlinePlus</td><td>1,629 (52)</td><td>869 (29)</td><td>16,581(26)</td></tr><tr><td>Wikipedia</td><td>2,608 (84)</td><td>2,624 (87)</td><td>34,911 (55)</td></tr><tr><td>Union of all resources</td><td>3,112</td><td>3,009</td><td>63,343</td></tr></tbody></table></blockquote>\r\n\r\n<p><a href=\"http://cdn.rawgit.com/dhimmel/indications/65dbe8ec52c1c882cecb4451f5bcf433d5b189c7/medi/index.html#resource-counts\">Our analysis</a> found different resource-specific counts. The comparison is complicated since the resource to numeric identifier mapping is unknown:</p>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th>resource</th><th>medications</th><th>diseases</th><th>indications</th></tr></thead><tbody><tr><td>1</td><td>3,091</td><td>2,985</td><td>53,615</td></tr><tr><td>2</td><td>1,648</td><td>1,075</td><td>6,279</td></tr><tr><td>3</td><td>984</td><td>551</td><td>2,497</td></tr><tr><td>4</td><td>447</td><td>222</td><td>952</td></tr><tr><td>all</td><td>3,112</td><td>3,009</td><td>63,343</td></tr><tr><td>hps</td><td>2,139</td><td>1,345</td><td>13,379</td></tr></tbody></table>\r\n\r\n<p>We will reach out to the MEDI authors for assistance. Currently the discrepancy seems to have a negligible effect on the high-precision subset.</p>",
      "body_md": "**Update: See reply -- this issue has been resolved. Links to our analyses and code have been changed to archived versions in this post.**\r\n\r\n***\r\n\r\nMEDI is a [publicly-available indication resource](http://knowledgemap.mc.vanderbilt.edu/research/content/MEDI) standardized to ICD9/UMLS concepts for diseases and RxNorm ingredients for drugs. The accompanying publication clearly and concisely presents the analysis, which follows a rational, resourceful, and thorough methodology [@10.1136/amiajnl-2012-001431].\r\n\r\nThe data is also already online, which [is not the case](http://thinklab.com/p/rephetio/how-should-we-construct-a-catalog-of-drug-indications/21) with some other indication resources we've evaluated. However, when [processing the data](http://cdn.rawgit.com/dhimmel/indications/65dbe8ec52c1c882cecb4451f5bcf433d5b189c7/medi/index.html) ([source on github](https://github.com/dhimmel/indications/tree/65dbe8ec52c1c882cecb4451f5bcf433d5b189c7/medi)), we came across a potential discrepancy between [Table 2 of the manuscript](http://dx.doi.org/10.1136/amiajnl-2012-001431#T1) and the [statistics we generated](http://cdn.rawgit.com/dhimmel/indications/65dbe8ec52c1c882cecb4451f5bcf433d5b189c7/medi/index.html#resource-counts). The problem could have arisen from a mistake in our data processing or in MEDI's data export.\r\n\r\nSpecifically, from the [manuscript](http://dx.doi.org/10.1136/amiajnl-2012-001431#T1) [@10.1136/amiajnl-2012-001431]:\r\n> **Table 2**: Number of unique medications, ICD9 codes, and indication pairs extracted from each resource\r\n>\r\n| Resource               | Medications (% of total) | ICD9 codes (% of total) | Indication pairs (% of total) |\r\n|------------------------|--------------------------|-------------------------|-------------------------------|\r\n| RxNorm                 | 1,726 (56)                | 999 (33)                | 8,040 (13)                     |\r\n| SIDER 2                | 1,554 (50)                | 1,703 (57)               | 17,702 (28)                    |\r\n| MedlinePlus            | 1,629 (52)                | 869 (29)                | 16,581(26)                     |\r\n| Wikipedia              | 2,608 (84)                | 2,624 (87)               | 34,911 (55)                    |\r\n| Union of all resources | 3,112                     | 3,009                    | 63,343                         |\r\n\r\n[Our analysis](http://cdn.rawgit.com/dhimmel/indications/65dbe8ec52c1c882cecb4451f5bcf433d5b189c7/medi/index.html#resource-counts) found different resource-specific counts. The comparison is complicated since the resource to numeric identifier mapping is unknown:\r\n\r\n| resource | medications | diseases | indications |\r\n|----------|-------------|----------|-------------|\r\n| 1        | 3,091        | 2,985     | 53,615       |\r\n| 2        | 1,648        | 1,075     | 6,279        |\r\n| 3        | 984         | 551      | 2,497        |\r\n| 4        | 447         | 222      | 952         |\r\n| all      | 3,112        | 3,009     | 63,343       |\r\n| hps      | 2,139        | 1,345     | 13,379       |\r\n\r\nWe will reach out to the MEDI authors for assistance. Currently the discrepancy seems to have a negligible effect on the high-precision subset.",
      "profile": 17,
      "published": "2015-02-17T02:21:47.753909Z",
      "thread": 31,
      "url": "/discussion/medi-indications-data-discrepancy-in-resource-specific-counts/31"
    },
    {
      "body_html": "<p>After contacting <a href=\"http://knowledgemap.mc.vanderbilt.edu/research/content/wei-qi-wei-mmed-phd\">Dr. Wei-Qi Wei</a>, we located the cause of the discrepancy. The integer values in the <code>MENTIONEDBYRESOURCES</code> column of <code>MEDI_01212013_0.csv</code> and <code>MEDI_01212013_UMLS.csv</code> refer to <em>how many</em> resources reported the indication. We had incorrectly assumed that this column referred to <em>which</em> resources reported the indication. Therefore, it appeared that each indication was only reported by a single resource.</p>\r\n\r\n<p>Resource-specific indications data is not available from the <a href=\"http://knowledgemap.mc.vanderbilt.edu/research/content/MEDI\">MEDI website</a>. However, the true counts for each resource combination are provided in <a href=\"http://dx.doi.org/10.1136/amiajnl-2012-001431#sec-8\">manuscript Figure 2</a> <span class=\"citation\">[<a href=\"/doi/10.1136/amiajnl-2012-001431\" class=\"citation\" data-key=\"10.1136/amiajnl-2012-001431\">1</a>]</span>:</p>\r\n\r\n<p><img src=\"http://jamia.oxfordjournals.org/content/jaminfo/20/5/954/F2.large.jpg\" alt=\"Weighted Venn diagram of the distribution of medications and indication pairs within the four resources\"></p>\r\n\r\n<p>We would like to thank the authors for their prompt response and clarification.</p>",
      "body_md": "After contacting [Dr. Wei-Qi Wei](http://knowledgemap.mc.vanderbilt.edu/research/content/wei-qi-wei-mmed-phd), we located the cause of the discrepancy. The integer values in the `MENTIONEDBYRESOURCES` column of `MEDI_01212013_0.csv` and `MEDI_01212013_UMLS.csv` refer to *how many* resources reported the indication. We had incorrectly assumed that this column referred to *which* resources reported the indication. Therefore, it appeared that each indication was only reported by a single resource.\r\n\r\nResource-specific indications data is not available from the [MEDI website](http://knowledgemap.mc.vanderbilt.edu/research/content/MEDI). However, the true counts for each resource combination are provided in [manuscript Figure 2](http://dx.doi.org/10.1136/amiajnl-2012-001431#sec-8) [@10.1136/amiajnl-2012-001431]:\r\n\r\n![Weighted Venn diagram of the distribution of medications and indication pairs within the four resources](http://jamia.oxfordjournals.org/content/jaminfo/20/5/954/F2.large.jpg)\r\n\r\nWe would like to thank the authors for their prompt response and clarification.",
      "profile": 17,
      "published": "2015-02-17T17:26:17.673816Z",
      "thread": 31,
      "url": "/discussion/medi-indications-data-discrepancy-in-resource-specific-counts/31#2"
    },
    {
      "body_html": "<p><a href=\"/u/dhimmel\" class=\"username\">@dhimmel</a> <br>From what I observed it seems that there are a few types of mistakes which, when combined, lead to a significant drop in the precision of indications. The biggest problem I saw in the SIDER data, and perhaps the easiest to fix, was that there were several \"indications\" which were not actually diseases (e.g. \"progression\", \"adverse reactions\", \"interactions\"). It should be possible to filter out these artifacts of text mining via reference to a disease ontology.</p>\r\n\r\n<p>The second repeated mistake I saw was that SIDER would on occasion mark a symptom of a real indication as an indication (e.g. agoraphobia, which sometimes accompanies panic disorder, was marked as an indication).</p>\r\n\r\n<p>Probably the hardest problem to sort out would be the cases where a disease is mentioned in the \"Indications and Usage\" section of a label, but is not actually treated by the drug (e.g. drug x treats disease y, in the case that patient has disease z, administer drug x at a slower rate). This is less common, but happened a few times in the random sample of 100 indications I analyzed.</p>\r\n\r\n<p>Lastly, an update to the drug list would be appropriate. There are only 888 drugs listed for SIDER, while the total number of FDA approved drugs is significantly higher. This difference could explain some of the gaps for indications.</p>\r\n\r\n<p>Hope this helps give context to some of the issues within the indications.<br>Leo</p>",
      "body_md": "@dhimmel \r\nFrom what I observed it seems that there are a few types of mistakes which, when combined, lead to a significant drop in the precision of indications. The biggest problem I saw in the SIDER data, and perhaps the easiest to fix, was that there were several \"indications\" which were not actually diseases (e.g. \"progression\", \"adverse reactions\", \"interactions\"). It should be possible to filter out these artifacts of text mining via reference to a disease ontology.\r\n\r\nThe second repeated mistake I saw was that SIDER would on occasion mark a symptom of a real indication as an indication (e.g. agoraphobia, which sometimes accompanies panic disorder, was marked as an indication).\r\n\r\nProbably the hardest problem to sort out would be the cases where a disease is mentioned in the \"Indications and Usage\" section of a label, but is not actually treated by the drug (e.g. drug x treats disease y, in the case that patient has disease z, administer drug x at a slower rate). This is less common, but happened a few times in the random sample of 100 indications I analyzed.\r\n\r\nLastly, an update to the drug list would be appropriate. There are only 888 drugs listed for SIDER, while the total number of FDA approved drugs is significantly higher. This difference could explain some of the gaps for indications.\r\n\r\nHope this helps give context to some of the issues within the indications.\r\nLeo",
      "profile": 21,
      "published": "2015-02-17T19:20:28.273189Z",
      "thread": 30,
      "url": "/discussion/assessing-the-quality-and-applicability-of-the-sider-2-resource/30#2"
    },
    {
      "body_html": "<p>In our previous project to <a href=\"http://het.io/diease-genes\">predict disease-associated</a> genes from a heterogeneous network <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1004259\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1004259\">1</a>]</span>, we used the <a href=\"http://www.genenames.org\">HGNC (HUGO Gene Nomenclature Committee) database</a> to encode genes <span class=\"citation\">[<a href=\"/doi/10.1007/s00439-001-0615-0\" class=\"citation\" data-key=\"10.1007/s00439-001-0615-0\">2</a>, <a href=\"/doi/10.1093/nar/gku1071\" class=\"citation\" data-key=\"10.1093/nar/gku1071\">3</a>]</span>. This resource, \"based at the European Bioinformatics Institute (EMBL-EBI), assigns unique symbols and names to human genes <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gku1071\" class=\"citation\" data-key=\"10.1093/nar/gku1071\">3</a>]</span>.\"</p>\r\n\r\n<p>For this project, we are considering switching to NCBI's <a href=\"http://www.ncbi.nlm.nih.gov/gene\">Entrez Gene</a> and would like feedback. \"The primary goals of Entrez Gene are to provide tracked, unique identifiers for genes and to report information associated with those identifiers for unrestricted public use. The identifier that is assigned (GeneID) is an integer, and is species specific <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gki031\" class=\"citation\" data-key=\"10.1093/nar/gki031\">4</a>]</span>.\"</p>\r\n\r\n<p>The main advantages of Entrez versus HGNC for gene identification include:</p>\r\n\r\n<ul><li>more species than human</li><li>GeneIDs which are less error prone than ambiguous gene symbols</li><li>integration with many other NCBI services such as <a href=\"http://www.ncbi.nlm.nih.gov/homologene\">HomoloGene</a>, which can relate orthologous genes across species</li></ul>\r\n\r\n<p>The main disadvantage is familiarity, as most biologists conceive human genes in terms of their HGNC symbols. Although symbol information is available in Entrez Gene, there is no guarantee that each Entrez Gene record has a single corresponding, current HGNC symbol.</p>\r\n\r\n<p>I am interested in:</p>\r\n\r\n<ul><li>the best way to retrieve, store, parse, and map to Entrez Gene records</li><li>how stable Entrez Gene identifiers are for protein-coding genes in humans</li><li>the difficulty of updating to new versions of the Entrez Gene database</li></ul>\r\n\r\n<p>Any advice or information would be appreciated!</p>",
      "body_md": "In our previous project to [predict disease-associated](http://het.io/diease-genes) genes from a heterogeneous network [@10.1371/journal.pcbi.1004259], we used the [HGNC (HUGO Gene Nomenclature Committee) database](http://www.genenames.org) to encode genes [@10.1007/s00439-001-0615-0 @10.1093/nar/gku1071]. This resource, \"based at the European Bioinformatics Institute (EMBL-EBI), assigns unique symbols and names to human genes [@10.1093/nar/gku1071].\"\r\n\r\nFor this project, we are considering switching to NCBI's [Entrez Gene](http://www.ncbi.nlm.nih.gov/gene) and would like feedback. \"The primary goals of Entrez Gene are to provide tracked, unique identifiers for genes and to report information associated with those identifiers for unrestricted public use. The identifier that is assigned (GeneID) is an integer, and is species specific [@10.1093/nar/gki031].\"\r\n\r\nThe main advantages of Entrez versus HGNC for gene identification include:\r\n\r\n+ more species than human\r\n+ GeneIDs which are less error prone than ambiguous gene symbols\r\n+ integration with many other NCBI services such as [HomoloGene](http://www.ncbi.nlm.nih.gov/homologene), which can relate orthologous genes across species\r\n\r\nThe main disadvantage is familiarity, as most biologists conceive human genes in terms of their HGNC symbols. Although symbol information is available in Entrez Gene, there is no guarantee that each Entrez Gene record has a single corresponding, current HGNC symbol.\r\n\r\nI am interested in:\r\n\r\n+ the best way to retrieve, store, parse, and map to Entrez Gene records\r\n+ how stable Entrez Gene identifiers are for protein-coding genes in humans\r\n+ the difficulty of updating to new versions of the Entrez Gene database\r\n\r\nAny advice or information would be appreciated!",
      "profile": 17,
      "published": "2015-02-27T19:35:36.741691Z",
      "thread": 34,
      "url": "/discussion/using-entrez-gene-as-our-gene-vocabulary/34"
    },
    {
      "body_html": "<p>We use Entrez internally in our lab at the moment. We store them in a SQL database with their associated features. We've started using ElasticSearch to perform mapping from sources that use multiple alternative identifiers or that also include aliases, but in general we try to convert to Entrez.</p>\r\n\r\n<p>There are some problems with Entrez &lt;-&gt; HGNC, but they are relatively minor — especially for protein coding genes. IMP, GIANT, Tribe, and our other servers use entrez internally and map to symbol for display purposes.</p>\r\n\r\n<p>I have talked to people who are discussing more sophisticated systems to generate identifiers that unify the databases through an automated process capable of resolving ambiguities, and I am hopeful that some of those projects will come to fruition.</p>",
      "body_md": "We use Entrez internally in our lab at the moment. We store them in a SQL database with their associated features. We've started using ElasticSearch to perform mapping from sources that use multiple alternative identifiers or that also include aliases, but in general we try to convert to Entrez.\r\n\r\nThere are some problems with Entrez <-> HGNC, but they are relatively minor -- especially for protein coding genes. IMP, GIANT, Tribe, and our other servers use entrez internally and map to symbol for display purposes.\r\n\r\nI have talked to people who are discussing more sophisticated systems to generate identifiers that unify the databases through an automated process capable of resolving ambiguities, and I am hopeful that some of those projects will come to fruition.",
      "profile": 22,
      "published": "2015-02-28T12:50:57.554844Z",
      "thread": 34,
      "url": "/discussion/using-entrez-gene-as-our-gene-vocabulary/34#2"
    },
    {
      "body_html": "<p>We have previously retrieved our human <a href=\"//geneontology.org/\">Gene Ontology</a> (GO) <span class=\"citation\">[<a href=\"/doi/10.1038/75556\" class=\"citation\" data-key=\"10.1038/75556\">1</a>]</span> annotations from <a href=\"//www.broadinstitute.org/gsea/msigdb/collections.jsp#C5\">MSigDB</a>. MSigDB was designed for gene set enrichment analyses and therefore GO terms with fewer than 10 annotations are excluded. Additionally, MSigDB is infrequently updated and only contains human gene sets.</p>\r\n\r\n<p>Therefore, we created a <a href=\"//git.dhimmel.com/gene-ontology/\">utility to provide GO annotations</a> for a variety of species using the most recent annotation data. The resource relies on Entrez Gene as the <a href=\"//thinklab.com/discussion/using-entrez-gene-as-our-gene-vocabulary/34\">main gene vocabulary</a>. The utility's <a href=\"https://github.com/dhimmel/gene-ontology\">source code is online</a>, but briefly, annotations are retrieved from the Entrez <code>gene2go.gz</code> file and the python <a href=\"https://github.com/tanghaibao/goatools\">goatools package</a> is used to parse <code>go-basic.obo</code> and propagate annotations.</p>\r\n\r\n<p>Propagating annotations refers to transferring annotations from a more specific GO term to its broader parent terms. The theoretical justification is described <span class=\"citation\">[<a href=\"/doi/10.1038/nrg2363\" class=\"citation\" data-key=\"10.1038/nrg2363\">2</a>]</span>:</p>\r\n\r\n<blockquote><p>When a gene is annotated to a term, associations between the gene and the terms' parents are implicitly inferred. Because GO annotations to a term inherit all the properties of the ancestors of those terms, every path from any term back to its root(s) must be biologically accurate or the ontology must be revised.</p></blockquote>\r\n\r\n<p>We allow the user to choose propagated or unpropagated annotations, gene identifiers as Entrez IDs or symbols, and protein-coding or all genes. Since this resource is meant to be maximally useful, any suggestions or feature requests are welcome.</p>",
      "body_md": "We have previously retrieved our human [Gene Ontology](//geneontology.org/) (GO) [@10.1038/75556] annotations from [MSigDB](//www.broadinstitute.org/gsea/msigdb/collections.jsp#C5). MSigDB was designed for gene set enrichment analyses and therefore GO terms with fewer than 10 annotations are excluded. Additionally, MSigDB is infrequently updated and only contains human gene sets.\r\n\r\nTherefore, we created a [utility to provide GO annotations](//git.dhimmel.com/gene-ontology/) for a variety of species using the most recent annotation data. The resource relies on Entrez Gene as the [main gene vocabulary](//thinklab.com/discussion/using-entrez-gene-as-our-gene-vocabulary/34). The utility's [source code is online](https://github.com/dhimmel/gene-ontology), but briefly, annotations are retrieved from the Entrez `gene2go.gz` file and the python [goatools package](https://github.com/tanghaibao/goatools) is used to parse `go-basic.obo` and propagate annotations.\r\n\r\nPropagating annotations refers to transferring annotations from a more specific GO term to its broader parent terms. The theoretical justification is described [@10.1038/nrg2363]:\r\n\r\n> When a gene is annotated to a term, associations between the gene and the terms' parents are implicitly inferred. Because GO annotations to a term inherit all the properties of the ancestors of those terms, every path from any term back to its root(s) must be biologically accurate or the ontology must be revised.\r\n\r\nWe allow the user to choose propagated or unpropagated annotations, gene identifiers as Entrez IDs or symbols, and protein-coding or all genes. Since this resource is meant to be maximally useful, any suggestions or feature requests are welcome.",
      "profile": 17,
      "published": "2015-03-12T16:26:39.610571Z",
      "thread": 39,
      "url": "/discussion/compiling-gene-ontology-annotations-into-an-easy-to-use-format/39"
    },
    {
      "body_html": "<p>Currently, we would like to integrate several drug resources that rely on different compound vocabularies. These resources include</p>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th>type</th><th>resource</th><th>vocabulary</th></tr></thead><tbody><tr><td>indication</td><td><a href=\"http://knowledgemap.mc.vanderbilt.edu/research/content/MEDI\">MEDI</a> <span class=\"citation\">[<a href=\"/doi/10.1136/amiajnl-2012-001431\" class=\"citation\" data-key=\"10.1136/amiajnl-2012-001431\">1</a>]</span></td><td><a href=\"http://www.nlm.nih.gov/research/umls/rxnorm/\">RxNorm</a> <span class=\"citation\">[<a href=\"/doi/10.1136/amiajnl-2011-000116\" class=\"citation\" data-key=\"10.1136/amiajnl-2011-000116\">2</a>]</span></td></tr><tr><td>indication</td><td><a href=\"http://ftp.ncbi.nlm.nih.gov/pub/lu/LabeledIn/\">LabeledIn</a> <span class=\"citation\">[<a href=\"/doi/10.1016/j.jbi.2014.08.004\" class=\"citation\" data-key=\"10.1016/j.jbi.2014.08.004\">3</a>, <a href=\"/doi/10.1093/database/bav016\" class=\"citation\" data-key=\"10.1093/database/bav016\">4</a>]</span></td><td><a href=\"http://www.nlm.nih.gov/research/umls/rxnorm/\">RxNorm</a> <span class=\"citation\">[<a href=\"/doi/10.1136/amiajnl-2011-000116\" class=\"citation\" data-key=\"10.1136/amiajnl-2011-000116\">2</a>]</span></td></tr><tr><td>transcriptional signatures</td><td><a href=\"http://www.lincscloud.org/\">LINCS</a></td><td>LINCS &amp; PubChem</td></tr><tr><td>target binding</td><td><a href=\"https://www.ebi.ac.uk/chembl/\">ChEMBL</a> <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkt1031\" class=\"citation\" data-key=\"10.1093/nar/gkt1031\">5</a>]</span></td><td>ChEMBL</td></tr><tr><td>side effects</td><td><a href=\"http://sideeffects.embl.de/download/\">SIDER 2</a> <span class=\"citation\">[<a href=\"/doi/10.1038/msb.2009.98\" class=\"citation\" data-key=\"10.1038/msb.2009.98\">6</a>]</span></td><td><a href=\"http://stitch.embl.de/\">STITCH</a> <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkt1207\" class=\"citation\" data-key=\"10.1093/nar/gkt1207\">7</a>]</span></td></tr><tr><td>side effects</td><td><a href=\"https://www.pharmgkb.org/downloads/\">OFFSIDES</a> <span class=\"citation\">[<a href=\"/doi/10.1126/scitranslmed.3003377\" class=\"citation\" data-key=\"10.1126/scitranslmed.3003377\">8</a>]</span></td><td><a href=\"http://stitch.embl.de/\">STITCH</a> <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkt1207\" class=\"citation\" data-key=\"10.1093/nar/gkt1207\">7</a>]</span></td></tr></tbody></table>\r\n\r\n<p>We are planning on using <a href=\"http://www.drugbank.ca/\">DrugBank</a> <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkt1068\" class=\"citation\" data-key=\"10.1093/nar/gkt1068\">9</a>]</span> as the primary vocabulary for compounds. While the <a href=\"http://www.drugbank.ca/stats\">coverage of DrugBank</a> is limited, DrugBank includes FDA-approved compounds and likely covers the majority of compounds that would be well-connected in the network. The main benefits of DrugBank are extensive information per compound and a level of granularity that matches our needs.</p>\r\n\r\n<p>We plan to use <a href=\"https://www.ebi.ac.uk/unichem/ucquery/listSources\">UniChem</a> <span class=\"citation\">[<a href=\"/doi/10.1186/1758-2946-5-3\" class=\"citation\" data-key=\"10.1186/1758-2946-5-3\">10</a>]</span> to map resources with available structures. Importantly, we will likely benefit from a permissive matching algorithm that ignores small structural variations <span class=\"citation\">[<a href=\"/doi/10.1016/j.ddtec.2015.01.005\" class=\"citation\" data-key=\"10.1016/j.ddtec.2015.01.005\">11</a>]</span>. UniChem has a connectivity mapping feature to perform fuzzy matching <span class=\"citation\">[<a href=\"/doi/10.1186/s13321-014-0043-5\" class=\"citation\" data-key=\"10.1186/s13321-014-0043-5\">12</a>]</span>.</p>",
      "body_md": "Currently, we would like to integrate several drug resources that rely on different compound vocabularies. These resources include\r\n\r\n| type | resource | vocabulary |\r\n| - | - | - |\r\n| indication | [MEDI](http://knowledgemap.mc.vanderbilt.edu/research/content/MEDI) [@10.1136/amiajnl-2012-001431] | [RxNorm](http://www.nlm.nih.gov/research/umls/rxnorm/) [@10.1136/amiajnl-2011-000116] |\r\n| indication | [LabeledIn](http://ftp.ncbi.nlm.nih.gov/pub/lu/LabeledIn/) [@10.1016/j.jbi.2014.08.004 @10.1093/database/bav016] | [RxNorm](http://www.nlm.nih.gov/research/umls/rxnorm/) [@10.1136/amiajnl-2011-000116] |\r\n| transcriptional signatures | [LINCS](http://www.lincscloud.org/) | LINCS & PubChem |\r\n| target binding | [ChEMBL](https://www.ebi.ac.uk/chembl/) [@10.1093/nar/gkt1031] | ChEMBL |\r\n| side effects | [SIDER 2](http://sideeffects.embl.de/download/) [@10.1038/msb.2009.98] | [STITCH](http://stitch.embl.de/) [@10.1093/nar/gkt1207] |\r\n| side effects | [OFFSIDES](https://www.pharmgkb.org/downloads/) [@10.1126/scitranslmed.3003377] | [STITCH](http://stitch.embl.de/) [@10.1093/nar/gkt1207] |\r\n\r\nWe are planning on using [DrugBank](http://www.drugbank.ca/) [@10.1093/nar/gkt1068] as the primary vocabulary for compounds. While the [coverage of DrugBank](http://www.drugbank.ca/stats) is limited, DrugBank includes FDA-approved compounds and likely covers the majority of compounds that would be well-connected in the network. The main benefits of DrugBank are extensive information per compound and a level of granularity that matches our needs.\r\n\r\nWe plan to use [UniChem](https://www.ebi.ac.uk/unichem/ucquery/listSources) [@10.1186/1758-2946-5-3] to map resources with available structures. Importantly, we will likely benefit from a permissive matching algorithm that ignores small structural variations [@10.1016/j.ddtec.2015.01.005]. UniChem has a connectivity mapping feature to perform fuzzy matching [@10.1186/s13321-014-0043-5].",
      "profile": 17,
      "published": "2015-03-16T23:22:11.518392Z",
      "thread": 40,
      "url": "/discussion/unifying-drug-vocabularies/40"
    },
    {
      "body_html": "<p>This paper <span class=\"citation\">[<a href=\"/doi/10.1016/j.jbi.2012.06.005\" class=\"citation\" data-key=\"10.1016/j.jbi.2012.06.005\">1</a>]</span> discusses the problem of mapping between medication vocabularies. They identify several major difficulties:</p>\r\n\r\n<blockquote><ol><li>the availability of up-to-date information to assess the suitability of a given terminological system for a particular use case, and to assess the quality and completeness of cross-terminology links</li><li>the difficulty of correctly using complex, rapidly evolving, modern terminologies</li><li>the time and effort required to complete and evaluate the mapping</li><li>the need to address differences in granularity between the source and target terminologies</li><li>the need to continuously update the mapping as terminological systems evolve</li></ol></blockquote>\r\n\r\n<p>They provide a helpful diagram (manuscript Fig. 2) that illustrates the connections between terminologies:</p>\r\n\r\n<blockquote><p><img src=\"http://www.j-biomed-inform.com/cms/attachment/2022323725/2041973432/gr2_lrg.jpg\" alt=\"\"></p></blockquote>\r\n\r\n<p>Since most of our resources include structural information, we will likely face a slightly different and more computationally-amenable set of mapping challenges. Nonetheless, the \"differences in granularity between the source and target terminologies\" will be an important consideration.</p>",
      "body_md": "This paper [@10.1016/j.jbi.2012.06.005] discusses the problem of mapping between medication vocabularies. They identify several major difficulties:\r\n\r\n>\r\n1. the availability of up-to-date information to assess the suitability of a given terminological system for a particular use case, and to assess the quality and completeness of cross-terminology links\r\n2. the difficulty of correctly using complex, rapidly evolving, modern terminologies\r\n3. the time and effort required to complete and evaluate the mapping\r\n4. the need to address differences in granularity between the source and target terminologies\r\n5. the need to continuously update the mapping as terminological systems evolve\r\n\r\nThey provide a helpful diagram (manuscript Fig. 2) that illustrates the connections between terminologies:\r\n> ![](http://www.j-biomed-inform.com/cms/attachment/2022323725/2041973432/gr2_lrg.jpg)\r\n\r\nSince most of our resources include structural information, we will likely face a slightly different and more computationally-amenable set of mapping challenges. Nonetheless, the \"differences in granularity between the source and target terminologies\" will be an important consideration.",
      "profile": 17,
      "published": "2015-03-17T17:52:01.860572Z",
      "thread": 40,
      "url": "/discussion/unifying-drug-vocabularies/40#2"
    },
    {
      "body_html": "<h1>Integrating RxNorm ingredients</h1>\r\n\r\n<p>The <a href=\"http://www.nlm.nih.gov/research/umls/rxnorm/\">RxNorm terminology</a> <span class=\"citation\">[<a href=\"/doi/10.1136/amiajnl-2011-000116\" class=\"citation\" data-key=\"10.1136/amiajnl-2011-000116\">1</a>]</span> does not contain chemical structures for ingredients. However, the terminology does cross-reference the NDF-RT and FDA-SRS. The FDA-SRS identifiers, called UNIIs (Unique Ingredient Identifiers), are <a href=\"https://www.ebi.ac.uk/unichem/ucquery/listSources\">included in UniChem</a>. Therefore to map RxNorm ingredients to other vocabularies, we will first convert RXCUIs to UNIIs.</p>\r\n\r\n<p>We downloaded the RxNorm data release and <a href=\"http://www.nlm.nih.gov/research/umls/rxnorm/docs/2015/rxnorm_doco_full_2015-1.html\">loaded it into a MySQL database</a>. We used the following query to produce a RXCUI–UNII mapping:</p>\r\n\r\n<p></p><pre><code class=\"sql\">SELECT DISTINCT RXCUI, CODE\r\nFROM rxnorm.RXNCONSO\r\nWHERE SAB = 'MTHSPL' AND TTY = 'SU' AND CODE != 'NOCODE';</code></pre>",
      "body_md": "# Integrating RxNorm ingredients\r\n\r\nThe [RxNorm terminology](http://www.nlm.nih.gov/research/umls/rxnorm/) [@10.1136/amiajnl-2011-000116] does not contain chemical structures for ingredients. However, the terminology does cross-reference the NDF-RT and FDA-SRS. The FDA-SRS identifiers, called UNIIs (Unique Ingredient Identifiers), are [included in UniChem](https://www.ebi.ac.uk/unichem/ucquery/listSources). Therefore to map RxNorm ingredients to other vocabularies, we will first convert RXCUIs to UNIIs.\r\n\r\nWe downloaded the RxNorm data release and [loaded it into a MySQL database](http://www.nlm.nih.gov/research/umls/rxnorm/docs/2015/rxnorm_doco_full_2015-1.html). We used the following query to produce a RXCUI--UNII mapping:\r\n\r\n```sql\r\nSELECT DISTINCT RXCUI, CODE\r\nFROM rxnorm.RXNCONSO\r\nWHERE SAB = 'MTHSPL' AND TTY = 'SU' AND CODE != 'NOCODE';\r\n```",
      "profile": 17,
      "published": "2015-03-17T19:40:52.373207Z",
      "thread": 40,
      "url": "/discussion/unifying-drug-vocabularies/40#3"
    },
    {
      "body_html": "<p>Would you consider hosting your knowledge network on WikiData ? <a href=\"https://www.wikidata.org/\" target=\"_blank\">https://www.wikidata.org/</a>.  WikiData is a new freebase-like open knowledge base being constructed by the Wikimedia Foundation.</p>\r\n\r\n<p>A couple reasons to think about this:<br>1) Our group has NIH funding to build many of the nodes and edges you need there and we have already started.<br>2) By working in WikiData, your project can benefit from an existing, large user/contributor community and from the WMF computational resources.<br>3) WikiData is fundamentally about open knowledge exchange.  Working in its context will ensure the greatest visibility and re-use for your network.  </p>\r\n\r\n<p>You probably wouldn't want to store computed probabilities there, but qualitative relationships would work well I think.  This would be a great use case for our own efforts..</p>",
      "body_md": "Would you consider hosting your knowledge network on WikiData ? https://www.wikidata.org/.  WikiData is a new freebase-like open knowledge base being constructed by the Wikimedia Foundation.\r\n\r\nA couple reasons to think about this:\r\n1) Our group has NIH funding to build many of the nodes and edges you need there and we have already started.\r\n2) By working in WikiData, your project can benefit from an existing, large user/contributor community and from the WMF computational resources.\r\n3) WikiData is fundamentally about open knowledge exchange.  Working in its context will ensure the greatest visibility and re-use for your network.  \r\n\r\nYou probably wouldn't want to store computed probabilities there, but qualitative relationships would work well I think.  This would be a great use case for our own efforts..\r\n",
      "profile": 48,
      "published": "2015-03-18T22:23:07.532207Z",
      "thread": 23,
      "url": "/discussion/enabling-reproducibility-and-reuse/23#7"
    },
    {
      "body_html": "<p>Daniel,<br>Regarding what to upload.  The figure you presented in your research plan sums this up nicely <a href=\"https://dl.dropboxusercontent.com/s/yp0gjh1v3329xji/metagraph.png\" target=\"_blank\">https://dl.dropboxusercontent.com/s/yp0gjh1v3329xji/metagraph.png</a>Aside from perhaps the MSigDB collection (which could be added), those are exactly the nodes and edges that we hope to see in WikiData.  </p>\r\n\r\n<p>To use WikiData to its full power, you would actually use it within your own application in the same way that you would use your own internal relational database.  Rather than thinking of it only as a repository to export to, you could think of it as the central staging area for the data that you (and the rest of the community) want to compute with.  </p>\r\n\r\n<p>For more about how we are working with wikidata, you can check out the series of blog posts that describes the funded grant here starting here:  <a href=\"http://sulab.org/2013/07/the-future-of-the-gene-wiki/\" target=\"_blank\">http://sulab.org/2013/07/the-future-of-the-gene-wiki/</a>  </p>",
      "body_md": "Daniel,  \r\nRegarding what to upload.  The figure you presented in your research plan sums this up nicely https://dl.dropboxusercontent.com/s/yp0gjh1v3329xji/metagraph.png\r\nAside from perhaps the MSigDB collection (which could be added), those are exactly the nodes and edges that we hope to see in WikiData.  \r\n\r\nTo use WikiData to its full power, you would actually use it within your own application in the same way that you would use your own internal relational database.  Rather than thinking of it only as a repository to export to, you could think of it as the central staging area for the data that you (and the rest of the community) want to compute with.  \r\n\r\nFor more about how we are working with wikidata, you can check out the series of blog posts that describes the funded grant here starting here:  http://sulab.org/2013/07/the-future-of-the-gene-wiki/  ",
      "profile": 48,
      "published": "2015-03-19T17:54:50.896960Z",
      "thread": 23,
      "url": "/discussion/enabling-reproducibility-and-reuse/23#8"
    },
    {
      "body_html": "<p>Instead of using <a href=\"http://www.brenda-enzymes.info/ontology.php?ontology_id=3\">Brenda Tissue Ontology</a>, I would suggest using <a href=\"http://uberon.org\">Uberon</a> for anatomy which incorporates <a href=\"http://cellontology.org/\">CL</a>.These ontologies provide greater breadth of anatomy and cell types for various species. In addition, I think that with integration with Disease Ontology into <a href=\"http://www.ebi.ac.uk/efo\">EFO</a> you can add more data sources to this network as well as links to further experiments. </p>\r\n\r\n<p>ENCODE makes use of Uberon, you can read more  <span class=\"citation\">[<a href=\"/doi/10.1093/database/bav010\" class=\"citation\" data-key=\"10.1093/database/bav010\">1</a>]</span></p>",
      "body_md": "Instead of using [Brenda Tissue Ontology](http://www.brenda-enzymes.info/ontology.php?ontology_id=3), I would suggest using [Uberon](http://uberon.org) for anatomy which incorporates [CL](http://cellontology.org/).These ontologies provide greater breadth of anatomy and cell types for various species. In addition, I think that with integration with Disease Ontology into [EFO](http://www.ebi.ac.uk/efo) you can add more data sources to this network as well as links to further experiments. \r\n\r\nENCODE makes use of Uberon, you can read more  [@10.1093/database/bav010]",
      "profile": 35,
      "published": "2015-03-19T19:15:53.264452Z",
      "thread": 41,
      "url": "/discussion/tissue-node/41"
    },
    {
      "body_html": "<p>Started discussion on changing ontology used for Tissue node. <a href=\"http://thinklab.com/discussion/tissue-node/41\">Discussion is here</a></p>",
      "body_md": "Started discussion on changing ontology used for Tissue node. [Discussion is here](http://thinklab.com/discussion/tissue-node/41)",
      "profile": 35,
      "published": "2015-03-19T19:17:04.573384Z",
      "thread": 22,
      "url": "/discussion/suggestions-for-additional-information-types/22#2"
    },
    {
      "body_html": "<p>Hi <a href=\"/u/vsmalladi\" class=\"username\">@vsmalladi</a>, thanks for the <a href=\"https://uberon.github.io/\">Uberon</a> <span class=\"citation\">[<a href=\"/doi/10.1186/gb-2012-13-1-r5\" class=\"citation\" data-key=\"10.1186/gb-2012-13-1-r5\">1</a>]</span> suggestion.</p>\r\n\r\n<p>The project has good documentation, a nice user interface, and is <a href=\"https://github.com/obophenotype/uberon/commits/master\">actively maintained</a> — three important features when choosing an ontology (and areas where the <a href=\"http://www.brenda-enzymes.info/ontology.php?ontology_id=3\">BRENDA Tissue Ontology</a> (BTO) <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkq968\" class=\"citation\" data-key=\"10.1093/nar/gkq968\">2</a>]</span> sometimes lags behind.</p>\r\n\r\n<p>In our previous network <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1004259\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1004259\">3</a>]</span>, we had under 100 tissues and only used BTO as a common vocabulary. Since the current project is in an early stage, it is difficult to know whether we will take full advantage of the rich structure and cross-referencing provided by next-generation ontologies. However, I agree it makes sense to build an extensible and forward-thinking network and Uberon will assist in these pursuits. We also prefer ontologies that will have the widest adoption and are free of restrictive licensing. Do you know Uberon's license?</p>\r\n\r\n<p>I noticed Uberon includes <a href=\"https://github.com/obophenotype/uberon/wiki/inter-anatomy-ontology-bridge-ontologies\">mappings to other ontologies</a>, called <em>bridges</em>. A BTO bridge doesn't currently exist, but perhaps we could contribute one for the 77 BTO terms (<a href=\"http://het.io/disease-genes/downloads/files/expression.txt.gz\">download link</a>) used in our disease network.</p>",
      "body_md": "Hi @vsmalladi, thanks for the [Uberon](https://uberon.github.io/) [@10.1186/gb-2012-13-1-r5] suggestion.\r\n\r\nThe project has good documentation, a nice user interface, and is [actively maintained](https://github.com/obophenotype/uberon/commits/master) -- three important features when choosing an ontology (and areas where the [BRENDA Tissue Ontology](http://www.brenda-enzymes.info/ontology.php?ontology_id=3) (BTO) [@10.1093/nar/gkq968] sometimes lags behind.\r\n\r\nIn our previous network [@10.1371/journal.pcbi.1004259], we had under 100 tissues and only used BTO as a common vocabulary. Since the current project is in an early stage, it is difficult to know whether we will take full advantage of the rich structure and cross-referencing provided by next-generation ontologies. However, I agree it makes sense to build an extensible and forward-thinking network and Uberon will assist in these pursuits. We also prefer ontologies that will have the widest adoption and are free of restrictive licensing. Do you know Uberon's license?\r\n\r\nI noticed Uberon includes [mappings to other ontologies](https://github.com/obophenotype/uberon/wiki/inter-anatomy-ontology-bridge-ontologies), called *bridges*. A BTO bridge doesn't currently exist, but perhaps we could contribute one for the 77 BTO terms ([download link](http://het.io/disease-genes/downloads/files/expression.txt.gz)) used in our disease network.",
      "profile": 17,
      "published": "2015-03-20T02:35:04.095121Z",
      "thread": 41,
      "url": "/discussion/tissue-node/41#2"
    },
    {
      "body_html": "<p>Hi <a href=\"/u/b_good\" class=\"username\">@b_good</a>, fascinating work. I was amazed that the <a href=\"https://en.wikipedia.org/wiki/Gene_Wiki\">Gene Wiki</a> <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pbio.0060175\" class=\"citation\" data-key=\"10.1371/journal.pbio.0060175\">1</a>]</span> <a href=\"http://sulab.org/2013/07/gene-wiki-progress-report/\">averages</a> \"two page views every single second\". This amazing traffic, and return on investement from the perspective of funders, illustrates the potential of open, crowdsourced, and collaborative undertakings.</p>\r\n\r\n<p>I think your wiki work capitalizes on a larger trend of computing moving to the web and browser. I recently made my <a href=\"http://slides.com/dhimmel/elevcan\">first html presentation</a>; my coding is now largely browser based thanks to <a href=\"http://jupyter.org/\">Jupyter</a> and <a href=\"http://www.rstudio.com/\">RStudio</a> and <a href=\"https://github.com/dhimmel\">hosted</a> online; API web-queries are now fundamental for information retrieval. It only makes sense that we adopt a information commons like Wikidata to integrate knowledge. The video below really sold me on the concept:</p>\r\n\r\n<p><iframe src=\"https://www.youtube.com/embed/Rww2dA-1Cqc\" width=\"640\" height=\"360\" frameborder=\"0\" allowfullscreen=\"true\"></iframe></p>\r\n\r\n<p>Since the initial stage of this project is highly prototypical, I am hesitant to immediately switch to Wikidata as our backend. However, I would love to work with you and your team to upload as much content as possible. Then for subsequent analyses, we could potentially pull from Wikidata.</p>",
      "body_md": "Hi @b_good, fascinating work. I was amazed that the [Gene Wiki](https://en.wikipedia.org/wiki/Gene_Wiki) [@10.1371/journal.pbio.0060175] [averages](http://sulab.org/2013/07/gene-wiki-progress-report/) \"two page views every single second\". This amazing traffic, and return on investement from the perspective of funders, illustrates the potential of open, crowdsourced, and collaborative undertakings.\r\n\r\nI think your wiki work capitalizes on a larger trend of computing moving to the web and browser. I recently made my [first html presentation](http://slides.com/dhimmel/elevcan); my coding is now largely browser based thanks to [Jupyter](http://jupyter.org/) and [RStudio](http://www.rstudio.com/) and [hosted](https://github.com/dhimmel) online; API web-queries are now fundamental for information retrieval. It only makes sense that we adopt a information commons like Wikidata to integrate knowledge. The video below really sold me on the concept:\r\n\r\n![:youtube](Rww2dA-1Cqc)\r\n\r\nSince the initial stage of this project is highly prototypical, I am hesitant to immediately switch to Wikidata as our backend. However, I would love to work with you and your team to upload as much content as possible. Then for subsequent analyses, we could potentially pull from Wikidata.",
      "profile": 17,
      "published": "2015-03-20T04:06:29.499007Z",
      "thread": 23,
      "url": "/discussion/enabling-reproducibility-and-reuse/23#9"
    },
    {
      "body_html": "<p>Hi <a href=\"/u/dantericci\" class=\"username\">@dantericci</a>, I believe there is no restrictive licensing and is open to use. </p>\r\n\r\n<p>As for adpotion, they have many big projects and consotrium adopting use of Uberon. Such as EBI, <a href=\"http://uberon.github.io/about/adopters.html\">other adoptors</a>.</p>\r\n\r\n<p>Uberon does have cross-references to BTO, so I don't think we need to make a bridge. </p>",
      "body_md": "Hi @dantericci, I believe there is no restrictive licensing and is open to use. \r\n\r\nAs for adpotion, they have many big projects and consotrium adopting use of Uberon. Such as EBI, [other adoptors](http://uberon.github.io/about/adopters.html).\r\n\r\nUberon does have cross-references to BTO, so I don't think we need to make a bridge. ",
      "profile": 35,
      "published": "2015-03-20T06:13:53.412703Z",
      "thread": 41,
      "url": "/discussion/tissue-node/41#3"
    },
    {
      "body_html": "<p>I <a href=\"http://thinklab.com/p/rephetio/plan/compare/cf0af88368d84640e6face839932af64a0963f5f/438864fde53c372dfa654dd59674f4bfc6cf249c\">updated the proposal</a> to replace BTO with Uberon.</p>\r\n\r\n<p>As for the BTO mapping, I didn't see <a href=\"https://github.com/obophenotype/uberon/tree/master/bridge\">any bridges on the GitHub</a> and the <a href=\"http://purl.obolibrary.org/obo/uberon/bridge/uberon-bridge-to-bto.owl\">BTO bridge download</a> produces an error. Am I looking in the wrong place or are the BTO mappings not populated yet?</p>",
      "body_md": "I [updated the proposal](http://thinklab.com/p/rephetio/plan/compare/cf0af88368d84640e6face839932af64a0963f5f/438864fde53c372dfa654dd59674f4bfc6cf249c) to replace BTO with Uberon.\r\n\r\nAs for the BTO mapping, I didn't see [any bridges on the GitHub](https://github.com/obophenotype/uberon/tree/master/bridge) and the [BTO bridge download](http://purl.obolibrary.org/obo/uberon/bridge/uberon-bridge-to-bto.owl) produces an error. Am I looking in the wrong place or are the BTO mappings not populated yet?",
      "profile": 17,
      "published": "2015-03-20T06:41:44.409926Z",
      "thread": 41,
      "url": "/discussion/tissue-node/41#4"
    },
    {
      "body_html": "<p><a href=\"/u/dhimmel\" class=\"username\">@dhimmel</a> Within the <a href=\"http://berkeleybop.org/ontologies/uberon.owl\">Uberon file</a> there are DbXref's. An example is for </p>\r\n\r\n<pre><code>&lt;owl:Class rdf:about=\"http://purl.obolibrary.org/obo/UBERON_0000002\"&gt;\r\n    &lt;rdfs:label rdf:datatype=\"http://www.w3.org/2001/XMLSchema#string\"&gt;uterine cervix&lt;/rdfs:label&gt;\r\n    &lt;oboInOwl:hasDbXref rdf:datatype=\"http://www.w3.org/2001/XMLSchema#string\"&gt;BTO:0001421&lt;/oboInOwl:hasDbXref&gt;\r\n    &lt;oboInOwl:hasDbXref rdf:datatype=\"http://www.w3.org/2001/XMLSchema#string\"&gt;BTO:0002249&lt;/oboInOwl:hasDbXref&gt;</code></pre>",
      "body_md": "@dhimmel Within the [Uberon file](http://berkeleybop.org/ontologies/uberon.owl) there are DbXref's. An example is for \r\n\r\n    <owl:Class rdf:about=\"http://purl.obolibrary.org/obo/UBERON_0000002\">\r\n        <rdfs:label rdf:datatype=\"http://www.w3.org/2001/XMLSchema#string\">uterine cervix</rdfs:label>\r\n        <oboInOwl:hasDbXref rdf:datatype=\"http://www.w3.org/2001/XMLSchema#string\">BTO:0001421</oboInOwl:hasDbXref>\r\n        <oboInOwl:hasDbXref rdf:datatype=\"http://www.w3.org/2001/XMLSchema#string\">BTO:0002249</oboInOwl:hasDbXref>",
      "profile": 35,
      "published": "2015-03-20T15:44:19.979836Z",
      "thread": 41,
      "url": "/discussion/tissue-node/41#5"
    },
    {
      "body_html": "<h1>The UniChem Connectivity Search</h1>\r\n\r\n<p>UniChem is a structure-centric search engine (based on InChI identifiers <span class=\"citation\">[<a href=\"/doi/10.1186/1758-2946-5-7\" class=\"citation\" data-key=\"10.1186/1758-2946-5-7\">1</a>]</span>) for compound unification <span class=\"citation\">[<a href=\"/doi/10.1186/1758-2946-5-3\" class=\"citation\" data-key=\"10.1186/1758-2946-5-3\">2</a>]</span>. UniChem includes a widesearch mode that matches compounds based on common connectivity <span class=\"citation\">[<a href=\"/doi/10.1186/s13321-014-0043-5\" class=\"citation\" data-key=\"10.1186/s13321-014-0043-5\">3</a>]</span>. We speculate that fuzzy matching will outperform a strict structural identity matching <span class=\"citation\">[<a href=\"/doi/10.1016/j.ddtec.2015.01.005\" class=\"citation\" data-key=\"10.1016/j.ddtec.2015.01.005\">4</a>]</span> because:</p>\r\n\r\n<ul><li>we will retain more information by integrating greater percentages of external databases</li><li>small chemical variations may have a minimal pharmacodynamic impact</li><li>many resources and pharmacologists conceptualize compounds with less granularity than exact chemical structure</li></ul>\r\n\r\n<h1>Mapping external resrouces to DrugBank</h1>\r\n\r\n<p>We would like to standardize all compound resources using DrugBank <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkt1068\" class=\"citation\" data-key=\"10.1093/nar/gkt1068\">5</a>]</span>. To accomplish this task, we first <a href=\"//nbviewer.ipython.org/url/git.dhimmel.com/drugbank/parse.ipynb\">parsed the DrugBank xml download</a> to extract basic compound information. Second, we <a href=\"//nbviewer.ipython.org/url/git.dhimmel.com/drugbank/unichem-map.ipynb\">mapped DrugBank compounds to each resource in UniChem</a> using the connectivity search [<a href=\"https://www.ebi.ac.uk/unichem/info/widesearchInfo\">docs</a>]. Third, we <a href=\"//git.dhimmel.com/drugbank/unichem-map.html\">assessed the mappings using a variety of metrics</a>. For this third step, we concentrated only on approved small molecules as these will be the most essential and connected in our network.</p>\r\n\r\n<p>The following findings were aparent:</p>\r\n\r\n<ul><li>DrugBank contained 1,600 approved small molecules, 51 of which were lacking structural information and could not be mapped.</li><li>108 DrugBank compounds mapped to multiple DrugBank compounds indicating the granularity of our connectivity search is not equivalent to the granularity of the DrugBank inclusion criteria.</li><li>93% of DrugBank compounds had atleast one match in ChEMBL, 77% matched FDA SRS (UNII), 95% matched PubChem, 57% matched LINCS</li><li>Zolmitriptan (DB00315) matched 768 PubChem compounds</li></ul>\r\n\r\n<p>Given these findings, we have the following <strong>questions for a chemist or cheminformaticist</strong>:</p>\r\n\r\n<ol><li>Given our focus on pharmacodynamics rather than pharmacokinetics and our desire to avoid duplicate entities in the network, did we <a href=\"//nbviewer.ipython.org/url/git.dhimmel.com/drugbank/unichem-map.ipynb\">properly construct our UniChem query</a>? </li><li>Given that some DrugBank compounds matched multiple DrugBank compounds (<a href=\"//git.dhimmel.com/drugbank/unichem-map.html\">see histograms</a>), should we instead use the First InChIKey Hash Block (FIKHB) as the primary compound identifier?</li><li>~20% of approved small molecules in DrugBank did not match a FDA-SRS (UNII) compound, which is troubling. Many of these unmatched compounds would have matched by name matching. We would like an explanation for this discrepency and will look into the issue further ourselves.</li><li>Should we use a tiered matching system, where we take only exact matches when available and then expand to connecitivity matches if necessary?</li><li>Should we adopt an even more permissive (or alternative) mapping strategy for LINCS to annotate more compounds with transcriptomic profiles?</li><li>Is the excessive number of PubChem matches for some DrugBank compounds indicative of a larger problem? The full mapping can be <a href=\"//git.dhimmel.com/drugbank/data/mapping.tsv.gz\">downloaded here</a>.</li></ol>",
      "body_md": "# The UniChem Connectivity Search\r\n\r\nUniChem is a structure-centric search engine (based on InChI identifiers [@10.1186/1758-2946-5-7]) for compound unification [@10.1186/1758-2946-5-3]. UniChem includes a widesearch mode that matches compounds based on common connectivity [@10.1186/s13321-014-0043-5]. We speculate that fuzzy matching will outperform a strict structural identity matching [@10.1016/j.ddtec.2015.01.005] because:\r\n\r\n+ we will retain more information by integrating greater percentages of external databases\r\n+ small chemical variations may have a minimal pharmacodynamic impact\r\n+ many resources and pharmacologists conceptualize compounds with less granularity than exact chemical structure\r\n\r\n# Mapping external resrouces to DrugBank\r\n\r\nWe would like to standardize all compound resources using DrugBank [@10.1093/nar/gkt1068]. To accomplish this task, we first [parsed the DrugBank xml download](//nbviewer.ipython.org/url/git.dhimmel.com/drugbank/parse.ipynb) to extract basic compound information. Second, we [mapped DrugBank compounds to each resource in UniChem](//nbviewer.ipython.org/url/git.dhimmel.com/drugbank/unichem-map.ipynb) using the connectivity search [[docs](https://www.ebi.ac.uk/unichem/info/widesearchInfo)]. Third, we [assessed the mappings using a variety of metrics](//git.dhimmel.com/drugbank/unichem-map.html). For this third step, we concentrated only on approved small molecules as these will be the most essential and connected in our network.\r\n\r\nThe following findings were aparent:\r\n\r\n+ DrugBank contained 1,600 approved small molecules, 51 of which were lacking structural information and could not be mapped.\r\n+ 108 DrugBank compounds mapped to multiple DrugBank compounds indicating the granularity of our connectivity search is not equivalent to the granularity of the DrugBank inclusion criteria.\r\n+ 93% of DrugBank compounds had atleast one match in ChEMBL, 77% matched FDA SRS (UNII), 95% matched PubChem, 57% matched LINCS\r\n+ Zolmitriptan (DB00315) matched 768 PubChem compounds\r\n\r\nGiven these findings, we have the following **questions for a chemist or cheminformaticist**:\r\n\r\n1. Given our focus on pharmacodynamics rather than pharmacokinetics and our desire to avoid duplicate entities in the network, did we [properly construct our UniChem query](//nbviewer.ipython.org/url/git.dhimmel.com/drugbank/unichem-map.ipynb)? \r\n2. Given that some DrugBank compounds matched multiple DrugBank compounds ([see histograms](//git.dhimmel.com/drugbank/unichem-map.html)), should we instead use the First InChIKey Hash Block (FIKHB) as the primary compound identifier?\r\n3. ~20% of approved small molecules in DrugBank did not match a FDA-SRS (UNII) compound, which is troubling. Many of these unmatched compounds would have matched by name matching. We would like an explanation for this discrepency and will look into the issue further ourselves.\r\n4. Should we use a tiered matching system, where we take only exact matches when available and then expand to connecitivity matches if necessary?\r\n5. Should we adopt an even more permissive (or alternative) mapping strategy for LINCS to annotate more compounds with transcriptomic profiles?\r\n6. Is the excessive number of PubChem matches for some DrugBank compounds indicative of a larger problem? The full mapping can be [downloaded here](//git.dhimmel.com/drugbank/data/mapping.tsv.gz).",
      "profile": 17,
      "published": "2015-03-20T21:37:04.117495Z",
      "thread": 40,
      "url": "/discussion/unifying-drug-vocabularies/40#4"
    },
    {
      "body_html": "<p>In order to make the disease data within the <em>Incomplete Interactome</em> easier to manipulate we have mapped the MeSH disease names used within the paper to their corresponding MeSH IDs. The outcome (<a href=\"https://raw.githubusercontent.com/LABrueggs/incomplete-interactome/master/disease_output.tsv\">here</a>) should make integrating data from this study simpler. The <a href=\"http://nbviewer.ipython.org/github/LABrueggs/incomplete-interactome/blob/master/SciencetoMESHterm.ipynb\">python program</a> and its associated input and output files can be found <a href=\"https://github.com/LABrueggs/incomplete-interactome\">here</a>.</p>",
      "body_md": "In order to make the disease data within the *Incomplete Interactome* easier to manipulate we have mapped the MeSH disease names used within the paper to their corresponding MeSH IDs. The outcome ([here](https://raw.githubusercontent.com/LABrueggs/incomplete-interactome/master/disease_output.tsv)) should make integrating data from this study simpler. The [python program](http://nbviewer.ipython.org/github/LABrueggs/incomplete-interactome/blob/master/SciencetoMESHterm.ipynb) and its associated input and output files can be found [here](https://github.com/LABrueggs/incomplete-interactome).",
      "profile": 21,
      "published": "2015-03-25T21:20:49.529810Z",
      "thread": 42,
      "url": "/discussion/mapping-incomplete-interactome-disease-names-to-mesh/42"
    },
    {
      "body_html": "<p><a href=\"//www.lincscloud.org/\">LINCS</a> (Library of Integrated Cellular Signatures) provided \"perturbational profiles across multiple cell and perturbation types, as well as read-outs, at a massive scale.\" We plan to compute transcriptional profiles, i.e. expression signatures — sets of up and down-regulated genes — for the compounds in our network. We will be following a workflow suggested to us by Ted Natoli during the <a href=\"//www.lincscloud.org/training/\">online office hours</a>.</p>\r\n\r\n<p>For a given compound in our network, there may be multiple matched LINCS compounds. Additionally, each LINCS compound may have been assayed across multiple cell lines, dosages, and replicates. To calculate a single consensus transcriptional profile across multiple signatures we will</p>\r\n\r\n<ol><li>calculate pairwise correlations between signatures</li><li>calculate mean correlation with other signatures for each signature</li><li>scale similarities to sum to 1</li><li>multiply z-score signature vectors by their similarity weights</li><li>sum weighted z-score signature vectors</li></ol>\r\n\r\n<p>The z-score signature vectors are retrieved from the <code>/xchip/cogs/data/build/a2y13q1/modzs.gctx</code> file on the <a href=\"http://c3.lincscloud.org/\">C3 cloud</a>.</p>",
      "body_md": "[LINCS](//www.lincscloud.org/) (Library of Integrated Cellular Signatures) provided \"perturbational profiles across multiple cell and perturbation types, as well as read-outs, at a massive scale.\" We plan to compute transcriptional profiles, i.e. expression signatures -- sets of up and down-regulated genes -- for the compounds in our network. We will be following a workflow suggested to us by Ted Natoli during the [online office hours](//www.lincscloud.org/training/).\r\n\r\nFor a given compound in our network, there may be multiple matched LINCS compounds. Additionally, each LINCS compound may have been assayed across multiple cell lines, dosages, and replicates. To calculate a single consensus transcriptional profile across multiple signatures we will\r\n\r\n1. calculate pairwise correlations between signatures\r\n+ calculate mean correlation with other signatures for each signature\r\n+ scale similarities to sum to 1\r\n+ multiply z-score signature vectors by their similarity weights\r\n+ sum weighted z-score signature vectors\r\n\r\nThe z-score signature vectors are retrieved from the `/xchip/cogs/data/build/a2y13q1/modzs.gctx` file on the [C3 cloud](http://c3.lincscloud.org/).",
      "profile": 17,
      "published": "2015-03-27T02:43:44.027312Z",
      "thread": 43,
      "url": "/discussion/computing-consensus-transcriptional-profiles-for-lincs-l1000-perturbations/43"
    },
    {
      "body_html": "<p>This discussion will explore how to unify the variety of disease vocabularies used by our resources. These resources are listed below:</p>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th>type</th><th>resource</th><th>vocabulary</th></tr></thead><tbody><tr><td>indications</td><td>MEDI <span class=\"citation\">[<a href=\"/doi/10.1136/amiajnl-2012-001431\" class=\"citation\" data-key=\"10.1136/amiajnl-2012-001431\">1</a>]</span></td><td>ICD-9</td></tr><tr><td>indications</td><td>LabeledIn <span class=\"citation\">[<a href=\"/doi/10.1016/j.jbi.2014.08.004\" class=\"citation\" data-key=\"10.1016/j.jbi.2014.08.004\">2</a>, <a href=\"/doi/10.1093/database/bav016\" class=\"citation\" data-key=\"10.1093/database/bav016\">3</a>]</span></td><td>UMLS <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkh061\" class=\"citation\" data-key=\"10.1093/nar/gkh061\">4</a>]</span></td></tr><tr><td>transcriptional signatures</td><td><a href=\"http://dev.stargeo.io/\">STAR-GEO</a></td><td>custom</td></tr><tr><td>symptoms</td><td>HSDN <span class=\"citation\">[<a href=\"/doi/10.1038/ncomms5212\" class=\"citation\" data-key=\"10.1038/ncomms5212\">5</a>]</span></td><td><a href=\"//www.nlm.nih.gov/mesh/filelist.html\">MeSH</a> <span class=\"citation\">[<a href=\"/doi/10.1001/jama.271.14.1103\" class=\"citation\" data-key=\"10.1001/jama.271.14.1103\">6</a>]</span> (2011 release)</td></tr><tr><td>gene associations</td><td><a href=\"//het.io/disease-genes/downloads/\">het.io</a> <span class=\"citation\">[<a href=\"/doi/10.1101/011569\" class=\"citation\" data-key=\"10.1101/011569\">7</a>]</span></td><td><a href=\"//disease-ontology.org/\">Disease Ontology</a> <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkr972\" class=\"citation\" data-key=\"10.1093/nar/gkr972\">8</a>, <a href=\"/doi/10.1093/nar/gku1011\" class=\"citation\" data-key=\"10.1093/nar/gku1011\">9</a>]</span></td></tr><tr><td>pathophysiology</td><td><a href=\"//het.io/disease-genes/downloads/\">het.io</a> <span class=\"citation\">[<a href=\"/doi/10.1101/011569\" class=\"citation\" data-key=\"10.1101/011569\">7</a>]</span></td><td><a href=\"//disease-ontology.org/\">Disease Ontology</a> <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkr972\" class=\"citation\" data-key=\"10.1093/nar/gkr972\">8</a>, <a href=\"/doi/10.1093/nar/gku1011\" class=\"citation\" data-key=\"10.1093/nar/gku1011\">9</a>]</span></td></tr><tr><td>tissue localization</td><td><a href=\"//het.io/disease-genes/downloads/\">het.io</a> <span class=\"citation\">[<a href=\"/doi/10.1101/011569\" class=\"citation\" data-key=\"10.1101/011569\">7</a>]</span></td><td><a href=\"//disease-ontology.org/\">Disease Ontology</a> <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkr972\" class=\"citation\" data-key=\"10.1093/nar/gkr972\">8</a>, <a href=\"/doi/10.1093/nar/gku1011\" class=\"citation\" data-key=\"10.1093/nar/gku1011\">9</a>]</span></td></tr></tbody></table>\r\n\r\n<p>Our current plan is to use the Disease Ontology (DO) as our primary vocabulary. Therefore, we will have to map resources to DO terms.</p>",
      "body_md": "This discussion will explore how to unify the variety of disease vocabularies used by our resources. These resources are listed below:\r\n\r\n| type | resource | vocabulary |\r\n| - | - | - |\r\n| indications | MEDI [@10.1136/amiajnl-2012-001431] | ICD-9 |\r\n| indications | LabeledIn [@10.1016/j.jbi.2014.08.004 @10.1093/database/bav016] | UMLS [@10.1093/nar/gkh061] |\r\n| transcriptional signatures | [STAR-GEO](http://dev.stargeo.io/) | custom |\r\n| symptoms | HSDN [@10.1038/ncomms5212] | [MeSH](//www.nlm.nih.gov/mesh/filelist.html) [@10.1001/jama.271.14.1103] (2011 release) |\r\n| gene associations | [het.io](//het.io/disease-genes/downloads/) [@10.1101/011569] | [Disease Ontology](//disease-ontology.org/) [@10.1093/nar/gkr972 @10.1093/nar/gku1011] |\r\n| pathophysiology | [het.io](//het.io/disease-genes/downloads/) [@10.1101/011569] | [Disease Ontology](//disease-ontology.org/) [@10.1093/nar/gkr972 @10.1093/nar/gku1011] |\r\n| tissue localization | [het.io](//het.io/disease-genes/downloads/) [@10.1101/011569] | [Disease Ontology](//disease-ontology.org/) [@10.1093/nar/gkr972 @10.1093/nar/gku1011] |\r\n\r\nOur current plan is to use the Disease Ontology (DO) as our primary vocabulary. Therefore, we will have to map resources to DO terms.",
      "profile": 17,
      "published": "2015-03-31T00:16:47.526338Z",
      "thread": 44,
      "url": "/discussion/unifying-disease-vocabularies/44"
    },
    {
      "body_html": "<p>The initial LabeledIn <span class=\"citation\">[<a href=\"/doi/10.1016/j.jbi.2014.08.004\" class=\"citation\" data-key=\"10.1016/j.jbi.2014.08.004\">1</a>]</span> resource used expert curators. The team behind this project tested crowdsourced curation using Amazon Mechanical Turk workers <span class=\"citation\">[<a href=\"/doi/10.1093/database/bav016\" class=\"citation\" data-key=\"10.1093/database/bav016\">2</a>]</span>. They found the majority vote of workers on whether a disease within a label was an indication had a high accuracy (96%).</p>\r\n\r\n<p>They <a href=\"ftp://ftp.ncbi.nlm.nih.gov/pub/lu/LabeledIn/Crowdsourcing/\">assessed</a> 3004 indications not already in LabeledIn corresponding to 706 new drug labels. We are looking to increase the coverage of the initial LabeledIn dataset by adding these crowdsourced indications.</p>",
      "body_md": "The initial LabeledIn [@10.1016/j.jbi.2014.08.004] resource used expert curators. The team behind this project tested crowdsourced curation using Amazon Mechanical Turk workers [@10.1093/database/bav016]. They found the majority vote of workers on whether a disease within a label was an indication had a high accuracy (96%).\r\n\r\nThey [assessed](ftp://ftp.ncbi.nlm.nih.gov/pub/lu/LabeledIn/Crowdsourcing/) 3004 indications not already in LabeledIn corresponding to 706 new drug labels. We are looking to increase the coverage of the initial LabeledIn dataset by adding these crowdsourced indications.",
      "profile": 17,
      "published": "2015-04-01T21:45:15.517786Z",
      "thread": 21,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#2"
    },
    {
      "body_html": "<p>The <a href=\"http://ftp.ncbi.nlm.nih.gov/pub/lu/LabeledIn/\">LabeledIn</a> resource consists of an expert curated <span class=\"citation\">[<a href=\"/doi/10.1016/j.jbi.2014.08.004\" class=\"citation\" data-key=\"10.1016/j.jbi.2014.08.004\">1</a>]</span> and crowdsourced <span class=\"citation\">[<a href=\"/doi/10.1093/database/bav016\" class=\"citation\" data-key=\"10.1093/database/bav016\">2</a>]</span> components. Here we will discuss parsing these resources to extract indications.</p>",
      "body_md": "The [LabeledIn](http://ftp.ncbi.nlm.nih.gov/pub/lu/LabeledIn/) resource consists of an expert curated [@10.1016/j.jbi.2014.08.004] and crowdsourced [@10.1093/database/bav016] components. Here we will discuss parsing these resources to extract indications.",
      "profile": 17,
      "published": "2015-04-02T17:16:17.491299Z",
      "thread": 46,
      "url": "/discussion/processing-labeledin-to-extract-indications/46"
    },
    {
      "body_html": "<h1>Disease Ontology Resources</h1>\r\n\r\n<p>Since we plan to use the DO as our primary disease vocabulary, I thought I would keep track of related papers and projects here. These may be slightly off-topic but valuable to keep track of.</p>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th>name</th><th>description</th><th>cite</th></tr></thead><tbody><tr><td><a href=\"//disease-ontology.org\">Disease Ontology</a></td><td>Main resource</td><td><span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkr972\" class=\"citation\" data-key=\"10.1093/nar/gkr972\">1</a>, <a href=\"/doi/10.1093/nar/gku1011\" class=\"citation\" data-key=\"10.1093/nar/gku1011\">2</a>]</span></td></tr><tr><td><a href=\"http://django.nubic.northwestern.edu/fundo/\">DOLite</a></td><td>DO terms are grouped using associated-gene similarity to produce a simplified vocabulary with little redundancy.</td><td><span class=\"citation\">[<a href=\"/doi/10.1093/bioinformatics/btp193\" class=\"citation\" data-key=\"10.1093/bioinformatics/btp193\">3</a>]</span></td></tr><tr><td>DO_cancer_slim</td><td>Created a DO subset named <code>TOPNodes_DOcancerslim</code> composed of 63 non-redundant upper-level cancer terms</td><td><span class=\"citation\">[<a href=\"/doi/10.1093/database/bav032\" class=\"citation\" data-key=\"10.1093/database/bav032\">4</a>]</span></td></tr><tr><td><a href=\"http://doa.nubic.northwestern.edu/pages/search.php\">DOAF</a></td><td>Provides gene annotations (extracted from GeneRIF) to the Disease Ontology.</td><td><span class=\"citation\">[<a href=\"/doi/10.1371/journal.pone.0049686\" class=\"citation\" data-key=\"10.1371/journal.pone.0049686\">5</a>]</span></td></tr><tr><td><a href=\"http://django.nubic.northwestern.edu/fundo/\">FunDO</a></td><td>Provides gene annotations (extracted from GeneRIF) to the Disease Ontology. Uses diseases from DOLite. Potentially outdated.</td><td><span class=\"citation\">[<a href=\"/doi/10.1186/1471-2164-10-S1-S6\" class=\"citation\" data-key=\"10.1186/1471-2164-10-S1-S6\">6</a>]</span></td></tr><tr><td><a href=\"https://github.com/GuangchuangYu/DOSE\">DOSE</a></td><td>DOSE is an R package to compute semantic similarity between DO terms. The result is pairwise similarities between DO terms based only on the ontology structure.</td><td><span class=\"citation\">[<a href=\"/doi/10.1093/bioinformatics/btu684\" class=\"citation\" data-key=\"10.1093/bioinformatics/btu684\">7</a>]</span></td></tr><tr><td><a href=\"http://cran.r-project.org/web/packages/DOSim/index.html\">DOsim</a></td><td>Similar to DOSE but allegedly unmaintained or outdated</td><td><span class=\"citation\">[<a href=\"/doi/10.1186/1471-2105-12-266\" class=\"citation\" data-key=\"10.1186/1471-2105-12-266\">8</a>]</span></td></tr></tbody></table>",
      "body_md": "# Disease Ontology Resources\r\n\r\nSince we plan to use the DO as our primary disease vocabulary, I thought I would keep track of related papers and projects here. These may be slightly off-topic but valuable to keep track of.\r\n\r\n| name | description | cite |\r\n| - | - | - |\r\n| [Disease Ontology](//disease-ontology.org) | Main resource | [@10.1093/nar/gkr972 @10.1093/nar/gku1011] |\r\n| [DOLite](http://django.nubic.northwestern.edu/fundo/) | DO terms are grouped using associated-gene similarity to produce a simplified vocabulary with little redundancy. | [@10.1093/bioinformatics/btp193] |\r\n| DO_cancer_slim | Created a DO subset named `TOPNodes_DOcancerslim` composed of 63 non-redundant upper-level cancer terms | [@10.1093/database/bav032] |\r\n| [DOAF](http://doa.nubic.northwestern.edu/pages/search.php) | Provides gene annotations (extracted from GeneRIF) to the Disease Ontology. | [@10.1371/journal.pone.0049686] |\r\n| [FunDO](http://django.nubic.northwestern.edu/fundo/) | Provides gene annotations (extracted from GeneRIF) to the Disease Ontology. Uses diseases from DOLite. Potentially outdated. | [@10.1186/1471-2164-10-S1-S6] |\r\n| [DOSE](https://github.com/GuangchuangYu/DOSE) | DOSE is an R package to compute semantic similarity between DO terms. The result is pairwise similarities between DO terms based only on the ontology structure.| [@10.1093/bioinformatics/btu684] |\r\n| [DOsim](http://cran.r-project.org/web/packages/DOSim/index.html) | Similar to DOSE but allegedly unmaintained or outdated | [@10.1186/1471-2105-12-266] |\r\n",
      "profile": 17,
      "published": "2015-04-02T22:13:31.868516Z",
      "thread": 44,
      "url": "/discussion/unifying-disease-vocabularies/44#2"
    },
    {
      "body_html": "<p>What data are you planning to use to validate your framework?  E.g. what are the positive controls?  Presumably you would have some collection of drug-disease pairs that would be divided into development, training, and testing sets? </p>",
      "body_md": "What data are you planning to use to validate your framework?  E.g. what are the positive controls?  Presumably you would have some collection of drug-disease pairs that would be divided into development, training, and testing sets? ",
      "profile": 48,
      "published": "2015-04-03T04:43:56.743898Z",
      "thread": 47,
      "url": "/discussion/evaluation-framework/47"
    },
    {
      "body_html": "<p>You say elsewhere that you want to avoid bias - that you want to work basically form experimental measurements as much as possible.  This has some merit, but it also seems like there must be quite a lot of value out there in the space of biased knowledge.. Some of that bias will be real signal.  Would be great to execute an experiment to empirically test the impact of bringing in prior knowledge (e.g. via text mining techniques for relation extraction).  Does it make it better or worse at the task at hand?</p>",
      "body_md": "You say elsewhere that you want to avoid bias - that you want to work basically form experimental measurements as much as possible.  This has some merit, but it also seems like there must be quite a lot of value out there in the space of biased knowledge.. Some of that bias will be real signal.  Would be great to execute an experiment to empirically test the impact of bringing in prior knowledge (e.g. via text mining techniques for relation extraction).  Does it make it better or worse at the task at hand?",
      "profile": 48,
      "published": "2015-04-03T04:53:35.489041Z",
      "thread": 48,
      "url": "/discussion/text-as-a-resource-for-network-population/48"
    },
    {
      "body_html": "<p>This dataset might be worth looking into.  Drug-indication links captured from physicians in an EHR system <span class=\"citation\">[<a href=\"/doi/10.1136/amiajnl-2012-000852\" class=\"citation\" data-key=\"10.1136/amiajnl-2012-000852\">1</a>]</span> .  Data appears to be available - though its in a 200+ page PDF!  <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3422843/bin/amiajnl-2012-000852-s1.pdf\" target=\"_blank\">http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3422843/bin/amiajnl-2012-000852-s1.pdf</a>(I'm sure that was a journal requirement).</p>",
      "body_md": "This dataset might be worth looking into.  Drug-indication links captured from physicians in an EHR system [@10.1136/amiajnl-2012-000852] .  Data appears to be available - though its in a 200+ page PDF!  http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3422843/bin/amiajnl-2012-000852-s1.pdf\r\n(I'm sure that was a journal requirement).\r\n",
      "profile": 48,
      "published": "2015-04-03T05:35:58.188298Z",
      "thread": 21,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#3"
    },
    {
      "body_html": "<p>Question: I was under the impression that the workers assessed individual indications rather than all indications within a specific label. Therefore each drug–disease (RxNORM–UMLS) pair should have it's own majority vote. However, the data release appears to be listed in terms of labels rather than indications. Some labels have multiple UMLS diseases but only report the outcome of a single vote. Majority votes should be in terms of indications rather than labels, right?</p>\r\n\r\n<p>Answer: You are right: each drug–disease (RxNORM–UMLS) pair should have it's own majority vote and majority votes should be in terms of indications rather than labels. The data is organized in this manner only (one entry = one drug-label/UMLSCUI pair). Each entry in the text file corresponds to one indication candidate (i.e. one disease UMLS-CUI) in a given drug label. The disease CUI is specified in the third field of the file. Also, as you have already noted that there for some entries with two CUIs in the third field. These correspond to composite mentions (e.g.\"Moderate to severe pain\"). Our <a href=\"http://metamap.nlm.nih.gov/\">disease NER module</a> detects two concepts for this phrase (\"moderate +pain\" and \"severe pain\") but we present this phrase as a single disease mention to the turkers and hence a single majority vote was computed for both UMLS-CUIs.</p>\r\n\r\n<p>We are happy to answer more questions! - LabeledIn Team</p>",
      "body_md": "Question: I was under the impression that the workers assessed individual indications rather than all indications within a specific label. Therefore each drug--disease (RxNORM--UMLS) pair should have it's own majority vote. However, the data release appears to be listed in terms of labels rather than indications. Some labels have multiple UMLS diseases but only report the outcome of a single vote. Majority votes should be in terms of indications rather than labels, right?\r\n \r\nAnswer: You are right: each drug--disease (RxNORM--UMLS) pair should have it's own majority vote and majority votes should be in terms of indications rather than labels. The data is organized in this manner only (one entry = one drug-label/UMLSCUI pair). Each entry in the text file corresponds to one indication candidate (i.e. one disease UMLS-CUI) in a given drug label. The disease CUI is specified in the third field of the file. Also, as you have already noted that there for some entries with two CUIs in the third field. These correspond to composite mentions (e.g.\"Moderate to severe pain\"). Our [disease NER module](http://metamap.nlm.nih.gov/) detects two concepts for this phrase (\"moderate +pain\" and \"severe pain\") but we present this phrase as a single disease mention to the turkers and hence a single majority vote was computed for both UMLS-CUIs.\r\n\r\nWe are happy to answer more questions! - LabeledIn Team",
      "profile": 72,
      "published": "2015-04-03T17:19:55.617887Z",
      "thread": 46,
      "url": "/discussion/processing-labeledin-to-extract-indications/46#2"
    },
    {
      "body_html": "<p>Thanks <a href=\"/u/ritukhare\" class=\"username\">@ritukhare</a>, we've <a href=\"//git.dhimmel.com/indications/labeledin/\">processed your datasets</a> and combined the expert <span class=\"citation\">[<a href=\"/doi/10.1016/j.jbi.2014.08.004\" class=\"citation\" data-key=\"10.1016/j.jbi.2014.08.004\">1</a>]</span> and crowdsourced <span class=\"citation\">[<a href=\"/doi/10.1093/database/bav016\" class=\"citation\" data-key=\"10.1093/database/bav016\">2</a>]</span> indications. The <a href=\"//git.dhimmel.com/indications/labeledin/data/indications.tsv\">resulting .tsv file is available for download</a>. We provide ingredient and disease names here <em>only for convenience</em>, since our simplistic lookup methodology left many identifiers unnamed. </p>\r\n\r\n<p>Specifically, we extracted 1,335 indications from the <a href=\"//ftp.ncbi.nlm.nih.gov/pub/lu/LabeledIn/LabeledIn_Structured_Results.txt\">expert data release</a> and 1,516 indications from the <a href=\"//ftp.ncbi.nlm.nih.gov/pub/lu/LabeledIn/Crowdsourcing/Crowdsourced_Results.txt\">crowdsourced data release</a>. The two sets shared one indication, so merging the two resources resulted in <code>2850 = 1335 + 1516 - 1</code> indications.</p>\r\n\r\n<p>We calculated the total number of labels reporting each indication. For this task, we assumed <code>study_drug_label_ID</code> was consistent across the expert and crowdsourced datasets. If this assumption is wrong, the effect would be minimal, since the two releases report almost entirely disjoint sets of indications.</p>",
      "body_md": "Thanks @ritukhare, we've [processed your datasets](//git.dhimmel.com/indications/labeledin/) and combined the expert [@10.1016/j.jbi.2014.08.004] and crowdsourced [@10.1093/database/bav016] indications. The [resulting .tsv file is available for download](//git.dhimmel.com/indications/labeledin/data/indications.tsv). We provide ingredient and disease names here *only for convenience*, since our simplistic lookup methodology left many identifiers unnamed. \r\n\r\nSpecifically, we extracted 1,335 indications from the [expert data release](//ftp.ncbi.nlm.nih.gov/pub/lu/LabeledIn/LabeledIn_Structured_Results.txt) and 1,516 indications from the [crowdsourced data release](//ftp.ncbi.nlm.nih.gov/pub/lu/LabeledIn/Crowdsourcing/Crowdsourced_Results.txt). The two sets shared one indication, so merging the two resources resulted in `2850 = 1335 + 1516 - 1` indications.\r\n\r\nWe calculated the total number of labels reporting each indication. For this task, we assumed `study_drug_label_ID` was consistent across the expert and crowdsourced datasets. If this assumption is wrong, the effect would be minimal, since the two releases report almost entirely disjoint sets of indications.",
      "profile": 17,
      "published": "2015-04-03T17:59:14.506499Z",
      "thread": 46,
      "url": "/discussion/processing-labeledin-to-extract-indications/46#3"
    },
    {
      "body_html": "<p>Hey <a href=\"/u/b_good\" class=\"username\">@b_good</a>, thanks for the suggestion <span class=\"citation\">[<a href=\"/doi/10.1136/amiajnl-2012-000852\" class=\"citation\" data-key=\"10.1136/amiajnl-2012-000852\">1</a>]</span> and tracking down the data supplement, which I cannot find on the <a href=\"http://dx.doi.org/10.1136/amiajnl-2012-000852\">article's JAMIA page</a>. Hereon, I will refer to this resource as <code>ehrlink</code>, unless anyone can find a previously-used or author-preferred nickname.</p>\r\n\r\n<p>This resource is noteworthy because it will capture off-label usages better than LabeledIn (which is explicitly on-label) and MEDI (whose inclusion criteria likely favor on-label indications)</p>\r\n\r\n<p>I <a href=\"http://nbviewer.ipython.org/github/dhimmel/indications/blob/gh-pages/ehrlink/convert-to-text.ipynb\">converted the pdf file into a tsv file</a>, which can be downloaded <a href=\"//git.dhimmel.com/indications/ehrlink/download/amiajnl-2012-000852-s1.tsv\">here</a>.</p>",
      "body_md": "Hey @b_good, thanks for the suggestion [@10.1136/amiajnl-2012-000852] and tracking down the data supplement, which I cannot find on the [article's JAMIA page](http://dx.doi.org/10.1136/amiajnl-2012-000852). Hereon, I will refer to this resource as `ehrlink`, unless anyone can find a previously-used or author-preferred nickname.\r\n\r\nThis resource is noteworthy because it will capture off-label usages better than LabeledIn (which is explicitly on-label) and MEDI (whose inclusion criteria likely favor on-label indications)\r\n\r\nI [converted the pdf file into a tsv file](http://nbviewer.ipython.org/github/dhimmel/indications/blob/gh-pages/ehrlink/convert-to-text.ipynb), which can be downloaded [here](//git.dhimmel.com/indications/ehrlink/download/amiajnl-2012-000852-s1.tsv).",
      "profile": 17,
      "published": "2015-04-03T20:10:47.043719Z",
      "thread": 21,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#4"
    },
    {
      "body_html": "<h1>Evaluation with known indications</h1>\r\n\r\n<p><a href=\"/u/b_good\" class=\"username\">@b_good</a>, the primary means of evaluation will be assessing performance on a masked subset of indications. Previously <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1004259\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1004259\">1</a>]</span>, we withheld 25% of observations for testing. During training (on the remaining 75% of observations), we can use cross-validation to identify optimal parameter values. We plan to measure performance using area under the ROC curve (AUROC). We will also consider using condensed-ROC curves <span class=\"citation\">[<a href=\"/doi/10.1093/bioinformatics/btq140\" class=\"citation\" data-key=\"10.1093/bioinformatics/btq140\">2</a>]</span>, which emphasize top predictions.</p>\r\n\r\n<p>The <a href=\"http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d21\">discussion you noted</a> discusses how to construct a catalog of high-confidence indications, also known as a <em>gold standard</em>. What we haven't discussed thus far is how to create a negative set, which is a necessary input for our classification approaches. The simplest way to generate negatives is to treat all non-positives as negatives: if a compound is not indicated for a disease, the compound-disease pair is considered a negative. Since our positive set is incomplete, some true but unknown indications will be considered negatives. Given that the overwhelming majority of negatives will truly be negatives, I expect the impact of improper negatives to be minimal. However, our past experiences show many people find this response unsatisfactory and would prefer us to exclude potential positives from the negative set. We will probably do that for this project, for example by omitting compound-disease pairs that are in the low-precision subset of MEDI <span class=\"citation\">[<a href=\"/doi/10.1136/amiajnl-2014-002954\" class=\"citation\" data-key=\"10.1136/amiajnl-2014-002954\">3</a>]</span>.</p>\r\n\r\n<p>In our previous project <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1004259\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1004259\">1</a>]</span>, we found that our classification approach was resistant to overfitting. In other words, our training and testing AUROCs were comparable. We should still continue a formal testing paradigm as good practice, but there is a larger issue that I will discuss in my next post.</p>",
      "body_md": "# Evaluation with known indications\r\n\r\n@b_good, the primary means of evaluation will be assessing performance on a masked subset of indications. Previously [@10.1371/journal.pcbi.1004259], we withheld 25% of observations for testing. During training (on the remaining 75% of observations), we can use cross-validation to identify optimal parameter values. We plan to measure performance using area under the ROC curve (AUROC). We will also consider using condensed-ROC curves [@10.1093/bioinformatics/btq140], which emphasize top predictions.\r\n\r\nThe [discussion you noted](http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21) discusses how to construct a catalog of high-confidence indications, also known as a *gold standard*. What we haven't discussed thus far is how to create a negative set, which is a necessary input for our classification approaches. The simplest way to generate negatives is to treat all non-positives as negatives: if a compound is not indicated for a disease, the compound-disease pair is considered a negative. Since our positive set is incomplete, some true but unknown indications will be considered negatives. Given that the overwhelming majority of negatives will truly be negatives, I expect the impact of improper negatives to be minimal. However, our past experiences show many people find this response unsatisfactory and would prefer us to exclude potential positives from the negative set. We will probably do that for this project, for example by omitting compound-disease pairs that are in the low-precision subset of MEDI [@10.1136/amiajnl-2014-002954].\r\n\r\nIn our previous project [@10.1371/journal.pcbi.1004259], we found that our classification approach was resistant to overfitting. In other words, our training and testing AUROCs were comparable. We should still continue a formal testing paradigm as good practice, but there is a larger issue that I will discuss in my next post.",
      "profile": 17,
      "published": "2015-04-07T17:58:47.060686Z",
      "thread": 47,
      "url": "/discussion/evaluation-framework/47#2"
    },
    {
      "body_html": "<p>You can access SemRep extracted semantic relations (e.g. treats, causes) based on all PubMed abstracts (updated bi-annually) via the semantic medline database.  With a UMLS login, you can get the complete MySQL dump via <a href=\"http://skr3.nlm.nih.gov/SemMedDB/\" target=\"_blank\">http://skr3.nlm.nih.gov/SemMedDB/</a> .  Main challenge here is in ensuring quality (as with any NLP output).</p>",
      "body_md": "You can access SemRep extracted semantic relations (e.g. treats, causes) based on all PubMed abstracts (updated bi-annually) via the semantic medline database.  With a UMLS login, you can get the complete MySQL dump via http://skr3.nlm.nih.gov/SemMedDB/ .  Main challenge here is in ensuring quality (as with any NLP output).",
      "profile": 48,
      "published": "2015-04-07T19:33:29.194446Z",
      "thread": 21,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#5"
    },
    {
      "body_html": "<h1>ehrlink problem and medication vocabularies</h1>\r\n\r\n<p>We have extracted the ehrlink <span class=\"citation\">[<a href=\"/doi/10.1136/amiajnl-2012-000852\" class=\"citation\" data-key=\"10.1136/amiajnl-2012-000852\">1</a>]</span> indication data (<a href=\"#104\">see above</a>). Unfortunately, I am unfamiliar with the identifiers used for problems (diseases) and medications (drugs). I've posted a sampling below in case anyone can figure out.</p>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th>problem_definition_id</th><th>problem</th></tr></thead><tbody><tr><td>63645</td><td>Complete D-transposition Of The Great Vessels</td></tr><tr><td>258894</td><td>Acromegaly</td></tr><tr><td>275590</td><td>Organic REM Sleep Behavior Disorder</td></tr><tr><td>62983</td><td>Arteriosclerotic Cardiovascular Disease (ASCVD)</td></tr><tr><td>75090</td><td>Cerebral Palsy</td></tr></tbody></table>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th>medication_definition_id</th><th>medication</th></tr></thead><tbody><tr><td>17938</td><td>Sodium Polystyrene Sulfonate Oral Powder</td></tr><tr><td>21707</td><td>Clotrimazole Anti-Fungal 1 % External Cream</td></tr><tr><td>18805</td><td>Niacin CR 1000 MG Oral Tablet Extended Release</td></tr><tr><td>19598</td><td>ClonazePAM 0.5 MG Oral Tablet</td></tr><tr><td>136143</td><td>AmLODIPine Besylate 2.5 MG Oral Tablet</td></tr></tbody></table>\r\n\r\n<p>My worry is that these identifiers may not correspond to a standardized vocabulary that we can access and easily map to. I will contact the authors for clarification.</p>",
      "body_md": "# ehrlink problem and medication vocabularies\r\n\r\nWe have extracted the ehrlink [@10.1136/amiajnl-2012-000852] indication data ([see above](#104)). Unfortunately, I am unfamiliar with the identifiers used for problems (diseases) and medications (drugs). I've posted a sampling below in case anyone can figure out.\r\n\r\n| problem_definition_id | problem                                         |\r\n|-----------------------|-------------------------------------------------|\r\n| 63645                 | Complete D-transposition Of The Great Vessels   |\r\n| 258894                | Acromegaly                                      |\r\n| 275590                | Organic REM Sleep Behavior Disorder             |\r\n| 62983                 | Arteriosclerotic Cardiovascular Disease (ASCVD) |\r\n| 75090                 | Cerebral Palsy                                  |\r\n\r\n| medication_definition_id | medication                                     |\r\n|--------------------------|------------------------------------------------|\r\n| 17938                    | Sodium Polystyrene Sulfonate Oral Powder       |\r\n| 21707                    | Clotrimazole Anti-Fungal 1 % External Cream    |\r\n| 18805                    | Niacin CR 1000 MG Oral Tablet Extended Release |\r\n| 19598                    | ClonazePAM 0.5 MG Oral Tablet                  |\r\n| 136143                   | AmLODIPine Besylate 2.5 MG Oral Tablet         |\r\n\r\nMy worry is that these identifiers may not correspond to a standardized vocabulary that we can access and easily map to. I will contact the authors for clarification.",
      "profile": 17,
      "published": "2015-04-08T01:53:46.278727Z",
      "thread": 21,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#6"
    },
    {
      "body_html": "<p>This is great. Thanks <a href=\"/u/dhimmel\" class=\"username\">@dhimmel</a>. There should be no confusion with the study_drug_label_ID between the two datasets: In expert-LabeledIn, the values are numbers and in crowd-LabeledIn, the values are concatenation of drug type and a number.</p>\r\n\r\n<p>I don't have a readily available mapping of ingredient and disease identifiers to names. Please note that the it would be more appropriate to use the title of drug label (SPL) instead of ingredient name as the title will also contain the dose form information of the drug (and we found that indications may be different between two drugs having same ingredient but different dose form). However, it's your decision.</p>",
      "body_md": "This is great. Thanks @dhimmel. There should be no confusion with the study_drug_label_ID between the two datasets: In expert-LabeledIn, the values are numbers and in crowd-LabeledIn, the values are concatenation of drug type and a number.\r\n\r\nI don't have a readily available mapping of ingredient and disease identifiers to names. Please note that the it would be more appropriate to use the title of drug label (SPL) instead of ingredient name as the title will also contain the dose form information of the drug (and we found that indications may be different between two drugs having same ingredient but different dose form). However, it's your decision.",
      "profile": 72,
      "published": "2015-04-06T18:57:35.693780Z",
      "thread": 46,
      "url": "/discussion/processing-labeledin-to-extract-indications/46#4"
    },
    {
      "body_html": "<h1>Evaluation with novel indications</h1>\r\n\r\n<p>Experienced chemoinformaticians stress that impressive testing performance on known positives does not always translate to predicting <em>novel</em> positives. The causes are several fold:</p>\r\n\r\n<ul><li>the patterns behind established positives are not generative — those patterns do not translate to unknown positives.</li><li>predictions are made for instances that are not well-represented in the training set. Understanding the applicability domain of your model is crucial here.</li><li>the current set of positives is synchronous with the current state of knowledge. If the method does not incorporate untapped knowledge, novel predictions may not be possible.</li></ul>\r\n\r\n<p>These issues are one reason why the community places such emphasis on novel discovery when appraising new methods. Absent experimental verification of our top predicted indications, there are a few approaches we could consider that begin to assess our ability to predict the \"unkown\". Several potential approaches were explored by a past repurposing study <span class=\"citation\">[<a href=\"/doi/10.1038/msb.2011.26\" class=\"citation\" data-key=\"10.1038/msb.2011.26\">1</a>]</span>:</p>\r\n\r\n<ul><li>Predicting indications currently undergoing clinical trials</li><li>Predicting potential indications that were not in our gold standard. For example, those indications in the MEDI low-precision subset or those identified using literature mining. </li></ul>\r\n\r\n<p>These approaches are both imperfect, but they are a start. Ideally, we could experimentally evaluate several predictions. This work could potentially be <a href=\"https://www.scienceexchange.com/\">outsourced</a> and thus parallelized. </p>",
      "body_md": "# Evaluation with novel indications\r\n\r\nExperienced chemoinformaticians stress that impressive testing performance on known positives does not always translate to predicting *novel* positives. The causes are several fold:\r\n\r\n+ the patterns behind established positives are not generative -- those patterns do not translate to unknown positives.\r\n+ predictions are made for instances that are not well-represented in the training set. Understanding the applicability domain of your model is crucial here.\r\n+ the current set of positives is synchronous with the current state of knowledge. If the method does not incorporate untapped knowledge, novel predictions may not be possible.\r\n\r\nThese issues are one reason why the community places such emphasis on novel discovery when appraising new methods. Absent experimental verification of our top predicted indications, there are a few approaches we could consider that begin to assess our ability to predict the \"unkown\". Several potential approaches were explored by a past repurposing study [@10.1038/msb.2011.26]:\r\n\r\n+ Predicting indications currently undergoing clinical trials\r\n+ Predicting potential indications that were not in our gold standard. For example, those indications in the MEDI low-precision subset or those identified using literature mining. \r\n\r\nThese approaches are both imperfect, but they are a start. Ideally, we could experimentally evaluate several predictions. This work could potentially be [outsourced](https://www.scienceexchange.com/) and thus parallelized. ",
      "profile": 17,
      "published": "2015-04-08T05:15:41.083774Z",
      "thread": 47,
      "url": "/discussion/evaluation-framework/47#3"
    },
    {
      "body_html": "<blockquote><p>we found that indications may different between two drugs having same ingredient but different dose form</p></blockquote>\r\n\r\n<p><a href=\"/u/ritukhare\" class=\"username\">@ritukhare</a>, interesting to hear that examples of repurposing frequently relied on different dose forms (and perhaps dosage levels as well). I think we would like to ignore this complexity. In other words, our predicted indications will not include dosage or dose form recommendations. I am comfortable leaving these details for the end users to investigate.</p>",
      "body_md": "> we found that indications may different between two drugs having same ingredient but different dose form\r\n\r\n@ritukhare, interesting to hear that examples of repurposing frequently relied on different dose forms (and perhaps dosage levels as well). I think we would like to ignore this complexity. In other words, our predicted indications will not include dosage or dose form recommendations. I am comfortable leaving these details for the end users to investigate.",
      "profile": 17,
      "published": "2015-04-08T05:23:57.025486Z",
      "thread": 46,
      "url": "/discussion/processing-labeledin-to-extract-indications/46#5"
    },
    {
      "body_html": "<p>Just to let you guys know that, at UNM, Oleg Ursu and I have been constructing such a catalog for nearly eight years. <br>Unfortunately, nobody funds this type of activity - or at least nobody has funded it so far - thus resources are somewhat limited.<br>Briefly, we manually curated all the active pharmaceutical ingredients APIs (over 4400; includes biologics), and mapped them to FDA approved drug labels (over 50000 ADLs). <br>From the ADLs one can extract/map indications, contra-indications, off-label indications... and to each API we mapped RxNorm [CUI], NPC, ATC, INN, plus targets, including numeric bioactivity &amp; type [MoA related; non-MoA assigned; as well as non-human targets]. We also mapped all our diseases to DOIDs - however, there are about 800 or so left that will take us a while to map.<br>A few pointers: <br>1) if you want to extract the data yourselves, you're in for a treat. There are diseases in \"indications\" that do NOT exist anywhere else [e.g., cancer XYZ with mutation A3999B, in other words it's not enough to have the disease, you need the right genotype!]; <br>2) you also have to deal with indications that are \"fringe\" (pregnancy is not a disease; neither is contraception)<br>3) indications etc. are not from PubMed - so please pay attention to approved labels<br>4) disease modifying is far from trivial - you need epi to show you that, X years after the Dx/Rx event, there was no recurrence [are steroids in anti-allergy disease modifying? probably not; are antibiotics in sinusitis disease-modifying? yes and no link_ref,[object Object],if it's chronic!</p>",
      "body_md": "Just to let you guys know that, at UNM, Oleg Ursu and I have been constructing such a catalog for nearly eight years. \r\nUnfortunately, nobody funds this type of activity - or at least nobody has funded it so far - thus resources are somewhat limited.\r\nBriefly, we manually curated all the active pharmaceutical ingredients APIs (over 4400; includes biologics), and mapped them to FDA approved drug labels (over 50000 ADLs). \r\nFrom the ADLs one can extract/map indications, contra-indications, off-label indications... and to each API we mapped RxNorm [CUI], NPC, ATC, INN, plus targets, including numeric bioactivity & type [MoA related; non-MoA assigned; as well as non-human targets]. We also mapped all our diseases to DOIDs - however, there are about 800 or so left that will take us a while to map.\r\nA few pointers: \r\n1) if you want to extract the data yourselves, you're in for a treat. There are diseases in \"indications\" that do NOT exist anywhere else [e.g., cancer XYZ with mutation A3999B, in other words it's not enough to have the disease, you need the right genotype!]; \r\n2) you also have to deal with indications that are \"fringe\" (pregnancy is not a disease; neither is contraception)\r\n3) indications etc. are not from PubMed - so please pay attention to approved labels\r\n4) disease modifying is far from trivial - you need epi to show you that, X years after the Dx/Rx event, there was no recurrence [are steroids in anti-allergy disease modifying? probably not; are antibiotics in sinusitis disease-modifying? yes and no [if it's chronic!]\r\n",
      "profile": 75,
      "published": "2015-04-08T14:53:34.682008Z",
      "thread": 21,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#7"
    },
    {
      "body_html": "<p>My colleagues and I have worked on multiple approaches to create this knowledge in the papers below:</p>\r\n\r\n<ol><li>Wright A, Chen ES, Maloney FL. An automated technique for identifying associations between medications, laboratory results and problems. J Biomed Inform. 2010 Dec;43(6):891–901. <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/20884377\" target=\"_blank\">http://www.ncbi.nlm.nih.gov/pubmed/20884377</a></li><li>McCoy AB, Wright A, Laxmisan A, Singh H, Sittig DF. A prototype knowledge base and SMART app to facilitate organization of patient medications by clinical problems. AMIA Annu Symp Proc. 2011;2011:888–94. <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/22195147\" target=\"_blank\">http://www.ncbi.nlm.nih.gov/pubmed/22195147</a></li><li>McCoy AB, Wright A, Laxmisan A, Ottosen MJ, McCoy JA, Butten D, et al. Development and evaluation of a crowdsourcing methodology for knowledge base construction: identifying relationships between clinical problems and medications. J Am Med Inform Assoc. 2012 Oct;19(5):713–8. <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/22582202\" target=\"_blank\">http://www.ncbi.nlm.nih.gov/pubmed/22582202</a></li><li>McCoy AB, Wright A, Rogith D, Fathiamini S, Ottenbacher AJ, Sittig DF. Development of a clinician reputation metric to identify appropriate problem-medication pairs in a crowdsourced knowledge base. J Biomed Inform. 2014 Apr;48:66–72. <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/24321170\" target=\"_blank\">http://www.ncbi.nlm.nih.gov/pubmed/24321170</a></li></ol>\r\n\r\n<p>In the JAMIA paper mentioned above, we used what we called a crowdsourcing approach to get this data. We have recently validated that approach at another site, and that publication is coming out in ACI soon. Unfortunately, in the original version, as you suspected, our medications and problems not mapped to any standardized terminology. The identifiers are local to the EHR, and while we have made some attempts to map them to RxNorm and SNOMED-CT, we were never able to get a really accurate set. However, the validation uses data from a different EHR, which I believe can be more easily mapped. Once the paper is out, I'll see if I can share that data.</p>",
      "body_md": "My colleagues and I have worked on multiple approaches to create this knowledge in the papers below:\r\n\r\n1. Wright A, Chen ES, Maloney FL. An automated technique for identifying associations between medications, laboratory results and problems. J Biomed Inform. 2010 Dec;43(6):891–901. http://www.ncbi.nlm.nih.gov/pubmed/20884377\r\n2. McCoy AB, Wright A, Laxmisan A, Singh H, Sittig DF. A prototype knowledge base and SMART app to facilitate organization of patient medications by clinical problems. AMIA Annu Symp Proc. 2011;2011:888–94. http://www.ncbi.nlm.nih.gov/pubmed/22195147\r\n3. McCoy AB, Wright A, Laxmisan A, Ottosen MJ, McCoy JA, Butten D, et al. Development and evaluation of a crowdsourcing methodology for knowledge base construction: identifying relationships between clinical problems and medications. J Am Med Inform Assoc. 2012 Oct;19(5):713–8. http://www.ncbi.nlm.nih.gov/pubmed/22582202\r\n4. McCoy AB, Wright A, Rogith D, Fathiamini S, Ottenbacher AJ, Sittig DF. Development of a clinician reputation metric to identify appropriate problem-medication pairs in a crowdsourced knowledge base. J Biomed Inform. 2014 Apr;48:66–72. http://www.ncbi.nlm.nih.gov/pubmed/24321170\r\n\r\nIn the JAMIA paper mentioned above, we used what we called a crowdsourcing approach to get this data. We have recently validated that approach at another site, and that publication is coming out in ACI soon. Unfortunately, in the original version, as you suspected, our medications and problems not mapped to any standardized terminology. The identifiers are local to the EHR, and while we have made some attempts to map them to RxNorm and SNOMED-CT, we were never able to get a really accurate set. However, the validation uses data from a different EHR, which I believe can be more easily mapped. Once the paper is out, I'll see if I can share that data.",
      "profile": 77,
      "published": "2015-04-08T17:40:12.289974Z",
      "thread": 21,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#8"
    },
    {
      "body_html": "<p>I find crowdsourcing useful when you use a team of experts. So, for example, a carefully selected team of experts, when working on the same problem, can give surprisingly interesting feedback on an otherwise difficult problem.<br><a href=\"http://www.nature.com/nchembio/journal/v5/n7/abs/nchembio0709-441.html\" target=\"_blank\">http://www.nature.com/nchembio/journal/v5/n7/abs/nchembio0709-441.html</a>Please note that this paper is not about data entry, but about polling experts for their opinion.</p>\r\n\r\n<p>I professionally supervised data entry for chemical structures, chemical bioactivities, as well as controlled vocabulary descriptions for assays, indexing medicinal chemistry literature. The average <em>trained</em> person loading data had an error rate of 5-10% - errors varied with period (e.g., the closer to the deadline, usually Christmas, the worse the quality). We used a 3-layer quality control system. And even so, we had a 1-2% error in our database, as revealed by comparison with two other systems. <br>See this paper <a href=\"http://pubs.acs.org/doi/abs/10.1021/ci400099q\" target=\"_blank\">http://pubs.acs.org/doi/abs/10.1021/ci400099q</a> for details (mine is the WOMBAT database).</p>\r\n\r\n<p>With this in mid, I want to point out that crowdsourcing problem medication pairs by clinicians is an intriguing effort, and if the data is publicly available I would like to learn more. There are risks because a) verification of data entry was probably not done at the entry level (was the clinician familiar with both the drug and the disease?); b) the person determining the problem would require training in pharmacovigilance, understanding of known side-effects, etc. I assume you have done that, and that you compared the sets? I apologize that I do not have time to access your papers right now. </p>",
      "body_md": "I find crowdsourcing useful when you use a team of experts. So, for example, a carefully selected team of experts, when working on the same problem, can give surprisingly interesting feedback on an otherwise difficult problem.\r\nhttp://www.nature.com/nchembio/journal/v5/n7/abs/nchembio0709-441.html\r\nPlease note that this paper is not about data entry, but about polling experts for their opinion.\r\n\r\nI professionally supervised data entry for chemical structures, chemical bioactivities, as well as controlled vocabulary descriptions for assays, indexing medicinal chemistry literature. The average *trained* person loading data had an error rate of 5-10% - errors varied with period (e.g., the closer to the deadline, usually Christmas, the worse the quality). We used a 3-layer quality control system. And even so, we had a 1-2% error in our database, as revealed by comparison with two other systems. \r\nSee this paper http://pubs.acs.org/doi/abs/10.1021/ci400099q for details (mine is the WOMBAT database).\r\n\r\nWith this in mid, I want to point out that crowdsourcing problem medication pairs by clinicians is an intriguing effort, and if the data is publicly available I would like to learn more. There are risks because a) verification of data entry was probably not done at the entry level (was the clinician familiar with both the drug and the disease?); b) the person determining the problem would require training in pharmacovigilance, understanding of known side-effects, etc. I assume you have done that, and that you compared the sets? I apologize that I do not have time to access your papers right now. ",
      "profile": 75,
      "published": "2015-04-08T18:05:35.112012Z",
      "thread": 21,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#9"
    },
    {
      "body_html": "<p>To clarify the crowdsourcing approach, in our study the clinicians are completing the task because it is required during routine care, not solely for the purpose of creating a knowledge base. They are entering the data into the EHR because they are prescribing a medication to a patient and are often required to link it to one or more of the patient's problems for billing purposes. We did not ask them to do any additional work outside of their own routine clinical practice.</p>",
      "body_md": "To clarify the crowdsourcing approach, in our study the clinicians are completing the task because it is required during routine care, not solely for the purpose of creating a knowledge base. They are entering the data into the EHR because they are prescribing a medication to a patient and are often required to link it to one or more of the patient's problems for billing purposes. We did not ask them to do any additional work outside of their own routine clinical practice.",
      "profile": 77,
      "published": "2015-04-08T18:16:23.522075Z",
      "thread": 21,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#10"
    },
    {
      "body_html": "<p>thank you - was wondering about that. this does make their work more reliable.</p>",
      "body_md": "thank you - was wondering about that. this does make their work more reliable.",
      "profile": 75,
      "published": "2015-04-08T19:29:35.198699Z",
      "thread": 21,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#11"
    },
    {
      "body_html": "<blockquote><p>there must be quite a lot of value out there in the space of biased knowledge</p></blockquote>\r\n\r\n<p>You are correct. In our current proposal, we do use text mining for the symptom-disease edges and tissue-disease edges. We also rely on literature curation for compound-target binding and protein interactions.</p>\r\n\r\n<blockquote><p>Would be great to execute an experiment to empirically test the impact of bringing in prior knowledge</p></blockquote>\r\n\r\n<p>I agree, we should fit a model that excludes all knowledge-biased domains. I reckon this model's performance on known indications will be drastically inferior. The worry with predicting known indications with known biology is that your testing performance becomes nearly perfect. However, your novel predictions are not interesting — they would be readily apparent to a pharmacologist.</p>\r\n\r\n<p>The more text mining data you include the larger your gap between testing performance and generative performance (see our <a href=\"http://thinklab.com/discussion/evaluation-framework/47#113\">discussion on evaluation</a>). Therefore, I like the following workflow:</p>\r\n\r\n<ol><li>Start with high-throughput resources that are not affected by knowledge bias (a.k.a. study bias)</li><li>If the algorithm performs significantly better than random, explore the top predictions.</li><li>If performance is mediocre, add a biased resource that provides orthogonal information (information not already included from a systematic resource)</li><li>Repeat.</li></ol>\r\n\r\n<p>One final note to help explain the insidiousness of the knowledge bias. In <a href=\"http://ctdbase.org/\">CTD</a> <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gku935\" class=\"citation\" data-key=\"10.1093/nar/gku935\">1</a>]</span>, curators may read that <code>compound_X</code> treats <code>disease_X</code> and also targets <code>gene_X</code> and therefore add interactions between all three entities to their knowledgebase. In reality the study hasn't proven that <code>gene_X</code> is associated with <code>disease_X</code> but this relationship was still extracted. This happens on a macro-scale across the entire compendium of published literature: specific network vicinities become well studied and the resulting disease-gene-compound triangles are more a result of attention rather than noteworthy biology.</p>",
      "body_md": "> there must be quite a lot of value out there in the space of biased knowledge\r\n\r\nYou are correct. In our current proposal, we do use text mining for the symptom-disease edges and tissue-disease edges. We also rely on literature curation for compound-target binding and protein interactions.\r\n\r\n> Would be great to execute an experiment to empirically test the impact of bringing in prior knowledge\r\n\r\nI agree, we should fit a model that excludes all knowledge-biased domains. I reckon this model's performance on known indications will be drastically inferior. The worry with predicting known indications with known biology is that your testing performance becomes nearly perfect. However, your novel predictions are not interesting -- they would be readily apparent to a pharmacologist.\r\n\r\nThe more text mining data you include the larger your gap between testing performance and generative performance (see our [discussion on evaluation](http://thinklab.com/discussion/evaluation-framework/47#113)). Therefore, I like the following workflow:\r\n\r\n1. Start with high-throughput resources that are not affected by knowledge bias (a.k.a. study bias)\r\n2. If the algorithm performs significantly better than random, explore the top predictions.\r\n3. If performance is mediocre, add a biased resource that provides orthogonal information (information not already included from a systematic resource)\r\n4. Repeat.\r\n\r\nOne final note to help explain the insidiousness of the knowledge bias. In [CTD](http://ctdbase.org/) [@10.1093/nar/gku935], curators may read that `compound_X` treats `disease_X` and also targets `gene_X` and therefore add interactions between all three entities to their knowledgebase. In reality the study hasn't proven that `gene_X` is associated with `disease_X` but this relationship was still extracted. This happens on a macro-scale across the entire compendium of published literature: specific network vicinities become well studied and the resulting disease-gene-compound triangles are more a result of attention rather than noteworthy biology.",
      "profile": 17,
      "published": "2015-04-08T22:22:49.866720Z",
      "thread": 48,
      "url": "/discussion/text-as-a-resource-for-network-population/48#2"
    },
    {
      "body_html": "<h1>Seeking a Slim DO with distinct terms.</h1>\r\n\r\n<p>The Disease Ontology is a hierarchy of human diseases. However, our current method has been designed for <em>distinct</em> nodes (especially with respect to diseases and compounds, since we will be predicting indications). By distinct we mean non-redundant — in a non-redundant set of terms, no terms should be an ancestor or descendant of any other term. </p>\r\n\r\n<p>Additionally, we would like to pick terms at an appropriate level of specificity. Below, I show lineages of DO terms and bold the term whose specificity I prefer:</p>\r\n\r\n<ul><li>cancer &gt; organ system cancer &gt; respiratory system cancer &gt; <strong>lung cancer</strong> &gt; Pancoast tumor &gt; lung superior sulcus carcinoma</li><li>neurodegenerative disease &gt; demyelinating disease &gt; <strong>multiple sclerosis</strong> &gt; relapsing-remitting multiple sclerosis</li><li>glucose metabolism disease &gt; diabetes mellitus &gt; <strong>type 2 diabetes mellitus</strong> &gt; diabetic peripheral angiopathy</li></ul>\r\n\r\n<p>Ideally, we chose a level of specificity such that:</p>\r\n\r\n<ul><li>a term and its descendants form a cohesive and differentiated disease concept</li><li>disease data is collected at a similar level of specificity</li><li>sufficient data exists for the term, after propagating data annotated to more specific terms</li></ul>\r\n\r\n<p>These are competing aims and we will most likely have to make difficult subjective decisions. In the past <span class=\"citation\">[<a href=\"/doi/10.1101/011569\" class=\"citation\" data-key=\"10.1101/011569\">1</a>]</span>, we identified <a href=\"http://het.io/disease-genes/downloads/files/diseases.txt\">108 distinct, complex diseases</a> by investigating only diseases with GWAS. However, GWAS may be too restrictive of a filter for the present study. We are more concerned with omitting diseases that would be poorly connected in the network. Some diseases may be well connected but lacking GWAS. We also would prefer to focus on diseases with indications as indications are needed to train our model.</p>\r\n\r\n<p>Two previous DO studies may be helpful here:</p>\r\n\r\n<ol><li><strong>DOLite</strong> took a data-driven approach to selecting a consolidated set of DO terms <span class=\"citation\">[<a href=\"/doi/10.1093/bioinformatics/btp193\" class=\"citation\" data-key=\"10.1093/bioinformatics/btp193\">2</a>]</span>. I have not been able to locate the list of DO identifiers composing DOLite. These terms may also be obsolete as the project is dated. <em><strong>Any information regarding DOLite would be appreciated.</strong></em></li><li><strong>DO_cancer_slim</strong> created a DO subset named <code>TOPNodes_DOcancerslim</code> composed of 63 non-redundant upper-level cancer terms <span class=\"citation\">[<a href=\"/doi/10.1093/database/bav032\" class=\"citation\" data-key=\"10.1093/database/bav032\">3</a>]</span>. <em><strong>We would like a similar term list but encompassing all diseases, not just cancers.</strong></em></li></ol>",
      "body_md": "# Seeking a Slim DO with distinct terms.\r\n\r\nThe Disease Ontology is a hierarchy of human diseases. However, our current method has been designed for *distinct* nodes (especially with respect to diseases and compounds, since we will be predicting indications). By distinct we mean non-redundant -- in a non-redundant set of terms, no terms should be an ancestor or descendant of any other term. \r\n\r\nAdditionally, we would like to pick terms at an appropriate level of specificity. Below, I show lineages of DO terms and bold the term whose specificity I prefer:\r\n\r\n+ cancer > organ system cancer > respiratory system cancer > **lung cancer** > Pancoast tumor > lung superior sulcus carcinoma\r\n+ neurodegenerative disease > demyelinating disease > **multiple sclerosis** > relapsing-remitting multiple sclerosis\r\n+ glucose metabolism disease > diabetes mellitus > **type 2 diabetes mellitus** > diabetic peripheral angiopathy\r\n\r\nIdeally, we chose a level of specificity such that:\r\n\r\n+ a term and its descendants form a cohesive and differentiated disease concept\r\n+ disease data is collected at a similar level of specificity\r\n+ sufficient data exists for the term, after propagating data annotated to more specific terms\r\n\r\nThese are competing aims and we will most likely have to make difficult subjective decisions. In the past [@10.1101/011569], we identified [108 distinct, complex diseases](http://het.io/disease-genes/downloads/files/diseases.txt) by investigating only diseases with GWAS. However, GWAS may be too restrictive of a filter for the present study. We are more concerned with omitting diseases that would be poorly connected in the network. Some diseases may be well connected but lacking GWAS. We also would prefer to focus on diseases with indications as indications are needed to train our model.\r\n\r\nTwo previous DO studies may be helpful here:\r\n\r\n1. **DOLite** took a data-driven approach to selecting a consolidated set of DO terms [@10.1093/bioinformatics/btp193]. I have not been able to locate the list of DO identifiers composing DOLite. These terms may also be obsolete as the project is dated. _**Any information regarding DOLite would be appreciated.**_\r\n+ **DO_cancer_slim** created a DO subset named `TOPNodes_DOcancerslim` composed of 63 non-redundant upper-level cancer terms [@10.1093/database/bav032]. _**We would like a similar term list but encompassing all diseases, not just cancers.**_",
      "profile": 17,
      "published": "2015-04-09T00:05:43.325198Z",
      "thread": 44,
      "url": "/discussion/unifying-disease-vocabularies/44#3"
    },
    {
      "body_html": "<p>While <a href=\"http://nbviewer.ipython.org/url/git.dhimmel.com/drugbank/unichem-map.ipynb\">mapping</a> DrugBank compounds to LINCS, we have noticed that UniChem's mapping is potentially outdated. UniChem maps to LINCS compound identifiers that begin with <code>LSM-</code> while the current LINCS small molecule identifiers (called <code>pert_id</code>) begin with <code>BRD-</code>.</p>\r\n\r\n<p>UniChem searches that match to LINCS, hyperlink to the <a href=\"http://life.ccs.miami.edu/life/\">LIFE resource</a>, which is not up to date with the released LINCS data (<a href=\"https://www.ebi.ac.uk/unichem/frontpage/results?queryText=ULSMZGGENQYXHL-UHFFFAOYSA-N&amp;kind=InChIKey&amp;sources=&amp;incl=exclude\">example search</a> and <a href=\"http://life.ccs.miami.edu/life/summary?mode=SmallMolecule&amp;source=LINCS&amp;input=LSM-3295\">example link</a>).</p>\r\n\r\n<p>LIFE contains ~9000 small molecules versus &gt;20000 at LINCS and also does not consistently supply the main pert_id used by LINCS. The <code>LINCS ID</code> used by LIFE doesn't appear in the compound information that is currently obtainable from the LINCS API, so it may be an obsolete ID system replaced by pert_id.</p>\r\n\r\n<p>We will contact UniChem to alert them and will solicit feedback from the LINCS team regarding the ID confusion.</p>",
      "body_md": "While [mapping](http://nbviewer.ipython.org/url/git.dhimmel.com/drugbank/unichem-map.ipynb) DrugBank compounds to LINCS, we have noticed that UniChem's mapping is potentially outdated. UniChem maps to LINCS compound identifiers that begin with `LSM-` while the current LINCS small molecule identifiers (called `pert_id`) begin with `BRD-`.\r\n\r\nUniChem searches that match to LINCS, hyperlink to the [LIFE resource](http://life.ccs.miami.edu/life/), which is not up to date with the released LINCS data ([example search](https://www.ebi.ac.uk/unichem/frontpage/results?queryText=ULSMZGGENQYXHL-UHFFFAOYSA-N&kind=InChIKey&sources=&incl=exclude) and [example link](http://life.ccs.miami.edu/life/summary?mode=SmallMolecule&source=LINCS&input=LSM-3295)).\r\n\r\nLIFE contains ~9000 small molecules versus >20000 at LINCS and also does not consistently supply the main pert_id used by LINCS. The `LINCS ID` used by LIFE doesn't appear in the compound information that is currently obtainable from the LINCS API, so it may be an obsolete ID system replaced by pert_id.\r\n\r\nWe will contact UniChem to alert them and will solicit feedback from the LINCS team regarding the ID confusion.",
      "profile": 21,
      "published": "2015-04-09T00:53:23.882384Z",
      "thread": 51,
      "url": "/discussion/unichem-mapping-to-lincs-small-molecules/51"
    },
    {
      "body_html": "<p>Perhaps this explains why 39% of DrugBank approved small molecules <a href=\"http://git.dhimmel.com/drugbank/unichem-map.html\">did not map</a> to a single LINCS compound: the LINCS resource was half it's current size.</p>\r\n\r\n<p>Until this issue is resolved, we have two workarounds:</p>\r\n\r\n<ol><li>Identify LINCS small molecules with their PubChem identifiers, which are provided for most compounds.</li><li>Match LINCS compounds to DrugBank using the provided InChIKeys and UniChem.</li></ol>",
      "body_md": "Perhaps this explains why 39% of DrugBank approved small molecules [did not map](http://git.dhimmel.com/drugbank/unichem-map.html) to a single LINCS compound: the LINCS resource was half it's current size.\r\n\r\nUntil this issue is resolved, we have two workarounds:\r\n\r\n1. Identify LINCS small molecules with their PubChem identifiers, which are provided for most compounds.\r\n2. Match LINCS compounds to DrugBank using the provided InChIKeys and UniChem.",
      "profile": 17,
      "published": "2015-04-09T01:14:34.957461Z",
      "thread": 51,
      "url": "/discussion/unichem-mapping-to-lincs-small-molecules/51#2"
    },
    {
      "body_html": "<blockquote><p>My colleagues and I have worked on multiple approaches to create this knowledge in the papers <span class=\"citation\">[<a href=\"/doi/10.1016/j.jbi.2010.09.009\" class=\"citation\" data-key=\"10.1016/j.jbi.2010.09.009\">1</a>, <a href=\"/doi/10.1136/amiajnl-2012-000852\" class=\"citation\" data-key=\"10.1136/amiajnl-2012-000852\">2</a>, <a href=\"/doi/10.1016/j.jbi.2013.11.010\" class=\"citation\" data-key=\"10.1016/j.jbi.2013.11.010\">3</a>]</span></p></blockquote>\r\n\r\n<p><a href=\"/u/allisonmccoy\" class=\"username\">@allisonmccoy</a>, thanks for the references. I like your approach because it captures what clinicians are actually using to treat diseases (and can provide indication prevalence — what percent of patients with problem X receive medication X). Too bad that the identifiers are local. We would definitely appreciate the validation data when available, especially if it can be mapped to standard terminologies.</p>\r\n\r\n<p>In terms of the mappings from the <a href=\"#104\">aforementioned</a> study <span class=\"citation\">[<a href=\"/doi/10.1136/amiajnl-2012-000852\" class=\"citation\" data-key=\"10.1136/amiajnl-2012-000852\">2</a>]</span>, we still may be able to extract some utility: for example, we could manually map indications for diseases where our indications were lacking. <a href=\"/u/allisonmccoy\" class=\"username\">@allisonmccoy</a>, did any of the other papers you highlighted release data that could add value here?</p>\r\n\r\n<p><a href=\"/u/TIOprea\" class=\"username\">@TIOprea</a> mentioned the difficulty of identifying disease-modifying indications, even in a carefully hand-curated database. <a href=\"/u/allisonmccoy\" class=\"username\">@allisonmccoy</a>, does your method favor disease-modifying links? For example, if modafinil were prescribed to treat MS-induced fatigue, would the clinicians link modafinil to multiple sclerosis or fatigue?</p>",
      "body_md": "> My colleagues and I have worked on multiple approaches to create this knowledge in the papers [@10.1016/j.jbi.2010.09.009 @10.1136/amiajnl-2012-000852 @10.1016/j.jbi.2013.11.010]\r\n\r\n@allisonmccoy, thanks for the references. I like your approach because it captures what clinicians are actually using to treat diseases (and can provide indication prevalence -- what percent of patients with problem X receive medication X). Too bad that the identifiers are local. We would definitely appreciate the validation data when available, especially if it can be mapped to standard terminologies.\r\n\r\nIn terms of the mappings from the [aforementioned](#104) study [@10.1136/amiajnl-2012-000852], we still may be able to extract some utility: for example, we could manually map indications for diseases where our indications were lacking. @allisonmccoy, did any of the other papers you highlighted release data that could add value here?\r\n\r\n@TIOprea mentioned the difficulty of identifying disease-modifying indications, even in a carefully hand-curated database. @allisonmccoy, does your method favor disease-modifying links? For example, if modafinil were prescribed to treat MS-induced fatigue, would the clinicians link modafinil to multiple sclerosis or fatigue?",
      "profile": 17,
      "published": "2015-04-09T01:28:15.012962Z",
      "thread": 21,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#12"
    },
    {
      "body_html": "<blockquote><p>Did any of the other papers you highlighted release data that could add value here?</p></blockquote>\r\n\r\n<p>I don't believe so. I also omitted one more paper that validated the approach in Wright, et al.:</p>\r\n\r\n<ol><li>Wright A, McCoy A, Henkin S, Flaherty M, Sittig D. Validation of an association rule mining-based method to infer associations between medications and problems. Appl Clin Inform. 2013;4(1):100–9. </li></ol>\r\n\r\n<p>The 2nd reference uses RxNorm, SNOMED-CT, and NDF-RT, all of which is freely available, so that knowledge base could easily be regenerated by another party.</p>\r\n\r\n<blockquote><p>Does your method favor disease-modifying links? For example, if modafinil were prescribed to treat MS-induced fatigue, would the clinicians link modafinil to multiple sclerosis or fatigue?</p></blockquote>\r\n\r\n<p>It could be either, but more than likely it would be linked to MS, because that's what would be on the problem list already and easily linked during e-prescribing, but in our evaluation, we would have counted either as correct. We actually had a lot of discussion about this while doing the evaluations, because it did occur frequently.</p>",
      "body_md": "> Did any of the other papers you highlighted release data that could add value here?\r\n\r\nI don't believe so. I also omitted one more paper that validated the approach in Wright, et al.:\r\n\r\n5. Wright A, McCoy A, Henkin S, Flaherty M, Sittig D. Validation of an association rule mining-based method to infer associations between medications and problems. Appl Clin Inform. 2013;4(1):100–9. \r\n\r\nThe 2nd reference uses RxNorm, SNOMED-CT, and NDF-RT, all of which is freely available, so that knowledge base could easily be regenerated by another party.\r\n\r\n> Does your method favor disease-modifying links? For example, if modafinil were prescribed to treat MS-induced fatigue, would the clinicians link modafinil to multiple sclerosis or fatigue?\r\n\r\nIt could be either, but more than likely it would be linked to MS, because that's what would be on the problem list already and easily linked during e-prescribing, but in our evaluation, we would have counted either as correct. We actually had a lot of discussion about this while doing the evaluations, because it did occur frequently.",
      "profile": 77,
      "published": "2015-04-09T01:43:12.201326Z",
      "thread": 21,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#13"
    },
    {
      "body_html": "<p><a href=\"/u/TIOprea\" class=\"username\">@TIOprea</a>, thanks for your insights. You touch on important points. In general our method may not require a perfect indication catalog to succeed, so I am hopeful despite the difficulties you mention. Specifically,</p>\r\n\r\n<blockquote><p>There are diseases in \"indications\" that do NOT exist anywhere else [e.g., cancer XYZ with mutation A3999B, in other words it's not enough to have the disease, you need the right genotype!]</p></blockquote>\r\n\r\n<p>In this case, \"cancer XYZ with mutation A3999B\" would likely not be in the Disease Ontology and if it were would probably lack cross-references. However, if the disease did map to the DO, we would <a href=\"http://thinklab.com/discussion/unifying-disease-vocabularies/44#121\">propagate the indication</a> to \"cancer XYZ\".</p>\r\n\r\n<blockquote><p>you also have to deal with indications that are \"fringe\" (pregnancy is not a disease; neither is contraception)</p></blockquote>\r\n\r\n<p>These indications would not make it into the network because they do not relate to an included disease term. Information loss is ); but we'll get over it (;</p>\r\n\r\n<blockquote><p>indications etc. are not from PubMed - so please pay attention to approved labels</p></blockquote>\r\n\r\n<p>Thanks for the perspective. We won't include these as part of our gold standard.</p>\r\n\r\n<blockquote><p>disease modifying is far from trivial - you need epi to show you that</p></blockquote>\r\n\r\n<p>This I think will be the biggest difficulty. One option could be to exclude drugs that mostly treat symptoms. We noticed that drugs with many indications tended to be of this category. For multiple sclerosis, disease modifying is an <a href=\"http://www.nationalmssociety.org/Treating-MS/Medications\">established concept with currently 12 drugs</a>. Unfortunately, the MS indications we've <a href=\"http://git.dhimmel.com/indications/medi/\">extracted from MEDI</a> and <a href=\"http://git.dhimmel.com/indications/labeledin/\">LabeledIn</a> are predominantly symptomatic. And to make matters worse, for most other diseases the DM status seems much more poorly defined.</p>",
      "body_md": "@TIOprea, thanks for your insights. You touch on important points. In general our method may not require a perfect indication catalog to succeed, so I am hopeful despite the difficulties you mention. Specifically,\r\n\r\n> There are diseases in \"indications\" that do NOT exist anywhere else [e.g., cancer XYZ with mutation A3999B, in other words it's not enough to have the disease, you need the right genotype!]\r\n\r\nIn this case, \"cancer XYZ with mutation A3999B\" would likely not be in the Disease Ontology and if it were would probably lack cross-references. However, if the disease did map to the DO, we would [propagate the indication](http://thinklab.com/discussion/unifying-disease-vocabularies/44#121) to \"cancer XYZ\".\r\n\r\n> you also have to deal with indications that are \"fringe\" (pregnancy is not a disease; neither is contraception)\r\n\r\nThese indications would not make it into the network because they do not relate to an included disease term. Information loss is ); but we'll get over it (;\r\n\r\n> indications etc. are not from PubMed - so please pay attention to approved labels\r\n\r\nThanks for the perspective. We won't include these as part of our gold standard.\r\n\r\n> disease modifying is far from trivial - you need epi to show you that\r\n\r\nThis I think will be the biggest difficulty. One option could be to exclude drugs that mostly treat symptoms. We noticed that drugs with many indications tended to be of this category. For multiple sclerosis, disease modifying is an [established concept with currently 12 drugs](http://www.nationalmssociety.org/Treating-MS/Medications). Unfortunately, the MS indications we've [extracted from MEDI](http://git.dhimmel.com/indications/medi/) and [LabeledIn](http://git.dhimmel.com/indications/labeledin/) are predominantly symptomatic. And to make matters worse, for most other diseases the DM status seems much more poorly defined.",
      "profile": 17,
      "published": "2015-04-09T01:53:53.624696Z",
      "thread": 21,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#14"
    },
    {
      "body_html": "<blockquote><p>The 2nd reference <span class=\"citation\">[<a href=\"http://www.ncbi.nlm.nih.gov/pubmed/22195147\" class=\"citation\" data-key=\"McCoy_2011\">1</a>]</span> uses RxNorm, SNOMED-CT, and NDF-RT, all of which is freely available, so that knowledge base could easily be regenerated by another party.</p></blockquote>\r\n\r\n<p><a href=\"/u/allisonmccoy\" class=\"username\">@allisonmccoy</a>, I believe when MEDI <span class=\"citation\">[<a href=\"/doi/10.1136/amiajnl-2012-001431\" class=\"citation\" data-key=\"10.1136/amiajnl-2012-001431\">2</a>]</span> extracts RxNorm indications, they are taking information from the NDF-RT. My belief is based on that in the introduction they state:</p>\r\n\r\n<blockquote><p>The integration of RxNorm with the National Drug File–Reference Terminology (NDF-RT) from the Veterans Health Administration has added significant indication information between single-ingredient medications and diseases through ‘may_treat’ and ‘may_prevent’ therapeutic relationships. NDF-RT includes both on-label and off-label indications, but its performance on indications has not been previously reported. Preliminary work with earlier versions of RxNorm and NDF-RT demonstrated that a number of medications were lacking indications.</p></blockquote>\r\n\r\n<p>Then in the methods they state:</p>\r\n\r\n<blockquote><p>To obtain indications of a medication from RxNorm, we retrieved all diseases that connect with the medication through either ‘may_be_treated_by’ or ‘may_be_prevented_by’ relationships.</p></blockquote>\r\n\r\n<p>Do you know whether the RxNorm portion of MEDI relied on the same underlying NDF-RT data that you collected for the 2011 AMIA Proceedings Paper <span class=\"citation\">[<a href=\"http://www.ncbi.nlm.nih.gov/pubmed/22195147\" class=\"citation\" data-key=\"McCoy_2011\">1</a>]</span>? </p>\r\n\r\n<p></p>",
      "body_md": "> The 2nd reference [@McCoy_2011] uses RxNorm, SNOMED-CT, and NDF-RT, all of which is freely available, so that knowledge base could easily be regenerated by another party.\r\n\r\n@allisonmccoy, I believe when MEDI [@10.1136/amiajnl-2012-001431] extracts RxNorm indications, they are taking information from the NDF-RT. My belief is based on that in the introduction they state:\r\n\r\n> The integration of RxNorm with the National Drug File–Reference Terminology (NDF-RT) from the Veterans Health Administration has added significant indication information between single-ingredient medications and diseases through ‘may_treat’ and ‘may_prevent’ therapeutic relationships. NDF-RT includes both on-label and off-label indications, but its performance on indications has not been previously reported. Preliminary work with earlier versions of RxNorm and NDF-RT demonstrated that a number of medications were lacking indications.\r\n\r\nThen in the methods they state:\r\n\r\n> To obtain indications of a medication from RxNorm, we retrieved all diseases that connect with the medication through either ‘may_be_treated_by’ or ‘may_be_prevented_by’ relationships.\r\n\r\nDo you know whether the RxNorm portion of MEDI relied on the same underlying NDF-RT data that you collected for the 2011 AMIA Proceedings Paper [@McCoy_2011]? \r\n\r\n[@McCoy_2011]: http://www.ncbi.nlm.nih.gov/pubmed/22195147 \"McCoy AB, Wright A, Laxmisan A, Singh H, Sittig DF. A prototype knowledge base and SMART app to facilitate organization of patient medications by clinical problems. AMIA Annu Symp Proc. 2011;2011:888–94.\"",
      "profile": 17,
      "published": "2015-04-09T02:14:45.918691Z",
      "thread": 21,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#15"
    },
    {
      "body_html": "<blockquote><p>Do you know whether the RxNorm portion of MEDI relied on the same underlying NDF-RT data that you collected for the 2011 AMIA Proceedings Paper [1]?</p></blockquote>\r\n\r\n<p><a href=\"/u/dhimmel\" class=\"username\">@dhimmel</a> We only used the may_treat relationship, but we also took advantage of the is_a hierarchy for problems and ingredient_of relationships between medications and expanded the original set of pairs. So there is some overlap between the two, but likely some pairs that exist in only one or the other.</p>",
      "body_md": "> Do you know whether the RxNorm portion of MEDI relied on the same underlying NDF-RT data that you collected for the 2011 AMIA Proceedings Paper [1]?\r\n\r\n@dhimmel We only used the may_treat relationship, but we also took advantage of the is_a hierarchy for problems and ingredient_of relationships between medications and expanded the original set of pairs. So there is some overlap between the two, but likely some pairs that exist in only one or the other.",
      "profile": 77,
      "published": "2015-04-09T02:18:37.037126Z",
      "thread": 21,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#16"
    },
    {
      "body_html": "<p>One of the edge types we plan to incorporate in our network is that between diseases and symptoms. This data will come from the work of <em>Zhou et al.</em> in their <a href=\"https://dx.doi.org/10.1038/ncomms5212\">\"Human symptoms–disease network\"</a> paper. The supplementary data released by <em>Zhou et al.</em> identifies diseases and symptoms by their MeSH names, but does not include the associated MeSH IDs. To ease interoperability we have performed the minor task of appending the relevant MeSH IDs to these files. The result can be found <a href=\"https://github.com/LABrueggs/HSDN\">here</a>.</p>",
      "body_md": "One of the edge types we plan to incorporate in our network is that between diseases and symptoms. This data will come from the work of *Zhou et al.* in their [\"Human symptoms–disease network\"](https://dx.doi.org/10.1038/ncomms5212) paper. The supplementary data released by *Zhou et al.* identifies diseases and symptoms by their MeSH names, but does not include the associated MeSH IDs. To ease interoperability we have performed the minor task of appending the relevant MeSH IDs to these files. The result can be found [here](https://github.com/LABrueggs/HSDN).",
      "profile": 21,
      "published": "2015-04-09T17:39:42.896099Z",
      "thread": 52,
      "url": "/discussion/human-symptom-disease-network-mesh-id-matching/52"
    },
    {
      "body_html": "<h1>Selecting the top nodes from DO Cancer Slim</h1>\r\n\r\n<p>I emailed <a href=\"http://medschool.umaryland.edu/FACULTYRESEARCHPROFILE/viewprofile.aspx?id=20337\">Lynn Schriml</a>, a lead investigator for the DO, asking about the creation of a consolidated and distinct set of high-level cancer terms. This work was <a href=\"https://dx.doi.org/10.1093/database/bav032\">recently published</a> in <em>Database</em> <span class=\"citation\">[<a href=\"/doi/10.1093/database/bav032\" class=\"citation\" data-key=\"10.1093/database/bav032\">1</a>]</span>. I wrote:</p>\r\n\r\n<blockquote><p>My main question is how did you decide which terms should be included in TopNodes_DOcancerslim? I want to generate a similar top-level term set but for all diseases. Was this a very difficult task that required medical expertise? Do you have any plans to create a DO-wide slim set? (Ideally, I would prefer to rely on an existing effort than to recreate the wheel).</p></blockquote>\r\n\r\n<p><strong>Response by Lynn Schriml</strong> (posted with permission):</p>\r\n\r\n<p>We are not planning on creating a DO wide slim at this time.</p>\r\n\r\n<p>Creating the DO cancer slim and the TOP nodes slim was a couple of months work by a team, including MDs, cancer and disease experts.</p>\r\n\r\n<p>We started with a set of terms from multiple cancer sources that we wanted align. We worked to identify how each term mapped to the current nomenclature of disease (disease names change over time), we then worked to define each term, figure out if the term was represented in DO, and where to place the term in DO, then we added the terms, creating DO definitions. We also reviewed and edited related terms. Once we had the larger set of terms defined, we looked at their parent nodes up to the top node for cancer. </p>\r\n\r\n<p>We wanted to identify body system level parents that reflected both the most specific we could be (not mapping the new TOP node parent all the way up to cancer), and for the TOP node to be biologically informative. There was also much discussion in our work group to finalize these choices. When that was done, we then edited the DO file, adding the new subtypes (slims) and adding each term in (one at a time).</p>",
      "body_md": "# Selecting the top nodes from DO Cancer Slim\r\n\r\nI emailed [Lynn Schriml](http://medschool.umaryland.edu/FACULTYRESEARCHPROFILE/viewprofile.aspx?id=20337), a lead investigator for the DO, asking about the creation of a consolidated and distinct set of high-level cancer terms. This work was [recently published](https://dx.doi.org/10.1093/database/bav032) in *Database* [@10.1093/database/bav032]. I wrote:\r\n\r\n> My main question is how did you decide which terms should be included in TopNodes_DOcancerslim? I want to generate a similar top-level term set but for all diseases. Was this a very difficult task that required medical expertise? Do you have any plans to create a DO-wide slim set? (Ideally, I would prefer to rely on an existing effort than to recreate the wheel).\r\n\r\n**Response by Lynn Schriml** (posted with permission):\r\n\r\nWe are not planning on creating a DO wide slim at this time.\r\n\r\nCreating the DO cancer slim and the TOP nodes slim was a couple of months work by a team, including MDs, cancer and disease experts.\r\n\r\nWe started with a set of terms from multiple cancer sources that we wanted align. We worked to identify how each term mapped to the current nomenclature of disease (disease names change over time), we then worked to define each term, figure out if the term was represented in DO, and where to place the term in DO, then we added the terms, creating DO definitions. We also reviewed and edited related terms. Once we had the larger set of terms defined, we looked at their parent nodes up to the top node for cancer. \r\n\r\nWe wanted to identify body system level parents that reflected both the most specific we could be (not mapping the new TOP node parent all the way up to cancer), and for the TOP node to be biologically informative. There was also much discussion in our work group to finalize these choices. When that was done, we then edited the DO file, adding the new subtypes (slims) and adding each term in (one at a time).",
      "profile": 17,
      "published": "2015-04-10T00:30:43.175427Z",
      "thread": 44,
      "url": "/discussion/unifying-disease-vocabularies/44#4"
    },
    {
      "body_html": "<h1>Reconstructing DO Lite</h1>\r\n\r\n<p>DO Lite is an outdated project which provided a consolidated disease terminology <span class=\"citation\">[<a href=\"/doi/10.1093/bioinformatics/btp193\" class=\"citation\" data-key=\"10.1093/bioinformatics/btp193\">1</a>]</span>. The only data release we could find is a mapping of disease names to implicated genes. Unfortunately, <a href=\"http://django.nubic.northwestern.edu/fundo/media/data/do_lite.txt\">this dataset</a> omits the actual DO identifiers.</p>\r\n\r\n<p>We extracted these disease names found 561 different terms. We <a href=\"http://nbviewer.ipython.org/github/dhimmel/disease-ontology/blob/gh-pages/dolite/dolite.ipynb\">used a text matching paradigm to match</a> these disease names to current DO terms. The paradigm consisted of the following steps:</p>\r\n\r\n<ol><li>creating a mapping of names (including synonyms) to DO identifiers [<a href=\"http://git.dhimmel.com/disease-ontology/data/term-names.tsv\">tsv</a>]</li><li>mapping DOLite names to current DO names (via exact lowercase match) [<a href=\"http://git.dhimmel.com/disease-ontology/dolite/dolite_to_doid.tsv\">tsv</a>]</li></ol>\r\n\r\n<p>A majority (<code>66.3% =  372 / 561</code>) of DOLite terms were matched to a current DO identifier. We may consider using these terms as a reference when manually constructing a DO Slim.</p>",
      "body_md": "# Reconstructing DO Lite\r\n\r\nDO Lite is an outdated project which provided a consolidated disease terminology [@10.1093/bioinformatics/btp193]. The only data release we could find is a mapping of disease names to implicated genes. Unfortunately, [this dataset](http://django.nubic.northwestern.edu/fundo/media/data/do_lite.txt) omits the actual DO identifiers.\r\n\r\nWe extracted these disease names found 561 different terms. We [used a text matching paradigm to match](http://nbviewer.ipython.org/github/dhimmel/disease-ontology/blob/gh-pages/dolite/dolite.ipynb) these disease names to current DO terms. The paradigm consisted of the following steps:\r\n\r\n1. creating a mapping of names (including synonyms) to DO identifiers [[tsv](http://git.dhimmel.com/disease-ontology/data/term-names.tsv)]\r\n2. mapping DOLite names to current DO names (via exact lowercase match) [[tsv](http://git.dhimmel.com/disease-ontology/dolite/dolite_to_doid.tsv)]\r\n\r\nA majority (`66.3% =  372 / 561`) of DOLite terms were matched to a current DO identifier. We may consider using these terms as a reference when manually constructing a DO Slim.",
      "profile": 17,
      "published": "2015-04-10T03:45:06.006320Z",
      "thread": 44,
      "url": "/discussion/unifying-disease-vocabularies/44#5"
    },
    {
      "body_html": "<p>The <a href=\"http://lincs-dcic.org/\">BD2K-LINCS Data Coordination and Integration Center</a>  is part of the Big Data to Knowledge (BD2K) NIH initiative, and it is the data coordination center for the NIH Common Fund's Library of Integrated Network-based Cellular Signatures (LINCS) program, which aims to characterize how a variety of human cells, tissues and the entire organism respond to perturbations by drugs and other molecular factors. </p>\r\n\r\n<p>The <a href=\"http://www.lincsproject.org/centers/bd2k-lincs-dcic/\">BD2K- LINCS DCIC</a> works closely with each <a href=\"http://www.lincsproject.org/centers/data-and-signature-generating-centers/\">LINCS Data and Signature Generation Centers (DSGC)</a>. The DCIC also collaborates with other other organizations and projects like EBI (UniChem, ChEMBL, ChEBI), PubChem.<br>The LINCS Production Phase (LP2) DSGC's are:<br>-  Drug Toxicity Signature Generation Center (Icahn School of Medicine at Mount Sinai)<br>-  HMS LINCS Center (Harvard Medical School)<br>-  LINCS Center for Transcriptomics (Broad Institute)<br>-  LINCS Proteomic Characterization Center for Signaling and Epigenetics (Broad Institute)<br>-  Microenvironment Perturbagen (MEP) LINCS Center (Oregon Health and Science University)<br>-  NeuroLINCS Center (University of California, Irvine)</p>\r\n\r\n<p>The UniChem chemical structure cross-reference is mapped against the standardized LINCS small molecule (LSM).  The DCIC uses a simple schema to combine LINCS and other data into a coherent and computable knowledge framework. The Center develops meta-data standards that enable data integration and representation across the data and signature generation centers (DGSCs). Members of the DCIC are actively developing a next generation integrated web-based platform for the LINCS project that will serve as the foundation for LINCS activities and federate LINCS data, signatures, analysis algorithms, pipelines, APIs and web tools.  </p>\r\n\r\n<p>UniChem cross-references the standardized LINCS Small Molecule ID (LSM ID).  The LSM IDs are mapped to each center's compound and / or sample identifiers.  One example of such Center-specific IDs are identifier with the prefix \"BRD\", which correspond to small molecule compound IDs from the LINCS Center for Transcriptomics (Broad Institute).</p>",
      "body_md": "The [BD2K-LINCS Data Coordination and Integration Center](http://lincs-dcic.org/)  is part of the Big Data to Knowledge (BD2K) NIH initiative, and it is the data coordination center for the NIH Common Fund's Library of Integrated Network-based Cellular Signatures (LINCS) program, which aims to characterize how a variety of human cells, tissues and the entire organism respond to perturbations by drugs and other molecular factors. \r\n  \r\n\r\nThe [BD2K- LINCS DCIC](http://www.lincsproject.org/centers/bd2k-lincs-dcic/) works closely with each [LINCS Data and Signature Generation Centers (DSGC)](http://www.lincsproject.org/centers/data-and-signature-generating-centers/). The DCIC also collaborates with other other organizations and projects like EBI (UniChem, ChEMBL, ChEBI), PubChem.\r\nThe LINCS Production Phase (LP2) DSGC's are:\r\n-  Drug Toxicity Signature Generation Center (Icahn School of Medicine at Mount Sinai)\r\n-  HMS LINCS Center (Harvard Medical School)\r\n-  LINCS Center for Transcriptomics (Broad Institute)\r\n-  LINCS Proteomic Characterization Center for Signaling and Epigenetics (Broad Institute)\r\n-  Microenvironment Perturbagen (MEP) LINCS Center (Oregon Health and Science University)\r\n-  NeuroLINCS Center (University of California, Irvine)\r\n\r\nThe UniChem chemical structure cross-reference is mapped against the standardized LINCS small molecule (LSM).  The DCIC uses a simple schema to combine LINCS and other data into a coherent and computable knowledge framework. The Center develops meta-data standards that enable data integration and representation across the data and signature generation centers (DGSCs). Members of the DCIC are actively developing a next generation integrated web-based platform for the LINCS project that will serve as the foundation for LINCS activities and federate LINCS data, signatures, analysis algorithms, pipelines, APIs and web tools.  \r\n\r\nUniChem cross-references the standardized LINCS Small Molecule ID (LSM ID).  The LSM IDs are mapped to each center's compound and / or sample identifiers.  One example of such Center-specific IDs are identifier with the prefix \"BRD\", which correspond to small molecule compound IDs from the LINCS Center for Transcriptomics (Broad Institute).",
      "profile": 79,
      "published": "2015-04-13T18:44:55.931707Z",
      "thread": 51,
      "url": "/discussion/unichem-mapping-to-lincs-small-molecules/51#3"
    },
    {
      "body_html": "<p>We have chosen to rely on <a href=\"http://www.bindingdb.org/bind/index.jsp\">BindingDB</a> <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkl999\" class=\"citation\" data-key=\"10.1093/nar/gkl999\">1</a>, <a href=\"/doi/10.2174/1386207013330670\" class=\"citation\" data-key=\"10.2174/1386207013330670\">2</a>]</span> for compound–protein binding (ligand–target affinity) information. Their website provides a <a href=\"http://www.bindingdb.org/bind/BindingDB-Intro2a.pdf\">nice background on this topic</a>. BindingDB includes several other databases including ChEMBL <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkr777\" class=\"citation\" data-key=\"10.1093/nar/gkr777\">3</a>, <a href=\"/doi/10.1093/nar/gkt1031\" class=\"citation\" data-key=\"10.1093/nar/gkt1031\">4</a>]</span> and PubChem BioAssay <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkp965\" class=\"citation\" data-key=\"10.1093/nar/gkp965\">5</a>, <a href=\"/doi/10.1093/nar/gkr1132\" class=\"citation\" data-key=\"10.1093/nar/gkr1132\">6</a>, <a href=\"/doi/10.1093/nar/gkt978\" class=\"citation\" data-key=\"10.1093/nar/gkt978\">7</a>]</span> and <a href=\"https://www.bindingdb.org/bind/chemsearch/marvin/SDFdownload.jsp?all_download=yes\">provides</a> well-annotated data in convenient formats.</p>\r\n\r\n<p>We have <a href=\"http://nbviewer.ipython.org/github/dhimmel/bindingdb/blob/gh-pages/process.ipynb\">begun our parsing of BindingDB</a> from the <code>BindingDB_All_2015m3</code> release. We have taken the following steps:</p>\r\n\r\n<ol><li>Some binding experiments refer to multichain protein complexes. However, <code>96% = 1073428 / 1115639</code> of experiments assayed a single chain protein. For simplicity, we decided to omit binding to complexes.</li><li>Of the remaining interactions, the protein targets for <code>20% = 215107 / 1073428 </code> did not map to a SwissProt identifier and were excluded.</li><li>Of the remaining interactions <code>78% = 673750 / 858321</code> were curated with <code>Homo sapiens</code> as the organism. While we will most likely only end up using the human targets, this filtering should naturally occur at a later stage.</li></ol>\r\n\r\n<p>The next step is translating BindingDB compounds to DrugBank compounds. Many BindingDB compounds may match a single DrugBank compound. Additionally multiple experiments may report affinities for the same compound–target pair. There are several affinity metrics (<code>Ki (nM)</code>, <code>IC50 (nM)</code>, <code>Kd (nM)</code>, <code>EC50 (nM)</code>) that are in nanomolar units. We would like to combine all affinities for a DrugBank–EntrezGene pair into a single molarity value (which can then be used as a network inclusion threshold).</p>\r\n\r\n<p>We would like input on how to combine affinities across experiments. Are some molarity measurements more precise? Are certain source databases less error prone? We are looking for simple and rational rules and are willing to accept a healthy dose of reductionism.</p>",
      "body_md": "We have chosen to rely on [BindingDB](http://www.bindingdb.org/bind/index.jsp) [@10.1093/nar/gkl999 @10.2174/1386207013330670] for compound--protein binding (ligand--target affinity) information. Their website provides a [nice background on this topic](http://www.bindingdb.org/bind/BindingDB-Intro2a.pdf). BindingDB includes several other databases including ChEMBL [@10.1093/nar/gkr777 @10.1093/nar/gkt1031] and PubChem BioAssay [@10.1093/nar/gkp965 @10.1093/nar/gkr1132 @10.1093/nar/gkt978] and [provides](https://www.bindingdb.org/bind/chemsearch/marvin/SDFdownload.jsp?all_download=yes) well-annotated data in convenient formats.\r\n\r\nWe have [begun our parsing of BindingDB](http://nbviewer.ipython.org/github/dhimmel/bindingdb/blob/gh-pages/process.ipynb) from the `BindingDB_All_2015m3` release. We have taken the following steps:\r\n\r\n1. Some binding experiments refer to multichain protein complexes. However, `96% = 1073428 / 1115639` of experiments assayed a single chain protein. For simplicity, we decided to omit binding to complexes.\r\n+ Of the remaining interactions, the protein targets for `20% = 215107 / 1073428 ` did not map to a SwissProt identifier and were excluded.\r\n+ Of the remaining interactions `78% = 673750 / 858321` were curated with `Homo sapiens` as the organism. While we will most likely only end up using the human targets, this filtering should naturally occur at a later stage.\r\n\r\nThe next step is translating BindingDB compounds to DrugBank compounds. Many BindingDB compounds may match a single DrugBank compound. Additionally multiple experiments may report affinities for the same compound--target pair. There are several affinity metrics (`Ki (nM)`, `IC50 (nM)`, `Kd (nM)`, `EC50 (nM)`) that are in nanomolar units. We would like to combine all affinities for a DrugBank--EntrezGene pair into a single molarity value (which can then be used as a network inclusion threshold).\r\n \r\nWe would like input on how to combine affinities across experiments. Are some molarity measurements more precise? Are certain source databases less error prone? We are looking for simple and rational rules and are willing to accept a healthy dose of reductionism.",
      "profile": 17,
      "published": "2015-04-13T20:32:22.786364Z",
      "thread": 53,
      "url": "/discussion/integrating-drug-target-information-from-bindingdb/53"
    },
    {
      "body_html": "<p>Hi Daniel,</p>\r\n\r\n<p>Most of the data in bindingDB are 50% inhibitor concentrations (IC50s), inhibition constants (Kis), with occasional dissocation constants (Kds), all in default units of nanomolar (nM).  The IC50 values are regarded as less rigorous measures of binding affinity as they depend to some degree on the association constant of the enzyme substrate used in measuring the IC50 value. The Ki and Kd values should be somewhat more rigorous. If a given compound and protein target have multiple measurements of different types, I'd probably use them in the following order of preference: Kd over Ki over IC50.  That said, most of the data are IC50s, so this case won't arise all that often.  </p>\r\n\r\n<p>Hope this helps!<br>Regards,<br>Mike</p>",
      "body_md": "Hi Daniel,\r\n\r\nMost of the data in bindingDB are 50% inhibitor concentrations (IC50s), inhibition constants (Kis), with occasional dissocation constants (Kds), all in default units of nanomolar (nM).  The IC50 values are regarded as less rigorous measures of binding affinity as they depend to some degree on the association constant of the enzyme substrate used in measuring the IC50 value. The Ki and Kd values should be somewhat more rigorous. If a given compound and protein target have multiple measurements of different types, I'd probably use them in the following order of preference: Kd over Ki over IC50.  That said, most of the data are IC50s, so this case won't arise all that often.  \r\n\r\nHope this helps!\r\nRegards,\r\nMike",
      "profile": 80,
      "published": "2015-04-14T00:34:03.123002Z",
      "thread": 53,
      "url": "/discussion/integrating-drug-target-information-from-bindingdb/53#2"
    },
    {
      "body_html": "<p>The table below categorizes each binding by the affinity measures reported in bindingDB. Some reports include multiple measures. After filtering multichain and unmapped proteins, the number of bindings per category is reported:</p>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th>IC50 (nM)</th><th>Ki (nM)</th><th>Kd (nM)</th><th>EC50 (nM)</th><th>count</th></tr></thead><tbody><tr><td>-</td><td>-</td><td>-</td><td>-</td><td>881</td></tr><tr><td>IC50</td><td>-</td><td>-</td><td>-</td><td>480,070</td></tr><tr><td>-</td><td>Ki</td><td>-</td><td>-</td><td>245,475</td></tr><tr><td>-</td><td>-</td><td>Kd</td><td>-</td><td>55,044</td></tr><tr><td>-</td><td>-</td><td>-</td><td>EC50</td><td>74,549</td></tr><tr><td>IC50</td><td>Ki</td><td>-</td><td>-</td><td>985</td></tr><tr><td>IC50</td><td>-</td><td>Kd</td><td>-</td><td>76</td></tr><tr><td>IC50</td><td>-</td><td>-</td><td>EC50</td><td>631</td></tr><tr><td>-</td><td>Ki</td><td>Kd</td><td>-</td><td>4</td></tr><tr><td>-</td><td>Ki</td><td>-</td><td>EC50</td><td>574</td></tr><tr><td>-</td><td>-</td><td>Kd</td><td>EC50</td><td>8</td></tr><tr><td>IC50</td><td>Ki</td><td>Kd</td><td>-</td><td>1</td></tr><tr><td>IC50</td><td>Ki</td><td>-</td><td>EC50</td><td>23</td></tr></tbody></table>\r\n\r\n<p>Definitions for reference:</p>\r\n\r\n<ul><li>IC50 - half maximal inhibitory concentration</li><li>Kd - dissociation constant</li><li>Ki - inhibitor constant</li><li>EC50 - half maximal effective concentration</li></ul>\r\n\r\n<p><a href=\"/u/mkgilson\" class=\"username\">@mkgilson</a>, thanks! What about EC50s, of which there are 74,549 reports? Can we include this information, and if so where should EC50s fall in the preference order? </p>\r\n\r\n<p>Also is it possible to infer whether a ligand is an agonist or antagonist based on which measures are reported?</p>",
      "body_md": "The table below categorizes each binding by the affinity measures reported in bindingDB. Some reports include multiple measures. After filtering multichain and unmapped proteins, the number of bindings per category is reported:\r\n\r\n| IC50 (nM) | Ki (nM) | Kd (nM) | EC50 (nM) | count |\r\n| - | - | - | - | - |\r\n| - | - | - | - | 881 |\r\n| IC50 | - | - | - | 480,070 |\r\n| - | Ki | - | - | 245,475 |\r\n|- | - | Kd | - | 55,044 |\r\n| - | - | - | EC50 | 74,549 |\r\n| IC50 | Ki | - | - | 985 |\r\n| IC50 | - | Kd | - | 76 |\r\n| IC50 | - | - | EC50 | 631 |\r\n| - | Ki | Kd | - | 4 |\r\n| - | Ki | - | EC50 | 574 |\r\n| - | - | Kd | EC50 | 8 |\r\n| IC50 | Ki | Kd | - | 1 |\r\n| IC50 | Ki | - | EC50 | 23 |\r\n\r\nDefinitions for reference:\r\n\r\n+ IC50 - half maximal inhibitory concentration\r\n+ Kd - dissociation constant\r\n+ Ki - inhibitor constant\r\n+ EC50 - half maximal effective concentration\r\n\r\n@mkgilson, thanks! What about EC50s, of which there are 74,549 reports? Can we include this information, and if so where should EC50s fall in the preference order? \r\n\r\nAlso is it possible to infer whether a ligand is an agonist or antagonist based on which measures are reported?",
      "profile": 17,
      "published": "2015-04-14T16:18:39.208944Z",
      "thread": 53,
      "url": "/discussion/integrating-drug-target-information-from-bindingdb/53#3"
    },
    {
      "body_html": "<p>Hi Dan,</p>\r\n\r\n<p>I'd significantly downweight or ignore the EC50s, as these are at greatest risk of not corresponding to a confirmed binding reaction.  For example, there might be a compound which binds, or is expected to bind,  a particular protein target; but the measurement done is to expose cells to the compound and report the concentration at which it is \"50% effective\" (hence EC50) in producing a biological effect supposedly due to binding of the protein target.</p>\r\n\r\n<p>Unfortunately, agonism/antagonism is not readily available.</p>\r\n\r\n<p>Regards,<br>Mike</p>",
      "body_md": "Hi Dan,\r\n\r\nI'd significantly downweight or ignore the EC50s, as these are at greatest risk of not corresponding to a confirmed binding reaction.  For example, there might be a compound which binds, or is expected to bind,  a particular protein target; but the measurement done is to expose cells to the compound and report the concentration at which it is \"50% effective\" (hence EC50) in producing a biological effect supposedly due to binding of the protein target.\r\n\r\nUnfortunately, agonism/antagonism is not readily available.\r\n\r\nRegards,\r\nMike",
      "profile": 80,
      "published": "2015-04-14T17:29:15.868281Z",
      "thread": 53,
      "url": "/discussion/integrating-drug-target-information-from-bindingdb/53#4"
    },
    {
      "body_html": "<p>The following tables will help you map between LINCS Centers to LINCS Small Molecule ID (LSM):<br><a href=\"http://lincs-dcic.org/metadata/SmallMolecules/LincsID2FacilityID_LINCS_StandardizedCmpds_LSMIDs.txt\">mapping of LINCS Small Molecules to LINCS Facility ID</a><br><a href=\"http://lincs-dcic.org/metadata/SmallMolecules/CompoundTable_LINCS_StandardizedCmpds_LSMIDs.txt\">LINCS compound table</a><br><a href=\"http://lincs-dcic.org/metadata/SmallMolecules/SampleTable_LincsID2FacilityID2CenterBatchID_LINCS_StandardizedCmpds_LSMIDs.txt\">LINCS by SM_Center_Sample_ID</a></p>",
      "body_md": "The following tables will help you map between LINCS Centers to LINCS Small Molecule ID (LSM):\r\n[mapping of LINCS Small Molecules to LINCS Facility ID](http://lincs-dcic.org/metadata/SmallMolecules/LincsID2FacilityID_LINCS_StandardizedCmpds_LSMIDs.txt)\r\n[LINCS compound table](http://lincs-dcic.org/metadata/SmallMolecules/CompoundTable_LINCS_StandardizedCmpds_LSMIDs.txt)\r\n[LINCS by SM_Center_Sample_ID](http://lincs-dcic.org/metadata/SmallMolecules/SampleTable_LincsID2FacilityID2CenterBatchID_LINCS_StandardizedCmpds_LSMIDs.txt)",
      "profile": 79,
      "published": "2015-04-15T19:25:54.569964Z",
      "thread": 51,
      "url": "/discussion/unichem-mapping-to-lincs-small-molecules/51#4"
    },
    {
      "body_html": "<p>The <a href=\"https://sites.google.com/site/bd2klincsdatascience/\">BD2K LINCS Data Science Research webinars</a> serve as a general forum to engage data scientists within and outside of LINCS project to work on problems related to LINCS data analysis and integration. </p>\r\n\r\n<p>The <a href=\"http://lincs-dcic.org/\">BD2K-LINCS DCIC</a> engages the research community by delivering high quality educational materials through the web as well as through mentoring, seminars and symposia. Also, Center investigators actively engage in the education of a new generation of Big Data Scientists by developing a graduate-level Big Data Science MOOC that will be delivered to graduate students in Big Data Biostatistics and other Biomedical Informatics graduate programs.</p>\r\n\r\n<p>See <a href=\"http://lincs-dcic.org/#/training\">BD2K-LINCS DCIC Training and Outreach</a>.</p>",
      "body_md": "The [BD2K LINCS Data Science Research webinars](https://sites.google.com/site/bd2klincsdatascience/) serve as a general forum to engage data scientists within and outside of LINCS project to work on problems related to LINCS data analysis and integration. \r\n\r\nThe [BD2K-LINCS DCIC](http://lincs-dcic.org/) engages the research community by delivering high quality educational materials through the web as well as through mentoring, seminars and symposia. Also, Center investigators actively engage in the education of a new generation of Big Data Scientists by developing a graduate-level Big Data Science MOOC that will be delivered to graduate students in Big Data Biostatistics and other Biomedical Informatics graduate programs.\r\n\r\nSee [BD2K-LINCS DCIC Training and Outreach](http://lincs-dcic.org/#/training).\r\n",
      "profile": 79,
      "published": "2015-04-15T19:32:12.014858Z",
      "thread": 43,
      "url": "/discussion/computing-consensus-transcriptional-profiles-for-lincs-l1000-perturbations/43#2"
    },
    {
      "body_html": "<p><a href=\"/u/cchung\" class=\"username\">@cchung</a>, Thank you for the useful links. Looking into the file you linked to named <a href=\"http://lincs-dcic.org/metadata/SmallMolecules/LincsID2FacilityID_LINCS_StandardizedCmpds_LSMIDs.txt\">mapping of LINCS Small Molecules to LINCS Facility ID</a> (<code>LincsID2FacilityID_LINCS_StandardizedCmpds_LSMIDs.txt</code>), we found that it contained 35,305 <code>BRD-</code> small molecule perturbagen IDs. Separate from this, we have a smaller set of <code>BRD-</code> small molecule perturbagen IDs (20,413) extracted from the <a href=\"http://api.lincscloud.org/\">LINCS L1000 API</a>. In comparing these two sets, we found only 13,796 common <code>BRD-</code> IDs.</p>\r\n\r\n<p>This means that many of the <code>BRD-</code> small molecule perturbagen IDs mapped to <code>LSM</code> IDs do not have transcriptional profiles from Broad. Additionally many compounds that were transcriptionally profiled are not mapped to the <code>LSM</code> ID set.</p>\r\n\r\n<p>Therefore, to integrate <code>BRD</code> compounds, we will rely on the supplied pubchem CIDs, which almost all compounds had.</p>",
      "body_md": "@cchung, Thank you for the useful links. Looking into the file you linked to named [mapping of LINCS Small Molecules to LINCS Facility ID](http://lincs-dcic.org/metadata/SmallMolecules/LincsID2FacilityID_LINCS_StandardizedCmpds_LSMIDs.txt) (`LincsID2FacilityID_LINCS_StandardizedCmpds_LSMIDs.txt`), we found that it contained 35,305 `BRD-` small molecule perturbagen IDs. Separate from this, we have a smaller set of `BRD-` small molecule perturbagen IDs (20,413) extracted from the [LINCS L1000 API](http://api.lincscloud.org/). In comparing these two sets, we found only 13,796 common `BRD-` IDs.\r\n\r\nThis means that many of the `BRD-` small molecule perturbagen IDs mapped to `LSM` IDs do not have transcriptional profiles from Broad. Additionally many compounds that were transcriptionally profiled are not mapped to the `LSM` ID set.\r\n\r\nTherefore, to integrate `BRD` compounds, we will rely on the supplied pubchem CIDs, which almost all compounds had.",
      "profile": 21,
      "published": "2015-04-15T21:48:29.611008Z",
      "thread": 51,
      "url": "/discussion/unichem-mapping-to-lincs-small-molecules/51#5"
    },
    {
      "body_html": "<p>The count difference is because the <a href=\"http://www.lincscloud.org/perturbagens/\">L1000 dataset contains  20,413 small-molecules</a> profiled as part of the LINCS program, the Broad Connectivity Map, NIH efforts such as CDRP, and other projects.  We are standardizing the remaining - will keep you posted on the release!</p>",
      "body_md": "The count difference is because the [L1000 dataset contains  20,413 small-molecules](http://www.lincscloud.org/perturbagens/) profiled as part of the LINCS program, the Broad Connectivity Map, NIH efforts such as CDRP, and other projects.  We are standardizing the remaining - will keep you posted on the release!\r\n",
      "profile": 79,
      "published": "2015-04-16T13:55:58.879377Z",
      "thread": 51,
      "url": "/discussion/unichem-mapping-to-lincs-small-molecules/51#6"
    },
    {
      "body_html": "<p><a href=\"/u/leobrueggeman\" class=\"username\">@leobrueggeman</a> and I attended the online LINCS office hours today. We spoke primarily with Ted who provided some helpful insights and suggestions.</p>\r\n\r\n<ol><li><p><strong>Spearman's correlation</strong> is preferred to the Pearson's correlation when calculating correlations between transcriptional profiles. Spearman's correlation is rank based and therefore less prone to excessive influence of extremes. We will switch from Pearson's to Spearman's correlation in step 2 <a href=\"#\">above</a>.</p></li><li><p><strong>Gold signatures</strong> — Multiple replicates (often 3) are performed for each signature. Higher quality signatures are considered gold (<code>is_gold</code>) based on a heuristic that values reproducibility across replicates and distinctness of that signature. They require the aggregate correlation within replicates to exceed a threshold among other criteria. Around half of the dataset is gold. The informativeness of nongold signatures is dubious, thus Ted suggests restricting to gold signatures.</p></li><li><p><strong>Z-score threshold</strong> — when setting a DEG (deferentially expressed gene) threshold, the LINCS team uses <code>&gt; 2.0</code> or <code>&lt; -2.0</code> and is pleased with the results.</p></li><li><p><strong>Signature number bias in DEG counts</strong> — We found that some perturbagens had an extremely high number of DEGs. We speculate that this occurs for perturbagens with few signatures, since with many signatures across varying contexts, a large set of consistently dysregulated genes seems unlikely. Two possible corrections for this bias are: a) choose a constant number of signatures to include per perturbagen. However, this would lead to information loss, which is undesirable. b) develop an empirical method for adjusting for expected DEG numbers. We will explore option (b) in a future post.</p></li><li><p><strong>Best inferred gene set</strong> — (<code>is_bing</code>) refers to genes that are reliably imputed from the 1000 assayed genes (L1000). We plan to switch to only included the ~7,500 bing genes in our DEG analysis because we would like to avoid unreliable data. This may also help reduce the excess of DEG for certain perturbagens. Whether a gene is part of the bing subset can be retrieved through the <code>geneinfo</code> <a href=\"http://api.lincscloud.org/a2/docs/geneinfo\">API query</a>. </p></li></ol>",
      "body_md": "@leobrueggeman and I attended the online LINCS office hours today. We spoke primarily with Ted who provided some helpful insights and suggestions.\r\n\r\n1. **Spearman's correlation** is preferred to the Pearson's correlation when calculating correlations between transcriptional profiles. Spearman's correlation is rank based and therefore less prone to excessive influence of extremes. We will switch from Pearson's to Spearman's correlation in step 2 [above](#).\r\n\r\n2. **Gold signatures** -- Multiple replicates (often 3) are performed for each signature. Higher quality signatures are considered gold (`is_gold`) based on a heuristic that values reproducibility across replicates and distinctness of that signature. They require the aggregate correlation within replicates to exceed a threshold among other criteria. Around half of the dataset is gold. The informativeness of nongold signatures is dubious, thus Ted suggests restricting to gold signatures.\r\n\r\n3. **Z-score threshold** -- when setting a DEG (deferentially expressed gene) threshold, the LINCS team uses `> 2.0` or `< -2.0` and is pleased with the results.\r\n\r\n4. **Signature number bias in DEG counts** -- We found that some perturbagens had an extremely high number of DEGs. We speculate that this occurs for perturbagens with few signatures, since with many signatures across varying contexts, a large set of consistently dysregulated genes seems unlikely. Two possible corrections for this bias are: a) choose a constant number of signatures to include per perturbagen. However, this would lead to information loss, which is undesirable. b) develop an empirical method for adjusting for expected DEG numbers. We will explore option (b) in a future post.\r\n\r\n5. **Best inferred gene set** -- (`is_bing`) refers to genes that are reliably imputed from the 1000 assayed genes (L1000). We plan to switch to only included the ~7,500 bing genes in our DEG analysis because we would like to avoid unreliable data. This may also help reduce the excess of DEG for certain perturbagens. Whether a gene is part of the bing subset can be retrieved through the `geneinfo` [API query](http://api.lincscloud.org/a2/docs/geneinfo). \r\n",
      "profile": 17,
      "published": "2015-04-16T18:56:14.776626Z",
      "thread": 43,
      "url": "/discussion/computing-consensus-transcriptional-profiles-for-lincs-l1000-perturbations/43#3"
    },
    {
      "body_html": "<h1>BindingDB Processing</h1>\r\n\r\n<p>For our network, we desire binding edges between entrez genes and drugbank compounds. We coerced the bindingDB to conform to our desires using the following steps:</p>\r\n\r\n<h3><a href=\"https://github.com/dhimmel/bindingdb/blob/95aa588c6e553d85f7bd9030956297076f0df5e3/process.ipynb\">Dataset cleanup and tidying</a>:</h3>\r\n\r\n<ol><li>downloading and reading bindingDB</li><li>removing interactions with multichain complexes or without uniprot protein IDs</li><li>converting binding affinities to floats</li><li>retrieving entrez genes corresponding to uniprot proteins (<a href=\"http://git.dhimmel.com/uniprot/data/map/GeneID.tsv.gz\">download mapping</a>)</li><li>gathering data so rows contain only a single affinity measurement, uniprot protein, and entrez gene (<a href=\"https://github.com/dhimmel/bindingdb/blob/95aa588c6e553d85f7bd9030956297076f0df5e3/data/binding.tsv.gz\">download tidied data</a>)</li></ol>\r\n\r\n<h3><a href=\"https://htmlpreview.github.io/?https://github.com/dhimmel/bindingdb/blob/95aa588c6e553d85f7bd9030956297076f0df5e3/collapse.html\">Collapsing bindingDB into compound-gene relationships</a>:</h3>\r\n\r\n<ol><li>restricting to human interactions</li><li>mapping bindingDB compounds to drugbank (<a href=\"http://git.dhimmel.com/drugbank/data/mapping/bindingdb.tsv\">download fuzzy mapping</a>)</li><li>multiple affinities for the same bindingdb–uniprot pairs were resolved by preferentially selecting Kd over Ki over IC50 and taking a geometric mean when there were multiple measurements of the same measure (<a href=\"https://github.com/dhimmel/bindingdb/blob/95aa588c6e553d85f7bd9030956297076f0df5e3/data/bindings-drugbank-collapsed.tsv\">download</a>)</li><li>collapsing into drugbank–gene pairs, taking the minimum affinity reported across grouped bindingdb–uniprot pairs (<a href=\"https://github.com/dhimmel/bindingdb/blob/95aa588c6e553d85f7bd9030956297076f0df5e3/data/bindings-drugbank-gene.tsv\">download</a>)</li></ol>\r\n\r\n<p>The resulting drugbank–gene dataset contained 21,617 interactions. Setting an affinity threshold at 1 micromolar (1000 nanomolar) — a threshold suggested by both <a href=\"/u/mkgilson\" class=\"username\">@mkgilson</a> and <a href=\"/u/alessandrodidonna\" class=\"username\">@alessandrodidonna</a> in conversation — retained ~20% of interactions. After thresholding at 1 micromolar, 890 genes and 1,634 drugbank compounds participated in 5,701 binding interactions.</p>",
      "body_md": "# BindingDB Processing\r\n\r\nFor our network, we desire binding edges between entrez genes and drugbank compounds. We coerced the bindingDB to conform to our desires using the following steps:\r\n\r\n### [Dataset cleanup and tidying](https://github.com/dhimmel/bindingdb/blob/95aa588c6e553d85f7bd9030956297076f0df5e3/process.ipynb):\r\n\r\n1. downloading and reading bindingDB\r\n2. removing interactions with multichain complexes or without uniprot protein IDs\r\n3. converting binding affinities to floats\r\n4. retrieving entrez genes corresponding to uniprot proteins ([download mapping](http://git.dhimmel.com/uniprot/data/map/GeneID.tsv.gz))\r\n5. gathering data so rows contain only a single affinity measurement, uniprot protein, and entrez gene ([download tidied data](https://github.com/dhimmel/bindingdb/blob/95aa588c6e553d85f7bd9030956297076f0df5e3/data/binding.tsv.gz))\r\n\r\n### [Collapsing bindingDB into compound-gene relationships](https://htmlpreview.github.io/?https://github.com/dhimmel/bindingdb/blob/95aa588c6e553d85f7bd9030956297076f0df5e3/collapse.html):\r\n\r\n1. restricting to human interactions\r\n2. mapping bindingDB compounds to drugbank ([download fuzzy mapping](http://git.dhimmel.com/drugbank/data/mapping/bindingdb.tsv))\r\n3. multiple affinities for the same bindingdb--uniprot pairs were resolved by preferentially selecting Kd over Ki over IC50 and taking a geometric mean when there were multiple measurements of the same measure ([download](https://github.com/dhimmel/bindingdb/blob/95aa588c6e553d85f7bd9030956297076f0df5e3/data/bindings-drugbank-collapsed.tsv))\r\n4. collapsing into drugbank--gene pairs, taking the minimum affinity reported across grouped bindingdb--uniprot pairs ([download](https://github.com/dhimmel/bindingdb/blob/95aa588c6e553d85f7bd9030956297076f0df5e3/data/bindings-drugbank-gene.tsv))\r\n\r\nThe resulting drugbank--gene dataset contained 21,617 interactions. Setting an affinity threshold at 1 micromolar (1000 nanomolar) -- a threshold suggested by both @mkgilson and @alessandrodidonna in conversation -- retained ~20% of interactions. After thresholding at 1 micromolar, 890 genes and 1,634 drugbank compounds participated in 5,701 binding interactions.",
      "profile": 17,
      "published": "2015-04-16T21:16:29.833048Z",
      "thread": 53,
      "url": "/discussion/integrating-drug-target-information-from-bindingdb/53#5"
    },
    {
      "body_html": "<h1>Creating a slim DO</h1>\r\n\r\n<p>We created a slim DO with <a href=\"https://github.com/dhimmel/disease-ontology/blob/75050ea2d4f60e745d3f3578ae03560a2cc0e444/data/slim-terms.tsv\" title=\"TSV of DO Slim diseases\">137 terms</a> where:</p>\r\n\r\n<ol><li>no terms were descendants/ancestors of other terms</li><li>terms were specific enough to be clinically relevant</li><li>terms were general enough to be well annotated</li></ol>\r\n\r\n<p>To create this slim term set, we combined the diseases from:</p>\r\n\r\n<ol><li>hetio <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1004259\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1004259\">1</a>]</span> — 108 complex diseases contained in the GWAS Catalog.</li><li>TOPNodes_DOcancerslim <span class=\"citation\">[<a href=\"/doi/10.1093/database/bav032\" class=\"citation\" data-key=\"10.1093/database/bav032\">2</a>]</span> — a body system focused set of 63 cancer terms.</li></ol>\r\n\r\n<p>We found that both sources contained overlapping nodes, and we removed 34 nodes to create a non-overlapping term set. We chose the following rules to resolve overlapping nodes:</p>\r\n\r\n<ol><li>For cancers in TOPNodes_DOcancerslim, retain only the most specific cancer</li><li>Remove hetio terms that descend from TOPNodes_DOcancerslim terms</li><li>For the remaining overlapping hetio terms, choose the term with greater clinical interest or GWAS annotations. For 4 out of the 5 conflicts under this rule, we chose to retain the more general term.</li></ol>\r\n\r\n<h3>Other notes:</h3>\r\n\r\n<p>The repository with our DO analysis is <a href=\"https://github.com/dhimmel/disease-ontology\">here</a> and contains notebooks for <a href=\"https://github.com/dhimmel/disease-ontology/blob/75050ea2d4f60e745d3f3578ae03560a2cc0e444/DO-xrefs.ipynb\">extracting xrefs</a> and <a href=\"https://github.com/dhimmel/disease-ontology/blob/75050ea2d4f60e745d3f3578ae03560a2cc0e444/slim.ipynb\">evaluating our slim DO</a> <span class=\"citation\">[<a href=\"/doi/10.5281/zenodo.45584\" class=\"citation\" data-key=\"10.5281/zenodo.45584\">3</a>]</span>.</p>\r\n\r\n<p>We plan to propagate annotations from more specific terms to our slim terms. To facilitate this process, we created a <a href=\"https://github.com/dhimmel/disease-ontology/blob/75050ea2d4f60e745d3f3578ae03560a2cc0e444/data/xrefs-prop-slim.tsv\">propagated DO slim xref mapping file</a>.</p>\r\n\r\n<p>Pleural cancer (<code>DOID:9917</code>) was a TOPNodes_DOcancerslim term but was not found in the ontology version we downloaded (subversion revision 2810).</p>",
      "body_md": "# Creating a slim DO\r\n\r\nWe created a slim DO with [137 terms](https://github.com/dhimmel/disease-ontology/blob/75050ea2d4f60e745d3f3578ae03560a2cc0e444/data/slim-terms.tsv \"TSV of DO Slim diseases\") where:\r\n\r\n1. no terms were descendants/ancestors of other terms\r\n2. terms were specific enough to be clinically relevant\r\n3. terms were general enough to be well annotated\r\n\r\nTo create this slim term set, we combined the diseases from:\r\n\r\n1. hetio [@10.1371/journal.pcbi.1004259] -- 108 complex diseases contained in the GWAS Catalog.\r\n2. TOPNodes_DOcancerslim [@10.1093/database/bav032] -- a body system focused set of 63 cancer terms.\r\n\r\nWe found that both sources contained overlapping nodes, and we removed 34 nodes to create a non-overlapping term set. We chose the following rules to resolve overlapping nodes:\r\n\r\n1. For cancers in TOPNodes_DOcancerslim, retain only the most specific cancer\r\n2. Remove hetio terms that descend from TOPNodes_DOcancerslim terms\r\n3. For the remaining overlapping hetio terms, choose the term with greater clinical interest or GWAS annotations. For 4 out of the 5 conflicts under this rule, we chose to retain the more general term.\r\n\r\n### Other notes:\r\n\r\nThe repository with our DO analysis is [here](https://github.com/dhimmel/disease-ontology) and contains notebooks for [extracting xrefs](https://github.com/dhimmel/disease-ontology/blob/75050ea2d4f60e745d3f3578ae03560a2cc0e444/DO-xrefs.ipynb) and [evaluating our slim DO](https://github.com/dhimmel/disease-ontology/blob/75050ea2d4f60e745d3f3578ae03560a2cc0e444/slim.ipynb) [@10.5281/zenodo.45584].\r\n\r\nWe plan to propagate annotations from more specific terms to our slim terms. To facilitate this process, we created a [propagated DO slim xref mapping file](https://github.com/dhimmel/disease-ontology/blob/75050ea2d4f60e745d3f3578ae03560a2cc0e444/data/xrefs-prop-slim.tsv).\r\n\r\nPleural cancer (`DOID:9917`) was a TOPNodes_DOcancerslim term but was not found in the ontology version we downloaded (subversion revision 2810).",
      "profile": 17,
      "published": "2015-04-17T18:45:07.422925Z",
      "thread": 44,
      "url": "/discussion/unifying-disease-vocabularies/44#6"
    },
    {
      "body_html": "<p>We mapped the MeSH diseases from the HSDN <span class=\"citation\">[<a href=\"/doi/10.1038/ncomms5212\" class=\"citation\" data-key=\"10.1038/ncomms5212\">1</a>]</span> to <a href=\"http://thinklab.com/discussion/unifying-disease-vocabularies/44#144\">our slim DO</a>. See the <a href=\"https://cdn.rawgit.com/dhimmel/hsdn/51d31d738fe3cad0a12c736b8def333729c3c7f4/index.html\">notebook</a> for more info or download the <a href=\"https://raw.githubusercontent.com/dhimmel/hsdn/51d31d738fe3cad0a12c736b8def333729c3c7f4/data/symptoms-DO.tsv\">mapped data</a>.</p>\r\n\r\n<p>Each disease-symptom relationship includes a <code>tfidf_score</code> (term frequency-inverse document frequency). This score, <span class=\"math\">$$w_{i,j}$$</span>, between symptom <em>i</em> and disease <em>j</em> was calculated with:</p>\r\n\r\n<p></p><div class=\"math\">$$$\r\nw_{i,j} = W_{i,j} \\times \\log{\\frac{N}{n_i}}\r\n$$$</div>\r\n\r\n<p>where <span class=\"math\">$$W_{i,j}$$</span> is the number of co-occurrences in PubMed, <em>N</em> is the total number of diseases, and <span class=\"math\">$$n_i$$</span> is the number of diseases where symptom <em>i</em> appears.</p>\r\n\r\n<p>At some point we will set an inclusion threshold for symptom edges based on their <code>tfidf_score</code>.</p>\r\n\r\n<p>We used a propagated slim DO mapping, so symptoms for MeSH term \"relapsing-remitting multiple sclerosis\" for example were included as symptoms for DO term \"multiple sclerosis\".</p>",
      "body_md": "We mapped the MeSH diseases from the HSDN [@10.1038/ncomms5212] to [our slim DO](http://thinklab.com/discussion/unifying-disease-vocabularies/44#144). See the [notebook](https://cdn.rawgit.com/dhimmel/hsdn/51d31d738fe3cad0a12c736b8def333729c3c7f4/index.html) for more info or download the [mapped data](https://raw.githubusercontent.com/dhimmel/hsdn/51d31d738fe3cad0a12c736b8def333729c3c7f4/data/symptoms-DO.tsv).\r\n\r\nEach disease-symptom relationship includes a `tfidf_score` (term frequency-inverse document frequency). This score, $$w_{i,j}$$, between symptom *i* and disease *j* was calculated with:\r\n\r\n$$$\r\nw_{i,j} = W_{i,j} \\times \\log{\\frac{N}{n_i}}\r\n$$$\r\n\r\nwhere $$W_{i,j}$$ is the number of co-occurrences in PubMed, *N* is the total number of diseases, and $$n_i$$ is the number of diseases where symptom *i* appears.\r\n\r\nAt some point we will set an inclusion threshold for symptom edges based on their `tfidf_score`.\r\n\r\nWe used a propagated slim DO mapping, so symptoms for MeSH term \"relapsing-remitting multiple sclerosis\" for example were included as symptoms for DO term \"multiple sclerosis\".",
      "profile": 17,
      "published": "2015-04-20T16:53:13.382327Z",
      "thread": 52,
      "url": "/discussion/human-symptom-disease-network-mesh-id-matching/52#2"
    },
    {
      "body_html": "<h1>PREDICT Indications</h1>\r\n\r\n<p>An existing computational repurposing approach called PREDICT <span class=\"citation\">[<a href=\"/doi/10.1038/msb.2011.26\" class=\"citation\" data-key=\"10.1038/msb.2011.26\">1</a>]</span>, compiled indications for their analysis. They describe their approach as:</p>\r\n\r\n<blockquote><p>The associations between drugs and UMLS disease concepts were integrated from four different sources using three different methods: (i) direct mapping to drugs, exploiting embedded UMLS links between concepts and drugs; (ii) drug–condition associations downloaded from <a href=\"http://drugs.com\" target=\"_blank\">http://drugs.com</a>, where conditions were mapped to UMLS concepts using MetaMap; and (iii) indication‐based mapping. For the latter, we extracted UMLS concepts using the MetaMap tool from textual drug indications downloaded from FDA package inserts (available in the DailyMed website, <a href=\"http://dailymed.nlm.nih.gov\" target=\"_blank\">http://dailymed.nlm.nih.gov</a>) and DrugBank. In addition, we manually added 44 associations occurring in phase IV (post‐marketing) clinical trials.</p><p>... Finally, performing a manual curation of the extracted UMLS concepts from textual description of drug indications, we observed that they are more prone to false positives. We thus required that associations extracted from drug indications appear also in at least one more source.</p></blockquote>\r\n\r\n<p>Compounds are from DrugBank and diseases are from OMIM and the UMLS, which are both cross-referenced by the DO. The study does not report the precision of their indications making it difficult to assess how the quality compares with MEDI-HPS and LabeledIn.</p>\r\n\r\n<p>We combined the supplementary datasets from the study to create a table of PREDICT indications (<a href=\"http://git.dhimmel.com/indications/msb-predict/\">notebook</a>, <a href=\"http://git.dhimmel.com/indications/msb-predict/data/indications-umls.tsv\">download</a>). We will further investigate including these indications.</p>",
      "body_md": "# PREDICT Indications\r\n\r\nAn existing computational repurposing approach called PREDICT [@10.1038/msb.2011.26], compiled indications for their analysis. They describe their approach as:\r\n\r\n> The associations between drugs and UMLS disease concepts were integrated from four different sources using three different methods: (i) direct mapping to drugs, exploiting embedded UMLS links between concepts and drugs; (ii) drug–condition associations downloaded from http://drugs.com, where conditions were mapped to UMLS concepts using MetaMap; and (iii) indication‐based mapping. For the latter, we extracted UMLS concepts using the MetaMap tool from textual drug indications downloaded from FDA package inserts (available in the DailyMed website, http://dailymed.nlm.nih.gov) and DrugBank. In addition, we manually added 44 associations occurring in phase IV (post‐marketing) clinical trials.\r\n\r\n>... Finally, performing a manual curation of the extracted UMLS concepts from textual description of drug indications, we observed that they are more prone to false positives. We thus required that associations extracted from drug indications appear also in at least one more source.\r\n\r\nCompounds are from DrugBank and diseases are from OMIM and the UMLS, which are both cross-referenced by the DO. The study does not report the precision of their indications making it difficult to assess how the quality compares with MEDI-HPS and LabeledIn.\r\n\r\nWe combined the supplementary datasets from the study to create a table of PREDICT indications ([notebook](http://git.dhimmel.com/indications/msb-predict/), [download](http://git.dhimmel.com/indications/msb-predict/data/indications-umls.tsv)). We will further investigate including these indications.",
      "profile": 17,
      "published": "2015-04-21T18:56:59.333381Z",
      "thread": 21,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#17"
    },
    {
      "body_html": "<h1>Indication Set</h1>\r\n\r\n<p>Now that we have decided which <a href=\"http://thinklab.com/discussion/unifying-disease-vocabularies/44#144\">diseases</a> and <a href=\"http://thinklab.com/discussion/unifying-drug-vocabularies/40\">compounds</a> to include in the network, we can map indications onto these nodes.</p>\r\n\r\n<p>Our indication set contains four indication resources:</p>\r\n\r\n<ol><li>MEDI-HPS — indications from the MEDI's high precision subset</li><li>MEDI-LPS — indications from the MEDI's low precision subset</li><li>LabeledIn — drug label indications extracted by experts or Mechanical Turks</li><li>PREDICT — indications compiled by the PREDICT study</li></ol>\r\n\r\n<p>We anticipate constructing our gold standard of indications from MEDI-HPS, LabeledIn, and PREDICT while omitting MEDI-LPS, which has a lower precision. We did not include ehrlink <span class=\"citation\">[<a href=\"/doi/10.1136/amiajnl-2012-000852\" class=\"citation\" data-key=\"10.1136/amiajnl-2012-000852\">1</a>]</span> because the vocabularies were not mapped. However, we would happily reward anyone who contributes a mapping of <a href=\"https://raw.githubusercontent.com/dhimmel/indications/f7ea9f9d932cf083c68cdb0395b474b215013280/ehrlink/data/problems.tsv\">the problems</a> to the DO and <a href=\"https://raw.githubusercontent.com/dhimmel/indications/f7ea9f9d932cf083c68cdb0395b474b215013280/ehrlink/data/medications.tsv\">the medications</a> to DrugBank.</p>\r\n\r\n<h3>Indication Links</h3>\r\n\r\n<ul><li><a href=\"https://cdn.rawgit.com/dhimmel/indications/f7ea9f9d932cf083c68cdb0395b474b215013280/merge.html\">analysis notebook</a> — includes a searchable table</li><li><a href=\"https://raw.githubusercontent.com/dhimmel/indications/f7ea9f9d932cf083c68cdb0395b474b215013280/data/indications.tsv\">data download</a> — tsv file</li></ul>\r\n\r\n<p>We would still like a way to differentiate disease-modifying from symptomatic indications and will explore manually classifying a subset of indications and training a model.</p>",
      "body_md": "# Indication Set\r\n\r\nNow that we have decided which [diseases](http://thinklab.com/discussion/unifying-disease-vocabularies/44#144) and [compounds](http://thinklab.com/discussion/unifying-drug-vocabularies/40) to include in the network, we can map indications onto these nodes.\r\n\r\nOur indication set contains four indication resources:\r\n\r\n1. MEDI-HPS -- indications from the MEDI's high precision subset\r\n2. MEDI-LPS -- indications from the MEDI's low precision subset\r\n2. LabeledIn -- drug label indications extracted by experts or Mechanical Turks\r\n3. PREDICT -- indications compiled by the PREDICT study\r\n\r\nWe anticipate constructing our gold standard of indications from MEDI-HPS, LabeledIn, and PREDICT while omitting MEDI-LPS, which has a lower precision. We did not include ehrlink [@10.1136/amiajnl-2012-000852] because the vocabularies were not mapped. However, we would happily reward anyone who contributes a mapping of [the problems](https://raw.githubusercontent.com/dhimmel/indications/f7ea9f9d932cf083c68cdb0395b474b215013280/ehrlink/data/problems.tsv) to the DO and [the medications](https://raw.githubusercontent.com/dhimmel/indications/f7ea9f9d932cf083c68cdb0395b474b215013280/ehrlink/data/medications.tsv) to DrugBank.\r\n\r\n### Indication Links\r\n\r\n+ [analysis notebook](https://cdn.rawgit.com/dhimmel/indications/f7ea9f9d932cf083c68cdb0395b474b215013280/merge.html) -- includes a searchable table\r\n+ [data download](https://raw.githubusercontent.com/dhimmel/indications/f7ea9f9d932cf083c68cdb0395b474b215013280/data/indications.tsv) -- tsv file\r\n\r\nWe would still like a way to differentiate disease-modifying from symptomatic indications and will explore manually classifying a subset of indications and training a model.",
      "profile": 17,
      "published": "2015-04-21T21:54:58.762400Z",
      "thread": 21,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#18"
    },
    {
      "body_html": "<p>Evolutionary rate covariation (ERC) assesses whether two genes have a similar evolutionary history. A recent study computed ERC values in humans and found that genes associated with the same disease were often tied together by similar evolutionary histories <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pgen.1004967\" class=\"citation\" data-key=\"10.1371/journal.pgen.1004967\">1</a>]</span>. The study based their gene sets on <a href=\"http://www.omim.org/\">OMIM</a> <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkn665\" class=\"citation\" data-key=\"10.1093/nar/gkn665\">2</a>]</span>, which focuses on Mendelian genetics, so whether ERC prioritizes disease-associated genes for complex diseases in unclear. However, this resource is attractive as an orthogonal, systematic, and unbiased indicator of common gene functionality.</p>\r\n\r\n<p>We began working with the human data from the <a href=\"http://csb.pitt.edu/erc_analysis/Methods.php\">website</a>. First, we parsed the data, converted from a matrix format to a tidy pairwise format, and mapped the UCSC gene ids to Entrez Gene (<a href=\"http://nbviewer.ipython.org/github/dhimmel/erc/blob/c1f19cc70758a4d880881221697f49ab0f41ee10/erc.ipynb\">notebook</a>). Next, we collapsed the values by Entrez Gene pairs (<a href=\"https://github.com/dhimmel/erc/blob/c1f19cc70758a4d880881221697f49ab0f41ee10/entrez-group.R\">code</a>). Almost all UCSC–Entrez mappings were one-to-one, but in the case of many-to-one, we took the average correlation value.</p>\r\n\r\n<p>Our goal is to extract gene pairs that share an evolutionary history. ERC values are provided for all gene pairs with sufficient data, but we are only interested in the small subset of biologically-meaningful correlations. Here we consider using the ERC value as a threshold:</p>\r\n\r\n<h3>Figure 1. Distribution of ERC values</h3>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/dhimmel/erc/c1f19cc70758a4d880881221697f49ab0f41ee10/figure/erc-value-dist.png\" alt=\"\"></p>\r\n\r\n<h3>Figure 2. Probability of positive or negative sign given absolute ERC value</h3>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/dhimmel/erc/c1f19cc70758a4d880881221697f49ab0f41ee10/figure/erc-signed-dist.png\" alt=\"\"></p>\r\n\r\n<p>Assuming that dissimilar evolutionary histories are not present, we can use Figure 2 to select an ERC threshold. Assuming a symmetric null distribution, selecting a threshold of <code>ERC &gt; 0.75</code> would lead to a false discovery rate of approximately 10%. We can also take an empirical approach and optimize the threshold based on performance.</p>\r\n\r\n<p>We would appreciate any community feedback on rational thresholding techniques. Additionally, we may want to consider a separate edge for dissimilar evolutionary history — is that a meaningful concept?</p>",
      "body_md": "Evolutionary rate covariation (ERC) assesses whether two genes have a similar evolutionary history. A recent study computed ERC values in humans and found that genes associated with the same disease were often tied together by similar evolutionary histories [@10.1371/journal.pgen.1004967]. The study based their gene sets on [OMIM](http://www.omim.org/) [@10.1093/nar/gkn665], which focuses on Mendelian genetics, so whether ERC prioritizes disease-associated genes for complex diseases in unclear. However, this resource is attractive as an orthogonal, systematic, and unbiased indicator of common gene functionality.\r\n\r\nWe began working with the human data from the [website](http://csb.pitt.edu/erc_analysis/Methods.php). First, we parsed the data, converted from a matrix format to a tidy pairwise format, and mapped the UCSC gene ids to Entrez Gene ([notebook](http://nbviewer.ipython.org/github/dhimmel/erc/blob/c1f19cc70758a4d880881221697f49ab0f41ee10/erc.ipynb)). Next, we collapsed the values by Entrez Gene pairs ([code](https://github.com/dhimmel/erc/blob/c1f19cc70758a4d880881221697f49ab0f41ee10/entrez-group.R)). Almost all UCSC--Entrez mappings were one-to-one, but in the case of many-to-one, we took the average correlation value.\r\n\r\nOur goal is to extract gene pairs that share an evolutionary history. ERC values are provided for all gene pairs with sufficient data, but we are only interested in the small subset of biologically-meaningful correlations. Here we consider using the ERC value as a threshold:\r\n\r\n### Figure 1. Distribution of ERC values\r\n![](https://raw.githubusercontent.com/dhimmel/erc/c1f19cc70758a4d880881221697f49ab0f41ee10/figure/erc-value-dist.png)\r\n\r\n### Figure 2. Probability of positive or negative sign given absolute ERC value\r\n![](https://raw.githubusercontent.com/dhimmel/erc/c1f19cc70758a4d880881221697f49ab0f41ee10/figure/erc-signed-dist.png)\r\n\r\nAssuming that dissimilar evolutionary histories are not present, we can use Figure 2 to select an ERC threshold. Assuming a symmetric null distribution, selecting a threshold of `ERC > 0.75` would lead to a false discovery rate of approximately 10%. We can also take an empirical approach and optimize the threshold based on performance.\r\n\r\nWe would appreciate any community feedback on rational thresholding techniques. Additionally, we may want to consider a separate edge for dissimilar evolutionary history -- is that a meaningful concept?",
      "profile": 17,
      "published": "2015-04-23T03:38:31.939487Z",
      "thread": 57,
      "url": "/discussion/selecting-informative-erc-evolutionary-rate-covariation-values-between-genes/57"
    },
    {
      "body_html": "<p>The above formula used to calculate <code>tfidf_score</code> adjusts for the frequency of the symptom, but not the frequency of disease. Therefore we speculate that the scores are comparable within but not across diseases. Since we want to adopt a single inclusion threshold for all symptom-disease pairs, we would like to reformulate the metric to adjust for disease frequency.</p>\r\n\r\n<p>We added a <a href=\"https://cdn.rawgit.com/dhimmel/hsdn/af2237af712be4b5319fa3669527e3fa1dbdfe44/index.html\">new visualization and table</a> to investigate a disease-frequency bias. It appears that diseases that occur in more PubMed records have a higher number of symptoms exceeding a given <code>tfidf_score</code>.</p>",
      "body_md": "The above formula used to calculate `tfidf_score` adjusts for the frequency of the symptom, but not the frequency of disease. Therefore we speculate that the scores are comparable within but not across diseases. Since we want to adopt a single inclusion threshold for all symptom-disease pairs, we would like to reformulate the metric to adjust for disease frequency.\r\n\r\nWe added a [new visualization and table](https://cdn.rawgit.com/dhimmel/hsdn/af2237af712be4b5319fa3669527e3fa1dbdfe44/index.html) to investigate a disease-frequency bias. It appears that diseases that occur in more PubMed records have a higher number of symptoms exceeding a given `tfidf_score`.",
      "profile": 17,
      "published": "2015-04-28T22:43:19.909493Z",
      "thread": 52,
      "url": "/discussion/human-symptom-disease-network-mesh-id-matching/52#3"
    },
    {
      "body_html": "<p>Another paper by the same group <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1004120\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1004120\">1</a>]</span> includes what I presume to be a subset of these diseases, so this mapping could be helpful there as well.</p>",
      "body_md": "Another paper by the same group [@10.1371/journal.pcbi.1004120] includes what I presume to be a subset of these diseases, so this mapping could be helpful there as well.\r\n",
      "profile": 17,
      "published": "2015-04-29T00:21:16.730403Z",
      "thread": 42,
      "url": "/discussion/mapping-incomplete-interactome-disease-names-to-mesh/42#2"
    },
    {
      "body_html": "<p>Hi Daniel,</p>\r\n\r\n<p>I spend some time solving your problem about mapping the drug names from one arbitrary system to a known ontology. As a matter of fact RxNorm proposes an <a href=\"http://rxnav.nlm.nih.gov/RxNormAPIs.html#\">API</a> which has an <a href=\"http://rxnav.nlm.nih.gov/RxNormAPIs.html#uLink=RxNorm_REST_getApproximateMatch\">endpoint</a> that can directly be queried for fuzzy matching - so that's useful. It will be helpful to look into the different endpoints of the API down the road, they provide many useful features (though poorly documented).</p>\r\n\r\n<p>I <a href=\"https://github.com/antoine-lizee/RRxNorm\">wrote a script (in R)</a> to match all the medication names in your file and get the related properties of the retrieved Rx concepts. It took an hour + to run because of stalling to avoid going over API quotas. The fuzzy-matching API returns several rxcui matches for <em>each medication</em>. A score, ranging from 0 to 100, is attached to every match.</p>\r\n\r\n<p>The main output file can have several concepts per medication names if (i) there is <strong>ambiguity</strong>, i.e. there is more than one best match for a medication or (ii) the best match is <strong>imperfect</strong>, i.e. the best score is not 100 (then the first three are reported).<br>The <a href=\"https://raw.githubusercontent.com/antoine-lizee/RRxNorm/master/Output/unambiguousMatches.csv\">final output</a> is a subset of this file with only the huge majority of unambiguous hits (and we thus have one concept per medication string).</p>\r\n\r\n<p>Here are some numbers:<br> 1. Only 2353 medications got matched with a valid concept, out of 2537 initially. Some names don't correspond to any medication and are filtered out.<br> 2. From these 2353 matched medications, 2281 (97%) have an unambiguous first match. These are in the final output.<br> 3. These 2281 unambiguous hits match to a total of 2148 different rxcuis.<br> 4. 1490 (63%) medications have at least one perfect match, with 1471 (63%) being unambiguous.<br> 5. These 1471 unambiguous perfect hits match to a total of 1442 different rxcuis.</p>\r\n\r\n<p><strong>QC</strong> is straightforward by <a href=\"https://github.com/antoine-lizee/RRxNorm/blob/master/Output/unambiguousMatchesForQC.csv\">comparing</a> the original medication names and the retrieved name of each matched rxcui. I quickly checked and even the non-perfect matches (with a score different than 100) seems on point, at the exception of the \"therapies\" that have very few equivalents in RxNorm and definitely match to the wrong concept.</p>\r\n\r\n<p>Potential future directions:<br>1. Assess the quality of the matches through a systematic check based on the <a href=\"https://github.com/antoine-lizee/RRxNorm/blob/master/Output/unambiguousMatchesForQC.csv\">QC file</a> mentioned above. <br>2. Enrich the final dataset by resolving ambiguity from the term types reported in the rxcui properties.</p>",
      "body_md": "Hi Daniel,\r\n\r\nI spend some time solving your problem about mapping the drug names from one arbitrary system to a known ontology. As a matter of fact RxNorm proposes an [API](http://rxnav.nlm.nih.gov/RxNormAPIs.html#) which has an [endpoint](http://rxnav.nlm.nih.gov/RxNormAPIs.html#uLink=RxNorm_REST_getApproximateMatch) that can directly be queried for fuzzy matching - so that's useful. It will be helpful to look into the different endpoints of the API down the road, they provide many useful features (though poorly documented).\r\n\r\nI [wrote a script (in R)](https://github.com/antoine-lizee/RRxNorm) to match all the medication names in your file and get the related properties of the retrieved Rx concepts. It took an hour + to run because of stalling to avoid going over API quotas. The fuzzy-matching API returns several rxcui matches for *each medication*. A score, ranging from 0 to 100, is attached to every match.\r\n\r\nThe main output file can have several concepts per medication names if (i) there is **ambiguity**, i.e. there is more than one best match for a medication or (ii) the best match is **imperfect**, i.e. the best score is not 100 (then the first three are reported).\r\nThe [final output](https://raw.githubusercontent.com/antoine-lizee/RRxNorm/master/Output/unambiguousMatches.csv) is a subset of this file with only the huge majority of unambiguous hits (and we thus have one concept per medication string).\r\n\r\nHere are some numbers:\r\n 1. Only 2353 medications got matched with a valid concept, out of 2537 initially. Some names don't correspond to any medication and are filtered out.\r\n 2. From these 2353 matched medications, 2281 (97%) have an unambiguous first match. These are in the final output.\r\n 3. These 2281 unambiguous hits match to a total of 2148 different rxcuis.\r\n 4. 1490 (63%) medications have at least one perfect match, with 1471 (63%) being unambiguous.\r\n 5. These 1471 unambiguous perfect hits match to a total of 1442 different rxcuis.\r\n\r\n**QC** is straightforward by [comparing](https://github.com/antoine-lizee/RRxNorm/blob/master/Output/unambiguousMatchesForQC.csv) the original medication names and the retrieved name of each matched rxcui. I quickly checked and even the non-perfect matches (with a score different than 100) seems on point, at the exception of the \"therapies\" that have very few equivalents in RxNorm and definitely match to the wrong concept.\r\n\r\nPotential future directions:\r\n1. Assess the quality of the matches through a systematic check based on the [QC file](https://github.com/antoine-lizee/RRxNorm/blob/master/Output/unambiguousMatchesForQC.csv) mentioned above. \r\n2. Enrich the final dataset by resolving ambiguity from the term types reported in the rxcui properties.",
      "profile": 23,
      "published": "2015-04-29T08:52:41.447548Z",
      "thread": 21,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#19"
    },
    {
      "body_html": "<p><em>Below, I've copied the <a href=\"http://www.nature.com/ncomms/2014/140626/ncomms5212/extref/ncomms5212-s1.pdf\">supplementary methods section</a> from the HSDN <span class=\"citation\">[<a href=\"/doi/10.1038/ncomms5212\" class=\"citation\" data-key=\"10.1038/ncomms5212\">1</a>]</span> describing how the literature mining was accomplished. I think a similar method could help us if we choose to perform our own <a href=\"http://thinklab.com/discussion/text-as-a-resource-for-network-population/48\">text mining</a> for network population.</em></p>\r\n\r\n<p>We use the Medical Subject Headings (MeSH) <span class=\"citation\">[<a href=\"/doi/10.1001/jama.1994.03510380059038\" class=\"citation\" data-key=\"10.1001/jama.1994.03510380059038\">2</a>]</span> terminology to generate symptom-disease relationships from the metadata extracted from PubMed <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkl1031\" class=\"citation\" data-key=\"10.1093/nar/gkl1031\">3</a>]</span> bibliographic records. PubMed is currently the most comprehensive literature database on biomedical sciences. It includes MEDLINE <span class=\"citation\">[<a href=\"/doi/10.3163/1536-5050.95.4.416\" class=\"citation\" data-key=\"10.3163/1536-5050.95.4.416\">4</a>]</span> and uses MeSH for each citation to facilitate information retrieval. MeSH is a controlled thesaurus that is used for the annotation of published articles, resulting in a high quality representation of their main topics and contributions. The MeSH terms are assigned manually by trained indexers and have been used in numerous biomedical text mining and literature-based discovery studies <span class=\"citation\">[<a href=\"/doi/10.1038/ng895\" class=\"citation\" data-key=\"10.1038/ng895\">5</a>, <a href=\"/doi/10.1093/bioinformatics/btr223\" class=\"citation\" data-key=\"10.1093/bioinformatics/btr223\">6</a>, <a href=\"/doi/10.1002/asi.20438\" class=\"citation\" data-key=\"10.1002/asi.20438\">7</a>, <a href=\"/doi/10.1038/nrg1768\" class=\"citation\" data-key=\"10.1038/nrg1768\">8</a>]</span>.</p>\r\n\r\n<p>We downloaded the <a href=\"http://www.nlm.nih.gov/mesh/2011/download/termscon.html\">2011 ASCII version of MeSH</a> that contains 26,142 distinct terms and their unified identifiers. The MeSH vocabulary is structured as a hierarchical tree with 16 top nodes, representing general categories, such as ‘Anatomy’, ‘Diseases’ and ‘Phenomena and Processes.’ The broad category ‘Diseases’ contains the sub-category ‘Symptoms and Signs’ (MeSH tree code C23.888) that incorporates terms related to clinical manifestations observed by physicians or perceived by patients. We used all terms contained in the ‘Disease’ category (Table S1), excluding ‘Animal diseases’, as well as twenty terms, which only represent unspecific disease information, such as ‘Diseases’ itself, ‘Syndrome’, ‘Chronic diseases’ and ‘Infection’. In total, we obtained 4,442 distinct MeSH disease terms and 327 distinct MeSH symptom terms to be used for the PubMed query. To ensure that we only retrieve records with the corresponding indexed disease terms as a major topic, we search MEDLINE with the constraint “[Majr:NoExp]”, which filters for bibliographic records with the study of a specific disease as a main contribution. Using the E-Utility API web service interface of the National Center for Biotechnology Information, we developed a JAVA program to automatically search all MEDLINE bibliographic records published between 1966 and October 2011 (Figure S4). The total number of corresponding PubMed records was 7,109,429, of which 6,553,494 included a disease and 1,405,038 a symptom term. The number of records that contain both a disease, as well as a symptom term was 849,103. They included all 4,442 MeSH disease terms and almost all (322, i.e. 98%) symptom terms.</p>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/dhimmel/hsdn/ca31229cf7174d8ee22567a455f386412a592144/figure/figS4.png\" alt=\"Figure S4 from the Human symptoms–disease network\"></p>",
      "body_md": "*Below, I've copied the [supplementary methods section](http://www.nature.com/ncomms/2014/140626/ncomms5212/extref/ncomms5212-s1.pdf) from the HSDN [@10.1038/ncomms5212] describing how the literature mining was accomplished. I think a similar method could help us if we choose to perform our own [text mining](http://thinklab.com/discussion/text-as-a-resource-for-network-population/48) for network population.*\r\n\r\nWe use the Medical Subject Headings (MeSH) [@10.1001/jama.1994.03510380059038] terminology to generate symptom-disease relationships from the metadata extracted from PubMed [@10.1093/nar/gkl1031] bibliographic records. PubMed is currently the most comprehensive literature database on biomedical sciences. It includes MEDLINE [@10.3163/1536-5050.95.4.416] and uses MeSH for each citation to facilitate information retrieval. MeSH is a controlled thesaurus that is used for the annotation of published articles, resulting in a high quality representation of their main topics and contributions. The MeSH terms are assigned manually by trained indexers and have been used in numerous biomedical text mining and literature-based discovery studies [@10.1038/ng895 @10.1093/bioinformatics/btr223 @10.1002/asi.20438 @10.1038/nrg1768].\r\n\r\nWe downloaded the [2011 ASCII version of MeSH](http://www.nlm.nih.gov/mesh/2011/download/termscon.html) that contains 26,142 distinct terms and their unified identifiers. The MeSH vocabulary is structured as a hierarchical tree with 16 top nodes, representing general categories, such as ‘Anatomy’, ‘Diseases’ and ‘Phenomena and Processes.’ The broad category ‘Diseases’ contains the sub-category ‘Symptoms and Signs’ (MeSH tree code C23.888) that incorporates terms related to clinical manifestations observed by physicians or perceived by patients. We used all terms contained in the ‘Disease’ category (Table S1), excluding ‘Animal diseases’, as well as twenty terms, which only represent unspecific disease information, such as ‘Diseases’ itself, ‘Syndrome’, ‘Chronic diseases’ and ‘Infection’. In total, we obtained 4,442 distinct MeSH disease terms and 327 distinct MeSH symptom terms to be used for the PubMed query. To ensure that we only retrieve records with the corresponding indexed disease terms as a major topic, we search MEDLINE with the constraint “[Majr:NoExp]”, which filters for bibliographic records with the study of a specific disease as a main contribution. Using the E-Utility API web service interface of the National Center for Biotechnology Information, we developed a JAVA program to automatically search all MEDLINE bibliographic records published between 1966 and October 2011 (Figure S4). The total number of corresponding PubMed records was 7,109,429, of which 6,553,494 included a disease and 1,405,038 a symptom term. The number of records that contain both a disease, as well as a symptom term was 849,103. They included all 4,442 MeSH disease terms and almost all (322, i.e. 98%) symptom terms.\r\n\r\n![Figure S4 from the Human symptoms–disease network](https://raw.githubusercontent.com/dhimmel/hsdn/ca31229cf7174d8ee22567a455f386412a592144/figure/figS4.png)",
      "profile": 17,
      "published": "2015-04-30T20:51:53.215222Z",
      "thread": 52,
      "url": "/discussion/human-symptom-disease-network-mesh-id-matching/52#4"
    },
    {
      "body_html": "<p>We would like to convert from RxNorm medications to their ingredients. RxNorm is an ontology, with terms connected by <a href=\"http://www.nlm.nih.gov/research/umls/rxnorm/docs/2015/appendix1.html\">relationship types</a>, which can be traversed to map ingredients. This is necessary to <a href=\"http://thinklab.com/discussion/unifying-drug-vocabularies/40\">map RxNorm medications to DrugBank</a>, enabling network inclusion. Additionally, we need to know when medications map to multiple ingredients as we are excluding combination therapies for the time being. Once the mapping is complete, we can proceed with <a href=\"http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#169\">integrating the ehrlink resource</a>. This discussion will follow our attempts to map concepts to ingredients.</p>",
      "body_md": "We would like to convert from RxNorm medications to their ingredients. RxNorm is an ontology, with terms connected by [relationship types](http://www.nlm.nih.gov/research/umls/rxnorm/docs/2015/appendix1.html), which can be traversed to map ingredients. This is necessary to [map RxNorm medications to DrugBank](http://thinklab.com/discussion/unifying-drug-vocabularies/40), enabling network inclusion. Additionally, we need to know when medications map to multiple ingredients as we are excluding combination therapies for the time being. Once the mapping is complete, we can proceed with [integrating the ehrlink resource](http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#169). This discussion will follow our attempts to map concepts to ingredients.",
      "profile": 17,
      "published": "2015-04-30T22:03:36.193097Z",
      "thread": 61,
      "url": "/discussion/retrieving-the-ingredients-in-rxnorm-concepts/61"
    },
    {
      "body_html": "<h1>RxNorm term types</h1>\r\n\r\n<p>RxNorm concepts each have a specified term type (<code>TTY</code>). RxNorm documentation is oftentimes difficult to navigate, so we provide definitions for <a href=\"http://rxnav.nlm.nih.gov/RxNormAPIs.html#uLink=RxNorm_REST_getTermTypes\">all term types</a> below. The descriptions are from <span class=\"citation\">[<a href=\"/doi/10.1136/amiajnl-2011-000116\" class=\"citation\" data-key=\"10.1136/amiajnl-2011-000116\">1</a>]</span>, while definitions were found <a href=\"http://rxnav.nlm.nih.gov/RxNavViews.html\">here</a>.</p>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th><code>TTY</code></th><th>Definition</th><th>Description</th></tr></thead><tbody><tr><td><code>BN</code></td><td>brand name</td><td></td></tr><tr><td><code>BPCK</code></td><td>branded pack</td><td></td></tr><tr><td><code>DF</code></td><td>dose form</td><td></td></tr><tr><td><code>DFG</code></td><td>dose form group</td><td></td></tr><tr><td><code>GPCK</code></td><td>generic pack</td><td></td></tr><tr><td><code>IN</code></td><td>ingredient</td><td>The term type (TTY) indicating that this name is that of the substance represented in an RxNorm name responsible for the medicinal activity. Also, the name and the substance.</td></tr><tr><td><code>MIN</code></td><td>multiple ingredients</td><td>The TTY indicating that this name is that of the ingredients of a combination product represented in an RxNorm name, where those ingredients are responsible for the medicinal activity. Also, the name and the substances.</td></tr><tr><td><code>PIN</code></td><td>precise ingredient</td><td>The TTY indicating that this name is that of the substance, expressed more precisely as a salt or ester of the ingredient, represented in an RxNorm name. Also, the name and the substance expressed precisely.</td></tr><tr><td><code>SBD</code></td><td>branded drug</td><td>The TTY indicating that this name is the normalized name created for a branded clinical drug. The name consists of ingredient, strength, and dose form, followed by a brand name in square brackets. Also, the name and the product.</td></tr><tr><td><code>SBDC</code></td><td>branded drug component</td><td></td></tr><tr><td><code>SBDF</code></td><td>branded dose form</td><td></td></tr><tr><td><code>SBDG</code></td><td>branded dose form group</td><td></td></tr><tr><td><code>SCD</code></td><td>clinical drug</td><td></td></tr><tr><td><code>SCDC</code></td><td>clinical drug component</td><td></td></tr><tr><td><code>SCDF</code></td><td>clinical dose form</td><td></td></tr><tr><td><code>SCDG</code></td><td>clinical dose form group</td><td></td></tr></tbody></table>",
      "body_md": "# RxNorm term types\r\n\r\nRxNorm concepts each have a specified term type (`TTY`). RxNorm documentation is oftentimes difficult to navigate, so we provide definitions for [all term types](http://rxnav.nlm.nih.gov/RxNormAPIs.html#uLink=RxNorm_REST_getTermTypes) below. The descriptions are from [@10.1136/amiajnl-2011-000116], while definitions were found [here](http://rxnav.nlm.nih.gov/RxNavViews.html).\r\n\r\n| `TTY` | Definition | Description |\r\n|--------|----------|----------------|\r\n| `BN` | brand name |  |\r\n| `BPCK` | branded pack |  |\r\n| `DF` | dose form |  |\r\n| `DFG` | dose form group |  |\r\n| `GPCK` | generic pack |  |\r\n| `IN` | ingredient | The term type (TTY) indicating that this name is that of the substance represented in an RxNorm name responsible for the medicinal activity. Also, the name and the substance. |\r\n| `MIN` | multiple ingredients | The TTY indicating that this name is that of the ingredients of a combination product represented in an RxNorm name, where those ingredients are responsible for the medicinal activity. Also, the name and the substances. |\r\n| `PIN` | precise ingredient | The TTY indicating that this name is that of the substance, expressed more precisely as a salt or ester of the ingredient, represented in an RxNorm name. Also, the name and the substance expressed precisely. |\r\n| `SBD` | branded drug | The TTY indicating that this name is the normalized name created for a branded clinical drug. The name consists of ingredient, strength, and dose form, followed by a brand name in square brackets. Also, the name and the product. |\r\n| `SBDC` | branded drug component |  |\r\n| `SBDF` | branded dose form |  |\r\n| `SBDG` | branded dose form group |  |\r\n| `SCD` | clinical drug |  |\r\n| `SCDC` | clinical drug component |  |\r\n| `SCDF` | clinical dose form |  |\r\n| `SCDG` | clinical dose form group |  |",
      "profile": 17,
      "published": "2015-04-30T22:04:16.067751Z",
      "thread": 61,
      "url": "/discussion/retrieving-the-ingredients-in-rxnorm-concepts/61#2"
    },
    {
      "body_html": "<h1>RxNorm API method</h1>\r\n\r\n<p>We found a method to retrieve ingredients using the <code>allrelated</code> <a href=\"http://rxnav.nlm.nih.gov/RxNormAPIs.html#uLink=RxNorm_REST_getAllRelatedInfo\">RxNorm API command</a>. An example query for rxcui <code>198440</code> looks like:</p>\r\n\r\n<p><code>http://rxnav.nlm.nih.gov/REST/rxcui/198440/allrelated</code></p>\r\n\r\n<p>Ingredients can be extracted from the returned xml with the following XPath query:</p>\r\n\r\n<p><code>./allRelatedGroup/conceptGroup[tty='IN']/conceptProperties</code></p>\r\n\r\n<p>We wrote a <a href=\"http://nbviewer.ipython.org/github/dhimmel/indications/blob/aa2405b1015be580a5452d5e318f8d3e72468713/ehrlink/rxnorm-ingredient-map.ipynb\">python script</a> to perform this operation on the ehrlink <span class=\"citation\">[<a href=\"/doi/10.1136/amiajnl-2012-000852\" class=\"citation\" data-key=\"10.1136/amiajnl-2012-000852\">1</a>]</span> RxNorm medications <a href=\"http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#169\">mapped</a> by <a href=\"/u/alizee\" class=\"username\">@alizee</a>. The resulting ingredient map is available for <a href=\"https://raw.githubusercontent.com/dhimmel/indications/aa2405b1015be580a5452d5e318f8d3e72468713/ehrlink/data/rxnorm-as-ingredient.tsv\">download</a>.</p>",
      "body_md": "# RxNorm API method\r\n\r\nWe found a method to retrieve ingredients using the `allrelated` [RxNorm API command](http://rxnav.nlm.nih.gov/RxNormAPIs.html#uLink=RxNorm_REST_getAllRelatedInfo). An example query for rxcui `198440` looks like:\r\n\r\n`http://rxnav.nlm.nih.gov/REST/rxcui/198440/allrelated`\r\n\r\nIngredients can be extracted from the returned xml with the following XPath query:\r\n\r\n`./allRelatedGroup/conceptGroup[tty='IN']/conceptProperties`\r\n\r\nWe wrote a [python script](http://nbviewer.ipython.org/github/dhimmel/indications/blob/aa2405b1015be580a5452d5e318f8d3e72468713/ehrlink/rxnorm-ingredient-map.ipynb) to perform this operation on the ehrlink [@10.1136/amiajnl-2012-000852] RxNorm medications [mapped](http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#169) by @alizee. The resulting ingredient map is available for [download](https://raw.githubusercontent.com/dhimmel/indications/aa2405b1015be580a5452d5e318f8d3e72468713/ehrlink/data/rxnorm-as-ingredient.tsv).",
      "profile": 17,
      "published": "2015-05-01T04:06:57.676423Z",
      "thread": 61,
      "url": "/discussion/retrieving-the-ingredients-in-rxnorm-concepts/61#3"
    },
    {
      "body_html": "<p>ehrlink is our name for a study where an EHR system prompted clinicians to report the problem that a medication was prescribed for <span class=\"citation\">[<a href=\"/doi/10.1136/amiajnl-2012-000852\" class=\"citation\" data-key=\"10.1136/amiajnl-2012-000852\">1</a>]</span>. The resulting high-confidence set contained 11,166 problem-medication pairs with precision exceeding 95%. Thus far, the comments pertaining to ehrlink have been scattered, so this discussion is meant to consolidate and provide a home for further analysis.</p>\r\n\r\n<p>Here is the history of this <em>collaborative integration effort</em>:</p>\r\n\r\n<ol><li><a href=\"/u/b_good\" class=\"username\">@b_good</a> initially <a href=\"//thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#101\">suggested</a> the resource and located the data supplement.</li><li><a href=\"/u/dhimmel\" class=\"username\">@dhimmel</a> converted the pdf data supplement to a tsv file (<a href=\"//thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#101\">comment</a>, <a href=\"http://nbviewer.ipython.org/github/dhimmel/indications/blob/gh-pages/ehrlink/convert-to-text.ipynb\">notebook</a>, <a href=\"//git.dhimmel.com/indications/ehrlink/download/amiajnl-2012-000852-s1.tsv\">download</a>).</li><li><a href=\"/u/dhimmel\" class=\"username\">@dhimmel</a> <a href=\"http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#111\">determined</a> the identifiers were not from a standard terminology</li><li><a href=\"/u/allisonmccoy\" class=\"username\">@allisonmccoy</a> <a href=\"http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#116\">joined</a> the discussion, confirming the proprietary identifiers and providing additional related studies.</li><li><a href=\"/u/allisonmccoy\" class=\"username\">@allisonmccoy</a> and <a href=\"/u/TIOprea\" class=\"username\">@TIOprea</a> <a href=\"http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#119\">discussed</a> the reliability of the resource.</li><li><a href=\"/u/alizee\" class=\"username\">@alizee</a> mapped the medication terms from ehrlink to RxNorm (<a href=\"http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#169\">comment</a>, <a href=\"https://github.com/antoine-lizee/RRxNorm\">repository</a>).</li><li><a href=\"/u/dhimmel\" class=\"username\">@dhimmel</a> mapped the RxNorm concepts matched by <a href=\"/u/alizee\" class=\"username\">@alizee</a> to RxNorm ingredients (<a href=\"//thinklab.com/discussion/retrieving-the-ingredients-in-rxnorm-concepts/61#179\">comment</a>, <a href=\"http://nbviewer.ipython.org/github/dhimmel/indications/blob/aa2405b1015be580a5452d5e318f8d3e72468713/ehrlink/rxnorm-ingredient-map.ipynb\">notebook</a>, <a href=\"https://raw.githubusercontent.com/dhimmel/indications/aa2405b1015be580a5452d5e318f8d3e72468713/ehrlink/data/rxnorm-as-ingredient.tsv\">download</a>).</li></ol>",
      "body_md": "ehrlink is our name for a study where an EHR system prompted clinicians to report the problem that a medication was prescribed for [@10.1136/amiajnl-2012-000852]. The resulting high-confidence set contained 11,166 problem-medication pairs with precision exceeding 95%. Thus far, the comments pertaining to ehrlink have been scattered, so this discussion is meant to consolidate and provide a home for further analysis.\r\n\r\nHere is the history of this *collaborative integration effort*:\r\n\r\n1. @b_good initially [suggested](//thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#101) the resource and located the data supplement.\r\n+ @dhimmel converted the pdf data supplement to a tsv file ([comment](//thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#101), [notebook](http://nbviewer.ipython.org/github/dhimmel/indications/blob/gh-pages/ehrlink/convert-to-text.ipynb), [download](//git.dhimmel.com/indications/ehrlink/download/amiajnl-2012-000852-s1.tsv)).\r\n+ @dhimmel [determined](http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#111) the identifiers were not from a standard terminology\r\n+ @allisonmccoy [joined](http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#116) the discussion, confirming the proprietary identifiers and providing additional related studies.\r\n+ @allisonmccoy and @TIOprea [discussed](http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#119) the reliability of the resource.\r\n+ @alizee mapped the medication terms from ehrlink to RxNorm ([comment](http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#169), [repository](https://github.com/antoine-lizee/RRxNorm)).\r\n+ @dhimmel mapped the RxNorm concepts matched by @alizee to RxNorm ingredients ([comment](//thinklab.com/discussion/retrieving-the-ingredients-in-rxnorm-concepts/61#179), [notebook](http://nbviewer.ipython.org/github/dhimmel/indications/blob/aa2405b1015be580a5452d5e318f8d3e72468713/ehrlink/rxnorm-ingredient-map.ipynb), [download](https://raw.githubusercontent.com/dhimmel/indications/aa2405b1015be580a5452d5e318f8d3e72468713/ehrlink/data/rxnorm-as-ingredient.tsv)).",
      "profile": 17,
      "published": "2015-05-01T14:51:24.555126Z",
      "thread": 62,
      "url": "/discussion/extracting-indications-from-the-ehrlink-resource/62"
    },
    {
      "body_html": "<p>To go further on resolving ambiguities when retrieving concepts, I had to look up abbreviations of the TTY too. I had seen <a href=\"http://rxnav.nlm.nih.gov/RxNavViews.html\">the very useful link you mention above</a> from the RxNav documentation, but I finally used the more general ressource that is the MetaThesaurus of UMLS. <a href=\"http://www.nlm.nih.gov/research/umls/knowledge_sources/metathesaurus/release/abbreviations.html#mrdoc_TTY\">This page</a> lists all the abbreviations used in the Rx system.</p>\r\n\r\n<p>I <a href=\"https://github.com/antoine-lizee/RRxNorm/blob/master/getTTYs.R\">extracted</a> from this page the table with all the TTY abbreviations into a reusable <a href=\"https://github.com/antoine-lizee/RRxNorm/blob/master/Output/TTYInSourceAbbreviations.csv\">csv file</a>. It could be useful down the road to other people.</p>",
      "body_md": "To go further on resolving ambiguities when retrieving concepts, I had to look up abbreviations of the TTY too. I had seen [the very useful link you mention above](http://rxnav.nlm.nih.gov/RxNavViews.html) from the RxNav documentation, but I finally used the more general ressource that is the MetaThesaurus of UMLS. [This page](http://www.nlm.nih.gov/research/umls/knowledge_sources/metathesaurus/release/abbreviations.html#mrdoc_TTY) lists all the abbreviations used in the Rx system.\r\n\r\nI [extracted](https://github.com/antoine-lizee/RRxNorm/blob/master/getTTYs.R) from this page the table with all the TTY abbreviations into a reusable [csv file](https://github.com/antoine-lizee/RRxNorm/blob/master/Output/TTYInSourceAbbreviations.csv). It could be useful down the road to other people.",
      "profile": 23,
      "published": "2015-05-01T20:00:14.399789Z",
      "thread": 61,
      "url": "/discussion/retrieving-the-ingredients-in-rxnorm-concepts/61#4"
    },
    {
      "body_html": "<h1>Mapping ehrlink diseases to the DO</h1>\r\n\r\n<p>The ehrlink high-confidence set contains indications for 1,596 problems (<a href=\"https://raw.githubusercontent.com/dhimmel/indications/968bd8c947fe045a56caefb9f08182dea6e20662/ehrlink/data/problems.tsv\">download</a>). We used a simplistic string matching scheme to map these terms to the disease ontology. Lowercase ehrlink problem names were matched to lowercase DO names and synonyms (<a href=\"http://nbviewer.ipython.org/github/dhimmel/indications/blob/968bd8c947fe045a56caefb9f08182dea6e20662/ehrlink/problem-map.ipynb\">notebook</a>, <a href=\"https://raw.githubusercontent.com/dhimmel/indications/968bd8c947fe045a56caefb9f08182dea6e20662/ehrlink/data/problem-to-doid.tsv\">results</a>).</p>\r\n\r\n<p><code>22.9% = 365 / 1596</code> of the ehrlink problems mapped to the disease ontology. Of the 137 DO slim terms, 50 had a matching ehrlink problem. When we include propagated matching to DO slim terms, 5 additional diseases get matched. While these recall numbers appear low, we do recover a decent extent of the major complex diseases with few to no false positives.</p>",
      "body_md": "# Mapping ehrlink diseases to the DO\r\n\r\nThe ehrlink high-confidence set contains indications for 1,596 problems ([download](https://raw.githubusercontent.com/dhimmel/indications/968bd8c947fe045a56caefb9f08182dea6e20662/ehrlink/data/problems.tsv)). We used a simplistic string matching scheme to map these terms to the disease ontology. Lowercase ehrlink problem names were matched to lowercase DO names and synonyms ([notebook](http://nbviewer.ipython.org/github/dhimmel/indications/blob/968bd8c947fe045a56caefb9f08182dea6e20662/ehrlink/problem-map.ipynb), [results](https://raw.githubusercontent.com/dhimmel/indications/968bd8c947fe045a56caefb9f08182dea6e20662/ehrlink/data/problem-to-doid.tsv)).\r\n\r\n`22.9% = 365 / 1596` of the ehrlink problems mapped to the disease ontology. Of the 137 DO slim terms, 50 had a matching ehrlink problem. When we include propagated matching to DO slim terms, 5 additional diseases get matched. While these recall numbers appear low, we do recover a decent extent of the major complex diseases with few to no false positives.",
      "profile": 17,
      "published": "2015-05-01T22:18:31.017689Z",
      "thread": 62,
      "url": "/discussion/extracting-indications-from-the-ehrlink-resource/62#2"
    },
    {
      "body_html": "<p>UPDATE:<br>I went forward on resolving the ambiguity, using the term source in type, and then the number of \"atoms\" that matches each medication name. </p>\r\n\r\n<p>This brings down the number of remaining ambiguity from 72 to 11 medications (0.5%).</p>\r\n\r\n<p>I understand you want to extract the ingredients from these concepts, so it doesn't necessarily matter that there are two \"top\" matches for one medication after trying to resolve ambiguity (both will likely lead to the same components). As a result I created both the file for the <a href=\"https://raw.githubusercontent.com/antoine-lizee/RRxNorm/master/Output/resolvedMatches.csv\">successfully resolved matches</a>, and the <a href=\"https://raw.githubusercontent.com/antoine-lizee/RRxNorm/master/Output/allResolvedMatches.csv\">file for all the best matches after trying to resolve them</a>. The latter has 100% of the medications, including the 11 ambiguous, for which I took arbitrarily one of the top concepts. This is the file you'll want to work off in the future.</p>\r\n\r\n<p>I also created the <a href=\"https://github.com/antoine-lizee/RRxNorm/blob/master/Output/resolvedMatchesForQC.csv\">QC file</a> for the ambiguity resolution step.</p>",
      "body_md": "UPDATE:\r\nI went forward on resolving the ambiguity, using the term source in type, and then the number of \"atoms\" that matches each medication name. \r\n\r\nThis brings down the number of remaining ambiguity from 72 to 11 medications (0.5%).\r\n\r\nI understand you want to extract the ingredients from these concepts, so it doesn't necessarily matter that there are two \"top\" matches for one medication after trying to resolve ambiguity (both will likely lead to the same components). As a result I created both the file for the [successfully resolved matches](https://raw.githubusercontent.com/antoine-lizee/RRxNorm/master/Output/resolvedMatches.csv), and the [file for all the best matches after trying to resolve them](https://raw.githubusercontent.com/antoine-lizee/RRxNorm/master/Output/allResolvedMatches.csv). The latter has 100% of the medications, including the 11 ambiguous, for which I took arbitrarily one of the top concepts. This is the file you'll want to work off in the future.\r\n\r\nI also created the [QC file](https://github.com/antoine-lizee/RRxNorm/blob/master/Output/resolvedMatchesForQC.csv) for the ambiguity resolution step.",
      "profile": 23,
      "published": "2015-05-03T20:54:06.377956Z",
      "thread": 21,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#20"
    },
    {
      "body_html": "<h1>Mapping ehrlink to DO and RxNorm ingredient terms</h1>\r\n\r\n<p>We created a version of ehrlink with the subset problem-medication pairs that mapped to standardized terminologies (<a href=\"https://cdn.rawgit.com/dhimmel/indications/169dd2bb5f1a77f7cd54f1b7d6181f914f666e89/ehrlink/index.html\">notebook</a>, <a href=\"https://raw.githubusercontent.com/dhimmel/indications/169dd2bb5f1a77f7cd54f1b7d6181f914f666e89/ehrlink/data/indications.tsv\">download</a>). We converted problems to DO terms (<a href=\"#184\">see above</a>). Then we converted medications to RxNorm concepts, using the <a href=\"http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#185\">mapping</a> produced by <a href=\"/u/alizee\" class=\"username\">@alizee</a>. We excluded any RxNorm matches with <code>score &lt; 55</code> as errors were observed below this threshold. Overall, the RxNorm <code>approximateTerm</code> <a href=\"http://rxnav.nlm.nih.gov/RxNormAPIs.html#uLink=RxNorm_REST_getApproximateMatch\">function of the API</a> performed impressively. Next we converted RxNorm concepts into their active ingredients and restricted to single-ingredient medications.</p>\r\n\r\n<p><code>33.3% = 3719 / 11166</code> of the original problem-medication pairs successfully mapped to an ingredient and DO term. Users should take note that our mapping procedure was motivated by precision and automation, rather than recall.</p>",
      "body_md": "# Mapping ehrlink to DO and RxNorm ingredient terms\r\n\r\nWe created a version of ehrlink with the subset problem-medication pairs that mapped to standardized terminologies ([notebook](https://cdn.rawgit.com/dhimmel/indications/169dd2bb5f1a77f7cd54f1b7d6181f914f666e89/ehrlink/index.html), [download](https://raw.githubusercontent.com/dhimmel/indications/169dd2bb5f1a77f7cd54f1b7d6181f914f666e89/ehrlink/data/indications.tsv)). We converted problems to DO terms ([see above](#184)). Then we converted medications to RxNorm concepts, using the [mapping](http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#185) produced by @alizee. We excluded any RxNorm matches with `score < 55` as errors were observed below this threshold. Overall, the RxNorm `approximateTerm` [function of the API](http://rxnav.nlm.nih.gov/RxNormAPIs.html#uLink=RxNorm_REST_getApproximateMatch) performed impressively. Next we converted RxNorm concepts into their active ingredients and restricted to single-ingredient medications.\r\n\r\n`33.3% = 3719 / 11166` of the original problem-medication pairs successfully mapped to an ingredient and DO term. Users should take note that our mapping procedure was motivated by precision and automation, rather than recall.",
      "profile": 17,
      "published": "2015-05-05T01:11:23.690583Z",
      "thread": 62,
      "url": "/discussion/extracting-indications-from-the-ehrlink-resource/62#3"
    },
    {
      "body_html": "<h1>Revised indications which include ehrlink</h1>\r\n\r\n<p>We were able to <a href=\"//thinklab.com/discussion/extracting-indications-from-the-ehrlink-resource/62#190\">collaboratively map</a> ehrlink to RxNorm and the DO.</p>\r\n\r\n<p>Our indication catalog, which only includes DO slim diseases and approved small molecules in DrugBank, now contains:</p>\r\n\r\n<ul><li>1,386 high-confidence indications retrieved from MEDI-HPS <span class=\"citation\">[<a href=\"/doi/10.1136/amiajnl-2012-001431\" class=\"citation\" data-key=\"10.1136/amiajnl-2012-001431\">1</a>]</span>, LabeledIn <span class=\"citation\">[<a href=\"/doi/10.1016/j.jbi.2014.08.004\" class=\"citation\" data-key=\"10.1016/j.jbi.2014.08.004\">2</a>, <a href=\"/doi/10.1093/database/bav016\" class=\"citation\" data-key=\"10.1093/database/bav016\">3</a>]</span>, PREDICT <span class=\"citation\">[<a href=\"/doi/10.1038/msb.2011.26\" class=\"citation\" data-key=\"10.1038/msb.2011.26\">4</a>]</span>, and ehrlink <span class=\"citation\">[<a href=\"/doi/10.1136/amiajnl-2012-000852\" class=\"citation\" data-key=\"10.1136/amiajnl-2012-000852\">5</a>]</span> covering 96 diseases and 602 compounds</li><li>1,113 low-confidence indications retrieved from MEDI-LPS <span class=\"citation\">[<a href=\"/doi/10.1136/amiajnl-2012-001431\" class=\"citation\" data-key=\"10.1136/amiajnl-2012-001431\">1</a>]</span></li></ul>\r\n\r\n<p>The combined high and low-confidence indication set covers 107 diseases and 744 compounds. For more information see the <a href=\"https://cdn.rawgit.com/dhimmel/indications/36f0c5f143618abbbf8c9eb94b7318cb5936fd61/merge.html\">notebook</a>, table of <a href=\"https://github.com/dhimmel/indications/blob/36f0c5f143618abbbf8c9eb94b7318cb5936fd61/data/indications-with-source.tsv\">indications with resource info</a>, or table of <a href=\"https://github.com/dhimmel/indications/blob/36f0c5f143618abbbf8c9eb94b7318cb5936fd61/data/indications.tsv\">collapsed indications</a>.</p>",
      "body_md": "# Revised indications which include ehrlink\r\n\r\nWe were able to [collaboratively map](//thinklab.com/discussion/extracting-indications-from-the-ehrlink-resource/62#190) ehrlink to RxNorm and the DO.\r\n\r\nOur indication catalog, which only includes DO slim diseases and approved small molecules in DrugBank, now contains:\r\n\r\n+ 1,386 high-confidence indications retrieved from MEDI-HPS [@10.1136/amiajnl-2012-001431], LabeledIn [@10.1016/j.jbi.2014.08.004 @10.1093/database/bav016], PREDICT [@10.1038/msb.2011.26], and ehrlink [@10.1136/amiajnl-2012-000852] covering 96 diseases and 602 compounds\r\n+ 1,113 low-confidence indications retrieved from MEDI-LPS [@10.1136/amiajnl-2012-001431]\r\n\r\nThe combined high and low-confidence indication set covers 107 diseases and 744 compounds. For more information see the [notebook](https://cdn.rawgit.com/dhimmel/indications/36f0c5f143618abbbf8c9eb94b7318cb5936fd61/merge.html), table of [indications with resource info](https://github.com/dhimmel/indications/blob/36f0c5f143618abbbf8c9eb94b7318cb5936fd61/data/indications-with-source.tsv), or table of [collapsed indications](https://github.com/dhimmel/indications/blob/36f0c5f143618abbbf8c9eb94b7318cb5936fd61/data/indications.tsv).",
      "profile": 17,
      "published": "2015-05-05T05:16:58.441919Z",
      "thread": 21,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#21"
    },
    {
      "body_html": "<h1>Compound Vocabulary</h1>\r\n\r\n<p>We have proceeded with a subset of DrugBank <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkt1068\" class=\"citation\" data-key=\"10.1093/nar/gkt1068\">1</a>]</span> as our compound vocabulary. <a href=\"https://github.com/dhimmel/drugbank/blob/503968ed700257215f7c81137d29f86ab71e7ac4/data/drugbank-slim.tsv\">Included compounds</a> meet the following criteria:</p>\r\n\r\n<ul><li>DrugBank type is <code>small molecule</code></li><li>DrugBank groups includes <code>approved</code></li><li>Have an InChI chemical structure</li></ul>\r\n\r\n<p>Other compound vocabularies are mapped to DrugBank with UniChem <span class=\"citation\">[<a href=\"/doi/10.1186/s13321-014-0043-5\" class=\"citation\" data-key=\"10.1186/s13321-014-0043-5\">2</a>]</span> using the most permissive matching scheme <a href=\"https://www.ebi.ac.uk/unichem/info/widesearchInfo\">available</a> (<code>B = 0</code> and <code>C = 4</code>). <code>B = 0</code> matches compounds using the FIKHB (First InChIKey Hash Block) which is based on atomic connectivity <span class=\"citation\">[<a href=\"/doi/10.1186/1758-2946-5-7\" class=\"citation\" data-key=\"10.1186/1758-2946-5-7\">3</a>]</span>. <code>C = 4</code> matches compounds which share a component with a component of the DrugBank compound, in order to ignore differences based on salts and acids.</p>",
      "body_md": "# Compound Vocabulary\r\n\r\nWe have proceeded with a subset of DrugBank [@10.1093/nar/gkt1068] as our compound vocabulary. [Included compounds](https://github.com/dhimmel/drugbank/blob/503968ed700257215f7c81137d29f86ab71e7ac4/data/drugbank-slim.tsv) meet the following criteria:\r\n\r\n+ DrugBank type is `small molecule`\r\n+ DrugBank groups includes `approved`\r\n+ Have an InChI chemical structure\r\n\r\nOther compound vocabularies are mapped to DrugBank with UniChem [@10.1186/s13321-014-0043-5] using the most permissive matching scheme [available](https://www.ebi.ac.uk/unichem/info/widesearchInfo) (`B = 0` and `C = 4`). `B = 0` matches compounds using the FIKHB (First InChIKey Hash Block) which is based on atomic connectivity [@10.1186/1758-2946-5-7]. `C = 4` matches compounds which share a component with a component of the DrugBank compound, in order to ignore differences based on salts and acids.",
      "profile": 17,
      "published": "2015-05-05T05:23:16.481212Z",
      "thread": 40,
      "url": "/discussion/unifying-drug-vocabularies/40#5"
    },
    {
      "body_html": "<p>We talked to Dave and Ted at the LINCS office hours today. Here are the meeting notes:</p>\r\n\r\n<h2>Probes and genes</h2>\r\n\r\n<p>There have been two versions of landmark genes (the genes that are measured by the L1000 platform). In the first version (<code>pr_pool_id = 'deltap'</code>), there were 979 landmark genes. In the current version (<code>pr_pool_id = 'epsilon'</code>), there are 978.</p>\r\n\r\n<p>The L1000 platform is designed to imitate the Affymetrix HG-U133A array <span class=\"citation\">[<a href=\"/doi/10.1186/gb-2006-7-7-r61\" class=\"citation\" data-key=\"10.1186/gb-2006-7-7-r61\">1</a>]</span>, so L1000 output is in probe-space. The LINCS team performs their analyses in probe-space. For landmark genes, the probe-to-gene correspondence is one-to-one. However, other genes may consist of multiple probes. We plan to average z-scores across probes to convert our observations into gene-space.</p>\r\n\r\n<h2>Computing consensus signatures</h2>\r\n\r\n<p>When drugs have multiple signatures, we find the average correlation value for each signature. We have noticed that some signatures have negative correlations and were thus contributing their inverse signature to the consensus. We are uncomfortable with negative weights and therefore plan to set a minimum correlation threshold for each signature. A minimum of <code>0</code> would exclude all negatively correlated signatures. Another option is <code>0.1</code>, which is used by the LINCS team when processing shRNA data.</p>\r\n\r\n<h1>Miscellaneous</h1>\r\n\r\n<ul><li>The API returns <code>-666</code> for missing values.</li><li>Instances refer to the replicates that compose a signature.</li><li><a href=\"/u/leobrueggeman\" class=\"username\">@leobrueggeman</a>, will be presenting on LINCS at lab meeting today (<a href=\"http://slides.com/leoo/lincs\">presentation</a>).</li><li><code>is_summly</code> refers to whether a perturbagen has been profiled across a broad range of cell lines.</li></ul>",
      "body_md": "We talked to Dave and Ted at the LINCS office hours today. Here are the meeting notes:\r\n\r\n## Probes and genes\r\n\r\nThere have been two versions of landmark genes (the genes that are measured by the L1000 platform). In the first version (`pr_pool_id = 'deltap'`), there were 979 landmark genes. In the current version (`pr_pool_id = 'epsilon'`), there are 978.\r\n\r\nThe L1000 platform is designed to imitate the Affymetrix HG-U133A array [@10.1186/gb-2006-7-7-r61], so L1000 output is in probe-space. The LINCS team performs their analyses in probe-space. For landmark genes, the probe-to-gene correspondence is one-to-one. However, other genes may consist of multiple probes. We plan to average z-scores across probes to convert our observations into gene-space.\r\n\r\n## Computing consensus signatures\r\n\r\nWhen drugs have multiple signatures, we find the average correlation value for each signature. We have noticed that some signatures have negative correlations and were thus contributing their inverse signature to the consensus. We are uncomfortable with negative weights and therefore plan to set a minimum correlation threshold for each signature. A minimum of `0` would exclude all negatively correlated signatures. Another option is `0.1`, which is used by the LINCS team when processing shRNA data.\r\n\r\n# Miscellaneous\r\n\r\n+ The API returns `-666` for missing values.\r\n+ Instances refer to the replicates that compose a signature.\r\n+ @leobrueggeman, will be presenting on LINCS at lab meeting today ([presentation](http://slides.com/leoo/lincs)).\r\n+ `is_summly` refers to whether a perturbagen has been profiled across a broad range of cell lines.",
      "profile": 17,
      "published": "2015-05-07T18:39:43.748862Z",
      "thread": 43,
      "url": "/discussion/computing-consensus-transcriptional-profiles-for-lincs-l1000-perturbations/43#4"
    },
    {
      "body_html": "<h2>Background reading on Gene Ontology annotations</h2>\r\n\r\n<ol><li>Gene Ontology Annotations and Resources <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gks1050\" class=\"citation\" data-key=\"10.1093/nar/gks1050\">1</a>]</span></li><li>Use and misuse of the gene ontology annotations <span class=\"citation\">[<a href=\"/doi/10.1038/nrg2363\" class=\"citation\" data-key=\"10.1038/nrg2363\">2</a>]</span></li><li>Understanding how and why the Gene Ontology and its annotations evolve: the GO within UniProt <span class=\"citation\">[<a href=\"/doi/10.1186/2047-217X-3-4\" class=\"citation\" data-key=\"10.1186/2047-217X-3-4\">3</a>]</span></li><li>Quality of Computationally Inferred Gene Ontology Annotations <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1002533\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1002533\">4</a>]</span></li></ol>\r\n\r\n<h3>Updates</h3>\r\n\r\n<ul><li><a href=\"http://arxiv.org/abs/1602.01876\" title=\"arXiv\">Primer on the Gene Ontology</a></li><li><a href=\"http://arxiv.org/abs/1602.01875\" title=\"arXiv\">Gene Ontology: Pitfalls, Biases, Remedies</a></li></ul>",
      "body_md": "## Background reading on Gene Ontology annotations\r\n\r\n1. Gene Ontology Annotations and Resources [@10.1093/nar/gks1050]\r\n+ Use and misuse of the gene ontology annotations [@10.1038/nrg2363]\r\n+ Understanding how and why the Gene Ontology and its annotations evolve: the GO within UniProt [@10.1186/2047-217X-3-4]\r\n+ Quality of Computationally Inferred Gene Ontology Annotations [@10.1371/journal.pcbi.1002533]\r\n\r\n### Updates\r\n\r\n+ [Primer on the Gene Ontology](http://arxiv.org/abs/1602.01876 \"arXiv\")\r\n+ [Gene Ontology: Pitfalls, Biases, Remedies](http://arxiv.org/abs/1602.01875 \"arXiv\")",
      "profile": 17,
      "published": "2015-05-08T18:06:54.819989Z",
      "thread": 39,
      "url": "/discussion/compiling-gene-ontology-annotations-into-an-easy-to-use-format/39#2"
    },
    {
      "body_html": "<p>We have proceeded with Entrez Gene for gene identification.</p>\r\n\r\n<p><a href=\"/u/caseygreene\" class=\"username\">@caseygreene</a>, do you have any advice or information on how to build the SQL database? I found <a href=\"http://jura.wi.mit.edu/entrez_gene/\">this site</a> which provides instructions and Perl scripts. Do you use the same <a href=\"http://jura.wi.mit.edu/entrez_gene/entrez_gene.pdf\">schema</a>?</p>",
      "body_md": "We have proceeded with Entrez Gene for gene identification.\r\n\r\n@caseygreene, do you have any advice or information on how to build the SQL database? I found [this site](http://jura.wi.mit.edu/entrez_gene/) which provides instructions and Perl scripts. Do you use the same [schema](http://jura.wi.mit.edu/entrez_gene/entrez_gene.pdf)?",
      "profile": 17,
      "published": "2015-05-08T18:58:07.489475Z",
      "thread": 34,
      "url": "/discussion/using-entrez-gene-as-our-gene-vocabulary/34#3"
    },
    {
      "body_html": "<p>We have done it a couple of ways. Currently we like to We have an EntrezID field that's an index, a systematic name that's an index (if you're human, this is HGNC identifiers), standard name (you won't need this for human only), the gene description, a foreign key to the organism (again, if only human, won't need this), the aliases (space separated list of previous/alternative names — used only for full text search), whether or not the gene is now obsolete (used during updates), and a few other things that we use for search.</p>\r\n\r\n<p>For other identifiers, we have a table of cross reference databases, which has a name (index) and a url. URL has characters in it that signify that the ID for the database is supposed to go there.</p>\r\n\r\n<p>We then have a table of cross references, which has foreign keys to both the cross reference database and gene, as well as a cross reference id (also db index).</p>\r\n\r\n<p>If you want python code to generate this and/or load identifiers using the Django ORM, we can supply it. We might also be able to open source it as part of a pip installable django app on pypi. This is on our to-do list, so we could potentially reprioritize if this is particularly useful to you.</p>",
      "body_md": "We have done it a couple of ways. Currently we like to We have an EntrezID field that's an index, a systematic name that's an index (if you're human, this is HGNC identifiers), standard name (you won't need this for human only), the gene description, a foreign key to the organism (again, if only human, won't need this), the aliases (space separated list of previous/alternative names -- used only for full text search), whether or not the gene is now obsolete (used during updates), and a few other things that we use for search.\r\n\r\nFor other identifiers, we have a table of cross reference databases, which has a name (index) and a url. URL has characters in it that signify that the ID for the database is supposed to go there.\r\n\r\nWe then have a table of cross references, which has foreign keys to both the cross reference database and gene, as well as a cross reference id (also db index).\r\n\r\nIf you want python code to generate this and/or load identifiers using the Django ORM, we can supply it. We might also be able to open source it as part of a pip installable django app on pypi. This is on our to-do list, so we could potentially reprioritize if this is particularly useful to you.",
      "profile": 22,
      "published": "2015-05-08T19:43:12.982725Z",
      "thread": 34,
      "url": "/discussion/using-entrez-gene-as-our-gene-vocabulary/34#4"
    },
    {
      "body_html": "<p><a href=\"/u/dhimmel\" class=\"username\">@dhimmel</a> I'd add this as particularly important for GO as well: <a href=\"http://wiki.geneontology.org/index.php/Transitive_closure\" target=\"_blank\">http://wiki.geneontology.org/index.php/Transitive_closure</a></p>\r\n\r\n<p>People frequently overlook this.</p>",
      "body_md": "@dhimmel I'd add this as particularly important for GO as well: http://wiki.geneontology.org/index.php/Transitive_closure\r\n\r\nPeople frequently overlook this.",
      "profile": 22,
      "published": "2015-05-08T19:46:56.057944Z",
      "thread": 39,
      "url": "/discussion/compiling-gene-ontology-annotations-into-an-easy-to-use-format/39#3"
    },
    {
      "body_html": "<p>DrugBank <a href=\"http://www.drugbank.ca/documentation\">contains</a> four types of drug-protein interactions:</p>\r\n\r\n<ul><li><strong>Target</strong>: A protein, macromolecule, nucleic acid, or small molecule to which a given drug binds, resulting in an alteration of the normal function of the bound molecule anda desirable therapeutic effect. Drug targets are most commonly proteins such as enzymes, ion channels, and receptors.</li><li><strong>Enzyme</strong>: A protein which catalyzes chemical reactions involving the a given drug (substrate). Most drugs are metabolized by the Cytochrome P450 enzymes.</li><li><strong>Transporter</strong>: A membrane bound protein which shuttles ions, small molecules or macromolecules across membranes, into cells or out of cells.</li><li><strong>Carrier</strong>: A secreted protein which binds to drugs, carrying them to cell transporters, where they are moved into the cell. Drug carriers may be used in drug design to increase the effectiveness of drug delivery to the target sites of pharmacological actions.</li></ul>\r\n\r\n<p>We extracted DrugBank-protein interactions (<a href=\"http://nbviewer.ipython.org/github/dhimmel/drugbank/blob/22d835b3cd0ed421c18f855a85a183a9c1349e8f/parse.ipynb\">notebook</a>, <a href=\"https://raw.githubusercontent.com/dhimmel/drugbank/22d835b3cd0ed421c18f855a85a183a9c1349e8f/data/proteins.tsv\">download</a>). Our resource includes all DrugBank interactions that met the following criteria:</p>\r\n\r\n<ol><li>The interaction is between a drug and <em>single</em> protein. A target which is \"protein group\" and contains multiple uniprot proteins would be excluded. Examples include the <a href=\"//www.drugbank.ca/biodb/bio_entities/BE0004797\">GABA-A receptor (anion channel)</a> and <a href=\"//www.drugbank.ca/biodb/bio_entities/BE0004956\">NMDA receptor</a>. Likewise, a target which is not a protein, such as <a href=\"//www.drugbank.ca/biodb/bio_entities/BE0004796\">DNA</a> or <a href=\"//www.drugbank.ca/biodb/bio_entities/BE0004815\">Phosphate</a>, would be excluded.</li><li>The protein maps to an entrez gene. Some uniprot proteins did not such as <a href=\"//www.uniprot.org/uniprot/Q7ZJM1\">Q7ZJM1</a>, <a href=\"//www.uniprot.org/uniprot/Q59GM9\">Q59GM9</a>, and <a href=\"//www.uniprot.org/uniprot/Q53ET4\">Q53ET4</a> — all TrEMBL (unreviewed) terms with low <a href=\"http://www.uniprot.org/help/annotation_score\">annotation scores</a>.</li></ol>\r\n\r\n<p>In total, we extracted 19,906 interactions for 5,878 drugs and 3,757 genes.</p>",
      "body_md": "DrugBank [contains](http://www.drugbank.ca/documentation) four types of drug-protein interactions:\r\n\r\n+ **Target**: A protein, macromolecule, nucleic acid, or small molecule to which a given drug binds, resulting in an alteration of the normal function of the bound molecule anda desirable therapeutic effect. Drug targets are most commonly proteins such as enzymes, ion channels, and receptors.\r\n+ **Enzyme**: A protein which catalyzes chemical reactions involving the a given drug (substrate). Most drugs are metabolized by the Cytochrome P450 enzymes.\r\n+ **Transporter**: A membrane bound protein which shuttles ions, small molecules or macromolecules across membranes, into cells or out of cells.\r\n+ **Carrier**: A secreted protein which binds to drugs, carrying them to cell transporters, where they are moved into the cell. Drug carriers may be used in drug design to increase the effectiveness of drug delivery to the target sites of pharmacological actions.\r\n\r\nWe extracted DrugBank-protein interactions ([notebook](http://nbviewer.ipython.org/github/dhimmel/drugbank/blob/22d835b3cd0ed421c18f855a85a183a9c1349e8f/parse.ipynb), [download](https://raw.githubusercontent.com/dhimmel/drugbank/22d835b3cd0ed421c18f855a85a183a9c1349e8f/data/proteins.tsv)). Our resource includes all DrugBank interactions that met the following criteria:\r\n\r\n1. The interaction is between a drug and *single* protein. A target which is \"protein group\" and contains multiple uniprot proteins would be excluded. Examples include the [GABA-A receptor (anion channel)](//www.drugbank.ca/biodb/bio_entities/BE0004797) and [NMDA receptor](//www.drugbank.ca/biodb/bio_entities/BE0004956). Likewise, a target which is not a protein, such as [DNA](//www.drugbank.ca/biodb/bio_entities/BE0004796) or [Phosphate](//www.drugbank.ca/biodb/bio_entities/BE0004815), would be excluded.\r\n2. The protein maps to an entrez gene. Some uniprot proteins did not such as [Q7ZJM1](//www.uniprot.org/uniprot/Q7ZJM1), [Q59GM9](//www.uniprot.org/uniprot/Q59GM9), and [Q53ET4](//www.uniprot.org/uniprot/Q53ET4) -- all TrEMBL (unreviewed) terms with low [annotation scores](http://www.uniprot.org/help/annotation_score).\r\n\r\nIn total, we extracted 19,906 interactions for 5,878 drugs and 3,757 genes.",
      "profile": 17,
      "published": "2015-05-09T06:26:06.869629Z",
      "thread": 65,
      "url": "/discussion/protein-target-carrier-transporter-and-enzyme-interactions-in-drugbank/65"
    },
    {
      "body_html": "<p>Larry Hunter just gave a talk here where he highlighted his group's work developing KaBOB:<br><a href=\"http://www.biomedcentral.com/1471-2105/16/126/abstract\" target=\"_blank\">http://www.biomedcentral.com/1471-2105/16/126/abstract</a></p>\r\n\r\n<p>Some of the work that they've done may help with processing of external databases. They unify some of the drug/target concepts in DrugBank and PharmGCB, for example, by developing abstract representations of genes, gene products, variants, etc.</p>",
      "body_md": "Larry Hunter just gave a talk here where he highlighted his group's work developing KaBOB:\r\nhttp://www.biomedcentral.com/1471-2105/16/126/abstract\r\n\r\nSome of the work that they've done may help with processing of external databases. They unify some of the drug/target concepts in DrugBank and PharmGCB, for example, by developing abstract representations of genes, gene products, variants, etc.",
      "profile": 22,
      "published": "2015-05-09T13:55:34.618123Z",
      "thread": 66,
      "url": "/discussion/kabob-knowledgebase/66"
    },
    {
      "body_html": "<p>Thanks for the recommendation <span class=\"citation\">[<a href=\"/doi/10.1186/s12859-015-0559-3\" class=\"citation\" data-key=\"10.1186/s12859-015-0559-3\">1</a>]</span>. Unless there is a specific contribution that this resource could immediately provide, I am wary to invest significant time in understanding and integrating it.</p>\r\n\r\n<p>For example, do we want to introduce a dependency on a <a href=\"https://github.com/UCDenver-ccp/datasource/blob/master/datasource-fileparsers/src/main/java/edu/ucdenver/ccp/datasource/fileparsers/drugbank/DrugBankDrugRecord.java\">1,665 line java package</a> to parse a DrugBank record, when we can retrieve the small subset of information we require with <a href=\"https://github.com/dhimmel/drugbank/blob/93d4974e05e238fc45d87e9a79d3f2b23cab58e1/parse.ipynb\">much simpler scripts</a>.</p>\r\n\r\n<p>My initial sense is that while this study tackles some important problems in biodata integration, there isn't a readily available way to easily retrieve and incorporate the unified vocabularies. It would be great to get feedback from the authors, in case I am wrong.</p>",
      "body_md": "Thanks for the recommendation [@10.1186/s12859-015-0559-3]. Unless there is a specific contribution that this resource could immediately provide, I am wary to invest significant time in understanding and integrating it.\r\n\r\nFor example, do we want to introduce a dependency on a [1,665 line java package](https://github.com/UCDenver-ccp/datasource/blob/master/datasource-fileparsers/src/main/java/edu/ucdenver/ccp/datasource/fileparsers/drugbank/DrugBankDrugRecord.java) to parse a DrugBank record, when we can retrieve the small subset of information we require with [much simpler scripts](https://github.com/dhimmel/drugbank/blob/93d4974e05e238fc45d87e9a79d3f2b23cab58e1/parse.ipynb).\r\n\r\nMy initial sense is that while this study tackles some important problems in biodata integration, there isn't a readily available way to easily retrieve and incorporate the unified vocabularies. It would be great to get feedback from the authors, in case I am wrong.",
      "profile": 17,
      "published": "2015-05-09T19:44:06.933442Z",
      "thread": 66,
      "url": "/discussion/kabob-knowledgebase/66#2"
    },
    {
      "body_html": "<h1>Transitive Closure</h1>\r\n\r\n<p><a href=\"/u/caseygreene\" class=\"username\">@caseygreene</a>, our resource has an option to propagate annotations to account for transitive closure. Briefly, transitive closure is defined through example as:</p>\r\n\r\n<blockquote><p>‘every kidney is located in some body’ follows from ‘every kidney is located in some abdomen’ and ‘every abdomen is located in some body’ <span class=\"citation\">[<a href=\"/doi/10.1093/bioinformatics/btr164\" class=\"citation\" data-key=\"10.1093/bioinformatics/btr164\">1</a>]</span></p></blockquote>\r\n\r\n<p>Our current propagation method transfers annotations across <code>is_a</code> relationships between terms in the <code>go-basic.obo</code> ontology. We rely on the <a href=\"https://github.com/tanghaibao/goatools\">goatools python package</a> to process the gene ontology. This package <a href=\"https://github.com/tanghaibao/goatools/blob/b7aab4ee4d242d67aa7f4eba2bb5015238875a6b/goatools/obo_parser.py#L91\">appears to discard all non-<code>is_a</code> relationships</a>. It sounds like our method would classify as the \"the old way\" according to <a href=\"http://wiki.geneontology.org/index.php/Transitive_closure\">your link</a>.</p>\r\n\r\n<p>Is there an easy way to retrieve a table of closure relationships that we should use for annotation propagation? The site mentions a \"pre-computed closure tsv\" but does not indicate whether it is currently available. If we do switch to a method that incorporates additional relationship types beyond <code>is_a</code>, which additional types do you recommend propagating on?</p>",
      "body_md": "# Transitive Closure\r\n\r\n@caseygreene, our resource has an option to propagate annotations to account for transitive closure. Briefly, transitive closure is defined through example as:\r\n\r\n> ‘every kidney is located in some body’ follows from ‘every kidney is located in some abdomen’ and ‘every abdomen is located in some body’ [@10.1093/bioinformatics/btr164]\r\n\r\nOur current propagation method transfers annotations across `is_a` relationships between terms in the `go-basic.obo` ontology. We rely on the [goatools python package](https://github.com/tanghaibao/goatools) to process the gene ontology. This package [appears to discard all non-`is_a` relationships](https://github.com/tanghaibao/goatools/blob/b7aab4ee4d242d67aa7f4eba2bb5015238875a6b/goatools/obo_parser.py#L91). It sounds like our method would classify as the \"the old way\" according to [your link](http://wiki.geneontology.org/index.php/Transitive_closure).\r\n\r\nIs there an easy way to retrieve a table of closure relationships that we should use for annotation propagation? The site mentions a \"pre-computed closure tsv\" but does not indicate whether it is currently available. If we do switch to a method that incorporates additional relationship types beyond `is_a`, which additional types do you recommend propagating on?",
      "profile": 17,
      "published": "2015-05-09T20:26:06.382072Z",
      "thread": 39,
      "url": "/discussion/compiling-gene-ontology-annotations-into-an-easy-to-use-format/39#4"
    },
    {
      "body_html": "<h2>Background</h2>\r\n\r\n<p>The National Library of Medicine (NLM) produces a catalog of 23 million journal articles called PubMed. PubMed contains <a href=\"//www.nlm.nih.gov/pubs/factsheets/dif_med_pub.html\">two subsets</a> that are relevant for literature mining:</p>\r\n\r\n<ol><li><strong>PubMed Central (PMC)</strong> — 3.4 million articles that include full texts, rather than just abstracts.</li><li><strong>MEDLINE</strong> — 21 million articles that are manually annotated with their topics. Topics are chosen from the MeSH vocabulary. 5,594 journals are <a href=\"http://www.ncbi.nlm.nih.gov/nlmcatalog/?term=currentlyindexed\">currently indexed</a>.</li></ol>\r\n\r\n<p>MeSH, which stands for Medical Subject Headings, is a broad terminology of ~27 thousand terms structured hierarchically to form an ontology. <em>Skilled subject analysts</em> at the NLM <a href=\"//www.ncbi.nlm.nih.gov/books/NBK3827/#pubmedhelp.MeSH_Terms_MH\">typically assign</a> 10–12 MeSH terms per article and denote a subset of these terms as <em>major topics</em>.</p>\r\n\r\n<h2>Application</h2>\r\n\r\n<p>Text mining, as <a href=\"//thinklab.com/discussion/text-as-a-resource-for-network-population/48\">suggested to us</a> by <a href=\"/u/b_good\" class=\"username\">@b_good</a>, is an intriguing technique because it is widely-applicable and draws from a knowledge base of epic proportions <span class=\"citation\">[<a href=\"/doi/10.1186/1742-5581-3-2\" class=\"citation\" data-key=\"10.1186/1742-5581-3-2\">1</a>]</span>.</p>\r\n\r\n<p>We would like to infer relationships between nodes in our network based on MEDLINE cooccurrence. We will search for pairs of MeSH terms that are assigned to the same articles beyond what would be expected if the terms were unrelated. This approach has successfully identified disease symptoms <span class=\"citation\">[<a href=\"/doi/10.1038/ncomms5212\" class=\"citation\" data-key=\"10.1038/ncomms5212\">2</a>]</span> (<a href=\"https://cdn.rawgit.com/dhimmel/hsdn/51d31d738fe3cad0a12c736b8def333729c3c7f4/index.html\">browse results</a>). The method is versatile and can be applied to any nodes which have been mapped to MeSH.</p>",
      "body_md": "## Background\r\n\r\nThe National Library of Medicine (NLM) produces a catalog of 23 million journal articles called PubMed. PubMed contains [two subsets](//www.nlm.nih.gov/pubs/factsheets/dif_med_pub.html) that are relevant for literature mining:\r\n\r\n1. **PubMed Central (PMC)** -- 3.4 million articles that include full texts, rather than just abstracts.\r\n2. **MEDLINE** -- 21 million articles that are manually annotated with their topics. Topics are chosen from the MeSH vocabulary. 5,594 journals are [currently indexed](http://www.ncbi.nlm.nih.gov/nlmcatalog/?term=currentlyindexed).\r\n\r\nMeSH, which stands for Medical Subject Headings, is a broad terminology of ~27 thousand terms structured hierarchically to form an ontology. *Skilled subject analysts* at the NLM [typically assign](//www.ncbi.nlm.nih.gov/books/NBK3827/#pubmedhelp.MeSH_Terms_MH) 10--12 MeSH terms per article and denote a subset of these terms as *major topics*.\r\n\r\n## Application\r\n\r\nText mining, as [suggested to us](//thinklab.com/discussion/text-as-a-resource-for-network-population/48) by @b_good, is an intriguing technique because it is widely-applicable and draws from a knowledge base of epic proportions [@10.1186/1742-5581-3-2].\r\n\r\nWe would like to infer relationships between nodes in our network based on MEDLINE cooccurrence. We will search for pairs of MeSH terms that are assigned to the same articles beyond what would be expected if the terms were unrelated. This approach has successfully identified disease symptoms [@10.1038/ncomms5212] ([browse results](https://cdn.rawgit.com/dhimmel/hsdn/51d31d738fe3cad0a12c736b8def333729c3c7f4/index.html)). The method is versatile and can be applied to any nodes which have been mapped to MeSH.",
      "profile": 17,
      "published": "2015-05-10T21:10:43.806808Z",
      "thread": 67,
      "url": "/discussion/mining-knowledge-from-medline-articles-and-their-indexed-mesh-terms/67"
    },
    {
      "body_html": "<p>The <a href=\"//disease-ontology.org/\">Disease Ontology</a> (DO) <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkr972\" class=\"citation\" data-key=\"10.1093/nar/gkr972\">1</a>, <a href=\"/doi/10.1093/nar/gku1011\" class=\"citation\" data-key=\"10.1093/nar/gku1011\">2</a>]</span> is an open source ontology of human diseases. We are using a <a href=\"//thinklab.com/discussion/unifying-disease-vocabularies/44#144\">subset of the DO</a>, which we refer to as DO slim, as our primary disease vocabulary.</p>\r\n\r\n<p>We plan on using this discussion to document <a href=\"//sourceforge.net/p/diseaseontology/feature-requests/\">DO feature requests</a> related to our project. Individuals who contribute to the DO to assist our project should post here so their efforts can be rewarded.</p>",
      "body_md": "The [Disease Ontology](//disease-ontology.org/) (DO) [@10.1093/nar/gkr972 @10.1093/nar/gku1011] is an open source ontology of human diseases. We are using a [subset of the DO](//thinklab.com/discussion/unifying-disease-vocabularies/44#144), which we refer to as DO slim, as our primary disease vocabulary.\r\n\r\nWe plan on using this discussion to document [DO feature requests](//sourceforge.net/p/diseaseontology/feature-requests/) related to our project. Individuals who contribute to the DO to assist our project should post here so their efforts can be rewarded.",
      "profile": 17,
      "published": "2015-05-12T02:41:44.069056Z",
      "thread": 68,
      "url": "/discussion/disease-ontology-feature-requests/68"
    },
    {
      "body_html": "<h1>MeSH cross-reference additions</h1>\r\n\r\n<p>Of our 137 <a href=\"//thinklab.com/discussion/unifying-disease-vocabularies/44#144\">DO slim</a> terms, 19 did not contain a MeSH (<code>MSH</code>) xref. We manually mapped 16 of these terms (<a href=\"https://raw.githubusercontent.com/dhimmel/disease-ontology/89a4fa3e8eb4703d0d3f2c6001ff807876f8b045/requests/DO-slim-to-mesh.tsv\">tsv download</a>):</p>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th>doid_code</th><th>doid_name</th><th>mesh_id</th><th>mesh_name</th></tr></thead><tbody><tr><td>DOID:0050741</td><td>alcohol dependence</td><td>D000437</td><td>Alcoholism</td></tr><tr><td>DOID:0050742</td><td>nicotine dependence</td><td>D014029</td><td>Tobacco Use Disorder</td></tr><tr><td>DOID:0060119</td><td>pharynx cancer</td><td>D010610</td><td>Pharyngeal Neoplasms</td></tr><tr><td>DOID:10021</td><td>duodenum cancer</td><td>D004379</td><td>Duodenal Neoplasms</td></tr><tr><td>DOID:10153</td><td>ileum cancer</td><td>D007078</td><td>Ileal Neoplasms</td></tr><tr><td>DOID:1115</td><td>sarcoma</td><td>D012509</td><td>Sarcoma</td></tr><tr><td>DOID:11615</td><td>penile cancer</td><td>D010412</td><td>Penile Neoplasms</td></tr><tr><td>DOID:11920</td><td>tracheal cancer</td><td>D014134</td><td>Tracheal Neoplasms</td></tr><tr><td>DOID:1324</td><td>lung cancer</td><td>D008175</td><td>Lung Neoplasms</td></tr><tr><td>DOID:1725</td><td>peritoneum cancer</td><td>D010534</td><td>Peritoneal Neoplasms</td></tr><tr><td>DOID:1781</td><td>thyroid cancer</td><td>D013964</td><td>Thyroid Neoplasms</td></tr><tr><td>DOID:4362</td><td>cervical cancer</td><td>D002583</td><td>Uterine Cervical Neoplasms</td></tr><tr><td>DOID:4481</td><td>allergic rhinitis</td><td>D065631</td><td>Rhinitis, Allergic</td></tr><tr><td>DOID:8398</td><td>osteoarthritis</td><td>D010003</td><td>Osteoarthritis</td></tr><tr><td>DOID:8893</td><td>psoriasis</td><td>D011565</td><td>Psoriasis</td></tr><tr><td>DOID:90</td><td>degenerative disc disease</td><td>D055959</td><td>Intervertebral Disc Degeneration</td></tr></tbody></table>\r\n\r\n<p>Comprehensive MeSH cross-references will enable <a href=\"http://thinklab.com/discussion/mining-knowledge-from-medline-articles-and-their-indexed-mesh-terms/67\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d67\">literature mining through MEDLINE</a>.</p>",
      "body_md": "# MeSH cross-reference additions\r\n\r\nOf our 137 [DO slim](//thinklab.com/discussion/unifying-disease-vocabularies/44#144) terms, 19 did not contain a MeSH (`MSH`) xref. We manually mapped 16 of these terms ([tsv download](https://raw.githubusercontent.com/dhimmel/disease-ontology/89a4fa3e8eb4703d0d3f2c6001ff807876f8b045/requests/DO-slim-to-mesh.tsv)):\r\n\r\n| doid_code | doid_name | mesh_id | mesh_name |\r\n|--------------|---------------------------|---------|----------------------------------|\r\n| DOID:0050741 | alcohol dependence | D000437 | Alcoholism |\r\n| DOID:0050742 | nicotine dependence | D014029 | Tobacco Use Disorder |\r\n| DOID:0060119 | pharynx cancer | D010610 | Pharyngeal Neoplasms |\r\n| DOID:10021 | duodenum cancer | D004379 | Duodenal Neoplasms |\r\n| DOID:10153 | ileum cancer | D007078 | Ileal Neoplasms |\r\n| DOID:1115 | sarcoma | D012509 | Sarcoma |\r\n| DOID:11615 | penile cancer | D010412 | Penile Neoplasms |\r\n| DOID:11920 | tracheal cancer | D014134 | Tracheal Neoplasms |\r\n| DOID:1324 | lung cancer | D008175 | Lung Neoplasms |\r\n| DOID:1725 | peritoneum cancer | D010534 | Peritoneal Neoplasms |\r\n| DOID:1781 | thyroid cancer | D013964 | Thyroid Neoplasms |\r\n| DOID:4362 | cervical cancer | D002583 | Uterine Cervical Neoplasms |\r\n| DOID:4481 | allergic rhinitis | D065631 | Rhinitis, Allergic |\r\n| DOID:8398 | osteoarthritis | D010003 | Osteoarthritis |\r\n| DOID:8893 | psoriasis | D011565 | Psoriasis |\r\n| DOID:90 | degenerative disc disease | D055959 | Intervertebral Disc Degeneration |\r\n\r\nComprehensive MeSH cross-references will enable [literature mining through MEDLINE](http://thinklab.com/discussion/mining-knowledge-from-medline-articles-and-their-indexed-mesh-terms/67).",
      "profile": 17,
      "published": "2015-05-12T02:52:52.494516Z",
      "thread": 68,
      "url": "/discussion/disease-ontology-feature-requests/68#2"
    },
    {
      "body_html": "<h1>Mistakes in xref resource abbreviations</h1>\r\n\r\n<p>MedDRA cross-references are inconsistently denoted with <code>MedDRA</code> and <code>MEDDRA</code>.</p>\r\n\r\n<p>The following examples include snippets from the <code>HumanDO.obo</code> (revision 2810). Unless otherwise noted, the errors are on the last copied line.</p>\r\n\r\n<p><code>IDC</code> should be <code>ICD</code> for the International Classification of Diseases (last two lines):<br></p>\r\n\r\n<p></p><pre><code class=\"no-highlight hljs\">[Term]\r\nid: DOID:0060236\r\nname: xanthinuria\r\ndef: \"A purine-pyrimidine metabolic disorder characterized by deficiency of xanthine oxidase, resulting in excretion of large amounts of xanthine in the urine and the formation of xanthine stones.\" [url:http\\://en.wikipedia.org/wiki/Xanthinuria, url:http\\://www.ncbi.nlm.nih.gov/pubmed/4369449]\r\ncomment: NT MGI.\r\nsubset: DO_MGI_slim\r\nsynonym: \"xanthine dehydrogenase deficiency\" EXACT []\r\nsynonym: \"xanthine oxidase deficiency\" EXACT []\r\nxref: HP:0010934\r\nxref: IDC10CM:E79.8\r\nxref: IDC9CM:277.2</code></pre>\r\n\r\n<p><code>IDC</code> should be <code>ICD</code>:<br></p>\r\n\r\n<p></p><pre><code class=\"no-highlight hljs\">[Term]\r\nid: DOID:0060332\r\nname: mitochondrial complex V (ATP synthase) deficiency nuclear type 3\r\ndef: \"A mitochondrial metabolism disease that has material basis in mutation in the ATP5E gene on chromosome 20q13.\" [url:http\\://omim.org/entry/614053]\r\nsubset: DO_MGI_slim\r\nsynonym: \"MC5DN3\" EXACT []\r\nxref: IDC10CM:E88.8</code></pre>\r\n\r\n<p><code>IDC</code> should be <code>ICD</code>:<br></p>\r\n\r\n<p></p><pre><code class=\"no-highlight hljs\">[Term]\r\nid: DOID:5212\r\nname: congenital disorder of glycosylation\r\ndef: \"A carbohydrate metabolic disorder that involves deficient or defective glycosylation of a variety of tissue proteins and/or lipids.\" [url:http\\://en.wikipedia.org/wiki/Congenital_disorder_of_glycosylation]\r\ncomment: Xref MGI.\r\nsubset: DO_MGI_slim\r\nsynonym: \"carbohydrate-deficient glycoprotein syndrome\" EXACT []\r\nxref: ICD10CM:77.8\r\nxref: IDC9CM:271.8</code></pre>\r\n\r\n<p><code>UML_CUI</code> should be <code>UMLS_CUI</code> for the Unified Medical Language System:<br></p>\r\n\r\n<p></p><pre><code class=\"no-highlight hljs\">[Term]\r\nid: DOID:0060313\r\nname: tracheomalacia\r\ndef: \"A tracheal disease characterized by flaccidity of the tracheal support cartilage.\" [url:http\\://en.wikipedia.org/wiki/Tracheomalacia]\r\ncomment: PRISM.\r\nsynonym: \"congenital tracheomalacia\" EXACT []\r\nxref: HP:0002779\r\nxref: ICD10CM:Q32.0\r\nxref: MSH:C557675\r\nxref: NCI:C98634\r\nxref: ORDO:95430\r\nxref: UML_CUI:C0392109</code></pre>\r\n\r\n<p><code>UMLS</code> should be <code>UMLS_CUI</code> for consistency:<br></p>\r\n\r\n<p></p><pre><code class=\"no-highlight hljs\">[Term]\r\nid: DOID:0060217\r\nname: Cogan-Reese syndrome\r\ndef: \"A rare eye disease characterized by variable iris atrophy, pigmented and pedunculated nodules located_in iris and attachment of the iris to the cornea (peripheral anterior synechiae) and characterized_by glaucoma.\" [url:http\\://en.wikipedia.org/wiki/Iridocorneal_endothelial_syndrome, url:http\\://rarediseases.info.nih.gov/gard/6125/cogan-reese-syndrome/resources/1]\r\nxref: ICD10CM:H21.1\r\nxref: MEDDRA:10059200\r\nxref: ORDO:98980\r\nxref: UMLS:C1168173</code></pre>",
      "body_md": "# Mistakes in xref resource abbreviations\r\n\r\nMedDRA cross-references are inconsistently denoted with `MedDRA` and `MEDDRA`.\r\n\r\nThe following examples include snippets from the `HumanDO.obo` (revision 2810). Unless otherwise noted, the errors are on the last copied line.\r\n\r\n`IDC` should be `ICD` for the International Classification of Diseases (last two lines):\r\n```\r\n[Term]\r\nid: DOID:0060236\r\nname: xanthinuria\r\ndef: \"A purine-pyrimidine metabolic disorder characterized by deficiency of xanthine oxidase, resulting in excretion of large amounts of xanthine in the urine and the formation of xanthine stones.\" [url:http\\://en.wikipedia.org/wiki/Xanthinuria, url:http\\://www.ncbi.nlm.nih.gov/pubmed/4369449]\r\ncomment: NT MGI.\r\nsubset: DO_MGI_slim\r\nsynonym: \"xanthine dehydrogenase deficiency\" EXACT []\r\nsynonym: \"xanthine oxidase deficiency\" EXACT []\r\nxref: HP:0010934\r\nxref: IDC10CM:E79.8\r\nxref: IDC9CM:277.2\r\n```\r\n`IDC` should be `ICD`:\r\n```\r\n[Term]\r\nid: DOID:0060332\r\nname: mitochondrial complex V (ATP synthase) deficiency nuclear type 3\r\ndef: \"A mitochondrial metabolism disease that has material basis in mutation in the ATP5E gene on chromosome 20q13.\" [url:http\\://omim.org/entry/614053]\r\nsubset: DO_MGI_slim\r\nsynonym: \"MC5DN3\" EXACT []\r\nxref: IDC10CM:E88.8\r\n```\r\n`IDC` should be `ICD`:\r\n```\r\n[Term]\r\nid: DOID:5212\r\nname: congenital disorder of glycosylation\r\ndef: \"A carbohydrate metabolic disorder that involves deficient or defective glycosylation of a variety of tissue proteins and/or lipids.\" [url:http\\://en.wikipedia.org/wiki/Congenital_disorder_of_glycosylation]\r\ncomment: Xref MGI.\r\nsubset: DO_MGI_slim\r\nsynonym: \"carbohydrate-deficient glycoprotein syndrome\" EXACT []\r\nxref: ICD10CM:77.8\r\nxref: IDC9CM:271.8\r\n```\r\n`UML_CUI` should be `UMLS_CUI` for the Unified Medical Language System:\r\n```\r\n[Term]\r\nid: DOID:0060313\r\nname: tracheomalacia\r\ndef: \"A tracheal disease characterized by flaccidity of the tracheal support cartilage.\" [url:http\\://en.wikipedia.org/wiki/Tracheomalacia]\r\ncomment: PRISM.\r\nsynonym: \"congenital tracheomalacia\" EXACT []\r\nxref: HP:0002779\r\nxref: ICD10CM:Q32.0\r\nxref: MSH:C557675\r\nxref: NCI:C98634\r\nxref: ORDO:95430\r\nxref: UML_CUI:C0392109\r\n```\r\n`UMLS` should be `UMLS_CUI` for consistency:\r\n```\r\n[Term]\r\nid: DOID:0060217\r\nname: Cogan-Reese syndrome\r\ndef: \"A rare eye disease characterized by variable iris atrophy, pigmented and pedunculated nodules located_in iris and attachment of the iris to the cornea (peripheral anterior synechiae) and characterized_by glaucoma.\" [url:http\\://en.wikipedia.org/wiki/Iridocorneal_endothelial_syndrome, url:http\\://rarediseases.info.nih.gov/gard/6125/cogan-reese-syndrome/resources/1]\r\nxref: ICD10CM:H21.1\r\nxref: MEDDRA:10059200\r\nxref: ORDO:98980\r\nxref: UMLS:C1168173\r\n```",
      "profile": 17,
      "published": "2015-05-12T03:11:16.676004Z",
      "thread": 68,
      "url": "/discussion/disease-ontology-feature-requests/68#3"
    },
    {
      "body_html": "<h1>UMLS cross-reference additions</h1>\r\n\r\n<p>Of our 137 DO slim terms, 6 did not contain a UMLS (<code>UMLS_CUI</code>) xref. We manually mapped 5 of these terms (<a href=\"https://raw.githubusercontent.com/dhimmel/disease-ontology/9fd75f14b17e01bebc97faf1bfa1b9025e9ce4de/requests/DO-slim-to-umls.tsv\">tsv download</a>):</p>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th>doid_code</th><th>doid_name</th><th>umls_cui</th><th>umls_name</th></tr></thead><tbody><tr><td>DOID:0050156</td><td>idiopathic pulmonary fibrosis</td><td>C1800706</td><td>Idiopathic Pulmonary Fibrosis</td></tr><tr><td>DOID:0050425</td><td>restless legs syndrome</td><td>C0035258</td><td>Restless Legs Syndrome</td></tr><tr><td>DOID:0050741</td><td>alcohol dependence</td><td>C0001973</td><td>Alcoholic Intoxication, Chronic</td></tr><tr><td>DOID:0050742</td><td>nicotine dependence</td><td>C0028043</td><td>Nicotine Dependence</td></tr><tr><td>DOID:0060119</td><td>pharynx cancer</td><td>C0031347</td><td>Pharyngeal Neoplasms</td></tr></tbody></table>",
      "body_md": "# UMLS cross-reference additions\r\n\r\nOf our 137 DO slim terms, 6 did not contain a UMLS (`UMLS_CUI`) xref. We manually mapped 5 of these terms ([tsv download](https://raw.githubusercontent.com/dhimmel/disease-ontology/9fd75f14b17e01bebc97faf1bfa1b9025e9ce4de/requests/DO-slim-to-umls.tsv)):\r\n\r\n| doid_code | doid_name | umls_cui | umls_name |\r\n|--------------|-------------------------------|----------|---------------------------------|\r\n| DOID:0050156 | idiopathic pulmonary fibrosis | C1800706 | Idiopathic Pulmonary Fibrosis |\r\n| DOID:0050425 | restless legs syndrome | C0035258 | Restless Legs Syndrome |\r\n| DOID:0050741 | alcohol dependence | C0001973 | Alcoholic Intoxication, Chronic |\r\n| DOID:0050742 | nicotine dependence | C0028043 | Nicotine Dependence |\r\n| DOID:0060119 | pharynx cancer | C0031347 | Pharyngeal Neoplasms |",
      "profile": 17,
      "published": "2015-05-12T03:37:08.730559Z",
      "thread": 68,
      "url": "/discussion/disease-ontology-feature-requests/68#4"
    },
    {
      "body_html": "<h1><em>Proof of concept</em> implementation</h1>\r\n\r\n<p>We implemented a topic cooccurrence calculator based on MEDLINE and used this method to identify <strong>disease-symptom relationships</strong> (<a href=\"//nbviewer.ipython.org/github/dhimmel/medline/blob/61590a7007d282429694e52a3a6743af99c1ab12/symptoms.ipynb\">notebook</a>, <a href=\"https://github.com/dhimmel/medline/blob/61590a7007d282429694e52a3a6743af99c1ab12/eutility.py\">API query script</a>, <a href=\"https://raw.githubusercontent.com/dhimmel/medline/gh-pages/data/disease-symptom-cooccurrence.tsv\">tsv of results</a>).</p>\r\n\r\n<p>First we created a <strong>disease set</strong> of 119 MeSH terms that mapped to DO slim diseases (<a href=\"https://github.com/dhimmel/medline/blob/61590a7007d282429694e52a3a6743af99c1ab12/data/DO-slim-to-mesh.tsv\">tsv of diseases</a>). Next, we created a <strong>symptom set</strong> of 438 MeSH terms by finding all descendants of <code>D012816</code> (Signs and Symptoms) (<a href=\"//nbviewer.ipython.org/github/dhimmel/mesh/blob/e561301360e6de2140dedeaa7c7e17ce4714eb7f/mesh.ipynb\">notebook</a>, <a href=\"https://github.com/dhimmel/mesh/blob/e561301360e6de2140dedeaa7c7e17ce4714eb7f/data/symptoms.tsv\">tsv of symptoms</a>).</p>\r\n\r\n<p>For each disease, we identified the articles where that disease was a major topic. For each symptom, we identified the articles where that symptom was a topic. We then identified the articles that contained both a disease major topic and symptom topic. We based further analysis only on these 392,397 articles that contain at least one disease–symptom cooccurrence.</p>\r\n\r\n<p>For each symptom–disease pair, we calculated:</p>\r\n\r\n<ul><li><code>cooccurrence</code> — the number of articles where the disease and symptom terms cooccurred.</li><li><code>expected</code> — the number of expected cooccurrences by chance based on each term's marginal frequency.</li><li><code>enrichment</code> — <code>cooccurrence</code> divided by <code>expected</code>.</li><li><code>odds_ratio</code> — the odds of <code>cooccurrence</code> divided by the odds of <code>expected</code>. This calculation appears to be slightly messed up due to non-integer expected counts.</li><li><code>p_fisher</code> — the p-value from Fisher's exact test evaluating whether the observed cooccurrence exceeded that expected by chance.</li></ul>\r\n\r\n<p><a href=\"/u/apankov\" class=\"username\">@apankov</a>, can you comment on the Fisher's exact test and whether there is a superior way to identify terms that significantly cooccur?</p>\r\n\r\n<p><a href=\"/u/b_good\" class=\"username\">@b_good</a> or others: do you know of better metrics for literature mining? One issue is that our approach may miss common symptoms that are not greatly enriched for any particular disease. The HSDN study <span class=\"citation\">[<a href=\"/doi/10.1038/ncomms5212\" class=\"citation\" data-key=\"10.1038/ncomms5212\">1</a>]</span> used a TF-IDF measure, but <a href=\"//thinklab.com/discussion/human-symptom-disease-network-mesh-id-matching/52#167\">we require</a> metrics that are comparable across diseases.</p>",
      "body_md": "# *Proof of concept* implementation\r\n\r\nWe implemented a topic cooccurrence calculator based on MEDLINE and used this method to identify **disease-symptom relationships** ([notebook](//nbviewer.ipython.org/github/dhimmel/medline/blob/61590a7007d282429694e52a3a6743af99c1ab12/symptoms.ipynb), [API query script](https://github.com/dhimmel/medline/blob/61590a7007d282429694e52a3a6743af99c1ab12/eutility.py), [tsv of results](https://raw.githubusercontent.com/dhimmel/medline/gh-pages/data/disease-symptom-cooccurrence.tsv)).\r\n\r\nFirst we created a **disease set** of 119 MeSH terms that mapped to DO slim diseases ([tsv of diseases](https://github.com/dhimmel/medline/blob/61590a7007d282429694e52a3a6743af99c1ab12/data/DO-slim-to-mesh.tsv)). Next, we created a **symptom set** of 438 MeSH terms by finding all descendants of `D012816` (Signs and Symptoms) ([notebook](//nbviewer.ipython.org/github/dhimmel/mesh/blob/e561301360e6de2140dedeaa7c7e17ce4714eb7f/mesh.ipynb), [tsv of symptoms](https://github.com/dhimmel/mesh/blob/e561301360e6de2140dedeaa7c7e17ce4714eb7f/data/symptoms.tsv)).\r\n\r\nFor each disease, we identified the articles where that disease was a major topic. For each symptom, we identified the articles where that symptom was a topic. We then identified the articles that contained both a disease major topic and symptom topic. We based further analysis only on these 392,397 articles that contain at least one disease--symptom cooccurrence.\r\n\r\nFor each symptom--disease pair, we calculated:\r\n\r\n+ `cooccurrence` -- the number of articles where the disease and symptom terms cooccurred.\r\n+ `expected` -- the number of expected cooccurrences by chance based on each term's marginal frequency.\r\n+ `enrichment` -- `cooccurrence` divided by `expected`.\r\n+ `odds_ratio` -- the odds of `cooccurrence` divided by the odds of `expected`. This calculation appears to be slightly messed up due to non-integer expected counts.\r\n+ `p_fisher` -- the p-value from Fisher's exact test evaluating whether the observed cooccurrence exceeded that expected by chance.\r\n\r\n@apankov, can you comment on the Fisher's exact test and whether there is a superior way to identify terms that significantly cooccur?\r\n\r\n@b_good or others: do you know of better metrics for literature mining? One issue is that our approach may miss common symptoms that are not greatly enriched for any particular disease. The HSDN study [@10.1038/ncomms5212] used a TF-IDF measure, but [we require](//thinklab.com/discussion/human-symptom-disease-network-mesh-id-matching/52#167) metrics that are comparable across diseases.",
      "profile": 17,
      "published": "2015-05-13T19:51:58.974363Z",
      "thread": 67,
      "url": "/discussion/mining-knowledge-from-medline-articles-and-their-indexed-mesh-terms/67#2"
    },
    {
      "body_html": "<p>I think the Fisher's exact test will be accepted well by reviewers, but Barnard's test could be a good alternative. Otherwise, if you can calculate a p-value based on permutation (or get a bootstrapped estimates for the variance of the number of expected cooccurrences) , that could be an easy, straightforward approach.</p>",
      "body_md": "I think the Fisher's exact test will be accepted well by reviewers, but Barnard's test could be a good alternative. Otherwise, if you can calculate a p-value based on permutation (or get a bootstrapped estimates for the variance of the number of expected cooccurrences) , that could be an easy, straightforward approach.\r\n\r\n",
      "profile": 84,
      "published": "2015-05-13T23:39:44.558228Z",
      "thread": 67,
      "url": "/discussion/mining-knowledge-from-medline-articles-and-their-indexed-mesh-terms/67#3"
    },
    {
      "body_html": "<p>Thanks <a href=\"/u/apankov\" class=\"username\">@apankov</a>. I couldn't find a python implementation of <a href=\"https://en.wikipedia.org/wiki/Barnard%27s_test\">Barnard's test</a> <span class=\"citation\">[<a href=\"/doi/10.1038/156177a0\" class=\"citation\" data-key=\"10.1038/156177a0\">1</a>, <a href=\"/doi/10.1093/biomet/34.1-2.123\" class=\"citation\" data-key=\"10.1093/biomet/34.1-2.123\">2</a>]</span>, so I think we'll stick with <a href=\"https://en.wikipedia.org/wiki/Fisher%27s_exact_test\">Fisher's exact test</a> <span class=\"citation\">[<a href=\"/doi/10.2307/2340521\" class=\"citation\" data-key=\"10.2307/2340521\">3</a>]</span> for simplicity. The fidelity of <em>p</em>-values is not a major concern here.</p>\r\n\r\n<p>However, it has occurred to me that in our above post, we incorrectly created the contingency table for the exact test. We now <a href=\"https://github.com/dhimmel/medline/blob/a3e35d7dd58fb64f4043247e661259c743dce7d5/cooccurrence.py#L44\">construct it</a> similarly to <a href=\"https://dx.doi.org/10.1016/j.jbi.2006.11.003#tbl1\">Table 1 of this paper</a> <span class=\"citation\">[<a href=\"/doi/10.1016/j.jbi.2006.11.003\" class=\"citation\" data-key=\"10.1016/j.jbi.2006.11.003\">4</a>]</span> so that the contingency table is:</p>\r\n\r\n<p></p><div class=\"math\">$$$\r\n\\begin{bmatrix}\r\na &amp; b\\\\\r\nc &amp; d\r\n\\end{bmatrix}\r\n$$$</div>\r\n\r\n<p>where</p>\r\n\r\n<ul><li><em>a</em> is the number of studies with both the disease and the symptom (<code>cooccurrence</code>)</li><li><em>b</em> is the number of studies with the disease and without the symptom</li><li><em>c</em> is the number of studies without the disease and with the symptom</li><li><em>d</em> is the number of studies without either the disease or symptom</li></ul>\r\n\r\n<p>The revised symptom–disease pair tsv file is <a href=\"https://raw.githubusercontent.com/dhimmel/medline/a3e35d7dd58fb64f4043247e661259c743dce7d5/data/disease-symptom-cooccurrence.tsv\">available here</a>.</p>",
      "body_md": "Thanks @apankov. I couldn't find a python implementation of [Barnard's test](https://en.wikipedia.org/wiki/Barnard%27s_test) [@10.1038/156177a0 @10.1093/biomet/34.1-2.123], so I think we'll stick with [Fisher's exact test](https://en.wikipedia.org/wiki/Fisher%27s_exact_test) [@10.2307/2340521] for simplicity. The fidelity of *p*-values is not a major concern here.\r\n\r\nHowever, it has occurred to me that in our above post, we incorrectly created the contingency table for the exact test. We now [construct it](https://github.com/dhimmel/medline/blob/a3e35d7dd58fb64f4043247e661259c743dce7d5/cooccurrence.py#L44) similarly to [Table 1 of this paper](https://dx.doi.org/10.1016/j.jbi.2006.11.003#tbl1) [@10.1016/j.jbi.2006.11.003] so that the contingency table is:\r\n\r\n$$$\r\n\\begin{bmatrix}\r\na & b\\\\\r\nc & d\r\n\\end{bmatrix}\r\n$$$\r\n\r\nwhere\r\n\r\n+ *a* is the number of studies with both the disease and the symptom (`cooccurrence`)\r\n+ *b* is the number of studies with the disease and without the symptom\r\n+ *c* is the number of studies without the disease and with the symptom\r\n+ *d* is the number of studies without either the disease or symptom\r\n\r\nThe revised symptom--disease pair tsv file is [available here](https://raw.githubusercontent.com/dhimmel/medline/a3e35d7dd58fb64f4043247e661259c743dce7d5/data/disease-symptom-cooccurrence.tsv).",
      "profile": 17,
      "published": "2015-05-15T22:54:27.488404Z",
      "thread": 67,
      "url": "/discussion/mining-knowledge-from-medline-articles-and-their-indexed-mesh-terms/67#4"
    },
    {
      "body_html": "<p>We have calculated molecular (aka chemical/structural) similarities between DrugBank compounds. First, we retrieved the compound structures as an SDF file from the <a href=\"http://www.drugbank.ca/downloads#structures\">download page</a>. Then we calculated extended connectivity fingerprints for each compound using the Morgan/circular method <span class=\"citation\">[<a href=\"/doi/10.1021/ci100050t\" class=\"citation\" data-key=\"10.1021/ci100050t\">1</a>]</span>. We chose a radius of 2, since \"Typically, two iterations is sufficient for fingerprints that will be used for similarity or clustering. <span class=\"citation\">[<a href=\"/doi/10.1021/ci100050t\" class=\"citation\" data-key=\"10.1021/ci100050t\">1</a>]</span>\" Finally, we computed all pairwise similarities <a href=\"http://www.rdkit.org/Python_Docs/rdkit.DataStructs.cDataStructs-module.html#DiceSimilarity\">using</a> the <a href=\"https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\">Dice coefficient</a> <span class=\"citation\">[<a href=\"/doi/10.2307/1932409\" class=\"citation\" data-key=\"10.2307/1932409\">2</a>]</span>.</p>\r\n\r\n<p>The similarities for the subset of DrugBank compounds included in our network is <a href=\"https://github.com/dhimmel/drugbank/blob/55587651ee9417e4621707dac559d84c984cf5fa/data/similarity-slim.tsv.gz\">available here</a>. We posted the full set of similarities (for all DrugBank compounds with structures) on <a href=\"https://dx.doi.org/10.6084/m9.figshare.1418386\">figshare</a> <span class=\"citation\">[<a href=\"/doi/10.6084/m9.figshare.1418386\" class=\"citation\" data-key=\"10.6084/m9.figshare.1418386\">3</a>]</span>.</p>\r\n\r\n<p>See the <a href=\"http://nbviewer.ipython.org/github/dhimmel/drugbank/blob/9eb1e15ada5e6579e5aea1cae784c10602037b05/similarity.ipynb\">notebook of the analysis</a> for more details.</p>",
      "body_md": "We have calculated molecular (aka chemical/structural) similarities between DrugBank compounds. First, we retrieved the compound structures as an SDF file from the [download page](http://www.drugbank.ca/downloads#structures). Then we calculated extended connectivity fingerprints for each compound using the Morgan/circular method [@10.1021/ci100050t]. We chose a radius of 2, since \"Typically, two iterations is sufficient for fingerprints that will be used for similarity or clustering. [@10.1021/ci100050t]\" Finally, we computed all pairwise similarities [using] (http://www.rdkit.org/Python_Docs/rdkit.DataStructs.cDataStructs-module.html#DiceSimilarity) the [Dice coefficient](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient) [@10.2307/1932409].\r\n\r\nThe similarities for the subset of DrugBank compounds included in our network is [available here](https://github.com/dhimmel/drugbank/blob/55587651ee9417e4621707dac559d84c984cf5fa/data/similarity-slim.tsv.gz). We posted the full set of similarities (for all DrugBank compounds with structures) on [figshare](https://dx.doi.org/10.6084/m9.figshare.1418386) [@10.6084/m9.figshare.1418386].\r\n\r\nSee the [notebook of the analysis](http://nbviewer.ipython.org/github/dhimmel/drugbank/blob/9eb1e15ada5e6579e5aea1cae784c10602037b05/similarity.ipynb) for more details.",
      "profile": 17,
      "published": "2015-05-19T02:56:31.081597Z",
      "thread": 70,
      "url": "/discussion/calculating-molecular-similarities-between-drugbank-compounds/70"
    },
    {
      "body_html": "<p>I came across the following paper that has useful information regarding the LINCS data integration standards:</p>\r\n\r\n<blockquote><p>Metadata Standard and Data Exchange Specifications to Describe, Model, and Integrate Complex and Diverse High-Throughput Screening Data from the Library of Integrated Network-based Cellular Signatures (LINCS). <span class=\"citation\">[<a href=\"/doi/10.1177/1087057114522514\" class=\"citation\" data-key=\"10.1177/1087057114522514\">1</a>]</span></p></blockquote>",
      "body_md": "I came across the following paper that has useful information regarding the LINCS data integration standards:\r\n\r\n> Metadata Standard and Data Exchange Specifications to Describe, Model, and Integrate Complex and Diverse High-Throughput Screening Data from the Library of Integrated Network-based Cellular Signatures (LINCS). [@10.1177/1087057114522514]",
      "profile": 17,
      "published": "2015-05-20T11:23:17.925877Z",
      "thread": 51,
      "url": "/discussion/unichem-mapping-to-lincs-small-molecules/51#7"
    },
    {
      "body_html": "<h1>Anatomy–Disease Relationships</h1>\r\n\r\n<p>The Uberon ontology <span class=\"citation\">[<a href=\"/doi/10.1186/gb-2012-13-1-r5\" class=\"citation\" data-key=\"10.1186/gb-2012-13-1-r5\">1</a>]</span> of anatomical structures includes MeSH <a href=\"https://github.com/dhimmel/uberon/blob/0c50839eb3e58a89e81018978f269210d2212d58/data/mesh-map.tsv\">cross-references</a>. Thus, we performed our MEDLINE cooccurrence analysis described above to find relationships between diseases and anatomical structures (<a href=\"http://nbviewer.ipython.org/github/dhimmel/medline/blob/ef0aef8b9c4bfcaa4cf46f03efe6d0ea0dc5d13b/tissues.ipynb\">notebook</a>, <a href=\"https://raw.githubusercontent.com/dhimmel/medline/ef0aef8b9c4bfcaa4cf46f03efe6d0ea0dc5d13b/data/disease-uberon-cooccurrence.tsv\">tsv download</a>).</p>\r\n\r\n<p>The ability of this method to capture disease localization was exceptional. For example, the top five terms by <em>p</em>-value for multiple sclerosis were:</p>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th>mesh_name</th><th>cooccurrence</th><th>expected</th><th>enrichment</th><th>odds_ratio</th><th>p_fisher</th></tr></thead><tbody><tr><td>Central Nervous System</td><td>881</td><td>38.6</td><td>22.8</td><td>34.3</td><td>0.000</td></tr><tr><td>Spinal Cord</td><td>1492</td><td>80.8</td><td>18.5</td><td>27.5</td><td>0.000</td></tr><tr><td>Myelin Sheath</td><td>1006</td><td>19.9</td><td>50.5</td><td>146.8</td><td>0.000</td></tr><tr><td>Brain</td><td>4777</td><td>778.3</td><td>6.1</td><td>11.5</td><td>0.000</td></tr><tr><td>Optic Nerve</td><td>372</td><td>36.5</td><td>10.2</td><td>11.9</td><td>0.000</td></tr></tbody></table>\r\n\r\n<p>One improvement would be to exclude Uberon terms that don't exist in humans such as venom (<code>UBERON:0007113</code>). Additionally, there are some <a href=\"https://github.com/obophenotype/uberon/issues/698#issuecomment-104079963\">Uberon–MeSH mapping issues</a> that should get resolved soon allowing us to update the analysis.</p>",
      "body_md": "# Anatomy--Disease Relationships\r\n\r\nThe Uberon ontology [@10.1186/gb-2012-13-1-r5] of anatomical structures includes MeSH [cross-references](https://github.com/dhimmel/uberon/blob/0c50839eb3e58a89e81018978f269210d2212d58/data/mesh-map.tsv). Thus, we performed our MEDLINE cooccurrence analysis described above to find relationships between diseases and anatomical structures ([notebook](http://nbviewer.ipython.org/github/dhimmel/medline/blob/ef0aef8b9c4bfcaa4cf46f03efe6d0ea0dc5d13b/tissues.ipynb), [tsv download](https://raw.githubusercontent.com/dhimmel/medline/ef0aef8b9c4bfcaa4cf46f03efe6d0ea0dc5d13b/data/disease-uberon-cooccurrence.tsv)).\r\n\r\nThe ability of this method to capture disease localization was exceptional. For example, the top five terms by *p*-value for multiple sclerosis were:\r\n\r\n| mesh_name | cooccurrence | expected | enrichment | odds_ratio | p_fisher |\r\n|------------------------|--------------|----------|------------|------------|----------|\r\n| Central Nervous System | 881 | 38.6 | 22.8 | 34.3 | 0.000 |\r\n| Spinal Cord | 1492 | 80.8 | 18.5 | 27.5 | 0.000 |\r\n| Myelin Sheath | 1006 | 19.9 | 50.5 | 146.8 | 0.000 |\r\n| Brain | 4777 | 778.3 | 6.1 | 11.5 | 0.000 |\r\n| Optic Nerve | 372 | 36.5 | 10.2 | 11.9 | 0.000 |\r\n\r\nOne improvement would be to exclude Uberon terms that don't exist in humans such as venom (`UBERON:0007113`). Additionally, there are some [Uberon--MeSH mapping issues](https://github.com/obophenotype/uberon/issues/698#issuecomment-104079963) that should get resolved soon allowing us to update the analysis.",
      "profile": 17,
      "published": "2015-05-21T03:35:08.975314Z",
      "thread": 67,
      "url": "/discussion/mining-knowledge-from-medline-articles-and-their-indexed-mesh-terms/67#5"
    },
    {
      "body_html": "<p>Hi <a href=\"/u/vsmalladi\" class=\"username\">@vsmalladi</a>, we have proceeded with Uberon and incorporated the MeSH cross-references. Specifically, we <a href=\"http://thinklab.com/d/67#229\">identified</a> disease–anatomy localization using literature mining.</p>\r\n\r\n<p>We would like a way to restrict terms to structures in humans. Does anyone know how to implement a species filter?</p>\r\n\r\n<p>We would also like to incorporate the Cell Ontology (CL) for cell information <span class=\"citation\">[<a href=\"/doi/10.1186/gb-2005-6-2-r21\" class=\"citation\" data-key=\"10.1186/gb-2005-6-2-r21\">1</a>]</span>. However, there is a <a href=\"https://code.google.com/p/cell-ontology/issues/detail?id=146#c2\">MeSH xref issue</a> that will need to be remedied first.</p>",
      "body_md": "Hi @vsmalladi, we have proceeded with Uberon and incorporated the MeSH cross-references. Specifically, we [identified](http://thinklab.com/d/67#229) disease--anatomy localization using literature mining.\r\n\r\nWe would like a way to restrict terms to structures in humans. Does anyone know how to implement a species filter?\r\n\r\nWe would also like to incorporate the Cell Ontology (CL) for cell information [@10.1186/gb-2005-6-2-r21]. However, there is a [MeSH xref issue](https://code.google.com/p/cell-ontology/issues/detail?id=146#c2) that will need to be remedied first.",
      "profile": 17,
      "published": "2015-05-21T03:46:33.583308Z",
      "thread": 41,
      "url": "/discussion/tissue-node/41#6"
    },
    {
      "body_html": "<h1>Combining z-scores across multiple signatures</h1>\r\n\r\n<p>To create consensus signatures for a compound, we have been taking a weighted average of z-scores (steps 4–5 <a href=\"#3\">above</a>). It has occurred to us that this is an underpowered method, just as averaging p-values is a weak method of meta-analysis. </p>\r\n\r\n<p>Instead we can use <a href=\"https://en.wikipedia.org/wiki/Fisher%27s_method#Relation_to_Stouffer.27s_Z-score_method\">Stouffer's method</a> to meta-analyze z-scores <span class=\"citation\">[<a href=\"http://press.princeton.edu/titles/2692.html\" class=\"citation\" data-key=\"stouffer\">1</a>]</span>. This method accepts weights (calculated in steps 2–3 <a href=\"#3\">above</a>). The formula is below for weight vector <em>w</em> and z-score vector <em>Z</em>:</p>\r\n\r\n<div class=\"math\">$$$\r\nZ \\sim \\frac{\\sum_{i=1}^k w_iZ_i}{\\sqrt{\\sum_{i=1}^k w_i^2}}\r\n$$$</div>\r\n\r\n",
      "body_md": "# Combining z-scores across multiple signatures\r\n\r\nTo create consensus signatures for a compound, we have been taking a weighted average of z-scores (steps 4--5 [above](#3)). It has occurred to us that this is an underpowered method, just as averaging p-values is a weak method of meta-analysis. \r\n\r\nInstead we can use [Stouffer's method](https://en.wikipedia.org/wiki/Fisher%27s_method#Relation_to_Stouffer.27s_Z-score_method) to meta-analyze z-scores [@stouffer]. This method accepts weights (calculated in steps 2--3 [above](#3)). The formula is below for weight vector *w* and z-score vector *Z*:\r\n\r\n$$$\r\nZ \\sim \\frac{\\sum_{i=1}^k w_iZ_i}{\\sqrt{\\sum_{i=1}^k w_i^2}}\r\n$$$\r\n\r\n[@stouffer]: http://press.princeton.edu/titles/2692.html \"Stouffer SA, Suchman EA, DeVinney LC, Star SA, Williams RM. (1949) *The American Soldier, Vol.1: Adjustment during Army Life*. Princeton University Press\"",
      "profile": 17,
      "published": "2015-05-21T05:04:41.405554Z",
      "thread": 43,
      "url": "/discussion/computing-consensus-transcriptional-profiles-for-lincs-l1000-perturbations/43#5"
    },
    {
      "body_html": "<p><a href=\"/u/dhimmel\" class=\"username\">@dhimmel</a> You can restrict structures by NCBI taxon ID Human would be Taxon:9606</p>\r\n\r\n<p>Can you elaborate on the MeSH issue? I might be able to help. </p>",
      "body_md": "@dhimmel You can restrict structures by NCBI taxon ID Human would be Taxon:9606\r\n\r\nCan you elaborate on the MeSH issue? I might be able to help. ",
      "profile": 35,
      "published": "2015-05-22T05:05:57.647307Z",
      "thread": 41,
      "url": "/discussion/tissue-node/41#7"
    },
    {
      "body_html": "<p><a href=\"/u/dhimmel\" class=\"username\">@dhimmel</a> To understand what relationships you should compute closure on I recommend reading <a href=\"http://geneontology.org/page/ontology-relations\" target=\"_blank\">http://geneontology.org/page/ontology-relations</a></p>\r\n\r\n<p>I would add <code>part_of</code><code> and maybe </code><code>has_part</code>` first before exploring the other relationships. </p>\r\n\r\n<p>Another option for reasoning that can take advantage of the relationships is <a href=\"https://github.com/owlcollab/owltools\" target=\"_blank\">https://github.com/owlcollab/owltools</a></p>",
      "body_md": "@dhimmel To understand what relationships you should compute closure on I recommend reading http://geneontology.org/page/ontology-relations\r\n\r\nI would add ```part_of```` and maybe ```has_part``` first before exploring the other relationships. \r\n\r\nAnother option for reasoning that can take advantage of the relationships is https://github.com/owlcollab/owltools",
      "profile": 35,
      "published": "2015-05-22T21:39:45.073691Z",
      "thread": 39,
      "url": "/discussion/compiling-gene-ontology-annotations-into-an-easy-to-use-format/39#5"
    },
    {
      "body_html": "<h1>Restricting to human terms</h1>\r\n\r\n<p><a href=\"/u/vsmalladi\" class=\"username\">@vsmalladi</a>, thanks for the pointer. In the <a href=\"//berkeleybop.org/ontologies/uberon/ext.obo\">obo</a> header I see:<br></p>\r\n\r\n<p></p><pre><code class=\"no-highlight hljs\">treat-xrefs-as-reverse-genus-differentia: DHBA part_of NCBITaxon:9606\r\ntreat-xrefs-as-reverse-genus-differentia: EHDAA2 part_of NCBITaxon:9606\r\ntreat-xrefs-as-reverse-genus-differentia: FMA part_of NCBITaxon:9606\r\ntreat-xrefs-as-reverse-genus-differentia: HBA part_of NCBITaxon:9606\r\ntreat-xrefs-as-reverse-genus-differentia: HsapDv part_of NCBITaxon:9606</code></pre>\r\n\r\n<p>Therefore I speculate the best way to identify human applicable terms would be to identify all terms with a cross-reference to the above resources and all broader terms in the hierarchy. Is that what you suggest?</p>",
      "body_md": "# Restricting to human terms\r\n\r\n@vsmalladi, thanks for the pointer. In the [obo](//berkeleybop.org/ontologies/uberon/ext.obo) header I see:\r\n```\r\ntreat-xrefs-as-reverse-genus-differentia: DHBA part_of NCBITaxon:9606\r\ntreat-xrefs-as-reverse-genus-differentia: EHDAA2 part_of NCBITaxon:9606\r\ntreat-xrefs-as-reverse-genus-differentia: FMA part_of NCBITaxon:9606\r\ntreat-xrefs-as-reverse-genus-differentia: HBA part_of NCBITaxon:9606\r\ntreat-xrefs-as-reverse-genus-differentia: HsapDv part_of NCBITaxon:9606\r\n```\r\nTherefore I speculate the best way to identify human applicable terms would be to identify all terms with a cross-reference to the above resources and all broader terms in the hierarchy. Is that what you suggest?",
      "profile": 17,
      "published": "2015-05-23T21:52:32.772955Z",
      "thread": 41,
      "url": "/discussion/tissue-node/41#8"
    },
    {
      "body_html": "<p><a href=\"/u/dhimmel\" class=\"username\">@dhimmel</a> </p>\r\n\r\n<p>Actually what you want to do is for each term filter on <code>present_in_taxon</code>:</p>\r\n\r\n<p>   property_value: present_in_taxon NCBITaxon:7777</p>",
      "body_md": "@dhimmel \r\n\r\nActually what you want to do is for each term filter on ```present_in_taxon```:\r\n\r\n   property_value: present_in_taxon NCBITaxon:7777\r\n\r\n",
      "profile": 35,
      "published": "2015-05-26T21:20:05.133071Z",
      "thread": 41,
      "url": "/discussion/tissue-node/41#9"
    },
    {
      "body_html": "<p>Our validation manuscript has been published, <a href=\"/u/dhimmel\" class=\"username\">@dhimmel</a>: <a href=\"http://aci.schattauer.de/en/contents/current-issue/issue/special/manuscript/24377/show.html\" target=\"_blank\">http://aci.schattauer.de/en/contents/current-issue/issue/special/manuscript/24377/show.html</a></p>\r\n\r\n<p>I'll see what I can do about sharing the data, but unfortunately I've got travel coming up along with several deadlines, so it may be a little while longer before I'm able to do that.</p>",
      "body_md": "Our validation manuscript has been published, @dhimmel: http://aci.schattauer.de/en/contents/current-issue/issue/special/manuscript/24377/show.html\r\n\r\nI'll see what I can do about sharing the data, but unfortunately I've got travel coming up along with several deadlines, so it may be a little while longer before I'm able to do that.",
      "profile": 77,
      "published": "2015-05-27T14:26:51.387135Z",
      "thread": 21,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#22"
    },
    {
      "body_html": "<p><a href=\"/u/vsmalladi\" class=\"username\">@vsmalladi</a>, <a href=\"http://berkeleybop.org/ontologies/uberon/ext.obo\">the obo</a> only contains <code>relationship: present_in_taxon NCBITaxon:9606</code> for four terms. Is there a more extensive listing of <code>present_in_taxon</code> relationships that you are aware of?</p>\r\n\r\n<p>Meanwhile the <a href=\"https://github.com/dhimmel/uberon/blob/50c311f3d15744e8c559ea76178bdd7542f6d16b/data/mesh-map.tsv\">results</a> of the <code>treat-xrefs-as-reverse-genus-differentia</code> method look satisfactory. Terms were annotated as human (<code>in_human = 1</code>) if they or any terms they subsumed (along <code>is_a</code>, <code>part_of</code>, and <code>develops_from</code> relationships) contained a cross-reference to any of the following resources: <code>DHBA</code>, <code>EHDAA2</code>, <code>FMA</code>, <code>HBA</code>, and <code>HsapDv</code> (<a href=\"https://github.com/dhimmel/uberon/blob/50c311f3d15744e8c559ea76178bdd7542f6d16b/mesh-map.ipynb\">notebook</a>). Most terms where <code>in_human = 0</code> are not appropriate for humans.</p>",
      "body_md": "@vsmalladi, [the obo](http://berkeleybop.org/ontologies/uberon/ext.obo) only contains `relationship: present_in_taxon NCBITaxon:9606` for four terms. Is there a more extensive listing of `present_in_taxon` relationships that you are aware of?\r\n\r\nMeanwhile the [results](https://github.com/dhimmel/uberon/blob/50c311f3d15744e8c559ea76178bdd7542f6d16b/data/mesh-map.tsv) of the `treat-xrefs-as-reverse-genus-differentia` method look satisfactory. Terms were annotated as human (`in_human = 1`) if they or any terms they subsumed (along `is_a`, `part_of`, and `develops_from` relationships) contained a cross-reference to any of the following resources: `DHBA`, `EHDAA2`, `FMA`, `HBA`, and `HsapDv` ([notebook](https://github.com/dhimmel/uberon/blob/50c311f3d15744e8c559ea76178bdd7542f6d16b/mesh-map.ipynb)). Most terms where `in_human = 0` are not appropriate for humans.\r\n\r\n\r\n",
      "profile": 17,
      "published": "2015-06-02T18:51:21.520906Z",
      "thread": 41,
      "url": "/discussion/tissue-node/41#10"
    },
    {
      "body_html": "<p><a href=\"/u/caseygreene\" class=\"username\">@caseygreene</a>, thanks for the description of your database setup. In the immediate term, I don't need any of the advanced features that your design accommodates such as elastic search and efficient lookup, so I just did a simple <a href=\"//nbviewer.ipython.org/github/dhimmel/entrez-gene/blob/2208158e9e9691d54d48faf3b6b9c2c8f69d7914/retrieve.ipynb\">parsing</a> of the human subset and exported three tsv files (<a href=\"https://github.com/dhimmel/entrez-gene/blob/2208158e9e9691d54d48faf3b6b9c2c8f69d7914/data/genes-human.tsv\">genes</a>, <a href=\"https://github.com/dhimmel/entrez-gene/blob/2208158e9e9691d54d48faf3b6b9c2c8f69d7914/data/symbols-human.tsv\">symbols</a>, <a href=\"https://github.com/dhimmel/entrez-gene/blob/2208158e9e9691d54d48faf3b6b9c2c8f69d7914/data/xrefs-human.tsv\">cross-references</a>).</p>\r\n\r\n<p>Therefore, don't reprioritize for me, but I think the pypi package is a great idea. It looks like your <a href=\"//tribe.greenelab.com/\">Tribe</a> API already supports Entrez gene lookup. However, I'm confused about the usage, since the <a href=\"//tribe.greenelab.com/#/demo/speak\">demo code</a> is equivalent to:</p>\r\n\r\n<p></p><pre><code class=\"python\">import requests\r\npayload = {'show_tip': 'true'}\r\nresponse = requests.get('http://tribe.greenelab.com/api/v1/geneset/', params=payload)</code></pre>\r\n\r\n<p>How do you specify the query string (the gene symbol/name for which you want the GeneID)? It may be the case that your API already provides most of the functionality a user may need. In that case, a local Entrez Gene database may not be needed at all.</p>\r\n\r\n<p>Summary: my vote is for a powerful, well-documented API as a primary resource, with an open source codebase.</p>",
      "body_md": "@caseygreene, thanks for the description of your database setup. In the immediate term, I don't need any of the advanced features that your design accommodates such as elastic search and efficient lookup, so I just did a simple [parsing](//nbviewer.ipython.org/github/dhimmel/entrez-gene/blob/2208158e9e9691d54d48faf3b6b9c2c8f69d7914/retrieve.ipynb) of the human subset and exported three tsv files ([genes](https://github.com/dhimmel/entrez-gene/blob/2208158e9e9691d54d48faf3b6b9c2c8f69d7914/data/genes-human.tsv), [symbols](https://github.com/dhimmel/entrez-gene/blob/2208158e9e9691d54d48faf3b6b9c2c8f69d7914/data/symbols-human.tsv), [cross-references](https://github.com/dhimmel/entrez-gene/blob/2208158e9e9691d54d48faf3b6b9c2c8f69d7914/data/xrefs-human.tsv)).\r\n\r\nTherefore, don't reprioritize for me, but I think the pypi package is a great idea. It looks like your [Tribe](//tribe.greenelab.com/) API already supports Entrez gene lookup. However, I'm confused about the usage, since the [demo code](//tribe.greenelab.com/#/demo/speak) is equivalent to:\r\n\r\n```python\r\nimport requests\r\npayload = {'show_tip': 'true'}\r\nresponse = requests.get('http://tribe.greenelab.com/api/v1/geneset/', params=payload)\r\n```\r\n\r\nHow do you specify the query string (the gene symbol/name for which you want the GeneID)? It may be the case that your API already provides most of the functionality a user may need. In that case, a local Entrez Gene database may not be needed at all.\r\n\r\nSummary: my vote is for a powerful, well-documented API as a primary resource, with an open source codebase.",
      "profile": 17,
      "published": "2015-06-07T23:11:43.668288Z",
      "thread": 34,
      "url": "/discussion/using-entrez-gene-as-our-gene-vocabulary/34#5"
    },
    {
      "body_html": "<h2>Background</h2>\r\n\r\n<p>GWAS uncover disease-associated loci, but due to sparse genotyping arrays and linkage disequilibrium (LD), identifying the specific SNP driving the association is difficult. Therefore, GWAS usually report the most significant hit as the single lead SNP for a loci, leaving the identification of a causal SNP for later research. Often multiple GWAS of the same disease will identify different lead SNPs in the same region, presumable all tagging the same causal variant. Therefore, around any lead SNP is a <em>region of indetermination</em>—a genomic window in which the SNP driving the association is likely to reside.</p>\r\n\r\n<h2>Application</h2>\r\n\r\n<p>When extracting disease-gene associations from the <a href=\"//www.ebi.ac.uk/gwas/\">GWAS Catalog</a> <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkt1229\" class=\"citation\" data-key=\"10.1093/nar/gkt1229\">1</a>]</span>, we collapse multiple associations for the same disease into loci (regions) <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1004259\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1004259\">2</a>]</span>. Starting with lead SNPs for each association, we find the corresponding windows and overlap them into genomically disjoint sets.</p>\r\n\r\n<p>Previously, we retrieved windows for GWAS lead-SNPs from the <a href=\"https://www.broadinstitute.org/mpg/dapple/dappleTMP.php\">DAPPLE</a> <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pgen.1001273\" class=\"citation\" data-key=\"10.1371/journal.pgen.1001273\">3</a>]</span> wingspan files. DAPPLE windows \"were calculated for each lead-SNP by finding the furthest upstream and downstream SNPs where <span class=\"math\">$$r^2 &gt; 0.5$$</span> and extending outwards to the next recombination hotspot <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1004259\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1004259\">2</a>]</span>.\"</p>\r\n\r\n<p>However, DAPPLE relied on HapMap <span class=\"citation\">[<a href=\"/doi/10.1038/nature04226\" class=\"citation\" data-key=\"10.1038/nature04226\">4</a>]</span> for LD data, which is now outdated. Many SNPs in the GWAS catalog are not in HapMap. Since HapMap is missing many SNPs, extending to the next recombination hotspot was necessary.</p>\r\n\r\n<h1>Questions</h1>\r\n\r\n<ol><li><strong>Given a lead SNP, how should we identify the furthest upstream and downstream SNPs with <span class=\"math\">$$r^2$$</span> exceeding a given threshold?</strong> Which data and tools should we use?</li><li>In the context of GWAS loci, is <span class=\"math\">$$r^2 &gt; 0.5$$</span> too low of a threshold for windows?</li><li>Is the recombination hotspot extension necessary?</li></ol>\r\n\r\n<p>We would like to identify windows for ~5000 SNPs which are identified in dbSNP build 142 rsids.</p>",
      "body_md": "## Background\r\n\r\nGWAS uncover disease-associated loci, but due to sparse genotyping arrays and linkage disequilibrium (LD), identifying the specific SNP driving the association is difficult. Therefore, GWAS usually report the most significant hit as the single lead SNP for a loci, leaving the identification of a causal SNP for later research. Often multiple GWAS of the same disease will identify different lead SNPs in the same region, presumable all tagging the same causal variant. Therefore, around any lead SNP is a *region of indetermination*---a genomic window in which the SNP driving the association is likely to reside.\r\n\r\n## Application\r\n\r\nWhen extracting disease-gene associations from the [GWAS Catalog](//www.ebi.ac.uk/gwas/) [@10.1093/nar/gkt1229], we collapse multiple associations for the same disease into loci (regions) [@10.1371/journal.pcbi.1004259]. Starting with lead SNPs for each association, we find the corresponding windows and overlap them into genomically disjoint sets.\r\n\r\nPreviously, we retrieved windows for GWAS lead-SNPs from the [DAPPLE](https://www.broadinstitute.org/mpg/dapple/dappleTMP.php) [@10.1371/journal.pgen.1001273] wingspan files. DAPPLE windows \"were calculated for each lead-SNP by finding the furthest upstream and downstream SNPs where $$r^2 > 0.5$$ and extending outwards to the next recombination hotspot [@10.1371/journal.pcbi.1004259].\"\r\n\r\nHowever, DAPPLE relied on HapMap [@10.1038/nature04226] for LD data, which is now outdated. Many SNPs in the GWAS catalog are not in HapMap. Since HapMap is missing many SNPs, extending to the next recombination hotspot was necessary.\r\n\r\n# Questions\r\n\r\n1. **Given a lead SNP, how should we identify the furthest upstream and downstream SNPs with $$r^2$$ exceeding a given threshold?** Which data and tools should we use?\r\n2. In the context of GWAS loci, is $$r^2 > 0.5$$ too low of a threshold for windows?\r\n3. Is the recombination hotspot extension necessary?\r\n\r\nWe would like to identify windows for ~5000 SNPs which are identified in dbSNP build 142 rsids.",
      "profile": 17,
      "published": "2015-06-08T18:54:53.853760Z",
      "thread": 71,
      "url": "/discussion/calculating-genomic-windows-for-gwas-lead-snps/71"
    },
    {
      "body_html": "<p>I would suggest using 1000 genomes for the LD calculation here with a more stringent r^2 cutoff (maybe 0.8?). Some LD information is available through their browser</p>\r\n\r\n<p><a href=\"http://browser.1000genomes.org/Homo_sapiens/Location/Genome?db=core;r=2:31451742-31452000\">http://browser.1000genomes.org/Homo_sapiens/Location/Genome?db=core;r=2:31451742-31452000</a></p>\r\n\r\n<p>Here is a thread discussing similar ideas:<br><a href=\"https://www.biostars.org/p/2909/\">https://www.biostars.org/p/2909/</a></p>\r\n\r\n<p>The other resource of interest is the ExAC dataset: <a href=\"http://exac.broadinstitute.org/\">http://exac.broadinstitute.org/</a> I don't think the LD data is available, but it's worthwhile reaching out to them!</p>",
      "body_md": "I would suggest using 1000 genomes for the LD calculation here with a more stringent r^2 cutoff (maybe 0.8?). Some LD information is available through their browser\r\n\r\nhttp://browser.1000genomes.org/Homo_sapiens/Location/Genome?db=core;r=2:31451742-31452000\r\n\r\nHere is a thread discussing similar ideas:\r\nhttps://www.biostars.org/p/2909/\r\n\r\nThe other resource of interest is the ExAC dataset: http://exac.broadinstitute.org/ I don't think the LD data is available, but it's worthwhile reaching out to them!",
      "profile": 103,
      "published": "2015-06-08T20:58:12.464481Z",
      "thread": 71,
      "url": "/discussion/calculating-genomic-windows-for-gwas-lead-snps/71#2"
    },
    {
      "body_html": "<p>For identifier mapping, you might want to check out <a href=\"http://bridgedb.org/\">BridgeDb</a>, which provides both mapping databases and libraries to add identifier mapping functionality to any project. It's 100% free and <a href=\"https://github.com/bridgedb/BridgeDb\">open source</a>.</p>\r\n\r\n<p>There are many ways to integrate BridgeDb into your own tool or resource. The easiest is simply to make web service calls, like:</p>\r\n\r\n<p><a href=\"http://webservice.bridgedb.org/Human/xrefs/H/CCR5\">http://webservice.bridgedb.org/Human/xrefs/H/CCR5</a><br>(for all mappings connected to HGNC \"CCR5\"), or</p>\r\n\r\n<p><a href=\"http://webservice.bridgedb.org/Human/xrefs/H/CCR5?dataSource=Entrez%20Gene\">http://webservice.bridgedb.org/Human/xrefs/H/CCR5?dataSource=Entrez%20Gene</a><br>(to only retrieve the Entrez Gene for \"CCR5\")</p>\r\n\r\n<p>Here are some docs for additional web service syntax: <a href=\"http://bridgedb.org/wiki/BridgeWebservice\">http://bridgedb.org/wiki/BridgeWebservice</a></p>\r\n\r\n<p>If performance is an issue, e.g., you want to query 10,000 times a day, then you can install the databases locally and implement the libs provided by the project into your tool and have complete control over database versions, etc.</p>",
      "body_md": "For identifier mapping, you might want to check out [BridgeDb](http://bridgedb.org/), which provides both mapping databases and libraries to add identifier mapping functionality to any project. It's 100% free and [open source](https://github.com/bridgedb/BridgeDb).\r\n\r\nThere are many ways to integrate BridgeDb into your own tool or resource. The easiest is simply to make web service calls, like:\r\n\r\nhttp://webservice.bridgedb.org/Human/xrefs/H/CCR5\r\n(for all mappings connected to HGNC \"CCR5\"), or\r\n\r\nhttp://webservice.bridgedb.org/Human/xrefs/H/CCR5?dataSource=Entrez%20Gene\r\n(to only retrieve the Entrez Gene for \"CCR5\")\r\n\r\nHere are some docs for additional web service syntax: http://bridgedb.org/wiki/BridgeWebservice\r\n\r\nIf performance is an issue, e.g., you want to query 10,000 times a day, then you can install the databases locally and implement the libs provided by the project into your tool and have complete control over database versions, etc.",
      "profile": 104,
      "published": "2015-06-09T00:52:06.974181Z",
      "thread": 34,
      "url": "/discussion/using-entrez-gene-as-our-gene-vocabulary/34#6"
    },
    {
      "body_html": "<p>For Table 1, you might consider adding another Gene Set resource based on curated pathways.  These are analogous to GO-Biological Process terms, but are much more focused and constrained. In fact, they also include small molecules and drugs, so they can serve as more than just <em>gene</em> sets.</p>\r\n\r\n<p>Likewise, for Table 2, you could add a number of interaction resources with pathway data.  As co-founder of <a href=\"http://wikipathways.org\">WikiPathways</a>, I have to recommend that one in particular! :)  It's 100% free, open source and open access.  I can also recommend <a href=\"http://www.reactome.org\">Reactome</a> and <a href=\"http://www.pathwaycommons.org/\">Pathway Commons</a>, the latter of which compiles pathway data from multiple sources into BioPAX data format.</p>\r\n\r\n<p>Again, these would not only provide high-quality gene-gene interactions for your network, but also direct drug-gene interactions. And in relation to your Figure 1, they would also provide Disease and Tissue associations.</p>\r\n\r\n<p>You can download all human pathways from WikiPathways <a href=\"http://wikipathways.org/index.php/Download_Pathways\">in multiple formats</a>, or parse just the Entrez Genes in Human pathways from <a href=\"http://www.pathvisio.org/data/bots/gmt/wikipathways.gmt\">this single dump file</a>. The advantage to the first option are that you are getting the original data, as curated by contributors; the disadvantage is that you have to perform the ID mapping to unify to Entrez and your preferred small molecule system. The advantage of the second option (the dump file) is that the Entrez ID unification has been done for you; the disadvantage is that anything that didn't map to Entrez is simply discarded (including drugs and small molecules!).</p>",
      "body_md": "For Table 1, you might consider adding another Gene Set resource based on curated pathways.  These are analogous to GO-Biological Process terms, but are much more focused and constrained. In fact, they also include small molecules and drugs, so they can serve as more than just *gene* sets.\r\n\r\nLikewise, for Table 2, you could add a number of interaction resources with pathway data.  As co-founder of [WikiPathways](http://wikipathways.org), I have to recommend that one in particular! :)  It's 100% free, open source and open access.  I can also recommend [Reactome](http://www.reactome.org) and [Pathway Commons](http://www.pathwaycommons.org/), the latter of which compiles pathway data from multiple sources into BioPAX data format.\r\n\r\nAgain, these would not only provide high-quality gene-gene interactions for your network, but also direct drug-gene interactions. And in relation to your Figure 1, they would also provide Disease and Tissue associations.\r\n\r\nYou can download all human pathways from WikiPathways [in multiple formats](http://wikipathways.org/index.php/Download_Pathways), or parse just the Entrez Genes in Human pathways from [this single dump file](http://www.pathvisio.org/data/bots/gmt/wikipathways.gmt). The advantage to the first option are that you are getting the original data, as curated by contributors; the disadvantage is that you have to perform the ID mapping to unify to Entrez and your preferred small molecule system. The advantage of the second option (the dump file) is that the Entrez ID unification has been done for you; the disadvantage is that anything that didn't map to Entrez is simply discarded (including drugs and small molecules!).\r\n",
      "profile": 104,
      "published": "2015-06-09T01:15:52.903978Z",
      "thread": 72,
      "url": "/discussion/adding-pathway-resources-to-your-network/72"
    },
    {
      "body_html": "<p><a href=\"/u/alexanderpico\" class=\"username\">@alexanderpico</a>, thanks for letting us know about the best current pathway resources.</p>\r\n\r\n<h2>MSigDB Canonical Pathways</h2>\r\n\r\n<p>In the past, we used the versions of <a href=\"http://www.reactome.org/\">Reactome</a>, <a href=\"http://www.genome.jp/kegg/pathway.html\">KEGG</a>, and <a href=\"http://www.biocarta.com/\">BioCarta</a> provided by MSigDB <span class=\"citation\">[<a href=\"/doi/10.1073/pnas.0506580102\" class=\"citation\" data-key=\"10.1073/pnas.0506580102\">1</a>, <a href=\"/doi/10.1093/bioinformatics/btr260\" class=\"citation\" data-key=\"10.1093/bioinformatics/btr260\">2</a>]</span>. MSigDB version 5.0 was <a href=\"https://www.broadinstitute.org/cancer/software/gsea/wiki/index.php/MSigDB_v5.0_Release_Notes\">released in April</a>, but it's unclear whether the pathway resources were updated. However, the \"C2: Canonical Pathways\" (CP) collection <a href=\"https://www.broadinstitute.org/gsea/msigdb/collection_details.jsp#CP\">integrates 9</a> pathway resources, so I think we should create a <em>C2: CP</em> metanode with a node for each MSigDB CP gene set.</p>\r\n\r\n<h2>WikiPathways</h2>\r\n\r\n<p>We can have a separate metanode for <a href=\"//www.wikipathways.org/\">WikiPathways</a> <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pbio.0060184\" class=\"citation\" data-key=\"10.1371/journal.pbio.0060184\">3</a>, <a href=\"/doi/10.1093/nar/gkr1074\" class=\"citation\" data-key=\"10.1093/nar/gkr1074\">4</a>]</span>. The open and crowdsourced nature of WikiPathways is ideal. The inclusion of compounds, tissues, diseases in addition to genes in these pathways could provide a major performance boost for our method. The benefit will depends on how frequently non-gene entities are included in these pathways. What percent of pathways include diseases, tissues, or drugs? Additionally, are non-gene entities identified as free text, or are they structured by a standardized vocabulary?</p>\r\n\r\n<h2>Pathway Commons</h2>\r\n\r\n<p>I like how Pathway Commons <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkq1039\" class=\"citation\" data-key=\"10.1093/nar/gkq1039\">5</a>]</span> brings a common format to <a href=\"http://www.pathwaycommons.org/pc2/datasources\">many resources</a>. One worry is that Pathway Commons contains edges, such as those from DrugBank, which will be included elsewhere in the network. One solution would be to <a href=\"http://www.pathwaycommons.org/pc2/downloads\">pick and chose</a> which source databases to integrate from Pathway Commons. After including <em>MSigDB C2: CP</em> and <em>WikiPathways</em>, will Pathway Commons contain much information not already captured? If not, we may just stick with the above resources.</p>\r\n\r\n<h2>General Questions</h2>\r\n\r\n<ol><li>How much do these pathway resources overlap? Does WikiPathways include pathways directly taken from other databases?</li><li>Do databases differ greatly in quality or type of pathways encoded? If the databases do differ, it may make sense to give each a separate metanode. Otherwise, we will organize all pathways by 1 or 2 metanodes.</li></ol>",
      "body_md": "@alexanderpico, thanks for letting us know about the best current pathway resources.\r\n\r\n## MSigDB Canonical Pathways\r\n\r\nIn the past, we used the versions of [Reactome](http://www.reactome.org/), [KEGG](http://www.genome.jp/kegg/pathway.html), and [BioCarta](http://www.biocarta.com/) provided by MSigDB [@10.1073/pnas.0506580102 @10.1093/bioinformatics/btr260]. MSigDB version 5.0 was [released in April](https://www.broadinstitute.org/cancer/software/gsea/wiki/index.php/MSigDB_v5.0_Release_Notes), but it's unclear whether the pathway resources were updated. However, the \"C2: Canonical Pathways\" (CP) collection [integrates 9](https://www.broadinstitute.org/gsea/msigdb/collection_details.jsp#CP) pathway resources, so I think we should create a *C2: CP* metanode with a node for each MSigDB CP gene set.\r\n\r\n## WikiPathways\r\n\r\nWe can have a separate metanode for [WikiPathways](//www.wikipathways.org/) [@10.1371/journal.pbio.0060184 @10.1093/nar/gkr1074]. The open and crowdsourced nature of WikiPathways is ideal. The inclusion of compounds, tissues, diseases in addition to genes in these pathways could provide a major performance boost for our method. The benefit will depends on how frequently non-gene entities are included in these pathways. What percent of pathways include diseases, tissues, or drugs? Additionally, are non-gene entities identified as free text, or are they structured by a standardized vocabulary?\r\n\r\n## Pathway Commons\r\n\r\nI like how Pathway Commons [@10.1093/nar/gkq1039] brings a common format to [many resources](http://www.pathwaycommons.org/pc2/datasources). One worry is that Pathway Commons contains edges, such as those from DrugBank, which will be included elsewhere in the network. One solution would be to [pick and chose](http://www.pathwaycommons.org/pc2/downloads) which source databases to integrate from Pathway Commons. After including *MSigDB C2: CP* and *WikiPathways*, will Pathway Commons contain much information not already captured? If not, we may just stick with the above resources.\r\n\r\n## General Questions\r\n\r\n1. How much do these pathway resources overlap? Does WikiPathways include pathways directly taken from other databases?\r\n+ Do databases differ greatly in quality or type of pathways encoded? If the databases do differ, it may make sense to give each a separate metanode. Otherwise, we will organize all pathways by 1 or 2 metanodes.",
      "profile": 17,
      "published": "2015-06-10T18:22:49.540745Z",
      "thread": 72,
      "url": "/discussion/adding-pathway-resources-to-your-network/72#2"
    },
    {
      "body_html": "<p><a href=\"/u/alexanderpico\" class=\"username\">@alexanderpico</a>, thanks for the BridgeDB suggestion. It looks like <a href=\"http://webservice.bridgedb.org/Human/targetDataSources\">several transcript/gene/protein resources</a> are integrated including HGNC, Entrez Gene, Affy, Illumina, WikiGenes, UniGene, UCSC Genome Browser, Uniprot, RefSeq, miRBase, and Ensembl. That's great — we may or may not need these mappings at this point.</p>\r\n\r\n<p>One worry I have is that the resource is outdated. The build date for human gene products is 2013-07-01. However, on 2014-11-21 version 2.0.0 was released. Does this mean the database was also rebuilt? In either case, I would like more frequent updates. Do you know the status of the project and whether it is actively maintained?</p>\r\n\r\n<p>One final note is that <a href=\"/u/caseygreene\" class=\"username\">@caseygreene</a>'s Tribe service allows free-text gene lookup, through elasticsearch. Currently, we do not need this feature. However, perhaps <a href=\"/u/alexanderpico\" class=\"username\">@alexanderpico</a> Pathways4Life project <span class=\"citation\">[<a href=\"/p/pathways4life\" class=\"citation\" data-key=\"10.15363/thinklab.8\">1</a>]</span> does. Also, perhaps <a href=\"http://tribe.greenelab.com/#/home\">Tribe</a>—a gene set wiki with a private option—would like to autopopulate <a href=\"//www.wikipathways.org/index.php/WikiPathways\">WikiPathways</a>.</p>",
      "body_md": "@alexanderpico, thanks for the BridgeDB suggestion. It looks like [several transcript/gene/protein resources](http://webservice.bridgedb.org/Human/targetDataSources) are integrated including HGNC, Entrez Gene, Affy, Illumina, WikiGenes, UniGene, UCSC Genome Browser, Uniprot, RefSeq, miRBase, and Ensembl. That's great -- we may or may not need these mappings at this point.\r\n\r\nOne worry I have is that the resource is outdated. The build date for human gene products is 2013-07-01. However, on 2014-11-21 version 2.0.0 was released. Does this mean the database was also rebuilt? In either case, I would like more frequent updates. Do you know the status of the project and whether it is actively maintained?\r\n\r\nOne final note is that @caseygreene's Tribe service allows free-text gene lookup, through elasticsearch. Currently, we do not need this feature. However, perhaps @alexanderpico Pathways4Life project [@10.15363/thinklab.8] does. Also, perhaps [Tribe](http://tribe.greenelab.com/#/home)---a gene set wiki with a private option---would like to autopopulate [WikiPathways](//www.wikipathways.org/index.php/WikiPathways).",
      "profile": 17,
      "published": "2015-06-10T18:47:13.538545Z",
      "thread": 34,
      "url": "/discussion/using-entrez-gene-as-our-gene-vocabulary/34#7"
    },
    {
      "body_html": "<p>Right. The database build system was recently updated from using Ensembl's Perl API to using BioMart. This will allow frequent updates; probably quarterly.</p>",
      "body_md": "Right. The database build system was recently updated from using Ensembl's Perl API to using BioMart. This will allow frequent updates; probably quarterly.",
      "profile": 104,
      "published": "2015-06-10T19:08:23.025368Z",
      "thread": 34,
      "url": "/discussion/using-entrez-gene-as-our-gene-vocabulary/34#8"
    },
    {
      "body_html": "<p>Figures 1 and 2 in <a href=\"http://thinklab.com/p/pathways4life\">the Pathways4Life proposal</a> will answer questions about overlap and frequency of updates.  Pathway Commons is not a primary source; their focus is on compiling from as many sources as possible. So, given their restriction to BioPAX, they definitely include more than any single resource.</p>",
      "body_md": "Figures 1 and 2 in [the Pathways4Life proposal](http://thinklab.com/p/pathways4life) will answer questions about overlap and frequency of updates.  Pathway Commons is not a primary source; their focus is on compiling from as many sources as possible. So, given their restriction to BioPAX, they definitely include more than any single resource.",
      "profile": 104,
      "published": "2015-06-10T19:12:41.943422Z",
      "thread": 72,
      "url": "/discussion/adding-pathway-resources-to-your-network/72#3"
    },
    {
      "body_html": "<p><a href=\"/u/alexanderpico\" class=\"username\">@alexanderpico</a>, thanks figures 1 and 2 do help, however I am more interested in <a href=\"http://thinklab.com/discussion/pathway-novelty-based-on-unique-relationships-rather-than-genes/75\">edge-based measures of overlap</a>. Do you have a general sense of whether the same pathways are represented in multiple databases?</p>\r\n\r\n<p>My interpretation of Figure 1 is that it provides a lower bound of uniqueness. The fact that there are many genes unique in KEGG, Reactome, and WikiPathways warrants the inclusion of all three resources. However, it doesn't answer whether the common genes are from duplicated pathways or not.</p>",
      "body_md": "@alexanderpico, thanks figures 1 and 2 do help, however I am more interested in [edge-based measures of overlap](http://thinklab.com/discussion/pathway-novelty-based-on-unique-relationships-rather-than-genes/75). Do you have a general sense of whether the same pathways are represented in multiple databases?\r\n\r\nMy interpretation of Figure 1 is that it provides a lower bound of uniqueness. The fact that there are many genes unique in KEGG, Reactome, and WikiPathways warrants the inclusion of all three resources. However, it doesn't answer whether the common genes are from duplicated pathways or not.",
      "profile": 17,
      "published": "2015-06-11T00:29:13.219315Z",
      "thread": 72,
      "url": "/discussion/adding-pathway-resources-to-your-network/72#4"
    },
    {
      "body_html": "<p><a href=\"/u/marinasirota\" class=\"username\">@marinasirota</a>, thanks for the advice.</p>\r\n\r\n<p>The <a href=\"https://www.broadinstitute.org/mpg/snap/ldsearch.php\">SNAP Proxy Search</a> <span class=\"citation\">[<a href=\"/doi/10.1093/bioinformatics/btn564\" class=\"citation\" data-key=\"10.1093/bioinformatics/btn564\">1</a>]</span> allows us to find all SNPs within 500kb and with LD above a provided threshold for the query SNP, using 1000 Genomes (KG) pilot data.</p>\r\n\r\n<p>One issue with KG is that the whole-genome sequencing was done at low depth (4x coverage) and that only 179 samples were sequenced: 60 CEU, 59 YRI, 30 CHB, and 30 JPT <span class=\"citation\">[<a href=\"/doi/10.1038/nature09534\" class=\"citation\" data-key=\"10.1038/nature09534\">2</a>]</span>. Therefore many low frequency or technically difficult variants were likely missed. Since GWAS have mostly focused on common variants, the puniness of 1000 Genomes pilot data may be acceptable.</p>\r\n\r\n<p>We went ahead and evaluated the SNAP LD information from KG for our GWAS lead SNPs. For each lead SNP, we found all SNPs in <span class=\"math\">$$r^2 \\geq 0.8$$</span> in the European subset of 60 individuals. The <a href=\"https://github.com/dhimmel/gwas-catalog/blob/568e063010d725c335a019ac20c2d023d000f5d4/windows.ipynb\">findings are as follows</a>:</p>\r\n\r\n<ul><li>Of 5,255 GWAS lead SNPs, 517 were not found by SNAP</li><li>SNPs with lower minor allele frequencies were more likely to have large windows (kilobase spans). We speculate this results from greater noise in <span class=\"math\">$$r^2$$</span> values when the number of minor alleles is low, enabling far away SNPs to appear in high LD by chance. </li><li>614 lead SNPs have a zero-length span — no SNPs were found with LD exceeding the threshold. Most likely this is due to the incompleteness of KG.</li><li>Window spans measured in kilobases are highly, positively correlated with spans measured in centimorgans. Therefore, we cannot chose a single centimorgan threshold to approximate windows calculated using the <span class=\"math\">$$r^2$$</span> method.</li></ul>\r\n\r\n<p>In conclusion, the KG data retrieved from SNAP is feasible but not ideal. We will look into larger datasets and have <a href=\"https://github.com/konradjk/exac_browser/issues/189\">reached out to the ExAC team</a>.</p>",
      "body_md": "@marinasirota, thanks for the advice.\r\n\r\nThe [SNAP Proxy Search](https://www.broadinstitute.org/mpg/snap/ldsearch.php) [@10.1093/bioinformatics/btn564] allows us to find all SNPs within 500kb and with LD above a provided threshold for the query SNP, using 1000 Genomes (KG) pilot data.\r\n\r\nOne issue with KG is that the whole-genome sequencing was done at low depth (4x coverage) and that only 179 samples were sequenced: 60 CEU, 59 YRI, 30 CHB, and 30 JPT [@10.1038/nature09534]. Therefore many low frequency or technically difficult variants were likely missed. Since GWAS have mostly focused on common variants, the puniness of 1000 Genomes pilot data may be acceptable.\r\n\r\nWe went ahead and evaluated the SNAP LD information from KG for our GWAS lead SNPs. For each lead SNP, we found all SNPs in $$r^2 \\geq 0.8$$ in the European subset of 60 individuals. The [findings are as follows](https://github.com/dhimmel/gwas-catalog/blob/568e063010d725c335a019ac20c2d023d000f5d4/windows.ipynb):\r\n\r\n+ Of 5,255 GWAS lead SNPs, 517 were not found by SNAP\r\n+ SNPs with lower minor allele frequencies were more likely to have large windows (kilobase spans). We speculate this results from greater noise in $$r^2$$ values when the number of minor alleles is low, enabling far away SNPs to appear in high LD by chance. \r\n+ 614 lead SNPs have a zero-length span -- no SNPs were found with LD exceeding the threshold. Most likely this is due to the incompleteness of KG.\r\n+ Window spans measured in kilobases are highly, positively correlated with spans measured in centimorgans. Therefore, we cannot chose a single centimorgan threshold to approximate windows calculated using the $$r^2$$ method.\r\n\r\nIn conclusion, the KG data retrieved from SNAP is feasible but not ideal. We will look into larger datasets and have [reached out to the ExAC team](https://github.com/konradjk/exac_browser/issues/189).",
      "profile": 17,
      "published": "2015-06-11T22:14:15.659411Z",
      "thread": 71,
      "url": "/discussion/calculating-genomic-windows-for-gwas-lead-snps/71#3"
    },
    {
      "body_html": "<p>That measure of overlap is fraught with caveats relating to exactly how edges are modeled. When each of the three resource mentioned here converts to a single exchange format, like BioPAX, for example, we each make a unique set of mapping decisions and compromises. Nevertheless, you're absolutely right that node overlap is a lower bound, but I don't have a good estimate for edge overlap. Just browsing the pathway titles is the most convincing way to see that we cover much of the same ground: metabolism, signaling and gene regulation.</p>",
      "body_md": "That measure of overlap is fraught with caveats relating to exactly how edges are modeled. When each of the three resource mentioned here converts to a single exchange format, like BioPAX, for example, we each make a unique set of mapping decisions and compromises. Nevertheless, you're absolutely right that node overlap is a lower bound, but I don't have a good estimate for edge overlap. Just browsing the pathway titles is the most convincing way to see that we cover much of the same ground: metabolism, signaling and gene regulation.",
      "profile": 104,
      "published": "2015-06-12T02:05:40.666985Z",
      "thread": 72,
      "url": "/discussion/adding-pathway-resources-to-your-network/72#5"
    },
    {
      "body_html": "<h1>Permissive <span class=\"math\">$$r^2$$</span> threshold when relying on low-powered LD data</h1>\r\n\r\n<p><a href=\"/u/marinasirota\" class=\"username\">@marinasirota</a>, I intuitively agree that for the modern GWAS assaying and imputing millions of SNPs, lead SNPs are likely be in <span class=\"math\">$$r^2 \\geq 0.8$$</span> with the SNPs driving the association. However, when using the 1000 Genomes Pilot data for LD, I think we should use a more permissive threshold of <span class=\"math\">$$r^2 \\geq 0.5$$</span>. The 0.8 threshold produces <a href=\"https://github.com/dhimmel/gwas-catalog/blob/568e063010d725c335a019ac20c2d023d000f5d4/windows.ipynb\">614</a> windows with zero-length spans compared to <a href=\"https://github.com/dhimmel/gwas-catalog/blob/cbc30cbe88bda38c7ebe9c32802b051436431065/windows.ipynb\">149</a> for the 0.5 threshold. Zero-length spans are equivalent to declaring that the lead SNP is the only SNP capable of creating the association. I would prefer to minimize these instances when we have such incomplete LD information.</p>",
      "body_md": "# Permissive $$r^2$$ threshold when relying on low-powered LD data\r\n\r\n@marinasirota, I intuitively agree that for the modern GWAS assaying and imputing millions of SNPs, lead SNPs are likely be in $$r^2 \\geq 0.8$$ with the SNPs driving the association. However, when using the 1000 Genomes Pilot data for LD, I think we should use a more permissive threshold of $$r^2 \\geq 0.5$$. The 0.8 threshold produces [614](https://github.com/dhimmel/gwas-catalog/blob/568e063010d725c335a019ac20c2d023d000f5d4/windows.ipynb) windows with zero-length spans compared to [149](https://github.com/dhimmel/gwas-catalog/blob/cbc30cbe88bda38c7ebe9c32802b051436431065/windows.ipynb) for the 0.5 threshold. Zero-length spans are equivalent to declaring that the lead SNP is the only SNP capable of creating the association. I would prefer to minimize these instances when we have such incomplete LD information.",
      "profile": 17,
      "published": "2015-06-12T18:15:23.590185Z",
      "thread": 71,
      "url": "/discussion/calculating-genomic-windows-for-gwas-lead-snps/71#4"
    },
    {
      "body_html": "<p><a href=\"/u/dhimmel\" class=\"username\">@dhimmel</a> - that makes sense.  </p>",
      "body_md": "@dhimmel - that makes sense.  ",
      "profile": 103,
      "published": "2015-06-12T20:00:07.588699Z",
      "thread": 71,
      "url": "/discussion/calculating-genomic-windows-for-gwas-lead-snps/71#5"
    },
    {
      "body_html": "<p>We recently worked on a (mini-)study to investigate the relationship between the ERC values of gene-pairs and the extent to which they share their interacting partners. The Jaccard coefficient was used to quantify the extent to which genes share interacting partners (higher the Jaccard, more the fraction of interacting partners shared). We used the yeast ERC dataset, and interestingly found that there is a weak, but significant positive correlation between ERC values of gene pairs and Jaccard coefficient (JC) of the interacting partners of the two genes. We additionally saw that using JC in conjunction with ERC has potential to reduce the number of false discoveries in interaction prediction. I could attach the full report of our investigation if it interests you.</p>\r\n\r\n<p>I think it would be interesting to refine the approach further, and apply the same in the human context as well - it may very well turn out to be more powerful than thresholding based on ERC values.</p>",
      "body_md": "We recently worked on a (mini-)study to investigate the relationship between the ERC values of gene-pairs and the extent to which they share their interacting partners. The Jaccard coefficient was used to quantify the extent to which genes share interacting partners (higher the Jaccard, more the fraction of interacting partners shared). We used the yeast ERC dataset, and interestingly found that there is a weak, but significant positive correlation between ERC values of gene pairs and Jaccard coefficient (JC) of the interacting partners of the two genes. We additionally saw that using JC in conjunction with ERC has potential to reduce the number of false discoveries in interaction prediction. I could attach the full report of our investigation if it interests you.\r\n\r\nI think it would be interesting to refine the approach further, and apply the same in the human context as well - it may very well turn out to be more powerful than thresholding based on ERC values.",
      "profile": 107,
      "published": "2015-06-16T18:28:41.237981Z",
      "thread": 57,
      "url": "/discussion/selecting-informative-erc-evolutionary-rate-covariation-values-between-genes/57#2"
    },
    {
      "body_html": "<p>The protein interaction project sounds interesting. Have you considered using a random walk with restart on the protein interaction network for PPI-similarity? I think you will find it preferable to the Jaccard coefficient, since it considers more than just first degree neighbors. I have some python code for the random walk that I can open source, if you can't find an implementation. What protein interaction network are you using? I think a systematic PPI network (that isn't ridden with knowledge bias) would be especially interesting. I suggest the HI-II-14 network from <a href=\"http://interactome.dfci.harvard.edu/H_sapiens/index.php?page=download\">here</a>.</p>\r\n\r\n<p>In terms of this project, we would like to keep the ERC edges independent of the PPI edges. The ERC values are attractive to us as a completely orthogonal resource to the protein interactions.</p>",
      "body_md": "The protein interaction project sounds interesting. Have you considered using a random walk with restart on the protein interaction network for PPI-similarity? I think you will find it preferable to the Jaccard coefficient, since it considers more than just first degree neighbors. I have some python code for the random walk that I can open source, if you can't find an implementation. What protein interaction network are you using? I think a systematic PPI network (that isn't ridden with knowledge bias) would be especially interesting. I suggest the HI-II-14 network from [here](http://interactome.dfci.harvard.edu/H_sapiens/index.php?page=download).\r\n\r\nIn terms of this project, we would like to keep the ERC edges independent of the PPI edges. The ERC values are attractive to us as a completely orthogonal resource to the protein interactions.",
      "profile": 17,
      "published": "2015-06-16T18:47:14.342149Z",
      "thread": 57,
      "url": "/discussion/selecting-informative-erc-evolutionary-rate-covariation-values-between-genes/57#3"
    },
    {
      "body_html": "<p>The <a href=\"https://www.ebi.ac.uk/gwas/\">GWAS Catalog</a> <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkt1229\" class=\"citation\" data-key=\"10.1093/nar/gkt1229\">1</a>]</span> compiles SNP associations from published genome-wide studies. We converted the catalog from SNP associations to gene associations. We classify each gene association as high or low confidence and as primary or secondary (based on whether the gene is assumed to drive the signal at a loci).</p>\r\n\r\n<p>We only extracted associations for diseases in <a href=\"http://thinklab.com/discussion/unifying-disease-vocabularies/44#144\">DO Slim</a>, which should cover most diseases in the catalog while excluding traits. Genes are restricted to protein-coding.</p>\r\n\r\n<h2>External resources</h2>\r\n\r\n<ul><li><a href=\"https://github.com/dhimmel/gwas-catalog/blob/0617ea7ea8268f21f5ca1b8dbe487dd12671fc7b/data/gene-associations.tsv\"><strong>compiled gene associations</strong></a></li><li>summary files with <a href=\"https://github.com/dhimmel/gwas-catalog/blob/0617ea7ea8268f21f5ca1b8dbe487dd12671fc7b/data/diseases.tsv\">associations per disease</a> and <a href=\"https://github.com/dhimmel/gwas-catalog/blob/0617ea7ea8268f21f5ca1b8dbe487dd12671fc7b/data/genes.tsv\">associations per gene</a></li><li><a href=\"https://github.com/dhimmel/gwas-catalog/blob/0617ea7ea8268f21f5ca1b8dbe487dd12671fc7b/data/snp-associations.tsv\">compiled SNP associations</a> — a processed subset of the GWAS Catalog</li><li><a href=\"https://github.com/dhimmel/gwas-catalog/blob/0617ea7ea8268f21f5ca1b8dbe487dd12671fc7b/loci.ipynb\">notebook</a> for processing loci</li><li><a href=\"http://thinklab.com/discussion/calculating-genomic-windows-for-gwas-lead-snps/71\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d71\">discussion</a> and <a href=\"https://github.com/dhimmel/gwas-catalog/blob/0617ea7ea8268f21f5ca1b8dbe487dd12671fc7b/windows.ipynb\">notebook</a> for calculating lead-SNP windows</li></ul>\r\n\r\n<h2>Method</h2>\r\n\r\n<p>The method for processing associations was taken from our previous work <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1004259\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1004259\">2</a>]</span>, which describes it as follows (modifications afterwards):</p>\r\n\r\n<blockquote><p>Disease-gene associations were extracted from the GWAS Catalog <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkt1229\" class=\"citation\" data-key=\"10.1093/nar/gkt1229\">1</a>]</span>, a compilation of GWAS associations where <span class=\"math\">$$p . First, associations were segregated by disease. GWAS Catalog phenotypes were converted to Experimental Factor Ontology (EFO) terms using mappings produced by the European Bioinformatics Institute. Associations mapping to multiple EFO terms were excluded to eliminate cross-phenotype studies. We manually mapped EFO to DO terms (now included in the DO as cross-references) and annotated each DO term with its associations.</span></p><p>Associations were classified as either high or low-confidence, where exceeding two thresholds granted high-confidence status. First, <span class=\"math\">$$p \\leq 5 × 10^{-8}$$</span> corresponding to  <span class=\"math\">$$p \\leq 0.05$$</span> after Bonferroni adjustment for one million comparisons (an approximate upper bound for the number of independent SNPs evaluated by most GWAS). Second, a minimum sample size (counting both cases and controls) of 1,000 was required, since studies below this size are underpowered <span class=\"citation\">[<a href=\"/doi/10.1093/brain/awn081\" class=\"citation\" data-key=\"10.1093/brain/awn081\">3</a>]</span>—i.e. any discovered associations are more likely than not to be false—for the majority of true effect size distributions commonly assumed to underlie complex disease etiology <span class=\"citation\">[<a href=\"/doi/10.1038/nrg2615\" class=\"citation\" data-key=\"10.1038/nrg2615\">4</a>]</span>.</p><p>Lead-SNPs were assigned windows—regions wherein the causal SNPs are assumed to lie—retrieved from the DAPPLE server <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pgen.1001273\" class=\"citation\" data-key=\"10.1371/journal.pgen.1001273\">5</a>]</span>. Windows were calculated for each lead-SNP by finding the furthest upstream and downstream SNPs where <span class=\"math\">$$r^2 &gt; 0.5$$</span> and extending outwards to the next recombination hotspot. Associations were ordered by confidence, sorting on following criteria: high/low confidence, p-value (low to high), and recency. In order of confidence, associations were overlapped by their windows into disease-specific loci. By organizing associations into loci, associations from multiple studies tagging the same underlying signal were condensed. A locus was classified as high-confidence if any of its composite associations were high-confidence and low-confidence otherwise.</p><p>For each disease-specific loci, we attempted to identify a primary gene. The primary gene was resolved in the following order:</p><ol><li>the mode author-reported gene</li><li>the containing gene for an intragenic lead-SNP</li><li>the mode author-reported gene for an intragenic lead-SNP (in the case of overlapping genes)</li><li>the mode author-reported gene of the most proximal up and downstream genes.</li></ol><p>Steps 2–4 were repeated on each association composing the loci, in order of confidence, until a single gene resolved as primary. Loci where ambiguity was unresolvable or where no genes were returned did not receive a primary gene. All non-primary genes—genes that were author-reported, overlapping the lead-SNP, or immediately up or downstream from the lead-SNP—were considered secondary.</p><p>Accordingly, four categories of processed associations were created: high-confidence primary, high-confidence secondary, low-confidence primary, and low-confidence secondary. We assume that our primary gene annotation for each loci represents the single causal gene responsible for the association.</p></blockquote>\r\n\r\n<h2>Method modifications</h2>\r\n\r\n<p>We <a href=\"http://thinklab.com/discussion/calculating-genomic-windows-for-gwas-lead-snps/71\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d71\">switched</a> from HapMap LD data provided by DAPPLE to 1000 Genomes LD data provided by SNAP and removed the recombination hotspot extensions. </p>",
      "body_md": "The [GWAS Catalog](https://www.ebi.ac.uk/gwas/) [@10.1093/nar/gkt1229] compiles SNP associations from published genome-wide studies. We converted the catalog from SNP associations to gene associations. We classify each gene association as high or low confidence and as primary or secondary (based on whether the gene is assumed to drive the signal at a loci).\r\n\r\nWe only extracted associations for diseases in [DO Slim](http://thinklab.com/discussion/unifying-disease-vocabularies/44#144), which should cover most diseases in the catalog while excluding traits. Genes are restricted to protein-coding.\r\n\r\n## External resources\r\n\r\n+ [**compiled gene associations**](https://github.com/dhimmel/gwas-catalog/blob/0617ea7ea8268f21f5ca1b8dbe487dd12671fc7b/data/gene-associations.tsv)\r\n+ summary files with [associations per disease](https://github.com/dhimmel/gwas-catalog/blob/0617ea7ea8268f21f5ca1b8dbe487dd12671fc7b/data/diseases.tsv) and [associations per gene](https://github.com/dhimmel/gwas-catalog/blob/0617ea7ea8268f21f5ca1b8dbe487dd12671fc7b/data/genes.tsv)\r\n+ [compiled SNP associations](https://github.com/dhimmel/gwas-catalog/blob/0617ea7ea8268f21f5ca1b8dbe487dd12671fc7b/data/snp-associations.tsv) -- a processed subset of the GWAS Catalog\r\n+ [notebook](https://github.com/dhimmel/gwas-catalog/blob/0617ea7ea8268f21f5ca1b8dbe487dd12671fc7b/loci.ipynb) for processing loci\r\n+ [discussion](http://thinklab.com/discussion/calculating-genomic-windows-for-gwas-lead-snps/71) and [notebook](https://github.com/dhimmel/gwas-catalog/blob/0617ea7ea8268f21f5ca1b8dbe487dd12671fc7b/windows.ipynb) for calculating lead-SNP windows\r\n\r\n## Method\r\n\r\nThe method for processing associations was taken from our previous work [@10.1371/journal.pcbi.1004259], which describes it as follows (modifications afterwards):\r\n\r\n> Disease-gene associations were extracted from the GWAS Catalog [@10.1093/nar/gkt1229], a compilation of GWAS associations where $$p < 10^{−5}$$. First, associations were segregated by disease. GWAS Catalog phenotypes were converted to Experimental Factor Ontology (EFO) terms using mappings produced by the European Bioinformatics Institute. Associations mapping to multiple EFO terms were excluded to eliminate cross-phenotype studies. We manually mapped EFO to DO terms (now included in the DO as cross-references) and annotated each DO term with its associations.\r\n\r\n> Associations were classified as either high or low-confidence, where exceeding two thresholds granted high-confidence status. First, $$p \\leq 5 × 10^{-8}$$ corresponding to  $$p \\leq 0.05$$ after Bonferroni adjustment for one million comparisons (an approximate upper bound for the number of independent SNPs evaluated by most GWAS). Second, a minimum sample size (counting both cases and controls) of 1,000 was required, since studies below this size are underpowered [@10.1093/brain/awn081]---i.e. any discovered associations are more likely than not to be false---for the majority of true effect size distributions commonly assumed to underlie complex disease etiology [@10.1038/nrg2615].\r\n\r\n> Lead-SNPs were assigned windows—regions wherein the causal SNPs are assumed to lie—retrieved from the DAPPLE server [@10.1371/journal.pgen.1001273]. Windows were calculated for each lead-SNP by finding the furthest upstream and downstream SNPs where $$r^2 > 0.5$$ and extending outwards to the next recombination hotspot. Associations were ordered by confidence, sorting on following criteria: high/low confidence, p-value (low to high), and recency. In order of confidence, associations were overlapped by their windows into disease-specific loci. By organizing associations into loci, associations from multiple studies tagging the same underlying signal were condensed. A locus was classified as high-confidence if any of its composite associations were high-confidence and low-confidence otherwise.\r\n\r\n> For each disease-specific loci, we attempted to identify a primary gene. The primary gene was resolved in the following order:\r\n>\r\n1. the mode author-reported gene\r\n2. the containing gene for an intragenic lead-SNP\r\n3. the mode author-reported gene for an intragenic lead-SNP (in the case of overlapping genes)\r\n4. the mode author-reported gene of the most proximal up and downstream genes.\r\n\r\n> Steps 2–4 were repeated on each association composing the loci, in order of confidence, until a single gene resolved as primary. Loci where ambiguity was unresolvable or where no genes were returned did not receive a primary gene. All non-primary genes—genes that were author-reported, overlapping the lead-SNP, or immediately up or downstream from the lead-SNP—were considered secondary.\r\n\r\n> Accordingly, four categories of processed associations were created: high-confidence primary, high-confidence secondary, low-confidence primary, and low-confidence secondary. We assume that our primary gene annotation for each loci represents the single causal gene responsible for the association.\r\n\r\n## Method modifications\r\n\r\nWe [switched](http://thinklab.com/discussion/calculating-genomic-windows-for-gwas-lead-snps/71) from HapMap LD data provided by DAPPLE to 1000 Genomes LD data provided by SNAP and removed the recombination hotspot extensions. ",
      "profile": 17,
      "published": "2015-06-16T20:43:50.492291Z",
      "thread": 80,
      "url": "/discussion/extracting-disease-gene-associations-from-the-gwas-catalog/80"
    },
    {
      "body_html": "<p>We would to know the expression level of each gene in as many tissues and cell types as possible. Humans only for now. What are the best resources and methods to go about this?</p>\r\n\r\n<p>Here are some relevant resources we compiled (in order of preference):</p>\r\n\r\n<ol><li>The Genotype-Tissue Expression project (<a href=\"http://www.gtexportal.org/home/\">GTEx</a>) <span class=\"citation\">[<a href=\"/doi/10.1126/science.aaa0355\" class=\"citation\" data-key=\"10.1126/science.aaa0355\">1</a>]</span></li><li>Baseline Expression Atlas (<a href=\"//www.ebi.ac.uk/gxa/help/baseline-atlas.html\">BEA</a>) <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkt1270\" class=\"citation\" data-key=\"10.1093/nar/gkt1270\">2</a>]</span></li><li>Human Protein Atlas (<a href=\"//www.proteinatlas.org/\">HPA</a>) <span class=\"citation\">[<a href=\"/doi/10.1126/science.1260419\" class=\"citation\" data-key=\"10.1126/science.1260419\">3</a>]</span></li><li>GNF Gene Expression Atlas (<a href=\"http://biogps.org/dataset/1/geneatlas-u133a-gcrma/\">BodyMap</a>) <span class=\"citation\">[<a href=\"/doi/10.1073/pnas.0400782101\" class=\"citation\" data-key=\"10.1073/pnas.0400782101\">4</a>]</span></li><li>Human Proteome Map (<a href=\"http://www.humanproteomemap.org/\">HPM</a>) <span class=\"citation\">[<a href=\"/doi/10.1038/nature13302\" class=\"citation\" data-key=\"10.1038/nature13302\">5</a>]</span></li><li>Gene Enrichment Profiler (<a href=\"//xavierlab2.mgh.harvard.edu/EnrichmentProfiler/\">GEP</a>) <span class=\"citation\">[<a href=\"/doi/10.1182/blood-2010-01-263855\" class=\"citation\" data-key=\"10.1182/blood-2010-01-263855\">6</a>]</span></li><li>A global map of human gene expression (<a href=\"https://www.ebi.ac.uk/arrayexpress/experiments/E-MTAB-62/\">E-MTAB-62</a>) <span class=\"citation\">[<a href=\"/doi/10.1038/nbt0410-322\" class=\"citation\" data-key=\"10.1038/nbt0410-322\">7</a>]</span></li><li>Unveiling RNA Sample Annotation (<a href=\"//ursa.princeton.edu/\">USRA</a>) <span class=\"citation\">[<a href=\"/doi/10.1093/bioinformatics/btt529\" class=\"citation\" data-key=\"10.1093/bioinformatics/btt529\">8</a>]</span></li><li>RNA-Seq Atlas <span class=\"citation\">[<a href=\"/doi/10.1093/bioinformatics/bts084\" class=\"citation\" data-key=\"10.1093/bioinformatics/bts084\">9</a>]</span></li></ol>\r\n\r\n<p><strong>Additional resources found since initial post</strong>:</p>\r\n\r\n<ul><li><a href=\"http://bgee.unil.ch/\">Bgee</a> <span class=\"citation\">[<a href=\"/doi/10.1007/978-3-540-69828-9_12\" class=\"citation\" data-key=\"10.1007/978-3-540-69828-9_12\">10</a>]</span></li><li><a href=\"http://tissues.jensenlab.org/About\">TISSUES</a> <span class=\"citation\">[<a href=\"/doi/10.7717/peerj.1054\" class=\"citation\" data-key=\"10.7717/peerj.1054\">11</a>]</span></li></ul>",
      "body_md": "We would to know the expression level of each gene in as many tissues and cell types as possible. Humans only for now. What are the best resources and methods to go about this?\r\n\r\nHere are some relevant resources we compiled (in order of preference):\r\n\r\n1. The Genotype-Tissue Expression project ([GTEx](http://www.gtexportal.org/home/)) [@10.1126/science.aaa0355]\r\n+ Baseline Expression Atlas ([BEA](//www.ebi.ac.uk/gxa/help/baseline-atlas.html)) [@10.1093/nar/gkt1270]\r\n+ Human Protein Atlas ([HPA](//www.proteinatlas.org/)) [@10.1126/science.1260419]\r\n+ GNF Gene Expression Atlas ([BodyMap](http://biogps.org/dataset/1/geneatlas-u133a-gcrma/)) [@10.1073/pnas.0400782101]\r\n+ Human Proteome Map ([HPM](http://www.humanproteomemap.org/)) [@10.1038/nature13302]\r\n+ Gene Enrichment Profiler ([GEP](//xavierlab2.mgh.harvard.edu/EnrichmentProfiler/)) [@10.1182/blood-2010-01-263855]\r\n+ A global map of human gene expression ([E-MTAB-62](https://www.ebi.ac.uk/arrayexpress/experiments/E-MTAB-62/)) [@10.1038/nbt0410-322]\r\n+ Unveiling RNA Sample Annotation ([USRA](//ursa.princeton.edu/)) [@10.1093/bioinformatics/btt529]\r\n+ RNA-Seq Atlas [@10.1093/bioinformatics/bts084]\r\n\r\n**Additional resources found since initial post**:\r\n\r\n+ [Bgee](http://bgee.unil.ch/) [@10.1007/978-3-540-69828-9_12]\r\n+ [TISSUES](http://tissues.jensenlab.org/About) [@10.7717/peerj.1054]",
      "profile": 17,
      "published": "2015-06-17T18:56:26.669391Z",
      "thread": 81,
      "url": "/discussion/tissue-specific-gene-expression-resources/81"
    },
    {
      "body_html": "<p><a href=\"/u/marinasirota\" class=\"username\">@marinasirota</a> recommended GTEx because of the large number samples per tissue and the use of RNA-seq.</p>",
      "body_md": "@marinasirota recommended GTEx because of the large number samples per tissue and the use of RNA-seq.",
      "profile": 17,
      "published": "2015-06-17T18:56:34.553541Z",
      "thread": 81,
      "url": "/discussion/tissue-specific-gene-expression-resources/81#2"
    },
    {
      "body_html": "<p>The Genotype-Tissue Expression project (<a href=\"http://www.gtexportal.org/home/\">GTEx</a>) RNA-sequenced <span class=\"citation\">[<a href=\"/doi/10.1126/science.aaa0355\" class=\"citation\" data-key=\"10.1126/science.aaa0355\">1</a>]</span>:</p>\r\n\r\n<blockquote><p>1641 samples from 175 individuals representing 43 sites: 29 solid organ tissues, 11 brain subregions, whole blood, and two cell lines: Epstein-Barr virus–transformed lymphocytes (LCL) and cultured fibroblasts from skin.</p></blockquote>\r\n\r\n<p>The data is <a href=\"http://www.gtexportal.org/home/datasets2\">available online</a>. Specifically, we are interested in the <code>GTEx_Analysis_V4_RNA-seq_RNA-SeQCv1.1.8_gene_rpkm.gct.gz</code> file that contains RPKM expression values for each sample. We would like to calculate a single expression value for each gene-tissue pair. Expression values should be comparable across tissues, not just within tissues.</p>\r\n\r\n<p>We will post our questions here. Advice appreciated.</p>",
      "body_md": "The Genotype-Tissue Expression project ([GTEx](http://www.gtexportal.org/home/)) RNA-sequenced [@10.1126/science.aaa0355]:\r\n\r\n> 1641 samples from 175 individuals representing 43 sites: 29 solid organ tissues, 11 brain subregions, whole blood, and two cell lines: Epstein-Barr virus–transformed lymphocytes (LCL) and cultured fibroblasts from skin.\r\n\r\nThe data is [available online](http://www.gtexportal.org/home/datasets2). Specifically, we are interested in the `GTEx_Analysis_V4_RNA-seq_RNA-SeQCv1.1.8_gene_rpkm.gct.gz` file that contains RPKM expression values for each sample. We would like to calculate a single expression value for each gene-tissue pair. Expression values should be comparable across tissues, not just within tissues.\r\n\r\nWe will post our questions here. Advice appreciated.",
      "profile": 17,
      "published": "2015-06-17T20:51:35.154243Z",
      "thread": 82,
      "url": "/discussion/processing-gtex-for-tissue-specific-gene-expression/82"
    },
    {
      "body_html": "<h1>Mapping GTEx sites to Uberon and CL</h1>\r\n\r\n<p>We <a href=\"//thinklab.com/discussion/tissue-node/41\">are using</a> <a href=\"https://uberon.github.io/\">Uberon</a> <span class=\"citation\">[<a href=\"/doi/10.1186/gb-2012-13-1-r5\" class=\"citation\" data-key=\"10.1186/gb-2012-13-1-r5\">1</a>]</span> terms to identify anatomical structures and <a href=\"https://github.com/obophenotype/cell-ontology\">Cell Ontology</a> (CL) <span class=\"citation\">[<a href=\"/doi/10.1186/gb-2005-6-2-r21\" class=\"citation\" data-key=\"10.1186/gb-2005-6-2-r21\">2</a>]</span> terms to identify cell types. Thus, we need to map GTEx sites to their corresponding ontology terms.</p>\r\n\r\n<p>From the sample attribute documentation (<code>GTEx_Data_V4_Annotations_SampleAttributesDS.txt</code>), we <a href=\"https://github.com/dhimmel/gtex/blob/8acb90fea2613f8b27814401e361bb6ae4b29078/gtex.ipynb\">identified</a> 54 sites using the <code>SMTSD</code> attribute. I have mapped about half of the sites to Uberon. The remainder would benefit from a skilled anatomist or GTEx consortium member.</p>\r\n\r\n<p><strong>Bounty:</strong> Add or correct <a href=\"https://docs.google.com/spreadsheets/d/1aXm_RvD4aywXRpQdxVxjBxwPAR_YBBq9f3xkHQaLZXo/edit?usp=sharing\">our mappings using this spreadsheet</a> and put your Thinklab username. Then leave a comment in this discussion, and we will rate its value <span class=\"math\">$$\\geq $4 \\times n$$</span>, where <em>n</em> is the number of mappings provided.</p>\r\n\r\n<p>Some additional sample site information is available in Table S1 (p. 58) of the <a href=\"//www.sciencemag.org/content/suppl/2015/05/06/348.6235.660.DC1/Mele.SM.pdf\">supplement</a>.</p>",
      "body_md": "# Mapping GTEx sites to Uberon and CL\r\n\r\nWe [are using](//thinklab.com/discussion/tissue-node/41) [Uberon](https://uberon.github.io/) [@10.1186/gb-2012-13-1-r5] terms to identify anatomical structures and [Cell Ontology](https://github.com/obophenotype/cell-ontology) (CL) [@10.1186/gb-2005-6-2-r21] terms to identify cell types. Thus, we need to map GTEx sites to their corresponding ontology terms.\r\n\r\nFrom the sample attribute documentation (`GTEx_Data_V4_Annotations_SampleAttributesDS.txt`), we [identified](https://github.com/dhimmel/gtex/blob/8acb90fea2613f8b27814401e361bb6ae4b29078/gtex.ipynb) 54 sites using the `SMTSD` attribute. I have mapped about half of the sites to Uberon. The remainder would benefit from a skilled anatomist or GTEx consortium member.\r\n\r\n**Bounty:** Add or correct [our mappings using this spreadsheet](https://docs.google.com/spreadsheets/d/1aXm_RvD4aywXRpQdxVxjBxwPAR_YBBq9f3xkHQaLZXo/edit?usp=sharing) and put your Thinklab username. Then leave a comment in this discussion, and we will rate its value $$\\geq $4 \\times n$$, where *n* is the number of mappings provided.\r\n\r\nSome additional sample site information is available in Table S1 (p. 58) of the [supplement](//www.sciencemag.org/content/suppl/2015/05/06/348.6235.660.DC1/Mele.SM.pdf).",
      "profile": 17,
      "published": "2015-06-17T21:22:53.938694Z",
      "thread": 82,
      "url": "/discussion/processing-gtex-for-tissue-specific-gene-expression/82#2"
    },
    {
      "body_html": "<p>So you probably don't want structures that are <em>uniquely</em> human or that evolved after the human-chimp common ancestor - there are probably only a handful of these, e.g. certain minor glands and brain regions.</p>\r\n\r\n<p>You probably want structures that are typically present in humans but not necessarily absent from other species. There are two ways to go about answering this, based on your tolerance for accidentally including a non-human structure vs accidentally excluding a human structure.</p>\r\n\r\n<ol><li>List structures that exclude those that are known not to be found in human</li><li>List structures for which there is evidence that the structure is found in humans.</li></ol>\r\n\r\n<p>Currently we are well geared up for answering (1), but you have to tolerate the occasional inclusion of some obscure brain region that was only actually observed in macaques or mice. We call these 'taxon modules'. We are not well geared up for (2) but this could be prioritized. Using cross-references to DHBA, EHDAA2, FMA, HBA, and HsapDv is a good start but you may still miss some things.</p>\r\n\r\n<p>Either way, you need to do something more specific than just look up the direct properties of the class. You need inference over both the anatomical graph, and the taxonomy graph. For (1) you need to make use of negative evidence, which intuitively 'reverses the flow' of inference. So if a larval stage is never found in amniotes, then any structure that necessarily exists at the larval stage is never found in any descendant of amniotes.</p>\r\n\r\n<p>You'd be better using an owl reasoner or some existing tooling for this.</p>\r\n\r\n<p>Some background on the taxon axioms:<br><a href=\"https://github.com/obophenotype/uberon/wiki/Taxon-constraints\">https://github.com/obophenotype/uberon/wiki/Taxon-constraints</a></p>\r\n\r\n<p>Hmm the taxon subsets files could do with better documentation:</p>\r\n\r\n<p><a href=\"http://uberon.github.io/downloads.html#subsets\">http://uberon.github.io/downloads.html#subsets</a></p>\r\n\r\n<p>(these follow (1) above) </p>\r\n\r\n<p>We should really make a ready-made human subset here. It would probably be more popular than the Aves one.</p>\r\n\r\n<p>These are built using <a href=\"https://github.com/owlcollab/owltools\">owltools</a></p>\r\n\r\n<p></p><pre><code class=\"no-highlight hljs\">owltools --use-catalog ext.owl--reasoner elk --make-species-subset -t NCBITaxon:9606 --remove-dangling --assert-inferred-subclass-axioms --useIsInferred --remove-dangling --set-ontology-id $(OBO)/uberon/subsets/human.owl -o human.owl</code></pre>",
      "body_md": "So you probably don't want structures that are *uniquely* human or that evolved after the human-chimp common ancestor - there are probably only a handful of these, e.g. certain minor glands and brain regions.\r\n\r\nYou probably want structures that are typically present in humans but not necessarily absent from other species. There are two ways to go about answering this, based on your tolerance for accidentally including a non-human structure vs accidentally excluding a human structure.\r\n\r\n 1. List structures that exclude those that are known not to be found in human\r\n 2. List structures for which there is evidence that the structure is found in humans.\r\n\r\nCurrently we are well geared up for answering (1), but you have to tolerate the occasional inclusion of some obscure brain region that was only actually observed in macaques or mice. We call these 'taxon modules'. We are not well geared up for (2) but this could be prioritized. Using cross-references to DHBA, EHDAA2, FMA, HBA, and HsapDv is a good start but you may still miss some things.\r\n\r\nEither way, you need to do something more specific than just look up the direct properties of the class. You need inference over both the anatomical graph, and the taxonomy graph. For (1) you need to make use of negative evidence, which intuitively 'reverses the flow' of inference. So if a larval stage is never found in amniotes, then any structure that necessarily exists at the larval stage is never found in any descendant of amniotes.\r\n\r\nYou'd be better using an owl reasoner or some existing tooling for this.\r\n\r\n\r\n\r\nSome background on the taxon axioms:\r\nhttps://github.com/obophenotype/uberon/wiki/Taxon-constraints\r\n\r\nHmm the taxon subsets files could do with better documentation:\r\n\r\nhttp://uberon.github.io/downloads.html#subsets\r\n\r\n(these follow (1) above) \r\n\r\nWe should really make a ready-made human subset here. It would probably be more popular than the Aves one.\r\n\r\nThese are built using [owltools](https://github.com/owlcollab/owltools)\r\n\r\n```\r\nowltools --use-catalog ext.owl--reasoner elk --make-species-subset -t NCBITaxon:9606 --remove-dangling --assert-inferred-subclass-axioms --useIsInferred --remove-dangling --set-ontology-id $(OBO)/uberon/subsets/human.owl -o human.owl\r\n```\r\n",
      "profile": 109,
      "published": "2015-06-17T23:55:39.120211Z",
      "thread": 41,
      "url": "/discussion/tissue-node/41#11"
    },
    {
      "body_html": "<p><em>R is a data scientist's dream but a programmer's nightmare.</em></p>\r\n\r\n<p>Here I'll describe the R programming principles and practices that <a href=\"/u/leobrueggeman\" class=\"username\">@leobrueggeman</a> and I will try to adhere to for this project. We are big believers in the <a href=\"https://barryrowlingson.github.io/hadleyverse\">Hadleyverse</a> — a philosophy of R programming, data analysis, and visualization — spearheaded by <a href=\"http://had.co.nz/\">Hadley Wickham</a>. I'll describe the basics as well as our modifications below.</p>\r\n\r\n<h1>Style</h1>\r\n\r\n<p>We follow the <a href=\"http://adv-r.had.co.nz/Style.html\">Hadley style guide</a>, which builds off the older <a href=\"https://google-styleguide.googlecode.com/svn/trunk/Rguide.xml\">Google style guide</a>. When calling functions from a package (any non-base function), use double colons to clarify function provenance (i.e. <code>dplyr::filter()</code> rather than just <code>filter()</code>).</p>\r\n\r\n<h1>Data format</h1>\r\n\r\n<p>The preferred storage format for tabular data is tab-separated with the <code>.tsv</code> function. Column names should always be included. For compression, use gzip with a <code>.gz</code> extension.</p>\r\n\r\n<p>Tabular data in R should be in data frames. Avoid relying on row names by making a dedicated column for the attribute. <code>options(stringsAsFactors = FALSE)</code> is imperative but may not need to be explicitly called if reading data with <a href=\"https://github.com/hadley/readr\"><code>readr</code></a> and manipulating data with <a href=\"http://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html\"><code>dplyr</code></a> and <a href=\"https://github.com/hadley/tidyr\"><code>tidyr</code></a>. Create data frames using <code>dplyr::data_frame()</code>.</p>\r\n\r\n<p>Tables should be tidy <span class=\"citation\">[<a href=\"/doi/10.18637/jss.v059.i10\" class=\"citation\" data-key=\"10.18637/jss.v059.i10\">1</a>]</span>:</p>\r\n\r\n<ol><li>Each variable forms a column.</li><li>Each observation forms a row.</li><li>Each type of observational unit forms a table.</li></ol>\r\n\r\n<h1>Piping</h1>\r\n\r\n<p>Consecutive commands should be chained together using the <a href=\"https://github.com/smbache/magrittr\"><code>magrittr</code></a> pipe (<code>%&gt;%</code>) when possible. Piping improves readability and avoids unnecessary variables cluttering the workspace.</p>\r\n\r\n<h1>Visualization</h1>\r\n\r\n<p><a href=\"http://docs.ggplot2.org/current/\"><code>ggplot2</code></a> is the preferred plotting package <span class=\"citation\">[<a href=\"/doi/10.1002/wics.147\" class=\"citation\" data-key=\"10.1002/wics.147\">2</a>, <a href=\"/doi/10.1198/jcgs.2009.07098\" class=\"citation\" data-key=\"10.1198/jcgs.2009.07098\">3</a>]</span>. Avoid the shortcut function <code>ggplot2::qplot()</code>. Consider <a href=\"http://cran.r-project.org/web/packages/cowplot/vignettes/introduction.html\"><code>cowplot</code></a> or <code>ggplot2::theme_bw()</code> to avoid the ugly default theme. Make sure all text is readable. If text is too small to read, remove it. Exporting to vector images (pdfs and svgs) is preferred unless intensive rendering requires png.</p>\r\n\r\n<h1>Development environment</h1>\r\n\r\n<p><a href=\"http://www.rstudio.com/products/RStudio/\">RStudio</a> is a mediocre mature development environment. For most prototyping work, notebooks are more powerful and straightforward. Consider using <a href=\"https://jupyter.org/\">Jupyter (IPython) notebooks</a> with <a href=\"https://irkernel.github.io/\">IRKernel</a>.</p>\r\n\r\n<h1>Version control</h1>\r\n\r\n<p>All code should be version controlled. We use <a href=\"https://git-scm.com/\">git</a> hosted on <a href=\"https://github.com/\">GitHub</a>. <a href=\"https://git-scm.com/videos\">These short videos</a> are recommended for inexperienced git users.</p>\r\n\r\n<p>Link to specific files on GitHub with the commit hash for immutability and durability.</p>\r\n\r\n<h1>Additional materials</h1>\r\n\r\n<p>Check out the rstudio <a href=\"https://github.com/rstudio/webinars\">webinars</a> as well as <a href=\"http://www.rstudio.com/resources/cheatsheets/\">cheatsheets</a>.</p>",
      "body_md": "*R is a data scientist's dream but a programmer's nightmare.*\r\n\r\nHere I'll describe the R programming principles and practices that @leobrueggeman and I will try to adhere to for this project. We are big believers in the [Hadleyverse](https://barryrowlingson.github.io/hadleyverse) -- a philosophy of R programming, data analysis, and visualization -- spearheaded by [Hadley Wickham](http://had.co.nz/). I'll describe the basics as well as our modifications below.\r\n\r\n# Style\r\n\r\nWe follow the [Hadley style guide](http://adv-r.had.co.nz/Style.html), which builds off the older [Google style guide](https://google-styleguide.googlecode.com/svn/trunk/Rguide.xml). When calling functions from a package (any non-base function), use double colons to clarify function provenance (i.e. `dplyr::filter()` rather than just `filter()`).\r\n\r\n# Data format\r\n\r\nThe preferred storage format for tabular data is tab-separated with the `.tsv` function. Column names should always be included. For compression, use gzip with a `.gz` extension.\r\n\r\nTabular data in R should be in data frames. Avoid relying on row names by making a dedicated column for the attribute. `options(stringsAsFactors = FALSE)` is imperative but may not need to be explicitly called if reading data with [`readr`](https://github.com/hadley/readr) and manipulating data with [`dplyr`](http://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html) and [`tidyr`](https://github.com/hadley/tidyr). Create data frames using `dplyr::data_frame()`.\r\n\r\nTables should be tidy [@10.18637/jss.v059.i10]:\r\n\r\n1. Each variable forms a column.\r\n2. Each observation forms a row.\r\n3. Each type of observational unit forms a table.\r\n\r\n# Piping\r\n\r\nConsecutive commands should be chained together using the [`magrittr`](https://github.com/smbache/magrittr) pipe (`%>%`) when possible. Piping improves readability and avoids unnecessary variables cluttering the workspace.\r\n\r\n# Visualization\r\n\r\n[`ggplot2`](http://docs.ggplot2.org/current/) is the preferred plotting package [@10.1002/wics.147 @10.1198/jcgs.2009.07098]. Avoid the shortcut function `ggplot2::qplot()`. Consider [`cowplot`](http://cran.r-project.org/web/packages/cowplot/vignettes/introduction.html) or `ggplot2::theme_bw()` to avoid the ugly default theme. Make sure all text is readable. If text is too small to read, remove it. Exporting to vector images (pdfs and svgs) is preferred unless intensive rendering requires png.\r\n\r\n# Development environment\r\n\r\n[RStudio](http://www.rstudio.com/products/RStudio/) is a mediocre mature development environment. For most prototyping work, notebooks are more powerful and straightforward. Consider using [Jupyter (IPython) notebooks](https://jupyter.org/) with [IRKernel](https://irkernel.github.io/).\r\n\r\n# Version control\r\n\r\nAll code should be version controlled. We use [git](https://git-scm.com/) hosted on [GitHub](https://github.com/). [These short videos](https://git-scm.com/videos) are recommended for inexperienced git users.\r\n\r\nLink to specific files on GitHub with the commit hash for immutability and durability.\r\n\r\n# Additional materials\r\n\r\nCheck out the rstudio [webinars](https://github.com/rstudio/webinars) as well as [cheatsheets](http://www.rstudio.com/resources/cheatsheets/).",
      "profile": 17,
      "published": "2015-06-18T04:18:56.860002Z",
      "thread": 83,
      "url": "/discussion/r-best-practices/83"
    },
    {
      "body_html": "<h1>Bgee</h1>\r\n\r\n<p>On a GitHub issues discussion on mapping GTEx data to Uberon, <a href=\"https://github.com/obophenotype/uberon/issues/704#issuecomment-113134400\">Frederic Bastian suggested Bgee</a> as a database for human tissue-specific expression measurements under normal conditions — exactly what we're looking for.</p>\r\n\r\n<p><a href=\"http://bgee.unil.ch/\">Bgee</a> was designed for comparative genomics <span class=\"citation\">[<a href=\"/doi/10.1007/978-3-540-69828-9_12\" class=\"citation\" data-key=\"10.1007/978-3-540-69828-9_12\">1</a>]</span> and therefore maps sample sites to standard vocabularies (like Uberon) and contains data for many species (not needed now but may be in the future).</p>\r\n\r\n<p>Genes are in Ensembl identifiers which we can easily convert to Entrez GeneIDs. Highly processed and relevant datasets <a href=\"http://bgee.unil.ch/?page=doc&amp;action=call_files#single_expr\">appear to be available</a>:</p>\r\n\r\n<ul><li>Presence/absence of expression</li><li>Over-/under-expression across anatomy or life stages</li></ul>\r\n\r\n<p>Specifically, we want to create three matrices with Entrez GeneID columns and Uberon/CL rows. We are interested in the adult development stage. The values should binary or continuous allowing us to pick an inclusion threshold later. The three desired matrices of tissue-specific expression are:</p>\r\n\r\n<ol><li>Transcript presence in adult </li><li>Transcript over-expression compared to other adult human sites</li><li>Transcript under-expression compared to other adult human sites</li></ol>\r\n\r\n<p>Bgee does not yet include GTEx RNA-Seq data but will integrate this data in the coming months. <strong>Question for Bgee team:</strong> do you compute transcript abundance from raw data? I am not stoked about the Flux Capacitor software used by GTEx for <a href=\"https://liorpachter.wordpress.com/2013/10/21/gtex/\">reasons pointed out by others</a>.</p>",
      "body_md": "# Bgee\r\n\r\nOn a GitHub issues discussion on mapping GTEx data to Uberon, [Frederic Bastian suggested Bgee](https://github.com/obophenotype/uberon/issues/704#issuecomment-113134400) as a database for human tissue-specific expression measurements under normal conditions -- exactly what we're looking for.\r\n\r\n[Bgee](http://bgee.unil.ch/) was designed for comparative genomics [@10.1007/978-3-540-69828-9_12] and therefore maps sample sites to standard vocabularies (like Uberon) and contains data for many species (not needed now but may be in the future).\r\n\r\nGenes are in Ensembl identifiers which we can easily convert to Entrez GeneIDs. Highly processed and relevant datasets [appear to be available](http://bgee.unil.ch/?page=doc&action=call_files#single_expr):\r\n\r\n+ Presence/absence of expression\r\n+ Over-/under-expression across anatomy or life stages\r\n\r\nSpecifically, we want to create three matrices with Entrez GeneID columns and Uberon/CL rows. We are interested in the adult development stage. The values should binary or continuous allowing us to pick an inclusion threshold later. The three desired matrices of tissue-specific expression are:\r\n\r\n1. Transcript presence in adult \r\n2. Transcript over-expression compared to other adult human sites\r\n3. Transcript under-expression compared to other adult human sites\r\n\r\nBgee does not yet include GTEx RNA-Seq data but will integrate this data in the coming months. **Question for Bgee team:** do you compute transcript abundance from raw data? I am not stoked about the Flux Capacitor software used by GTEx for [reasons pointed out by others](https://liorpachter.wordpress.com/2013/10/21/gtex/).",
      "profile": 17,
      "published": "2015-06-18T16:33:27.976269Z",
      "thread": 81,
      "url": "/discussion/tissue-specific-gene-expression-resources/81#3"
    },
    {
      "body_html": "<h1>Human constraint: <em>positive</em> versus <em>no negative</em> evidence</h1>\r\n\r\n<p><a href=\"/u/chrismungall\" class=\"username\">@chrismungall</a>, could not have hoped for a more informative response!</p>\r\n\r\n<p>I am going to refer to your two methods as:</p>\r\n\r\n<ol><li>no negative evidence, which should include all human structures and some non-human structures</li><li>positive evidence, which should include some human structures and exclude all non-human structures</li></ol>\r\n\r\n<p>I created a comparison of the two methods for Uberon terms, using <a href=\"/u/fbastian\" class=\"username\">@fbastian</a>'s <a href=\"https://github.com/obophenotype/uberon/issues/703#issuecomment-113131156\">implementation</a> of <a href=\"/u/chrismungall\" class=\"username\">@chrismungall</a>'s owltools command <a href=\"#276\">above</a> for (1) and my <a href=\"https://github.com/dhimmel/uberon/blob/08cbf5beef80a68cf880a459a49097e39afade08/human-constraint.ipynb\">implementation</a> of <em>positive evidence</em> for (2). My implementation has not been vetted, and if there is an owltools command for this functionality, we should switch. </p>\r\n\r\n<p>I created two tsv files: one with <a href=\"https://github.com/dhimmel/uberon/blob/08cbf5beef80a68cf880a459a49097e39afade08/data/human-constraint.tsv\">all uberon terms</a> and another with only <a href=\"https://github.com/dhimmel/uberon/blob/08cbf5beef80a68cf880a459a49097e39afade08/data/mesh-map.tsv\">MeSH-mapping uberon terms</a>.</p>\r\n\r\n<p>Since I'm primarily concerned with terms in MeSH, I looked through MeSH-mapping uberon terms with no positive evidence (<code>positive_evidence == 0</code>) and also no negative evidence (<code>no_negative_evidence == 1</code>). Looking through this subset, there were a few notable terms where we would like negative evidence to exist:</p>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th>uberon_id</th><th>uberon_name</th></tr></thead><tbody><tr><td>UBERON:0013196</td><td>strand of wool</td></tr><tr><td>UBERON:0002415</td><td>tail</td></tr><tr><td>UBERON:0007113</td><td>venom</td></tr><tr><td>UBERON:0005079</td><td>eggshell</td></tr><tr><td>UBERON:0001011</td><td>hemolymph</td></tr><tr><td>UBERON:0006378</td><td>strand of vibrissa hair</td></tr><tr><td>UBERON:0004758</td><td>salt gland</td></tr><tr><td>UBERON:0011123</td><td>stifle joint</td></tr></tbody></table>\r\n\r\n<p>However, overall the <em>no negative evidence</em> method appeared to have higher accuracy than the <em>positive evidence</em> method. Therefore, we will proceed using <em>no negative evidence</em>, which should improve over time as Uberon matures.</p>",
      "body_md": "# Human constraint: *positive* versus *no negative* evidence\r\n\r\n@chrismungall, could not have hoped for a more informative response!\r\n\r\nI am going to refer to your two methods as:\r\n\r\n1. no negative evidence, which should include all human structures and some non-human structures\r\n2. positive evidence, which should include some human structures and exclude all non-human structures\r\n\r\nI created a comparison of the two methods for Uberon terms, using @fbastian's [implementation](https://github.com/obophenotype/uberon/issues/703#issuecomment-113131156) of @chrismungall's owltools command [above](#276) for (1) and my [implementation](https://github.com/dhimmel/uberon/blob/08cbf5beef80a68cf880a459a49097e39afade08/human-constraint.ipynb) of *positive evidence* for (2). My implementation has not been vetted, and if there is an owltools command for this functionality, we should switch. \r\n\r\nI created two tsv files: one with [all uberon terms](https://github.com/dhimmel/uberon/blob/08cbf5beef80a68cf880a459a49097e39afade08/data/human-constraint.tsv) and another with only [MeSH-mapping uberon terms](https://github.com/dhimmel/uberon/blob/08cbf5beef80a68cf880a459a49097e39afade08/data/mesh-map.tsv).\r\n\r\nSince I'm primarily concerned with terms in MeSH, I looked through MeSH-mapping uberon terms with no positive evidence (`positive_evidence == 0`) and also no negative evidence (`no_negative_evidence == 1`). Looking through this subset, there were a few notable terms where we would like negative evidence to exist:\r\n\r\n| uberon_id | uberon_name |\r\n|----------------|-------------------------|\r\n| UBERON:0013196 | strand of wool |\r\n| UBERON:0002415 | tail |\r\n| UBERON:0007113 | venom |\r\n| UBERON:0005079 | eggshell |\r\n| UBERON:0001011 | hemolymph |\r\n| UBERON:0006378 | strand of vibrissa hair |\r\n| UBERON:0004758 | salt gland |\r\n| UBERON:0011123 | stifle joint |\r\n\r\nHowever, overall the *no negative evidence* method appeared to have higher accuracy than the *positive evidence* method. Therefore, we will proceed using *no negative evidence*, which should improve over time as Uberon matures.",
      "profile": 17,
      "published": "2015-06-18T21:09:23.190526Z",
      "thread": 41,
      "url": "/discussion/tissue-node/41#12"
    },
    {
      "body_html": "<h1>Bgee parameters</h1>\r\n\r\n<p>The <a href=\"http://bgee.unil.ch/?page=download&amp;action=expr_calls#id9606\">Bgee human downloads</a> are user-friendly and should be easy to process. We just have a few questions:</p>\r\n\r\n<h2>Transcript presence</h2>\r\n\r\n<p>For determining expression presence, we want to pick a single and broad <strong>developmental stage</strong>, such as adult. The <a href=\"http://bgee.unil.ch/?page=doc&amp;action=call_files#single_expr\">propagation</a> up the developmental ontology means we don't need to pick a stage that has been directly assayed by many studies. Post-juvenile adult stage (<code>UBERON:0000113</code>) is used for the differential expression analysis. Which stage should we choose?</p>\r\n\r\n<p>Next, what <strong>evidence threshold</strong> should we require to consider a gene expressed I was thinking requiring <code>call_quality == 'high quality'</code> and <code>expression in {'present', 'low ambiguity'}</code>. We could also use a more complex metric on the <a href=\"http://bgee.unil.ch/?page=doc&amp;action=call_files#single_expr\">complete file</a>. </p>\r\n\r\n<h2>Differential expression</h2>\r\n\r\n<p>We would like to include two differential expression edges in our network: one for under-expression and one for over-expression. For the over-expression dataset, is <code>call_quality == 'high quality'</code> and <code>differential_expression  == 'over-expression'</code> the right filter?</p>",
      "body_md": "# Bgee parameters\r\n\r\nThe [Bgee human downloads](http://bgee.unil.ch/?page=download&action=expr_calls#id9606) are user-friendly and should be easy to process. We just have a few questions:\r\n\r\n## Transcript presence\r\n\r\nFor determining expression presence, we want to pick a single and broad **developmental stage**, such as adult. The [propagation](http://bgee.unil.ch/?page=doc&action=call_files#single_expr) up the developmental ontology means we don't need to pick a stage that has been directly assayed by many studies. Post-juvenile adult stage (`UBERON:0000113`) is used for the differential expression analysis. Which stage should we choose?\r\n\r\nNext, what **evidence threshold** should we require to consider a gene expressed I was thinking requiring `call_quality == 'high quality'` and `expression in {'present', 'low ambiguity'}`. We could also use a more complex metric on the [complete file](http://bgee.unil.ch/?page=doc&action=call_files#single_expr). \r\n\r\n## Differential expression\r\n\r\nWe would like to include two differential expression edges in our network: one for under-expression and one for over-expression. For the over-expression dataset, is `call_quality == 'high quality'` and `differential_expression  == 'over-expression'` the right filter?",
      "profile": 17,
      "published": "2015-06-18T23:49:43.252238Z",
      "thread": 81,
      "url": "/discussion/tissue-specific-gene-expression-resources/81#4"
    },
    {
      "body_html": "<h1>Handing over GTEx processing responsibility to Bgee</h1>\r\n\r\n<p>We have <a href=\"http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#278\">decided to use Bgee</a> for tissue-specific transcript presence, over-, and under-expression. Bgee doesn't currently include GTEx data but <a href=\"https://github.com/obophenotype/uberon/issues/704#issuecomment-113134400\">will soon</a>.</p>\r\n\r\n<p>Therefore, we are not going to proceed with GTEx data directly for this project. However, we did already process the data into a usable gene × site format (<a href=\"https://github.com/dhimmel/gtex/blob/93c3800cef1ced48b100d57b8d6efab831f8533b/gtex.ipynb\">notebook</a>, <a href=\"https://github.com/dhimmel/gtex/blob/93c3800cef1ced48b100d57b8d6efab831f8533b/data/expression-SMTSD.tsv.gz\">download</a>). We converted genes to Entrez GeneIDs. The sites are still in GTEx strings rather than Uberon terms. Expression values are log-transformed. Check out the notebook for a visualization of tissue-specific transcript abundance distributions.</p>\r\n\r\n<p><strong>Bounty:</strong> we will keep the GTEx–Uberon mapping bounty going until June 25, 2015 because these mappings will help the Bgee team and eventually us. <a href=\"/u/chrismungall\" class=\"username\">@chrismungall</a>, do you want to add a comment here, so you can get rewarded for your 3 mappings?</p>",
      "body_md": "# Handing over GTEx processing responsibility to Bgee\r\n\r\nWe have [decided to use Bgee](http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#278) for tissue-specific transcript presence, over-, and under-expression. Bgee doesn't currently include GTEx data but [will soon](https://github.com/obophenotype/uberon/issues/704#issuecomment-113134400).\r\n\r\nTherefore, we are not going to proceed with GTEx data directly for this project. However, we did already process the data into a usable gene × site format ([notebook](https://github.com/dhimmel/gtex/blob/93c3800cef1ced48b100d57b8d6efab831f8533b/gtex.ipynb), [download](https://github.com/dhimmel/gtex/blob/93c3800cef1ced48b100d57b8d6efab831f8533b/data/expression-SMTSD.tsv.gz)). We converted genes to Entrez GeneIDs. The sites are still in GTEx strings rather than Uberon terms. Expression values are log-transformed. Check out the notebook for a visualization of tissue-specific transcript abundance distributions.\r\n\r\n**Bounty:** we will keep the GTEx--Uberon mapping bounty going until June 25, 2015 because these mappings will help the Bgee team and eventually us. @chrismungall, do you want to add a comment here, so you can get rewarded for your 3 mappings?",
      "profile": 17,
      "published": "2015-06-19T00:00:40.757557Z",
      "thread": 82,
      "url": "/discussion/processing-gtex-for-tissue-specific-gene-expression/82#3"
    },
    {
      "body_html": "<p>I think the Bgee team will do a great job. Just a few general comments:</p>\r\n\r\n<p>most of the GTEx terms correspond to 'wild-type' structures as can be found in uberon/cl. There are however, two subclasses of skin: exposed and unexposed. We <em>could</em> add these as subclasses in uberon, but this would be unusual. It would be better to either post-compose these, or to have some kind of ancillary 'sample' ontology where this is composed.</p>\r\n\r\n<p>For 'hippocampus', the safest option is to map to the broadest term, 'hippocampal formation', but if it can be shown than the GTEx sample excludes bits of the dentate gyrus then the more specific 'ammons horn' can be used.</p>\r\n\r\n<p>Finally, it's always best when ontologies are used prospectively rather than retrospectively, maybe future rounds of GTEx will follow the lead of FANTOM5 and ENCODE in doing this.</p>",
      "body_md": "I think the Bgee team will do a great job. Just a few general comments:\r\n\r\nmost of the GTEx terms correspond to 'wild-type' structures as can be found in uberon/cl. There are however, two subclasses of skin: exposed and unexposed. We *could* add these as subclasses in uberon, but this would be unusual. It would be better to either post-compose these, or to have some kind of ancillary 'sample' ontology where this is composed.\r\n\r\nFor 'hippocampus', the safest option is to map to the broadest term, 'hippocampal formation', but if it can be shown than the GTEx sample excludes bits of the dentate gyrus then the more specific 'ammons horn' can be used.\r\n\r\nFinally, it's always best when ontologies are used prospectively rather than retrospectively, maybe future rounds of GTEx will follow the lead of FANTOM5 and ENCODE in doing this.",
      "profile": 109,
      "published": "2015-06-19T00:12:47.995112Z",
      "thread": 82,
      "url": "/discussion/processing-gtex-for-tissue-specific-gene-expression/82#4"
    },
    {
      "body_html": "<p>IMO, the \"exposed/unexposed\" state is an experimental factor, and should not be annotated using a new anatomical term, it should be an additional \"column\" in an annotation (using, e.g., EFO).</p>\r\n\r\n<p>Daniel, we will discuss next week during our lab meeting the timescale to annotate GTEx data, but as you said, it should be fast. Our problem is that we requested access to the data, and are waiting for an answer. </p>\r\n\r\n<p>We could start working on the mapping you have, but we'd rather go through the information for each sample, to check for normality, etc. This is how we usually do. <br>(do they provide GEO or SRA identifiers for the samples BTW?)</p>",
      "body_md": "IMO, the \"exposed/unexposed\" state is an experimental factor, and should not be annotated using a new anatomical term, it should be an additional \"column\" in an annotation (using, e.g., EFO).\r\n\r\nDaniel, we will discuss next week during our lab meeting the timescale to annotate GTEx data, but as you said, it should be fast. Our problem is that we requested access to the data, and are waiting for an answer. \r\n\r\nWe could start working on the mapping you have, but we'd rather go through the information for each sample, to check for normality, etc. This is how we usually do. \r\n(do they provide GEO or SRA identifiers for the samples BTW?)",
      "profile": 111,
      "published": "2015-06-19T12:48:14.856994Z",
      "thread": 82,
      "url": "/discussion/processing-gtex-for-tissue-specific-gene-expression/82#5"
    },
    {
      "body_html": "<blockquote><p>do you compute transcript abundance from raw data? I am not stoked about the Flux Capacitor software used by GTEx for reasons pointed out by others.</p></blockquote>\r\n\r\n<p>Yes, we do. We map reads to transcriptome using TopHat2; read counts extracted using HTseq; length of longest annotated transcript used.<br>(edit: woops, you said transcript abundance. We only provide expression data \"per gene\")</p>\r\n\r\n<p>Currently, we do not use TMM normalization between samples for the RPKM values we provide, but we will in the next release. We also want to get away from RPKM values, and provide TPM values (for motivation, see <span class=\"citation\">[<a href=\"/doi/10.1007/s12064-012-0162-3\" class=\"citation\" data-key=\"10.1007/s12064-012-0162-3\">1</a>]</span>).</p>\r\n\r\n<p>To generate differential expression calls, we use TMM normalization, then voom + limma. This has been shown to perform well (see <span class=\"citation\">[<a href=\"/doi/10.1093/bib/bbt086\" class=\"citation\" data-key=\"10.1093/bib/bbt086\">2</a>]</span>). </p>\r\n\r\n<p>We will provide complete documentation by July. </p>\r\n\r\n<blockquote><p>The values should binary or continuous allowing us to pick an inclusion threshold later.</p></blockquote>\r\n\r\n<p>So have you noticed that we also provide RPKM values, not only the qualitative \"calls\"?</p>\r\n\r\n<blockquote><p>Post-juvenile adult stage (UBERON:0000113) is used for the differential expression analysis. Which stage should we choose?</p></blockquote>\r\n\r\n<p>Post-juvenile in human also includes 13-18 yo, so I don't know if you want to include those. HsapDv:0000087 \"human adult stage\" would be a \"true\" adult stage. But I think we don't use it for \"differential expression across anatomy\", we could correct that if you need it. It should be correctly used for \"differential expression across development\", if we have the data. And of course it is correctly used for the \"presence/absence\" calls.</p>\r\n\r\n<p>Note that \"differential expression\" calls are not propagated, so you might still need to examine all stages for \"differential expression across development\". But for \"presence/absence\", you can safely rely on the propagation, and only retrieve results for your stage of interest. </p>\r\n\r\n<p>You might want to use the \"complete\" file, you will have more data propagated. Also, if you use the \"complete\" file, you might decide to rely only on RNA-Seq data, and avoid the \"ambiguity\" states.</p>\r\n\r\n<blockquote><p>what evidence threshold should we require to consider a gene expressed I was thinking requiring call_quality == 'high quality' and expression in {'present', 'low ambiguity'}. We could also use a more complex metric on the complete file. </p></blockquote>\r\n\r\n<p>Presence low quality seems also to give good results, but here it is up to you to decide whether you want to be more on the false negative side, or more on the false positive side.</p>\r\n\r\n<blockquote><p>For the over-expression dataset, is call_quality == 'high quality' and differential_expression == 'over-expression' the right filter?</p></blockquote>\r\n\r\n<p>I would definitely use the \"low quality\" as well here. Because the overall call generated is based on a voting system weighted by p-values, so even if it is \"low quality\" because of conflicting analyses, the best p-value has won anyway. <br>Again, if you use the \"complete\" file, you might decide to rely only on RNA-Seq data, and avoid the \"ambiguity\" states.</p>\r\n\r\n<p>Edit: oh, I didn't notice you where mentioning \"transcript abundance\". We only provide expression data \"per gene\", not \"per transcript\".</p>",
      "body_md": "> do you compute transcript abundance from raw data? I am not stoked about the Flux Capacitor software used by GTEx for reasons pointed out by others.\r\n\r\nYes, we do. We map reads to transcriptome using TopHat2; read counts extracted using HTseq; length of longest annotated transcript used.\r\n(edit: woops, you said transcript abundance. We only provide expression data \"per gene\")\r\n\r\nCurrently, we do not use TMM normalization between samples for the RPKM values we provide, but we will in the next release. We also want to get away from RPKM values, and provide TPM values (for motivation, see [@10.1007/s12064-012-0162-3]).\r\n\r\nTo generate differential expression calls, we use TMM normalization, then voom + limma. This has been shown to perform well (see [@10.1093/bib/bbt086]). \r\n\r\nWe will provide complete documentation by July. \r\n\r\n> The values should binary or continuous allowing us to pick an inclusion threshold later.\r\n\r\nSo have you noticed that we also provide RPKM values, not only the qualitative \"calls\"?\r\n\r\n> Post-juvenile adult stage (UBERON:0000113) is used for the differential expression analysis. Which stage should we choose?\r\n\r\nPost-juvenile in human also includes 13-18 yo, so I don't know if you want to include those. HsapDv:0000087 \"human adult stage\" would be a \"true\" adult stage. But I think we don't use it for \"differential expression across anatomy\", we could correct that if you need it. It should be correctly used for \"differential expression across development\", if we have the data. And of course it is correctly used for the \"presence/absence\" calls.\r\n\r\nNote that \"differential expression\" calls are not propagated, so you might still need to examine all stages for \"differential expression across development\". But for \"presence/absence\", you can safely rely on the propagation, and only retrieve results for your stage of interest. \r\n\r\nYou might want to use the \"complete\" file, you will have more data propagated. Also, if you use the \"complete\" file, you might decide to rely only on RNA-Seq data, and avoid the \"ambiguity\" states.\r\n\r\n> what evidence threshold should we require to consider a gene expressed I was thinking requiring call_quality == 'high quality' and expression in {'present', 'low ambiguity'}. We could also use a more complex metric on the complete file. \r\n\r\nPresence low quality seems also to give good results, but here it is up to you to decide whether you want to be more on the false negative side, or more on the false positive side.\r\n\r\n> For the over-expression dataset, is call_quality == 'high quality' and differential_expression == 'over-expression' the right filter?\r\n\r\nI would definitely use the \"low quality\" as well here. Because the overall call generated is based on a voting system weighted by p-values, so even if it is \"low quality\" because of conflicting analyses, the best p-value has won anyway. \r\nAgain, if you use the \"complete\" file, you might decide to rely only on RNA-Seq data, and avoid the \"ambiguity\" states.\r\n\r\nEdit: oh, I didn't notice you where mentioning \"transcript abundance\". We only provide expression data \"per gene\", not \"per transcript\".",
      "profile": 111,
      "published": "2015-06-19T12:52:20.262962Z",
      "thread": 81,
      "url": "/discussion/tissue-specific-gene-expression-resources/81#5"
    },
    {
      "body_html": "<blockquote><p>do they provide GEO or SRA identifiers for the samples BTW?</p></blockquote>\r\n\r\n<p><a href=\"/u/fbastian\" class=\"username\">@fbastian</a>, I do not see any other sample identifiers than <code>SAMPID</code> (GTEx Public Sample ID) in the <a href=\"http://www.gtexportal.org/static/datasets/gtex_analysis_v4/annotations/GTEx_Data_V4_Annotations_SampleAttributesDD.xlsx\">sample attributes documentation spreadsheet</a>. The IDs are formatted like <code>GTEX-N7MS-0007-SM-2D7W1</code> — not sure whether that corresponds with other databases.</p>\r\n\r\n<blockquote><p>We could start working on the mapping you have, but we'd rather go through the information for each sample, to check for normality, etc.</p></blockquote>\r\n\r\n<p><a href=\"/u/fbastian\" class=\"username\">@fbastian</a>, do whatever is best for you! And let it be known that your painstaking and thorough integration efforts are appreciated.</p>\r\n\r\n<blockquote><p>There are however, two subclasses of skin: exposed and unexposed.</p></blockquote>\r\n\r\n<p><a href=\"/u/chrismungall\" class=\"username\">@chrismungall</a>, by post-compose do you mean contacting GTEx and asking for more details on the skin sample sites? That seems the best to me as I assume the sample collectors had specific instructions. The skin sites are specified as <em>suprapubic</em> for sun unexposed and <em>lower leg</em> for sun exposed.</p>",
      "body_md": "> do they provide GEO or SRA identifiers for the samples BTW?\r\n\r\n@fbastian, I do not see any other sample identifiers than `SAMPID` (GTEx Public Sample ID) in the [sample attributes documentation spreadsheet](http://www.gtexportal.org/static/datasets/gtex_analysis_v4/annotations/GTEx_Data_V4_Annotations_SampleAttributesDD.xlsx). The IDs are formatted like `GTEX-N7MS-0007-SM-2D7W1` -- not sure whether that corresponds with other databases.\r\n\r\n> We could start working on the mapping you have, but we'd rather go through the information for each sample, to check for normality, etc.\r\n\r\n@fbastian, do whatever is best for you! And let it be known that your painstaking and thorough integration efforts are appreciated.\r\n\r\n> There are however, two subclasses of skin: exposed and unexposed.\r\n\r\n@chrismungall, by post-compose do you mean contacting GTEx and asking for more details on the skin sample sites? That seems the best to me as I assume the sample collectors had specific instructions. The skin sites are specified as *suprapubic* for sun unexposed and *lower leg* for sun exposed.",
      "profile": 17,
      "published": "2015-06-19T18:00:24.304953Z",
      "thread": 82,
      "url": "/discussion/processing-gtex-for-tissue-specific-gene-expression/82#6"
    },
    {
      "body_html": "<h1>Nomenclature</h1>\r\n\r\n<p>We have been referring to the metanode (node type) for uberon as Tissue. Is Tissue a misnomer? Does Tissue encompass all uberon nodes? If we add CL terms under the same metanode, what term can we use that encompass uberon terms and cell types?</p>\r\n\r\n<p>A single word is preferred to a compound term. Some options I can think of are</p>\r\n\r\n<ul><li>Tissue</li><li>Anatomy</li><li>Structure</li><li>Anatomical Structure</li></ul>\r\n\r\n<p><a href=\"/u/chrismungall\" class=\"username\">@chrismungall</a>, do you have an inclination?</p>",
      "body_md": "# Nomenclature\r\n\r\nWe have been referring to the metanode (node type) for uberon as Tissue. Is Tissue a misnomer? Does Tissue encompass all uberon nodes? If we add CL terms under the same metanode, what term can we use that encompass uberon terms and cell types?\r\n\r\nA single word is preferred to a compound term. Some options I can think of are\r\n\r\n+ Tissue\r\n+ Anatomy\r\n+ Structure\r\n+ Anatomical Structure\r\n\r\n@chrismungall, do you have an inclination?",
      "profile": 17,
      "published": "2015-06-19T18:37:26.847611Z",
      "thread": 41,
      "url": "/discussion/tissue-node/41#13"
    },
    {
      "body_html": "<p>tissue is too specific, but people will still know what you mean. Uberon follows the CARO upper level ontology:</p>\r\n\r\n<ul><li><a href=\"http://purl.obolibrary.org/obo/CARO_0030000\">CARO:0030000</a> ! biological entity<ul><li><a href=\"http://purl.obolibrary.org/obo/CARO_0000000\">CARO:0000000</a> ! anatomical entity<ul><li><a href=\"http://purl.obolibrary.org/obo/CARO_0000006\">CARO:0000006</a> ! material anatomical entity<ul><li><a href=\"http://purl.obolibrary.org/obo/CARO_0000003\">CARO:0000003</a> ! connected anatomical structure (aka anatomical structure)<ul><li><a href=\"http://purl.obolibrary.org/obo/CARO_0000013\">CARO:0000013</a> ! cell </li><li><a href=\"http://purl.obolibrary.org/obo/CARO_0010000\">CARO:0010000</a> ! multicellular anatomical structure<ul><li><a href=\"http://purl.obolibrary.org/obo/CARO_0000043\">CARO:0000043</a> ! tissue  </li><li><a href=\"http://purl.obolibrary.org/obo/CARO_0020004\">CARO:0020004</a> ! organ  </li><li><a href=\"http://purl.obolibrary.org/obo/CARO_0000043\">CARO:0000043</a> ! tissue  </li></ul></li></ul></li></ul></li></ul></li></ul></li></ul>\r\n\r\n<p>So formally speaking 'anatomical structure' would exclude non-material entities such as lumens, 2D boundaries... but you should never have gene expression in any of these sites</p>",
      "body_md": "tissue is too specific, but people will still know what you mean. Uberon follows the CARO upper level ontology:\r\n\r\n *  [CARO:0030000](http://purl.obolibrary.org/obo/CARO_0030000) ! biological entity\r\n    *  [CARO:0000000](http://purl.obolibrary.org/obo/CARO_0000000) ! anatomical entity\r\n       *  [CARO:0000006](http://purl.obolibrary.org/obo/CARO_0000006) ! material anatomical entity\r\n          *  [CARO:0000003](http://purl.obolibrary.org/obo/CARO_0000003) ! connected anatomical structure (aka anatomical structure)\r\n              *  [CARO:0000013](http://purl.obolibrary.org/obo/CARO_0000013) ! cell \r\n              *  [CARO:0010000](http://purl.obolibrary.org/obo/CARO_0010000) ! multicellular anatomical structure\r\n                 *  [CARO:0000043](http://purl.obolibrary.org/obo/CARO_0000043) ! tissue  \r\n                 *  [CARO:0020004](http://purl.obolibrary.org/obo/CARO_0020004) ! organ  \r\n                 *  [CARO:0000043](http://purl.obolibrary.org/obo/CARO_0000043) ! tissue  \r\n\r\nSo formally speaking 'anatomical structure' would exclude non-material entities such as lumens, 2D boundaries... but you should never have gene expression in any of these sites",
      "profile": 109,
      "published": "2015-06-19T19:07:48.480887Z",
      "thread": 41,
      "url": "/discussion/tissue-node/41#14"
    },
    {
      "body_html": "<p><a href=\"/u/dhimmel\" class=\"username\">@dhimmel</a>, thanks for the information. Did your lab formally request access to the data to get the actual annotations? (GTEx_Data_V4_Annotations_SampleAttributesDS.txt in your notebook)</p>\r\n\r\n<p>Otherwise, Chris is speaking about ontology term post-composition, a way of creating a new ontology concept on-the-fly, that doesn't have any identifier or IRI (\"anonymous class expression\"), and that is made of the \"composition\" of several other terms. That would allow you to create on the fly a new concept for \"exposed skin\".</p>\r\n\r\n<p>See for instance, in zebrafish ontology: <a href=\"https://zfin.org/action/ontology/post-composed-term-detail?superTermID=ZFA:0001117&amp;subTermID=ZFA:0000155\">https://zfin.org/action/ontology/post-composed-term-detail?superTermID=ZFA:0001117&amp;subTermID=ZFA:0000155</a><br>There is no term \"post-vent region somite\" in the ontology, but the concept is represented by using the existing terms \"post-vent region\" and \"somite\".</p>",
      "body_md": "@dhimmel, thanks for the information. Did your lab formally request access to the data to get the actual annotations? (GTEx_Data_V4_Annotations_SampleAttributesDS.txt in your notebook)\r\n\r\nOtherwise, Chris is speaking about ontology term post-composition, a way of creating a new ontology concept on-the-fly, that doesn't have any identifier or IRI (\"anonymous class expression\"), and that is made of the \"composition\" of several other terms. That would allow you to create on the fly a new concept for \"exposed skin\".\r\n\r\nSee for instance, in zebrafish ontology: https://zfin.org/action/ontology/post-composed-term-detail?superTermID=ZFA:0001117&subTermID=ZFA:0000155\r\nThere is no term \"post-vent region somite\" in the ontology, but the concept is represented by using the existing terms \"post-vent region\" and \"somite\".",
      "profile": 111,
      "published": "2015-06-20T01:04:36.970291Z",
      "thread": 82,
      "url": "/discussion/processing-gtex-for-tissue-specific-gene-expression/82#7"
    },
    {
      "body_html": "<p>We have done some initial analyses on the Bgee data (<a href=\"http://nbviewer.ipython.org/github/dhimmel/bgee/blob/b26d396bbe6b137372731f031f544c7da47e314c/bgee.ipynb\">notebook</a>). We stuck with the simple files because:</p>\r\n\r\n<ul><li>we would like to outsource as much of the difficult decision-making as possible.</li><li>we want a method that is resilient to changing technologies and Bgee revisions — not a method that is only optimal for a specific Bgee release.</li></ul>\r\n\r\n<p>Here are our initial findings.</p>\r\n\r\n<h2>Gene presence</h2>\r\n\r\n<p>We chose the filter:</p>\r\n\r\n<p></p><pre><code class=\"python\">call_quality in {'high quality', 'low quality'} and expression in {'present', 'low ambiguity'}</code></pre>\r\n\r\n<p>We chose a permissive filter that is not limited to RNA-Seq. In the future, when Bgee contains more sources, we could increase these thresholds.</p>\r\n\r\n<p>The developmental stage \"25-44 year-old human stage\" (<code>HsapDv:0000090</code>) has the most present genes. \"Human adult stage\" (<code>HsapDv:0000087</code>) is comparatively quite underpopulated with major organs like heart having no present genes (<a href=\"http://nbviewer.ipython.org/github/dhimmel/bgee/blob/b26d396bbe6b137372731f031f544c7da47e314c/bgee.ipynb#Figure-1:-Number-of-genes-present-by-developmental-stage-and-anatomical-entity\">Figure 1</a>). I was surprised because I thought adult would subsume 25-44 and the expression data would be propagated. <a href=\"/u/fbastian\" class=\"username\">@fbastian</a>, any idea what is going on? Should we take 25-44 as our developmental context or should we collapse data across adult developmental stages. Is the differential-expression data lacking if it's only calculated on \"post-juvenile adult stage\"?</p>\r\n\r\n<p>Compared to the distribution of expressed genes in the GNF Expression Atlas (<a href=\"http://nbviewer.ipython.org/github/dhimmel/bgee/blob/b26d396bbe6b137372731f031f544c7da47e314c/bgee.ipynb#Figure-3:-Distibution-of-genes-present-per-tissue-in-GNF-Expression-Atlas\">Figure 3</a>), Bgee had much more variation by tissue. This is expected since Bgee integrates diverse data of varied throughput and a is an acceptable sacrifice for broader input data.</p>\r\n\r\n<h2>Differential expression</h2>\r\n\r\n<p>We identified differential expression with <code>call_quality in {'low quality', 'high quality'}</code>. <a href=\"http://nbviewer.ipython.org/github/dhimmel/bgee/blob/b26d396bbe6b137372731f031f544c7da47e314c/bgee.ipynb#Figure-2:-Number-of-differntially-expressed-genes-present-by-anatomical-entity\">Figure 2</a> shows that we identified a large number of DE-genes across a variety of anatomical entities. Some tissues or cell types, such as leukocytes and testis, have over 10,000 under/over-expressed genes (almost every gene). Is this biologically meaningful or the result of data coverage or other experimental artifacts?</p>",
      "body_md": "We have done some initial analyses on the Bgee data ([notebook](http://nbviewer.ipython.org/github/dhimmel/bgee/blob/b26d396bbe6b137372731f031f544c7da47e314c/bgee.ipynb)). We stuck with the simple files because:\r\n\r\n+ we would like to outsource as much of the difficult decision-making as possible.\r\n+ we want a method that is resilient to changing technologies and Bgee revisions -- not a method that is only optimal for a specific Bgee release.\r\n\r\nHere are our initial findings.\r\n\r\n## Gene presence\r\n\r\nWe chose the filter:\r\n\r\n```python\r\ncall_quality in {'high quality', 'low quality'} and expression in {'present', 'low ambiguity'}\r\n```\r\n\r\nWe chose a permissive filter that is not limited to RNA-Seq. In the future, when Bgee contains more sources, we could increase these thresholds.\r\n\r\nThe developmental stage \"25-44 year-old human stage\" (`HsapDv:0000090`) has the most present genes. \"Human adult stage\" (`HsapDv:0000087`) is comparatively quite underpopulated with major organs like heart having no present genes ([Figure 1](http://nbviewer.ipython.org/github/dhimmel/bgee/blob/b26d396bbe6b137372731f031f544c7da47e314c/bgee.ipynb#Figure-1:-Number-of-genes-present-by-developmental-stage-and-anatomical-entity)). I was surprised because I thought adult would subsume 25-44 and the expression data would be propagated. @fbastian, any idea what is going on? Should we take 25-44 as our developmental context or should we collapse data across adult developmental stages. Is the differential-expression data lacking if it's only calculated on \"post-juvenile adult stage\"?\r\n\r\nCompared to the distribution of expressed genes in the GNF Expression Atlas ([Figure 3](http://nbviewer.ipython.org/github/dhimmel/bgee/blob/b26d396bbe6b137372731f031f544c7da47e314c/bgee.ipynb#Figure-3:-Distibution-of-genes-present-per-tissue-in-GNF-Expression-Atlas)), Bgee had much more variation by tissue. This is expected since Bgee integrates diverse data of varied throughput and a is an acceptable sacrifice for broader input data.\r\n\r\n## Differential expression\r\n\r\nWe identified differential expression with `call_quality in {'low quality', 'high quality'}`. [Figure 2](http://nbviewer.ipython.org/github/dhimmel/bgee/blob/b26d396bbe6b137372731f031f544c7da47e314c/bgee.ipynb#Figure-2:-Number-of-differntially-expressed-genes-present-by-anatomical-entity) shows that we identified a large number of DE-genes across a variety of anatomical entities. Some tissues or cell types, such as leukocytes and testis, have over 10,000 under/over-expressed genes (almost every gene). Is this biologically meaningful or the result of data coverage or other experimental artifacts?",
      "profile": 17,
      "published": "2015-06-20T03:01:42.139093Z",
      "thread": 81,
      "url": "/discussion/tissue-specific-gene-expression-resources/81#6"
    },
    {
      "body_html": "<p><a href=\"/u/fbastian\" class=\"username\">@fbastian</a>, <code>GTEx_Data_V4_Annotations_SampleAttributesDS.txt</code> is available from the GTEx <a href=\"http://www.gtexportal.org/home/datasets2\">download page</a> which requires an account. However, <a href=\"http://www.gtexportal.org/static/datasets/gtex_analysis_v4/annotations/GTEx_Data_V4_Annotations_SampleAttributesDS.txt\">this direct link</a> circumvents the login page. How long has it been since you submitted the data access request?</p>",
      "body_md": "@fbastian, `GTEx_Data_V4_Annotations_SampleAttributesDS.txt` is available from the GTEx [download page](http://www.gtexportal.org/home/datasets2) which requires an account. However, [this direct link](http://www.gtexportal.org/static/datasets/gtex_analysis_v4/annotations/GTEx_Data_V4_Annotations_SampleAttributesDS.txt) circumvents the login page. How long has it been since you submitted the data access request?",
      "profile": 17,
      "published": "2015-06-20T03:14:11.820349Z",
      "thread": 82,
      "url": "/discussion/processing-gtex-for-tissue-specific-gene-expression/82#8"
    },
    {
      "body_html": "<p>Not very long, a week or so.</p>\r\n\r\n<p>Just to anticipate, do they provide more detailed information somewhere else, like, e.g., <a href=\"http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSM81022\">http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSM81022</a> ? (extraction protocols, detailed information about the anatomical structure, etc)</p>",
      "body_md": "Not very long, a week or so.\r\n\r\nJust to anticipate, do they provide more detailed information somewhere else, like, e.g., http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSM81022 ? (extraction protocols, detailed information about the anatomical structure, etc)",
      "profile": 111,
      "published": "2015-06-20T11:51:59.295307Z",
      "thread": 82,
      "url": "/discussion/processing-gtex-for-tissue-specific-gene-expression/82#9"
    },
    {
      "body_html": "<p>(on a side note, I didn't know IPython/Jupyter, I discovered it thanks to you and I fell in love with it, this is amazing; also, it's great to have other people to investigate our data, and check their quality)</p>\r\n\r\n<blockquote><p>I was surprised because I thought adult would subsume 25-44 and the expression data would be propagated</p></blockquote>\r\n\r\n<p>So, we propagate the data, but in the simple file we only display conditions that were actually observed in experimental data (no conditions appearing from propagation only). It means that we never had an experiment studying \"heart\" at stage \"human adult stage\". Which makes sense, because annotating experimental data with such a broad term basically mean \"we know it was a heart from an adult, but no idea which age\"; we often have more detailed information.</p>\r\n\r\n<p>This is why the complete file would work better for you, as we also display conditions from \"propagation only\"; you should see lots of data for \"heart\" at \"human adult stage\". Of note, even in the complete file you get the \"global\" call generated by us, see column 7 and 8: <a href=\"http://bgee.org/?page=doc&amp;action=call_files#single_expr_complete\">http://bgee.org/?page=doc&amp;action=call_files#single_expr_complete</a><br>So you wouldn't have to do the decision-making, even when using the complete file.</p>\r\n\r\n<p>But your problem is interesting, maybe we should \"relax\" the filtering of conditions in the simple file, I will think about it.<br>This difference in filtering was explained in the documentation, should we clarify it?</p>\r\n\r\n<blockquote><p>Some tissues or cell types, such as leukocytes and testis, have over 10,000 under/over-expressed genes (almost every gene). Is this biologically meaningful or the result of data coverage or other experimental artifacts?</p></blockquote>\r\n\r\n<p>I think it is meaningful. First, it is not almost every gene, from the complete file (where you can also find genes never shown to have differential expression) you can see that we have about 20,000 genes tested for differential expression; so, in the majority of tissues you get a reasonable percentage of genes differentially expressed. Second, I'm not surprised by the outlier tissues you found, for instance we know that testis and brain have very specific expression. Third, you almost always found more genes under-expressed than over-expressed, and this is actually an interesting pattern, that we have observed by other methods.</p>\r\n\r\n<p>I will discuss this with my colleagues to be sure they get to the same conclusion, and will get back to you. But I think everything looks good. </p>",
      "body_md": "(on a side note, I didn't know IPython/Jupyter, I discovered it thanks to you and I fell in love with it, this is amazing; also, it's great to have other people to investigate our data, and check their quality)\r\n\r\n> I was surprised because I thought adult would subsume 25-44 and the expression data would be propagated\r\n\r\nSo, we propagate the data, but in the simple file we only display conditions that were actually observed in experimental data (no conditions appearing from propagation only). It means that we never had an experiment studying \"heart\" at stage \"human adult stage\". Which makes sense, because annotating experimental data with such a broad term basically mean \"we know it was a heart from an adult, but no idea which age\"; we often have more detailed information.\r\n\r\nThis is why the complete file would work better for you, as we also display conditions from \"propagation only\"; you should see lots of data for \"heart\" at \"human adult stage\". Of note, even in the complete file you get the \"global\" call generated by us, see column 7 and 8: http://bgee.org/?page=doc&action=call_files#single_expr_complete\r\nSo you wouldn't have to do the decision-making, even when using the complete file.\r\n\r\nBut your problem is interesting, maybe we should \"relax\" the filtering of conditions in the simple file, I will think about it.\r\nThis difference in filtering was explained in the documentation, should we clarify it?\r\n\r\n> Some tissues or cell types, such as leukocytes and testis, have over 10,000 under/over-expressed genes (almost every gene). Is this biologically meaningful or the result of data coverage or other experimental artifacts?\r\n\r\nI think it is meaningful. First, it is not almost every gene, from the complete file (where you can also find genes never shown to have differential expression) you can see that we have about 20,000 genes tested for differential expression; so, in the majority of tissues you get a reasonable percentage of genes differentially expressed. Second, I'm not surprised by the outlier tissues you found, for instance we know that testis and brain have very specific expression. Third, you almost always found more genes under-expressed than over-expressed, and this is actually an interesting pattern, that we have observed by other methods.\r\n\r\nI will discuss this with my colleagues to be sure they get to the same conclusion, and will get back to you. But I think everything looks good. ",
      "profile": 111,
      "published": "2015-06-20T12:36:57.943554Z",
      "thread": 81,
      "url": "/discussion/tissue-specific-gene-expression-resources/81#7"
    },
    {
      "body_html": "<h1>Completed Bgee analysis — version 0</h1>\r\n\r\n<h2>Tissue-specific gene presence</h2>\r\n\r\n<blockquote><p>we propagate the data, but in the simple file we only display conditions that were actually observed in experimental data (no conditions appearing from propagation only)</p></blockquote>\r\n\r\n<p>Okay I switched to the complete file for presence/absence. The updated <a href=\"http://nbviewer.ipython.org/github/dhimmel/bgee/blob/bd50da6c931675a0316a71ab5c6d7d1bbd35f8bd/bgee.ipynb#Figure-1:-Number-of-genes-present-by-developmental-stage-and-anatomical-entity\">Figure 1</a> makes much more sense now. We chose to go with \"human adult stage\" (<code>HsapDv:0000087</code>) as the developmental stage, which now has broad coverage across tissues, however with fewer present genes than \"life cycle\" as expected. We converted to entrez genes and ended up with a matrix of 18,997 genes (16,278 coding) × 666 anatomical entities (<a href=\"https://github.com/dhimmel/bgee/blob/bd50da6c931675a0316a71ab5c6d7d1bbd35f8bd/data/present-in-adult.tsv.gz\">download</a>). On average, 41.1% of genes were expressed in a given anatomical entity.</p>\r\n\r\n<blockquote><p>But your problem is interesting, maybe we should \"relax\" the filtering of conditions in the simple file, I will think about it. This difference in filtering was explained in the documentation, should we clarify it?</p></blockquote>\r\n\r\n<p>I misinterpreted the <a href=\"http://bgee.unil.ch/?page=doc&amp;action=call_files#single_expr\">documentation</a> by assuming that if a developmental stage had any annotations, propagated values would be provided for all anatomies of that stage. Instead, propagated values are only included for stage–anatomy combinations with annotations. Perhaps the documentation should make it more clear that many use cases will require the complete file due to this issue. Another consideration is that processing the complete file was consuming up to 80 GB of RAM at points — not an issue <em>personally</em> but could be for others.</p>\r\n\r\n<h2>Tissue-specific differential expression</h2>\r\n\r\n<p>We converted the differential expression to entrez genes and ended up with a matrix of 18,620 genes (16,184 coding) × 98 anatomical entities (<a href=\"http://nbviewer.ipython.org/github/dhimmel/bgee/blob/bd50da6c931675a0316a71ab5c6d7d1bbd35f8bd/bgee.ipynb#Read-and-process-differential-expression-data\">notebook</a>, <a href=\"https://github.com/dhimmel/bgee/blob/bd50da6c931675a0316a71ab5c6d7d1bbd35f8bd/data/diffex.tsv.gz\">download</a>).</p>\r\n\r\n<h1>General Bgee feedback</h1>\r\n\r\n<p><a href=\"/u/fbastian\" class=\"username\">@fbastian</a>, in the presence download, for a given gene–stage–anatomy combination is there at most one row? For the anatomy-based differential expression download, for a given gene–anatomy combination is there at most one row? Better documentation of what uniquely defines an observation (row) would be helpful.</p>\r\n\r\n<p>Since the downloaded zip files only contain a single file, I think gzip compression makes more sense.</p>\r\n\r\n<p>Finally, I prefer lowercase column names with underscores rather than spaces. This naming convention avoids frustrating <a href=\"https://github.com/hadley/readr/blob/efd422504762d703bc8c67a40c36e52466e275b3/README.md#output\">R munging</a> and enables unquoted variable reference in R and python. Understandably, you may not want to change for compatibility issues.</p>",
      "body_md": "# Completed Bgee analysis -- version 0\r\n\r\n## Tissue-specific gene presence\r\n\r\n> we propagate the data, but in the simple file we only display conditions that were actually observed in experimental data (no conditions appearing from propagation only)\r\n\r\nOkay I switched to the complete file for presence/absence. The updated [Figure 1](http://nbviewer.ipython.org/github/dhimmel/bgee/blob/bd50da6c931675a0316a71ab5c6d7d1bbd35f8bd/bgee.ipynb#Figure-1:-Number-of-genes-present-by-developmental-stage-and-anatomical-entity) makes much more sense now. We chose to go with \"human adult stage\" (`HsapDv:0000087`) as the developmental stage, which now has broad coverage across tissues, however with fewer present genes than \"life cycle\" as expected. We converted to entrez genes and ended up with a matrix of 18,997 genes (16,278 coding) × 666 anatomical entities ([download](https://github.com/dhimmel/bgee/blob/bd50da6c931675a0316a71ab5c6d7d1bbd35f8bd/data/present-in-adult.tsv.gz)). On average, 41.1% of genes were expressed in a given anatomical entity.\r\n\r\n> But your problem is interesting, maybe we should \"relax\" the filtering of conditions in the simple file, I will think about it. This difference in filtering was explained in the documentation, should we clarify it?\r\n\r\nI misinterpreted the [documentation](http://bgee.unil.ch/?page=doc&action=call_files#single_expr) by assuming that if a developmental stage had any annotations, propagated values would be provided for all anatomies of that stage. Instead, propagated values are only included for stage--anatomy combinations with annotations. Perhaps the documentation should make it more clear that many use cases will require the complete file due to this issue. Another consideration is that processing the complete file was consuming up to 80 GB of RAM at points -- not an issue *personally* but could be for others.\r\n\r\n## Tissue-specific differential expression\r\n\r\nWe converted the differential expression to entrez genes and ended up with a matrix of 18,620 genes (16,184 coding) × 98 anatomical entities ([notebook](http://nbviewer.ipython.org/github/dhimmel/bgee/blob/bd50da6c931675a0316a71ab5c6d7d1bbd35f8bd/bgee.ipynb#Read-and-process-differential-expression-data), [download](https://github.com/dhimmel/bgee/blob/bd50da6c931675a0316a71ab5c6d7d1bbd35f8bd/data/diffex.tsv.gz)).\r\n\r\n# General Bgee feedback\r\n\r\n@fbastian, in the presence download, for a given gene--stage--anatomy combination is there at most one row? For the anatomy-based differential expression download, for a given gene--anatomy combination is there at most one row? Better documentation of what uniquely defines an observation (row) would be helpful.\r\n\r\nSince the downloaded zip files only contain a single file, I think gzip compression makes more sense.\r\n\r\nFinally, I prefer lowercase column names with underscores rather than spaces. This naming convention avoids frustrating [R munging](https://github.com/hadley/readr/blob/efd422504762d703bc8c67a40c36e52466e275b3/README.md#output) and enables unquoted variable reference in R and python. Understandably, you may not want to change for compatibility issues.",
      "profile": 17,
      "published": "2015-06-20T20:46:05.585228Z",
      "thread": 81,
      "url": "/discussion/tissue-specific-gene-expression-resources/81#8"
    },
    {
      "body_html": "<blockquote><p>do they provide more detailed information somewhere else</p></blockquote>\r\n\r\n<p>No idea, I did email the GTEx support with a link to this thread, so perhaps they'll provide some clarification.</p>",
      "body_md": "> do they provide more detailed information somewhere else\r\n\r\nNo idea, I did email the GTEx support with a link to this thread, so perhaps they'll provide some clarification.",
      "profile": 17,
      "published": "2015-06-20T22:26:16.860996Z",
      "thread": 82,
      "url": "/discussion/processing-gtex-for-tissue-specific-gene-expression/82#10"
    },
    {
      "body_html": "<blockquote><p>formally speaking 'anatomical structure' would exclude non-material entities such as lumens, 2D boundaries</p></blockquote>\r\n\r\n<p>Nodes that don't have expression may still be connected via a disease localization edge. These edges are currently <a href=\"http://thinklab.com/discussion/mining-knowledge-from-medline-articles-and-their-indexed-mesh-terms/67#229\">created via MEDLINE cooccurrence</a> and can connect any uberon or CL terms that map to MeSH.</p>\r\n\r\n<p>I am leaning towards calling the metanode <em>Anatomy</em>. We would end up with metapaths (path types) like: <strong>C</strong>ompound–<strong>t</strong>arget–<strong>G</strong>ene–<strong>e</strong>xpression–<strong>A</strong>natomy–<strong>l</strong>ocalization–<strong>D</strong>isease (abbreviated as <em>CtGeAlD</em>). This metapath refers to when a compound targets a gene that is expressed in an anatomy/tissue/cell-type where the disease is localized. Does that seem like a misuse of the word Anatomy?</p>",
      "body_md": "> formally speaking 'anatomical structure' would exclude non-material entities such as lumens, 2D boundaries\r\n\r\nNodes that don't have expression may still be connected via a disease localization edge. These edges are currently [created via MEDLINE cooccurrence](http://thinklab.com/discussion/mining-knowledge-from-medline-articles-and-their-indexed-mesh-terms/67#229) and can connect any uberon or CL terms that map to MeSH.\r\n\r\nI am leaning towards calling the metanode *Anatomy*. We would end up with metapaths (path types) like: **C**ompound--**t**arget--**G**ene--**e**xpression--**A**natomy--**l**ocalization--**D**isease (abbreviated as *CtGeAlD*). This metapath refers to when a compound targets a gene that is expressed in an anatomy/tissue/cell-type where the disease is localized. Does that seem like a misuse of the word Anatomy?",
      "profile": 17,
      "published": "2015-06-20T23:02:20.747870Z",
      "thread": 41,
      "url": "/discussion/tissue-node/41#15"
    },
    {
      "body_html": "<p>FYI, our curator Anne Niknejad has started editing your mapping, she will also create the issues on the Uberon tracker to request new terms.</p>",
      "body_md": "FYI, our curator Anne Niknejad has started editing your mapping, she will also create the issues on the Uberon tracker to request new terms.",
      "profile": 111,
      "published": "2015-06-22T11:50:04.721482Z",
      "thread": 82,
      "url": "/discussion/processing-gtex-for-tissue-specific-gene-expression/82#11"
    },
    {
      "body_html": "<blockquote><p>Some tissues or cell types, such as leukocytes and testis, have over 10,000 under/over-expressed genes (almost every gene). Is this biologically meaningful or the result of data coverage or other experimental artifacts?</p></blockquote>\r\n\r\n<p>I discussed this with my colleagues as promised, they agree that everything looks fine. We just get to the conclusion that maybe we could use a FDR correction over all analyses (currently, p-values are corrected on a \"per analysis\" basis), to decrease the number of differentially expressed genes in the most studied organs. </p>\r\n\r\n<blockquote><p>matrix of 18,997 genes (16,278 coding) × 666 anatomical entities</p></blockquote>\r\n\r\n<p>Great, I just want to warn you that lots of these structures are not independent (e.g., \"cerebellum\" is not independent from \"brain\"). I don't know what type of analyses you plan, but this can sometimes be problematic. If you need it, I can provide you a script accepting a list of organs as argument, and returning only the most-precise \"independent\" organs.</p>\r\n\r\n<blockquote><p>in the presence download, for a given gene–stage–anatomy combination is there at most one row</p></blockquote>\r\n\r\n<p>Yes.</p>\r\n\r\n<blockquote><p>For the anatomy-based differential expression download, for a given gene–anatomy combination is there at most one row?</p></blockquote>\r\n\r\n<p>No, because an analysis could compare organ A, B, C at stage embryo, another one compare the organs A, B, C at stage adult. So, for a given gene, you would have two entries for organ A: one at stage embryo, another one at stage adult.<br>So, it is as for presence download, it is at most one row for a given gene–stage–anatomy combination.</p>\r\n\r\n<blockquote><p>I think gzip compression makes more sense.</p></blockquote>\r\n\r\n<p>Not for Windows users (yes, it exists :p)</p>\r\n\r\n<p>Thank you for your feedback, we will take it into account, it is much appreciated. Notably we will remove the spaces in header, will update the documentation, and will change the filtering in simple files.</p>",
      "body_md": "> Some tissues or cell types, such as leukocytes and testis, have over 10,000 under/over-expressed genes (almost every gene). Is this biologically meaningful or the result of data coverage or other experimental artifacts?\r\n\r\nI discussed this with my colleagues as promised, they agree that everything looks fine. We just get to the conclusion that maybe we could use a FDR correction over all analyses (currently, p-values are corrected on a \"per analysis\" basis), to decrease the number of differentially expressed genes in the most studied organs. \r\n\r\n> matrix of 18,997 genes (16,278 coding) × 666 anatomical entities\r\n\r\nGreat, I just want to warn you that lots of these structures are not independent (e.g., \"cerebellum\" is not independent from \"brain\"). I don't know what type of analyses you plan, but this can sometimes be problematic. If you need it, I can provide you a script accepting a list of organs as argument, and returning only the most-precise \"independent\" organs.\r\n\r\n> in the presence download, for a given gene–stage–anatomy combination is there at most one row\r\n\r\nYes.\r\n\r\n> For the anatomy-based differential expression download, for a given gene–anatomy combination is there at most one row?\r\n\r\nNo, because an analysis could compare organ A, B, C at stage embryo, another one compare the organs A, B, C at stage adult. So, for a given gene, you would have two entries for organ A: one at stage embryo, another one at stage adult.\r\nSo, it is as for presence download, it is at most one row for a given gene–stage–anatomy combination.\r\n\r\n> I think gzip compression makes more sense.\r\n\r\nNot for Windows users (yes, it exists :p)\r\n\r\nThank you for your feedback, we will take it into account, it is much appreciated. Notably we will remove the spaces in header, will update the documentation, and will change the filtering in simple files.",
      "profile": 111,
      "published": "2015-06-22T15:16:41.680643Z",
      "thread": 81,
      "url": "/discussion/tissue-specific-gene-expression-resources/81#9"
    },
    {
      "body_html": "<h1>1000 Genomes Phase 3 data</h1>\r\n\r\n<p>We have <a href=\"http://www.1000genomes.org/announcements/phase-3-variant-set-additional-allele-frequencies-functional-annotation-and-other-data\">become aware</a> that more recent and comprehensive 1000 Genomes data exists, it's just not included in SNAP. The phase 3 dataset contains ~2500 individuals with whole-genome sequencing.</p>\r\n\r\n<p>The phase 3 data was <a href=\"http://www.ensembl.info/blog/2015/06/18/1000-genomes-phase-3-frequencies-genotypes-and-ld-data/#comments\">recently added</a> to the Ensembl database. Ensembl has a <a href=\"http://uswest.ensembl.org/info/docs/api/variation/true\">perl API</a>, which should be able to find all SNPs in LD with a lead SNP.</p>\r\n\r\n<p>We found <a href=\"https://www.biostars.org/p/109785/#110102\">example code</a> and have <a href=\"https://www.biostars.org/p/109785/#147784\">reached out</a> for advice because our implementation is <a href=\"http://nbviewer.ipython.org/github/dhimmel/ensembl-api/blob/5ee80e036a8dd4e5416c2af6ddd5ae7a1a9c5a44/linkage.ipynb\">currently failing</a>.</p>",
      "body_md": "# 1000 Genomes Phase 3 data\r\n\r\nWe have [become aware](http://www.1000genomes.org/announcements/phase-3-variant-set-additional-allele-frequencies-functional-annotation-and-other-data) that more recent and comprehensive 1000 Genomes data exists, it's just not included in SNAP. The phase 3 dataset contains ~2500 individuals with whole-genome sequencing.\r\n\r\nThe phase 3 data was [recently added](http://www.ensembl.info/blog/2015/06/18/1000-genomes-phase-3-frequencies-genotypes-and-ld-data/#comments) to the Ensembl database. Ensembl has a [perl API](http://uswest.ensembl.org/info/docs/api/variation/true), which should be able to find all SNPs in LD with a lead SNP.\r\n\r\nWe found [example code](https://www.biostars.org/p/109785/#110102) and have [reached out](https://www.biostars.org/p/109785/#147784) for advice because our implementation is [currently failing](http://nbviewer.ipython.org/github/dhimmel/ensembl-api/blob/5ee80e036a8dd4e5416c2af6ddd5ae7a1a9c5a44/linkage.ipynb).",
      "profile": 17,
      "published": "2015-06-23T18:26:55.244063Z",
      "thread": 71,
      "url": "/discussion/calculating-genomic-windows-for-gwas-lead-snps/71#6"
    },
    {
      "body_html": "<blockquote><p>we could use a FDR correction over all analyses</p></blockquote>\r\n\r\n<p>I support the correction for multiple testing.</p>\r\n\r\n<blockquote><p>So, it is as for presence download, it is at most one row for a given gene–stage–anatomy combination.</p></blockquote>\r\n\r\n<p>I thought all rows in the \"Over-/Under-expression across anatomy\" download were for \"post-juvenile adult stage\", so row uniqueness depends only on gene and anatomy?</p>\r\n\r\n<blockquote><p>lots of these structures are not independent (e.g., \"cerebellum\" is not independent from \"brain\"). I don't know what type of analyses you plan, but this can sometimes be problematic.</p></blockquote>\r\n\r\n<p>We enforce uniqueness for the metanodes where we are predicting edges (<a href=\"http://thinklab.com/discussion/unifying-drug-vocabularies/40#192\">compounds</a> and <a href=\"http://thinklab.com/discussion/unifying-disease-vocabularies/44#144\">diseases</a>). We are not planning on eradicating term overlap for other metanodes such as <a href=\"http://thinklab.com/discussion/compiling-gene-ontology-annotations-into-an-easy-to-use-format/39\">GO Domains</a>, <a href=\"http://thinklab.com/discussion/tissue-node/41#286\">Anatomy</a>, and <a href=\"http://thinklab.com/discussion/mining-knowledge-from-medline-articles-and-their-indexed-mesh-terms/67#222\">Symptom</a>. The consequence of this duplicity is unknown and something that <a href=\"http://het.io/hnep/\">HNEP</a> researchers should eventually confront.</p>\r\n\r\n<p>Our method downweights paths through high-degree nodes, which reduces the impact of highly-redundant supernodes such as \"anatomical entity\", \"anatomical structure\", and \"anatomical system\". However, our implementation has not been optimized and may get bogged down by all these expression edges. Therefore we may consider removing anatomies that are too broad to be meaningful. We could also consider pruning for independence.</p>\r\n\r\n<blockquote><p>If you need it, I can provide you a script accepting a list of organs as argument, and returning only the most-precise \"independent\" organs.</p></blockquote>\r\n\r\n<p>Yes, I don't <em>need</em> it but <em>want</em> to see the script out of interest. Primarily, I am curious about how your algorithm, since I often encounter these problems.</p>",
      "body_md": "> we could use a FDR correction over all analyses\r\n\r\nI support the correction for multiple testing.\r\n\r\n> So, it is as for presence download, it is at most one row for a given gene–stage–anatomy combination.\r\n\r\nI thought all rows in the \"Over-/Under-expression across anatomy\" download were for \"post-juvenile adult stage\", so row uniqueness depends only on gene and anatomy?\r\n\r\n> lots of these structures are not independent (e.g., \"cerebellum\" is not independent from \"brain\"). I don't know what type of analyses you plan, but this can sometimes be problematic.\r\n\r\nWe enforce uniqueness for the metanodes where we are predicting edges ([compounds](http://thinklab.com/discussion/unifying-drug-vocabularies/40#192) and [diseases](http://thinklab.com/discussion/unifying-disease-vocabularies/44#144)). We are not planning on eradicating term overlap for other metanodes such as [GO Domains](http://thinklab.com/discussion/compiling-gene-ontology-annotations-into-an-easy-to-use-format/39), [Anatomy](http://thinklab.com/discussion/tissue-node/41#286), and [Symptom](http://thinklab.com/discussion/mining-knowledge-from-medline-articles-and-their-indexed-mesh-terms/67#222). The consequence of this duplicity is unknown and something that [HNEP](http://het.io/hnep/) researchers should eventually confront.\r\n\r\nOur method downweights paths through high-degree nodes, which reduces the impact of highly-redundant supernodes such as \"anatomical entity\", \"anatomical structure\", and \"anatomical system\". However, our implementation has not been optimized and may get bogged down by all these expression edges. Therefore we may consider removing anatomies that are too broad to be meaningful. We could also consider pruning for independence.\r\n\r\n> If you need it, I can provide you a script accepting a list of organs as argument, and returning only the most-precise \"independent\" organs.\r\n\r\nYes, I don't *need* it but *want* to see the script out of interest. Primarily, I am curious about how your algorithm, since I often encounter these problems.",
      "profile": 17,
      "published": "2015-06-23T19:29:01.737789Z",
      "thread": 81,
      "url": "/discussion/tissue-specific-gene-expression-resources/81#10"
    },
    {
      "body_html": "<p>Python is our first-line language because it is powerful, elegant, and widely adopted. In combination with <a href=\"https://jupyter.org/\">Jupyter notebooks</a>, python is a data science jackhammer, while also being general purpose.</p>\r\n\r\n<p>Python 3 was released in 2008 and contains small incompatibilities with Python 2. 3 is <a href=\"https://youtu.be/f_6vDi7ywuA\">superior</a> to 2. Many training resources and codebases are still in 2, but new users should begin with 3.</p>\r\n\r\n<h2>Installation</h2>\r\n\r\n<p>We will use Anaconda for package management. Anaconda makes installing packages easier and includes most important ones by default. It also supports environments — distinct and independent installations — which allow specific installations for specific purposes. Anaconda creates a default environment (root) that becomes your primary python distribution. Here, we create a root python 3 environment and an elective python 2 environment that can be activated when needed. </p>\r\n\r\n<p>Download <a href=\"http://continuum.io/downloads\">anaconda for python 3</a>. Avoid the graphical installer which installs unneeded GUI programs. Install according to the defaults. Once installed run the following terminal command for updates:</p>\r\n\r\n<pre><code class=\"no-highlight hljs\">conda update --all</code></pre>\r\n\r\n<p>When needed, additional packages should be installed like <code>conda install seaborn</code>, which installs the <a href=\"https://web.stanford.edu/~mwaskom/software/seaborn/\">seaborn</a> visualization package.</p>\r\n\r\n<p>To install python 2.7, we will create a new environment called <code>py27</code> with the following command:</p>\r\n\r\n<pre><code class=\"no-highlight hljs\">conda create -n py27 python=2.7 anaconda</code></pre>\r\n\r\n<p>Activate the python 2 environment with <code>source activate py27</code> on linux or mac and <code>activate py27</code> on windows. Once activated, run <code>conda update --all</code> to update packages and <code>jupyter kernelspec install-self --user</code> to make the Python 2 kernel available in Jupyter. Then run <code>source deactivate</code> (or <code>deactivate</code> on windows) to return to the default python 3 environment.</p>\r\n\r\n<p>Python 2 should only be used when necessitated by legacy codebases. Otherwise, we use Python 3.</p>\r\n\r\n<p>If you need <a href=\"http://www.rdkit.org/\">rdkit</a> for chemoinformatics, you should follow the installation <a href=\"https://github.com/rdkit/conda-rdkit\">instructions here</a>, which creates a dedicated rdkit environment with InChI support.</p>\r\n\r\n<h2>Usage</h2>\r\n\r\n<p>We recommend using Jupyter notebooks for most analyses. Launch jupyter by running <code>jupyter notebook</code> in the terminal. Dedicated .py files can be edited using the Jupyter text editor or <a href=\"https://atom.io/\">atom</a>.</p>\r\n\r\n<p>Familiarize yourself with the <a href=\"https://www.python.org/dev/peps/pep-0008/\">PEP 0008</a> style guide.</p>\r\n\r\n<h2>Local development</h2>\r\n\r\n<p>When developing a package locally, run <code>pip install -e .</code> from the package's root directory. The package will then be importable from within your conda environment. The <code>-e</code> flag specifies editable mode and makes the package automatically update when you modify the source.</p>\r\n\r\n<h2>Packages</h2>\r\n\r\n<ul><li><p><a href=\"http://pandas.pydata.org/\"><code>pandas</code></a> for dataframes — important functions include <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html\"><code>DataFrame.merge</code></a>, <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.melt.html\"><code>melt</code></a>, <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pivot_table.html\"><code>DataFrame.pivot_table</code></a>, <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\"><code>DataFrame.groupby</code></a>, <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_table.html\"><code>pandas.read_table</code></a>, <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html\"><code>DataFrame.to_csv(path, sep='\\t', index=False)</code></a>, <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.isnull.html\"><code>pandas.isnull</code></a>, and <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.head.html\"><code>DataFrame.head</code></a>. Be forewarned of <a href=\"http://pandas.pydata.org/pandas-docs/stable/gotchas.html#support-for-integer-na\">the horrors</a> of int to float conversion when missing values are present.</p></li><li><p><a href=\"https://web.stanford.edu/~mwaskom/software/seaborn/\"><code>seaborn</code></a> for data visualization.</p></li><li><p><a href=\"http://www.numpy.org/\"><code>numpy</code></a> for arrays and linear algebra.</p></li><li><p><a href=\"http://docs.python-requests.org/en/latest/\"><code>requests</code></a> for http calls.</p></li><li><p><a href=\"http://scikit-learn.org/stable/\"><code>sklearn</code></a> for machine learning and classification.</p></li></ul>\r\n\r\n<p>For example code, <a href=\"https://github.com/search?l=&amp;o=desc&amp;q=user%3Adhimmel+extension%3Aipynb&amp;ref=advsearch&amp;s=indexed&amp;type=Code&amp;utf8=%E2%9C%93\">check out</a> my notebooks.</p>",
      "body_md": "Python is our first-line language because it is powerful, elegant, and widely adopted. In combination with [Jupyter notebooks](https://jupyter.org/), python is a data science jackhammer, while also being general purpose.\r\n\r\nPython 3 was released in 2008 and contains small incompatibilities with Python 2. 3 is [superior](https://youtu.be/f_6vDi7ywuA) to 2. Many training resources and codebases are still in 2, but new users should begin with 3.\r\n\r\n## Installation\r\n\r\nWe will use Anaconda for package management. Anaconda makes installing packages easier and includes most important ones by default. It also supports environments -- distinct and independent installations -- which allow specific installations for specific purposes. Anaconda creates a default environment (root) that becomes your primary python distribution. Here, we create a root python 3 environment and an elective python 2 environment that can be activated when needed. \r\n\r\nDownload [anaconda for python 3](http://continuum.io/downloads). Avoid the graphical installer which installs unneeded GUI programs. Install according to the defaults. Once installed run the following terminal command for updates:\r\n\r\n```sh\r\nconda update --all\r\n```\r\n\r\nWhen needed, additional packages should be installed like `conda install seaborn`, which installs the [seaborn](https://web.stanford.edu/~mwaskom/software/seaborn/) visualization package.\r\n\r\nTo install python 2.7, we will create a new environment called `py27` with the following command:\r\n\r\n```sh\r\nconda create -n py27 python=2.7 anaconda\r\n```\r\n\r\nActivate the python 2 environment with `source activate py27` on linux or mac and `activate py27` on windows. Once activated, run `conda update --all` to update packages and `jupyter kernelspec install-self --user` to make the Python 2 kernel available in Jupyter. Then run `source deactivate` (or `deactivate` on windows) to return to the default python 3 environment.\r\n\r\nPython 2 should only be used when necessitated by legacy codebases. Otherwise, we use Python 3.\r\n\r\nIf you need [rdkit](http://www.rdkit.org/) for chemoinformatics, you should follow the installation [instructions here](https://github.com/rdkit/conda-rdkit), which creates a dedicated rdkit environment with InChI support.\r\n\r\n## Usage\r\n\r\nWe recommend using Jupyter notebooks for most analyses. Launch jupyter by running `jupyter notebook` in the terminal. Dedicated .py files can be edited using the Jupyter text editor or [atom](https://atom.io/).\r\n\r\nFamiliarize yourself with the [PEP 0008](https://www.python.org/dev/peps/pep-0008/) style guide.\r\n\r\n## Local development\r\n\r\nWhen developing a package locally, run `pip install -e .` from the package's root directory. The package will then be importable from within your conda environment. The `-e` flag specifies editable mode and makes the package automatically update when you modify the source.\r\n\r\n## Packages\r\n\r\n+ [`pandas`](http://pandas.pydata.org/) for dataframes -- important functions include [`DataFrame.merge`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html), [`melt`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.melt.html), [`DataFrame.pivot_table`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pivot_table.html), [`DataFrame.groupby`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html), [`pandas.read_table`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_table.html), [`DataFrame.to_csv(path, sep='\\t', index=False)`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html), [`pandas.isnull`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.isnull.html), and [`DataFrame.head`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.head.html). Be forewarned of [the horrors](http://pandas.pydata.org/pandas-docs/stable/gotchas.html#support-for-integer-na) of int to float conversion when missing values are present.\r\n\r\n+ [`seaborn`](https://web.stanford.edu/~mwaskom/software/seaborn/) for data visualization.\r\n\r\n+ [`numpy`](http://www.numpy.org/) for arrays and linear algebra.\r\n\r\n+ [`requests`](http://docs.python-requests.org/en/latest/) for http calls.\r\n\r\n+ [`sklearn`](http://scikit-learn.org/stable/) for machine learning and classification.\r\n\r\nFor example code, [check out](https://github.com/search?l=&o=desc&q=user%3Adhimmel+extension%3Aipynb&ref=advsearch&s=indexed&type=Code&utf8=%E2%9C%93) my notebooks.",
      "profile": 17,
      "published": "2015-06-24T05:08:18.216459Z",
      "thread": 84,
      "url": "/discussion/python-for-the-modern-biodata-scientist/84"
    },
    {
      "body_html": "<p>Thank you. This is very helpful.</p>",
      "body_md": "Thank you. This is very helpful.",
      "profile": 113,
      "published": "2015-06-25T04:17:27.147552Z",
      "thread": 84,
      "url": "/discussion/python-for-the-modern-biodata-scientist/84#2"
    },
    {
      "body_html": "<h1>Initial human pathway collection</h1>\r\n\r\n<p>We have downloaded, parsing, and combined MSigDB and WikiPathways (<a href=\"https://github.com/dhimmel/pathways/blob/5eb835b33629ffed4e22978e98a702e30ebd7076/merge-resources.ipynb\">notebook</a>, <a href=\"https://github.com/dhimmel/pathways/blob/5eb835b33629ffed4e22978e98a702e30ebd7076/data/pathways.tsv\">tsv results</a>). In total we identified, 1,516 human pathways after removing a single duplicated pathway. Most pathways have below 100 genes but some have up to 1,000.</p>\r\n\r\n<h2>WikiPathways Method</h2>\r\n\r\n<p>We extracted pathways from the previously-suggested <a href=\"http://www.pathvisio.org/data/bots/gmt/wikipathways.gmt\">dump file</a>. We removed pathways without any human genes. From a total of 669 wikipathways, 187 were human. <a href=\"/u/alexanderpico\" class=\"username\">@alexanderpico</a>, can you confirm that these are the expected numbers?</p>\r\n\r\n<p>Above I asked:</p>\r\n\r\n<blockquote><p>Additionally, are non-gene entities identified as free text, or are they structured by a standardized vocabulary?</p></blockquote>\r\n\r\n<p>It appears that other entities besides genes are unstandardized in WikiPathway models. Therefore, we chose to not connect pathways to diseases, drugs, and tissues.</p>\r\n\r\n<h2>MSigDB Method</h2>\r\n\r\n<p>We used the <em>C2: CP</em> collection from MSigDB 5.0 which yielded 1,330 pathways. The sources and counts of these pathways are below:</p>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th>MsigDB ID</th><th>Name</th><th>Pathways</th></tr></thead><tbody><tr><td>REACTOME</td><td><a href=\"http://www.reactome.org/\">Reactome</a></td><td>674</td></tr><tr><td>BIOCARTA</td><td><a href=\"http://www.biocarta.com/\">BioCarta</a></td><td>217</td></tr><tr><td>PID</td><td><a href=\"http://pid.nci.nih.gov/\">Pathway Interaction Database</a></td><td>196</td></tr><tr><td>KEGG</td><td><a href=\"http://www.genome.jp/kegg/\">KEGG</a></td><td>186</td></tr><tr><td>ST</td><td><a href=\"http://stke.sciencemag.org/\">Signaling Transduction KE</a></td><td>28</td></tr><tr><td>SA</td><td><a href=\"http://www.sigmaaldrich.com/life-science.html\">SigmaAldrich</a></td><td>10</td></tr><tr><td>NABA</td><td><a href=\"http://matrisomeproject.mit.edu/\">Matrisome</a></td><td>10</td></tr><tr><td>SIG</td><td><a href=\"http://www.signaling-gateway.org/molecule/\">Signaling Gateway</a></td><td>8</td></tr><tr><td>WNT</td><td><a href=\"http://superarray.com/\">SuperArray</a></td><td>1</td></tr></tbody></table>",
      "body_md": "# Initial human pathway collection\r\n\r\nWe have downloaded, parsing, and combined MSigDB and WikiPathways ([notebook](https://github.com/dhimmel/pathways/blob/5eb835b33629ffed4e22978e98a702e30ebd7076/merge-resources.ipynb), [tsv results](https://github.com/dhimmel/pathways/blob/5eb835b33629ffed4e22978e98a702e30ebd7076/data/pathways.tsv)). In total we identified, 1,516 human pathways after removing a single duplicated pathway. Most pathways have below 100 genes but some have up to 1,000.\r\n\r\n## WikiPathways Method\r\n\r\nWe extracted pathways from the previously-suggested [dump file](http://www.pathvisio.org/data/bots/gmt/wikipathways.gmt). We removed pathways without any human genes. From a total of 669 wikipathways, 187 were human. @alexanderpico, can you confirm that these are the expected numbers?\r\n\r\nAbove I asked:\r\n\r\n> Additionally, are non-gene entities identified as free text, or are they structured by a standardized vocabulary?\r\n\r\nIt appears that other entities besides genes are unstandardized in WikiPathway models. Therefore, we chose to not connect pathways to diseases, drugs, and tissues.\r\n\r\n## MSigDB Method\r\n\r\nWe used the *C2: CP* collection from MSigDB 5.0 which yielded 1,330 pathways. The sources and counts of these pathways are below:\r\n\r\n| MsigDB ID | Name | Pathways |\r\n|---------------------|--------------------------------------|----------|\r\n| REACTOME | [Reactome](http://www.reactome.org/) | 674 |\r\n| BIOCARTA | [BioCarta](http://www.biocarta.com/) | 217 |\r\n| PID | [Pathway Interaction Database](http://pid.nci.nih.gov/) | 196 |\r\n| KEGG | [KEGG](http://www.genome.jp/kegg/) | 186 |\r\n| ST | [Signaling Transduction KE](http://stke.sciencemag.org/) | 28 |\r\n| SA | [SigmaAldrich](http://www.sigmaaldrich.com/life-science.html) | 10 |\r\n| NABA | [Matrisome](http://matrisomeproject.mit.edu/) | 10 |\r\n| SIG | [Signaling Gateway](http://www.signaling-gateway.org/molecule/) | 8 |\r\n| WNT | [SuperArray](http://superarray.com/) | 1 |",
      "profile": 17,
      "published": "2015-06-26T21:40:14.598959Z",
      "thread": 72,
      "url": "/discussion/adding-pathway-resources-to-your-network/72#6"
    },
    {
      "body_html": "<blockquote><p>We extracted pathways from the previously-suggested dump file. We removed pathways without any human genes. From a total of 669 wikipathways, 187 were human. <a href=\"/u/alexanderpico\" class=\"username\">@alexanderpico</a>, can you confirm that these are the expected numbers?</p></blockquote>\r\n\r\n<p>Hmm... You are correct that the dump file contains 187 human pathways (just did a browser FIND on the page for 'homo sapiens'), but there are ~293 human pathways in the standard collection. You can access these on the <a href=\"http://wikipathways.org/index.php/Download_Pathways\">bulk download page</a> in multiple formats, including plain text lists of (non-unified) datanode identifiers.  This number is climbing as folks continue to add new content. For example, we have over 300 additional human pathways that are in the works at various stages of completion (or disrepair) that are not included in these bulk downloads.</p>\r\n\r\n<p>Sorry for suggesting the dump file. I thought that would make it easier since the identifiers are unified to Entrez, but it's apparently incomplete. I'm not sure why...</p>",
      "body_md": "> We extracted pathways from the previously-suggested dump file. We removed pathways without any human genes. From a total of 669 wikipathways, 187 were human. @alexanderpico, can you confirm that these are the expected numbers?\r\n\r\nHmm... You are correct that the dump file contains 187 human pathways (just did a browser FIND on the page for 'homo sapiens'), but there are ~293 human pathways in the standard collection. You can access these on the [bulk download page](http://wikipathways.org/index.php/Download_Pathways) in multiple formats, including plain text lists of (non-unified) datanode identifiers.  This number is climbing as folks continue to add new content. For example, we have over 300 additional human pathways that are in the works at various stages of completion (or disrepair) that are not included in these bulk downloads.\r\n\r\nSorry for suggesting the dump file. I thought that would make it easier since the identifiers are unified to Entrez, but it's apparently incomplete. I'm not sure why...\r\n",
      "profile": 104,
      "published": "2015-06-27T17:57:03.429585Z",
      "thread": 72,
      "url": "/discussion/adding-pathway-resources-to-your-network/72#7"
    },
    {
      "body_html": "<p>The WikiPathways team found the error, corrected it and updated the dump file, which now contains 290 human pathways with gene identifiers unified to Entrez. Same location: <a href=\"http://www.pathvisio.org/data/bots/gmt/wikipathways.gmt\">http://www.pathvisio.org/data/bots/gmt/wikipathways.gmt</a></p>",
      "body_md": "The WikiPathways team found the error, corrected it and updated the dump file, which now contains 290 human pathways with gene identifiers unified to Entrez. Same location: http://www.pathvisio.org/data/bots/gmt/wikipathways.gmt\r\n\r\n",
      "profile": 104,
      "published": "2015-06-29T01:34:02.291021Z",
      "thread": 72,
      "url": "/discussion/adding-pathway-resources-to-your-network/72#8"
    },
    {
      "body_html": "<h1>Human pathway collection revision</h1>\r\n\r\n<p>We updated our pathway resource with the updated WikiPathways data (<a href=\"https://github.com/dhimmel/pathways/blob/032036f91a8395eabd0dab2d9d1ee3252ba140f8/merge-resources.ipynb\">notebook</a>, <a href=\"https://github.com/dhimmel/pathways/blob/032036f91a8395eabd0dab2d9d1ee3252ba140f8/data/pathways.tsv\">tsv results</a>). The new total for human pathways is 1,619 with 289 of those from WikiPathways.</p>",
      "body_md": "# Human pathway collection revision\r\n\r\nWe updated our pathway resource with the updated WikiPathways data ([notebook](https://github.com/dhimmel/pathways/blob/032036f91a8395eabd0dab2d9d1ee3252ba140f8/merge-resources.ipynb), [tsv results](https://github.com/dhimmel/pathways/blob/032036f91a8395eabd0dab2d9d1ee3252ba140f8/data/pathways.tsv)). The new total for human pathways is 1,619 with 289 of those from WikiPathways.",
      "profile": 17,
      "published": "2015-06-29T17:19:07.142553Z",
      "thread": 72,
      "url": "/discussion/adding-pathway-resources-to-your-network/72#9"
    },
    {
      "body_html": "<p><a href=\"/u/fbastian\" class=\"username\">@fbastian</a> and Anne Niknejad.</p>\r\n\r\n<p>On June 17th I emailed <code>gtex-help@broadinstitute.org</code> asking for a GTEX–Uberon mapping. Today, Tim Sullivan responded and attached <a href=\"https://github.com/dhimmel/gtex/blob/30096fb519efba939eeb0c5681ba20ad8d43660a/download/GTEx_Uberon_Terms.xlsx\">this mapping file</a>.</p>\r\n\r\n<p>He didn't mention the methodology used, but you may want to crosscheck your work.</p>\r\n\r\n<p>I've reproduced Tim's mapping below for quick reference:</p>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th>Tissue Site Detail</th><th>Uberon Code</th><th>Uberon Term</th></tr></thead><tbody><tr><td>Adipose - Subcutaneous</td><td>0002190</td><td>subcutaneous adipose tissue</td></tr><tr><td>Adipose - Visceral (Omentum)</td><td>0010414</td><td>omental fat pad</td></tr><tr><td>Adrenal Gland</td><td>0002369</td><td>adrenal gland</td></tr><tr><td>Artery - Aorta</td><td>0001496</td><td>ascending aorta</td></tr><tr><td>Artery - Coronary</td><td>0001621</td><td>coronary artery</td></tr><tr><td>Artery - Tibial</td><td>0001323</td><td>tibial nerve</td></tr><tr><td>Artery - Tibial</td><td>0007610</td><td>tibial artery</td></tr><tr><td>Bladder</td><td>0001255</td><td>urinary bladder</td></tr><tr><td>Brain - Amygdala</td><td>0001876</td><td>amygdala</td></tr><tr><td>Brain - Anterior cingulate cortex (BA24)</td><td>0009835</td><td>anterior cingulate cortex</td></tr><tr><td>Brain - Caudate (basal ganglia)</td><td>0001873</td><td>caudate nucleus</td></tr><tr><td>Brain - Cerebellar Hemisphere</td><td>0002037</td><td>cerebellum</td></tr><tr><td>Brain - Cerebellum</td><td>0002037</td><td>cerebellum</td></tr><tr><td>Brain - Cortex</td><td>0001870</td><td>frontal cortex</td></tr><tr><td>Brain - Frontal Cortex (BA9)</td><td>0009834</td><td>dorsolateral prefrontal cortex</td></tr><tr><td>Brain - Hippocampus</td><td>0001954</td><td>Ammon's horn</td></tr><tr><td>Brain - Hypothalamus</td><td>0001898</td><td>hypothalamus</td></tr><tr><td>Brain - Nucleus accumbens (basal ganglia)</td><td>0001882</td><td>nucleus accumbens</td></tr><tr><td>Brain - Putamen (basal ganglia)</td><td>0001874</td><td>putamen</td></tr><tr><td>Brain - Spinal cord (cervical c-1)</td><td>0006469</td><td>first cervical spinal cord segment</td></tr><tr><td>Brain - Substantia nigra</td><td>0002038</td><td>substantia nigra</td></tr><tr><td>Breast - Mammary Tissue</td><td>0008367</td><td>breast epithelium</td></tr><tr><td>Cells - EBV-transformed lymphocytes</td><td>EFO_0000572</td><td>lymphoblast</td></tr><tr><td>Cells - Leukemia cell line (CML)</td><td>EFO_0002067</td><td>K562</td></tr><tr><td>Cells - Transformed fibroblasts</td><td>EFO_0000496</td><td>fibroblast</td></tr><tr><td>Cervix - Ectocervix</td><td>0012249</td><td>ectocervix</td></tr><tr><td>Cervix - Endocervix</td><td>0000458</td><td>endocervix</td></tr><tr><td>Colon - Sigmoid</td><td>0001159</td><td>sigmoid colon</td></tr><tr><td>Colon - Transverse</td><td>0001157</td><td>transverse colon</td></tr><tr><td>Esophagus - Gastroesophageal Junction</td><td>0004550</td><td>gastroesophageal sphincter</td></tr><tr><td>Esophagus - Mucosa</td><td>0006920</td><td>esophagus squamous epithelium</td></tr><tr><td>Esophagus - Muscularis</td><td>0004648</td><td>esophagus muscularis mucosa</td></tr><tr><td>Fallopian Tube</td><td>0003889</td><td>fallopian tube</td></tr><tr><td>Heart - Atrial Appendage</td><td>0006631</td><td>right atrium auricular region</td></tr><tr><td>Heart - Left Ventricle</td><td>0006566</td><td>left ventricle myocardium</td></tr><tr><td>Kidney - Cortex</td><td>0001225</td><td>cortex of kidney</td></tr><tr><td>Liver</td><td>0001114</td><td>right lobe of liver</td></tr><tr><td>Lung</td><td>0008952</td><td>upper lobe of left lung</td></tr><tr><td>Minor Salivary Gland</td><td>0006330</td><td>anterior lingual gland</td></tr><tr><td>Muscle - Skeletal</td><td>0011907</td><td>gastrocnemius medialis</td></tr><tr><td>Nerve - Tibial</td><td>0001323</td><td>tibial nerve</td></tr><tr><td>Nerve - Tibial</td><td>0007610</td><td>tibial artery</td></tr><tr><td>Ovary</td><td>0002119</td><td>left ovary</td></tr><tr><td>Pancreas</td><td>0001150</td><td>body of pancreas</td></tr><tr><td>Pituitary</td><td>0000007</td><td>pituitary gland</td></tr><tr><td>Prostate</td><td>0002367</td><td>prostate gland</td></tr><tr><td>Skin - Not Sun Exposed (Suprapubic)</td><td>0001416</td><td>skin of abdomen</td></tr><tr><td>Skin - Sun Exposed (Lower leg)</td><td>0001511</td><td>skin of leg</td></tr><tr><td>Small Intestine - Terminal Ileum</td><td>0001211</td><td>Peyer's patch</td></tr><tr><td>Spleen</td><td>0002106</td><td>spleen</td></tr><tr><td>Stomach</td><td>0000945</td><td>stomach</td></tr><tr><td>Testis</td><td>0000473</td><td>testis</td></tr><tr><td>Thyroid</td><td>0002046</td><td>thyroid gland</td></tr><tr><td>Uterus</td><td>0000995</td><td>uterus</td></tr><tr><td>Vagina</td><td>0000996</td><td>vagina</td></tr><tr><td>Whole Blood</td><td>0013756</td><td>venous blood</td></tr></tbody></table>",
      "body_md": "@fbastian and Anne Niknejad.\r\n\r\nOn June 17th I emailed `gtex-help@broadinstitute.org` asking for a GTEX--Uberon mapping. Today, Tim Sullivan responded and attached [this mapping file](https://github.com/dhimmel/gtex/blob/30096fb519efba939eeb0c5681ba20ad8d43660a/download/GTEx_Uberon_Terms.xlsx).\r\n\r\nHe didn't mention the methodology used, but you may want to crosscheck your work.\r\n\r\nI've reproduced Tim's mapping below for quick reference:\r\n\r\n| Tissue Site Detail | Uberon Code | Uberon Term |\r\n|-------------------------------------------|-------------|------------------------------------|\r\n| Adipose - Subcutaneous | 0002190 | subcutaneous adipose tissue |\r\n| Adipose - Visceral (Omentum) | 0010414 | omental fat pad |\r\n| Adrenal Gland | 0002369 | adrenal gland |\r\n| Artery - Aorta | 0001496 | ascending aorta |\r\n| Artery - Coronary | 0001621 | coronary artery |\r\n| Artery - Tibial | 0001323 | tibial nerve |\r\n| Artery - Tibial | 0007610 | tibial artery |\r\n| Bladder | 0001255 | urinary bladder |\r\n| Brain - Amygdala | 0001876 | amygdala |\r\n| Brain - Anterior cingulate cortex (BA24) | 0009835 | anterior cingulate cortex |\r\n| Brain - Caudate (basal ganglia) | 0001873 | caudate nucleus |\r\n| Brain - Cerebellar Hemisphere | 0002037 | cerebellum |\r\n| Brain - Cerebellum | 0002037 | cerebellum |\r\n| Brain - Cortex | 0001870 | frontal cortex |\r\n| Brain - Frontal Cortex (BA9) | 0009834 | dorsolateral prefrontal cortex |\r\n| Brain - Hippocampus | 0001954 | Ammon's horn |\r\n| Brain - Hypothalamus | 0001898 | hypothalamus |\r\n| Brain - Nucleus accumbens (basal ganglia) | 0001882 | nucleus accumbens |\r\n| Brain - Putamen (basal ganglia) | 0001874 | putamen |\r\n| Brain - Spinal cord (cervical c-1) | 0006469 | first cervical spinal cord segment |\r\n| Brain - Substantia nigra | 0002038 | substantia nigra |\r\n| Breast - Mammary Tissue | 0008367 | breast epithelium |\r\n| Cells - EBV-transformed lymphocytes | EFO_0000572 | lymphoblast |\r\n| Cells - Leukemia cell line (CML) | EFO_0002067 | K562 |\r\n| Cells - Transformed fibroblasts | EFO_0000496 | fibroblast |\r\n| Cervix - Ectocervix | 0012249 | ectocervix |\r\n| Cervix - Endocervix | 0000458 | endocervix |\r\n| Colon - Sigmoid | 0001159 | sigmoid colon |\r\n| Colon - Transverse | 0001157 | transverse colon |\r\n| Esophagus - Gastroesophageal Junction | 0004550 | gastroesophageal sphincter |\r\n| Esophagus - Mucosa | 0006920 | esophagus squamous epithelium |\r\n| Esophagus - Muscularis | 0004648 | esophagus muscularis mucosa |\r\n| Fallopian Tube | 0003889 | fallopian tube |\r\n| Heart - Atrial Appendage | 0006631 | right atrium auricular region |\r\n| Heart - Left Ventricle | 0006566 | left ventricle myocardium |\r\n| Kidney - Cortex | 0001225 | cortex of kidney |\r\n| Liver | 0001114 | right lobe of liver |\r\n| Lung | 0008952 | upper lobe of left lung |\r\n| Minor Salivary Gland | 0006330 | anterior lingual gland |\r\n| Muscle - Skeletal | 0011907 | gastrocnemius medialis |\r\n| Nerve - Tibial | 0001323 | tibial nerve |\r\n| Nerve - Tibial | 0007610 | tibial artery |\r\n| Ovary | 0002119 | left ovary |\r\n| Pancreas | 0001150 | body of pancreas |\r\n| Pituitary | 0000007 | pituitary gland |\r\n| Prostate | 0002367 | prostate gland |\r\n| Skin - Not Sun Exposed (Suprapubic) | 0001416 | skin of abdomen |\r\n| Skin - Sun Exposed (Lower leg) | 0001511 | skin of leg |\r\n| Small Intestine - Terminal Ileum | 0001211 | Peyer's patch |\r\n| Spleen | 0002106 | spleen |\r\n| Stomach | 0000945 | stomach |\r\n| Testis | 0000473 | testis |\r\n| Thyroid | 0002046 | thyroid gland |\r\n| Uterus | 0000995 | uterus |\r\n| Vagina | 0000996 | vagina |\r\n| Whole Blood | 0013756 | venous blood |",
      "profile": 17,
      "published": "2015-06-30T19:15:05.994937Z",
      "thread": 82,
      "url": "/discussion/processing-gtex-for-tissue-specific-gene-expression/82#12"
    },
    {
      "body_html": "<p>This is very helpful and something I can use to improve my python workflow. </p>",
      "body_md": "This is very helpful and something I can use to improve my python workflow. ",
      "profile": 35,
      "published": "2015-06-30T20:00:13.809016Z",
      "thread": 84,
      "url": "/discussion/python-for-the-modern-biodata-scientist/84#3"
    },
    {
      "body_html": "<p>Some seem slightly more specific than the label suggests - sometimes the increased specificity is trivial (ie their ovary sample was from a left ovary), sometimes relevant (their representative skeletal muscle sample was from gastrocnemius medialis, the esophagus mucosa sample was taken from the epithelium rather than lamina propria).</p>",
      "body_md": "Some seem slightly more specific than the label suggests - sometimes the increased specificity is trivial (ie their ovary sample was from a left ovary), sometimes relevant (their representative skeletal muscle sample was from gastrocnemius medialis, the esophagus mucosa sample was taken from the epithelium rather than lamina propria).",
      "profile": 109,
      "published": "2015-07-01T14:32:41.223751Z",
      "thread": 82,
      "url": "/discussion/processing-gtex-for-tissue-specific-gene-expression/82#13"
    },
    {
      "body_html": "<p>We would like to create a catalog of interactions between proteins (PPIs). I am currently leaning towards focusing on physical interactions, since other types of interactions will be captured by other metanodes and metaedges. If some non-physical interactions are included that is acceptable but not the goal.</p>\r\n\r\n<p>Some previous studies have compiled PPI catalogs:</p>\r\n\r\n<ul><li><p>the Incomplete Interactome (II) <span class=\"citation\">[<a href=\"/doi/10.1126/science.1257601\" class=\"citation\" data-key=\"10.1126/science.1257601\">1</a>]</span> — compiled protein interactions of seven types</p></li><li><p>the Human Interaction Database (<a href=\"http://interactome.dfci.harvard.edu/H_sapiens/index.php?page=download\">HID</a>) <span class=\"citation\">[<a href=\"/doi/10.1016/j.cell.2014.10.050\" class=\"citation\" data-key=\"10.1016/j.cell.2014.10.050\">2</a>]</span> — systematic experimental approach for identifying PPIs</p></li><li><p>our disease-gene association study (<a href=\"http://het.io/disease-genes/downloads/\">hetio</a>) <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1004259\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1004259\">3</a>]</span> — interactions from <a href=\"http://irefindex.org/wiki/index.php?title=iRefIndex\">iRefIndex</a>, which compiles records from primary databases, processed using <a href=\"http://www.ncbi.nlm.nih.gov/CBBresearch/Yu/downloads/ppiTrim.html\">ppiTrim</a> <span class=\"citation\">[<a href=\"/doi/10.1093/database/bar036\" class=\"citation\" data-key=\"10.1093/database/bar036\">4</a>]</span>.</p></li></ul>\r\n\r\n<p>Suggestions for other resources are welcome.</p>",
      "body_md": "We would like to create a catalog of interactions between proteins (PPIs). I am currently leaning towards focusing on physical interactions, since other types of interactions will be captured by other metanodes and metaedges. If some non-physical interactions are included that is acceptable but not the goal.\r\n\r\nSome previous studies have compiled PPI catalogs:\r\n\r\n+ the Incomplete Interactome (II) [@10.1126/science.1257601] --- compiled protein interactions of seven types\r\n\r\n+ the Human Interaction Database ([HID](http://interactome.dfci.harvard.edu/H_sapiens/index.php?page=download)) [@10.1016/j.cell.2014.10.050] --- systematic experimental approach for identifying PPIs\r\n\r\n+ our disease-gene association study ([hetio](http://het.io/disease-genes/downloads/)) [@10.1371/journal.pcbi.1004259] -- interactions from [iRefIndex](http://irefindex.org/wiki/index.php?title=iRefIndex), which compiles records from primary databases, processed using [ppiTrim](http://www.ncbi.nlm.nih.gov/CBBresearch/Yu/downloads/ppiTrim.html) [@10.1093/database/bar036].\r\n\r\nSuggestions for other resources are welcome.",
      "profile": 17,
      "published": "2015-07-02T00:44:39.854416Z",
      "thread": 85,
      "url": "/discussion/creating-a-catalog-of-protein-interactions/85"
    },
    {
      "body_html": "<h1>Methods for the <em>Incomplete Interactome</em> PPI catalog</h1>\r\n\r\n<p><em>Here, we reproduce the methods section from the <a href=\"http://www.sciencemag.org/content/suppl/2015/02/18/347.6224.1257601.DC1/Menche_SM.pdf\">supplement</a> of the Incomplete Interactome publication <span class=\"citation\">[<a href=\"/doi/10.1126/science.1257601\" class=\"citation\" data-key=\"10.1126/science.1257601\">1</a>]</span> describing how their PPI catalog was constructed:</em></p>\r\n\r\n<p>In building the interactome, we rely only physical protein interactions with experimental support, hence we do not include interactions extracted from gene expression data or evolutionary considerations. In order to obtain an interactome as complete as currently feasible, we combine several databases with various kinds of physical interactions:</p>\r\n\r\n<ol><li><strong>Regulatory interactions</strong>: We use the TRANSFAC database <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkg108\" class=\"citation\" data-key=\"10.1093/nar/gkg108\">2</a>]</span> that lists interactions derived from the presence of a transcription factor binding site in the promoter region of a certain gene. The resulting network consists of 271 transcription factors regulating 564 genes via 1,335 interactions.</li><li><strong>Binary interactions</strong>: We combine several yeast-two-hybrid high-throughput datasets <span class=\"citation\">[<a href=\"/doi/10.1016/j.cell.2014.10.050\" class=\"citation\" data-key=\"10.1016/j.cell.2014.10.050\">3</a>, <a href=\"/doi/10.1038/nmeth.1280\" class=\"citation\" data-key=\"10.1038/nmeth.1280\">4</a>, <a href=\"/doi/10.1016/j.cell.2005.08.029\" class=\"citation\" data-key=\"10.1016/j.cell.2005.08.029\">5</a>, <a href=\"/doi/10.1038/nmeth.1597\" class=\"citation\" data-key=\"10.1038/nmeth.1597\">6</a>, <a href=\"/doi/10.1038/nature04209\" class=\"citation\" data-key=\"10.1038/nature04209\">7</a>, <a href=\"\" class=\"citation\" data-key=\"55\">8</a>]</span> with binary interactions from IntAct <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkp878\" class=\"citation\" data-key=\"10.1093/nar/gkp878\">9</a>]</span> and MINT databases <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkp983\" class=\"citation\" data-key=\"10.1093/nar/gkp983\">10</a>]</span>. The sum of these data sources yields 28,653 interactions between 8,120 proteins. Note that IntAct and MINT provide interactions derived from both literature curation and direct submissions. </li><li><strong>Literature curated interactions</strong>: These interactions, typically obtained by low throughput experiments, are manually curated from the literature. We use IntAct <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkp878\" class=\"citation\" data-key=\"10.1093/nar/gkp878\">9</a>]</span>, MINT <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkp983\" class=\"citation\" data-key=\"10.1093/nar/gkp983\">10</a>]</span>, BioGRID <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkq1116\" class=\"citation\" data-key=\"10.1093/nar/gkq1116\">11</a>]</span> and HPRD <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkn892\" class=\"citation\" data-key=\"10.1093/nar/gkn892\">12</a>]</span>, resulting in 88,349 interactions between 11,798 proteins.</li><li><strong>Metabolic enzyme-coupled interactions</strong>: Two enzymes are assumed to be coupled if they share adjacent reactions in the KEGG and BIGG databases. In total, we use 5,325 such metabolic links between 921 enzymes from <span class=\"citation\">[<a href=\"/doi/10.1073/pnas.0802208105\" class=\"citation\" data-key=\"10.1073/pnas.0802208105\">13</a>]</span>.</li><li><strong>Protein complexes</strong>: Protein complexes are single molecular units that integrate multiple gene products. The CORUM database <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkp914\" class=\"citation\" data-key=\"10.1093/nar/gkp914\">14</a>]</span> is a collection of mammalian complexes derived from a variety of experimental tools, from co-immunoprecipitation to co-sedimentation and ion exchange chromatography. In total, CORUM yields 2,837 complexes with 2,069 proteins connected by 31,276 links.</li><li><strong>Kinase network (kinase-substrate pairs)</strong>: Protein kinases are important regulators in different biological processes, such as signal transduction. PhosphositePlus <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkr1122\" class=\"citation\" data-key=\"10.1093/nar/gkr1122\">15</a>]</span> provides a network of peptides that can be bound by kinases, yielding in total 6,066 interactions between 1,843 proteins.</li><li><strong>Signaling interactions</strong>: The dataset from <span class=\"citation\">[<a href=\"/doi/10.1126/scisignal.2001699\" class=\"citation\" data-key=\"10.1126/scisignal.2001699\">16</a>]</span> provides 32,706 interactions between 6,339 proteins that integrate several sources, both high-throughput and literature curation, into a directed network in which cellular signals are transmitted by proteins-protein interactions.</li></ol>\r\n\r\n<p>The union of all interactions obtained from (i)-(vii) yields a network of 13,460 proteins that are interconnected by 141,296 physical interactions. Note that we treat the interactome as an undirected network (see Section 2.3 for a discussion of the impact of directionality). The interactome is approximately scale-free (Figure S1a) and shows other typical characteristics as observed previously in many other biological networks <span class=\"citation\">[<a href=\"/doi/10.1103/RevModPhys.74.47\" class=\"citation\" data-key=\"10.1103/RevModPhys.74.47\">17</a>, <a href=\"/doi/10.1137/S003614450342480\" class=\"citation\" data-key=\"10.1137/S003614450342480\">18</a>]</span>, such as high clustering and short pathlengths (Figure S1c)</p>\r\n\r\n<p></p>",
      "body_md": "# Methods for the *Incomplete Interactome* PPI catalog\r\n\r\n*Here, we reproduce the methods section from the [supplement](http://www.sciencemag.org/content/suppl/2015/02/18/347.6224.1257601.DC1/Menche_SM.pdf) of the Incomplete Interactome publication [@10.1126/science.1257601] describing how their PPI catalog was constructed:*\r\n\r\nIn building the interactome, we rely only physical protein interactions with experimental support, hence we do not include interactions extracted from gene expression data or evolutionary considerations. In order to obtain an interactome as complete as currently feasible, we combine several databases with various kinds of physical interactions:\r\n\r\n1. **Regulatory interactions**: We use the TRANSFAC database [@10.1093/nar/gkg108] that lists interactions derived from the presence of a transcription factor binding site in the promoter region of a certain gene. The resulting network consists of 271 transcription factors regulating 564 genes via 1,335 interactions.\r\n+ **Binary interactions**: We combine several yeast-two-hybrid high-throughput datasets [@10.1016/j.cell.2014.10.050 @10.1038/nmeth.1280 @10.1016/j.cell.2005.08.029 @10.1038/nmeth.1597 @10.1038/nature04209 @55] with binary interactions from IntAct [@10.1093/nar/gkp878] and MINT databases [@10.1093/nar/gkp983]. The sum of these data sources yields 28,653 interactions between 8,120 proteins. Note that IntAct and MINT provide interactions derived from both literature curation and direct submissions. \r\n+ **Literature curated interactions**: These interactions, typically obtained by low throughput experiments, are manually curated from the literature. We use IntAct [@10.1093/nar/gkp878], MINT [@10.1093/nar/gkp983], BioGRID [@10.1093/nar/gkq1116] and HPRD [@10.1093/nar/gkn892], resulting in 88,349 interactions between 11,798 proteins.\r\n+ **Metabolic enzyme-coupled interactions**: Two enzymes are assumed to be coupled if they share adjacent reactions in the KEGG and BIGG databases. In total, we use 5,325 such metabolic links between 921 enzymes from [@10.1073/pnas.0802208105].\r\n+ **Protein complexes**: Protein complexes are single molecular units that integrate multiple gene products. The CORUM database [@10.1093/nar/gkp914] is a collection of mammalian complexes derived from a variety of experimental tools, from co-immunoprecipitation to co-sedimentation and ion exchange chromatography. In total, CORUM yields 2,837 complexes with 2,069 proteins connected by 31,276 links.\r\n+ **Kinase network (kinase-substrate pairs)**: Protein kinases are important regulators in different biological processes, such as signal transduction. PhosphositePlus [@10.1093/nar/gkr1122] provides a network of peptides that can be bound by kinases, yielding in total 6,066 interactions between 1,843 proteins.\r\n+ **Signaling interactions**: The dataset from [@10.1126/scisignal.2001699] provides 32,706 interactions between 6,339 proteins that integrate several sources, both high-throughput and literature curation, into a directed network in which cellular signals are transmitted by proteins-protein interactions.\r\n\r\nThe union of all interactions obtained from (i)-(vii) yields a network of 13,460 proteins that are interconnected by 141,296 physical interactions. Note that we treat the interactome as an undirected network (see Section 2.3 for a discussion of the impact of directionality). The interactome is approximately scale-free (Figure S1a) and shows other typical characteristics as observed previously in many other biological networks [@10.1103/RevModPhys.74.47 @10.1137/S003614450342480], such as high clustering and short pathlengths (Figure S1c)\r\n\r\n[@55]: \"Center for Cancer Systems Biology, Hi-2012 prepublication\"",
      "profile": 17,
      "published": "2015-07-02T00:47:21.811599Z",
      "thread": 85,
      "url": "/discussion/creating-a-catalog-of-protein-interactions/85#2"
    },
    {
      "body_html": "<h1>GWAS associations for all DO diseases</h1>\r\n\r\n<p>We repeated the above analysis for all diseases, not just DO slim diseases. The EFO terms added to the GWAS Catalog by the EBI are still converted to DO terms: therefore, associations whose EFO terms are not cross-referenced in the DO are omitted.</p>\r\n\r\n<p>In total, the dataset contains 1447 high-confidence primary gene-disease associations. Counting both confidence levels, associations exist for 124 diseases and 4142 genes.</p>\r\n\r\n<p>Download the <a href=\"https://github.com/dhimmel/gwas-catalog/blob/a5aa4910708a3995501ebe4136d8b9d601463fa1/data/gene-associations.tsv\">compiled gene associations</a> or see the <a href=\"https://github.com/dhimmel/gwas-catalog/tree/a5aa4910708a3995501ebe4136d8b9d601463fa1\">repository</a> for more information.</p>",
      "body_md": "# GWAS associations for all DO diseases\r\n\r\nWe repeated the above analysis for all diseases, not just DO slim diseases. The EFO terms added to the GWAS Catalog by the EBI are still converted to DO terms: therefore, associations whose EFO terms are not cross-referenced in the DO are omitted.\r\n\r\nIn total, the dataset contains 1447 high-confidence primary gene-disease associations. Counting both confidence levels, associations exist for 124 diseases and 4142 genes.\r\n\r\nDownload the [compiled gene associations](https://github.com/dhimmel/gwas-catalog/blob/a5aa4910708a3995501ebe4136d8b9d601463fa1/data/gene-associations.tsv) or see the [repository](https://github.com/dhimmel/gwas-catalog/tree/a5aa4910708a3995501ebe4136d8b9d601463fa1) for more information.",
      "profile": 17,
      "published": "2015-07-09T20:59:26.433170Z",
      "thread": 80,
      "url": "/discussion/extracting-disease-gene-associations-from-the-gwas-catalog/80#2"
    },
    {
      "body_html": "<p>Last week a seminal paper on tissue-specificity of the transciptome, proteome, knowledgeome, and literome was published <span class=\"citation\">[<a href=\"/doi/10.7717/peerj.1054\" class=\"citation\" data-key=\"10.7717/peerj.1054\">1</a>]</span>, along with an <a href=\"http://tissues.jensenlab.org/\">accompanying webapp</a>. This study is notable for intelligently merging expression studies and performing informative comparisons between studies and data types.</p>\r\n\r\n<p>We have already <a href=\"http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#8\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d81\">processed Bgee</a> for anatomy-specific transcription. Compared to Bgee, the anatomical coverage of TISSUES is much lower, but edges are likely of higher quality. Therefore I think Bgee and TISSUES will be complimentary. Additionally, TISSUES contains other measures of tissue-specific gene expression such as UniProtKB, proteomics, and literature mining. We are particularly interested in including these data sources.</p>\r\n\r\n<p>Our first step is to retrieve the data for each of the four methods. We want entities encoded using identifiers rather than names or symbols. We will need to pick a confidence threshold. And for each method, we would like the broadest tissue coverage permitted by that method. Advice appreciated!</p>",
      "body_md": "Last week a seminal paper on tissue-specificity of the transciptome, proteome, knowledgeome, and literome was published [@10.7717/peerj.1054], along with an [accompanying webapp](http://tissues.jensenlab.org/). This study is notable for intelligently merging expression studies and performing informative comparisons between studies and data types.\r\n\r\nWe have already [processed Bgee](http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#8) for anatomy-specific transcription. Compared to Bgee, the anatomical coverage of TISSUES is much lower, but edges are likely of higher quality. Therefore I think Bgee and TISSUES will be complimentary. Additionally, TISSUES contains other measures of tissue-specific gene expression such as UniProtKB, proteomics, and literature mining. We are particularly interested in including these data sources.\r\n\r\nOur first step is to retrieve the data for each of the four methods. We want entities encoded using identifiers rather than names or symbols. We will need to pick a confidence threshold. And for each method, we would like the broadest tissue coverage permitted by that method. Advice appreciated!",
      "profile": 17,
      "published": "2015-07-10T18:15:03.724833Z",
      "thread": 91,
      "url": "/discussion/the-tissues-resource-for-the-tissue-specificity-of-genes/91"
    },
    {
      "body_html": "<h1>SciPy 2015</h1>\r\n\r\n<p>Some incredible presentations and materials are being unleashed at the <a href=\"http://scipy2015.scipy.org/\">SciPy 2015 conference</a> as I type. The full <a href=\"https://www.youtube.com/playlist?list=PLYx7XA2nY5Gcpabmu61kKcToLz0FapmHu\">playlist of presentations</a> is on YouTube.</p>\r\n\r\n<p>One excellent presentation is <em>State of the Stack</em> (<a href=\"https://youtu.be/5GlNDD7qbP4\">video</a>, <a href=\"https://speakerdeck.com/jakevdp/the-state-of-the-stack-scipy-2015-keynote\">slides</a>) by Jake Vanderplas, which details the latest developments in python tools for data science.</p>\r\n\r\n<p>Some additional projects of interest are:</p>\r\n\r\n<ul><li><a href=\"https://youtu.be/X0pAhJgySxk\">xray</a></li><li><a href=\"https://youtu.be/iMPfLz6kKv8\">beaker notebook</a></li></ul>",
      "body_md": "# SciPy 2015\r\n\r\nSome incredible presentations and materials are being unleashed at the [SciPy 2015 conference](http://scipy2015.scipy.org/) as I type. The full [playlist of presentations](https://www.youtube.com/playlist?list=PLYx7XA2nY5Gcpabmu61kKcToLz0FapmHu) is on YouTube.\r\n\r\nOne excellent presentation is *State of the Stack* ([video](https://youtu.be/5GlNDD7qbP4), [slides](https://speakerdeck.com/jakevdp/the-state-of-the-stack-scipy-2015-keynote)) by Jake Vanderplas, which details the latest developments in python tools for data science.\r\n\r\nSome additional projects of interest are:\r\n\r\n+ [xray](https://youtu.be/X0pAhJgySxk)\r\n+ [beaker notebook](https://youtu.be/iMPfLz6kKv8)",
      "profile": 17,
      "published": "2015-07-10T19:06:19.048687Z",
      "thread": 84,
      "url": "/discussion/python-for-the-modern-biodata-scientist/84#4"
    },
    {
      "body_html": "<h1>Human Interaction Database</h1>\r\n\r\n<p>The Human Interaction Database (HID) 2014 <a href=\"http://interactome.dfci.harvard.edu/H_sapiens/index.php?page=download\">release</a> <span class=\"citation\">[<a href=\"/doi/10.1016/j.cell.2014.10.050\" class=\"citation\" data-key=\"10.1016/j.cell.2014.10.050\">1</a>]</span> contained two PPI datasets:</p>\r\n\r\n<ul><li><code>HI-II-14</code> — 13,944 interactions — proteome-scale map of the human binary interactome network generated by systematically screening Space-II</li><li><code>Lit-BM-13</code> — 11,045 interactions — high-quality recurated literature binary interactions extracted from 7 public repository in 2013</li></ul>\r\n\r\n<p>We <a href=\"https://github.com/dhimmel/ppi/blob/77862798448c4272c55e9c718323a3ec5d8db571/compile-PPIs.ipynb\">compared</a> interactions from HID to interactions from the Incomplete Interactome (II). We found that <code>HI-II-14</code> was a subset of <code>II_binary</code>. However, only 78.2% of <code>Lit-BM-13</code> was included in <code>II_literature</code>.</p>\r\n\r\n<p><em>To better understand <code>Lit-BM-13</code>, we have added the relevant sections of the paper <a href=\"http://www.sciencedirect.com/science/MiamiMultiMediaURL/1-s2.0-S0092867414014226/1-s2.0-S0092867414014226-mmc1.pdf/272196/html/S0092867414014226/2b892ecda8f249667d75023be6d13c7b/mmc1.pdf\">supplement</a> below, omitting the \"assignment of experimental method\" sections <span class=\"citation\">[<a href=\"/doi/10.1016/j.cell.2014.10.050\" class=\"citation\" data-key=\"10.1016/j.cell.2014.10.050\">1</a>]</span>:</em></p>\r\n\r\n<p><strong>Literature datasets:</strong> We generated two datasets from literature-curated protein-protein interactions. A first dataset was generated in 2010 and used for all experiments, concomitantly with our mapping experiment, and a second dataset was extracted in 2013 to provide an updated version for all computational analyses.</p>\r\n\r\n<p><strong>Obtaining the Lit-2010 dataset:</strong> The Lit-2010 dataset extracts human protein-protein interactions (PPIs), annotated through December 2010, from seven primary source databases: BIND <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkg056\" class=\"citation\" data-key=\"10.1093/nar/gkg056\">2</a>]</span>, BioGRID <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gks1158\" class=\"citation\" data-key=\"10.1093/nar/gks1158\">3</a>]</span>, DIP <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkh086\" class=\"citation\" data-key=\"10.1093/nar/gkh086\">4</a>]</span>, HPRD <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkn892\" class=\"citation\" data-key=\"10.1093/nar/gkn892\">5</a>]</span>, MINT <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkr930\" class=\"citation\" data-key=\"10.1093/nar/gkr930\">6</a>]</span>, IntAct <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkr1088\" class=\"citation\" data-key=\"10.1093/nar/gkr1088\">7</a>]</span> and PDB <span class=\"citation\">[<a href=\"/doi/10.1093/nar/28.1.235\" class=\"citation\" data-key=\"10.1093/nar/28.1.235\">8</a>]</span>. For each reported PPI the interacting proteins were mapped to UniProt protein identifiers and then converted to NCBI Entrez gene ID pairs using an ID mapping table downloaded on January 12, 2012 from uniprot.org. Information about the specific publications reporting each interaction was retained and reported interactions that did not have an associated PubMed ID (PMID) were not included in the Lit-2010 dataset.</p>\r\n\r\n<p><strong>Identification of binary interactions:</strong> We divided Lit-2010 into the PPIs reported by<br>systematic high-throughput binary human interactome mapping efforts <span class=\"citation\">[<a href=\"/doi/10.1038/nature04209\" class=\"citation\" data-key=\"10.1038/nature04209\">9</a>, <a href=\"/doi/10.1016/j.cell.2005.08.029\" class=\"citation\" data-key=\"10.1016/j.cell.2005.08.029\">10</a>, <a href=\"/doi/10.1038/nmeth.1280\" class=\"citation\" data-key=\"10.1038/nmeth.1280\">11</a>]</span> and those detected in small- or medium-scale experiments. A small number of PPIs that had been detected in both systematic and other studies could appear in both datasets. Removing the PPIs only seen in systematic studies resulted in a dataset of 56,743 human PPIs.</p>\r\n\r\n<p>Next we attempted to distinguish binary interactions (direct biophysical contact between two proteins) <span class=\"citation\">[<a href=\"/doi/10.1002/pmic.201100598\" class=\"citation\" data-key=\"10.1002/pmic.201100598\">12</a>, <a href=\"/doi/10.1002/pmic.201100563\" class=\"citation\" data-key=\"10.1002/pmic.201100563\">13</a>]</span> from indirect associations (associations between two proteins that are in the same complex, but may or may not directly interact) <span class=\"citation\">[<a href=\"/doi/10.1016/j.sbi.2013.02.008\" class=\"citation\" data-key=\"10.1016/j.sbi.2013.02.008\">14</a>]</span>. We evaluated each experimental interaction detection method in the PSI-MI 2.5 and classified them as binary, that is, primarily detects binary interactions, versus indirect, that is, primarily detects association of proteins within a complex (Table S1C). Where an experimental method could be viewed as either, depending on the specific experimental implementation then the method was conservatively classified as indirect. Fewer methods were classified as binary here than in previous <span class=\"citation\">[<a href=\"/doi/10.1038/nmeth.1284\" class=\"citation\" data-key=\"10.1038/nmeth.1284\">15</a>, <a href=\"/doi/10.1126/science.1158684\" class=\"citation\" data-key=\"10.1126/science.1158684\">16</a>]</span> or parallel <span class=\"citation\">[<a href=\"/doi/10.1186/1752-0509-6-92\" class=\"citation\" data-key=\"10.1186/1752-0509-6-92\">17</a>]</span> efforts to ensure the highest confidence binary Lit dataset possible.</p>\r\n\r\n<p>After parsing all PPI data from the source databases we obtained a binary human<br>dataset of 13,962 PPIs that contained at least one piece of binary evidence supporting each PPI (there could be other pieces of experimental evidences that were either binary or indirect) and a non-binary dataset containing 42,781 PPIs for which none of the experimental methods are binary (Lit-NB-10).</p>\r\n\r\n<p>A paper curated independently by two or more different PPI databases is commonly annotated to different PSI-MI terms, generally to terms of different depth on the same branch of the PSI-MI ontology tree <span class=\"citation\">[<a href=\"/doi/10.1093/database/baq026\" class=\"citation\" data-key=\"10.1093/database/baq026\">18</a>, <a href=\"/doi/10.1038/nbt.1867\" class=\"citation\" data-key=\"10.1038/nbt.1867\">19</a>]</span>. If not corrected for, these annotations would count as two or more pieces of evidence for the PPI, when actually there is only one piece of supporting evidence. For example, a yeast two-hybrid experiment might be annotated to the deeper term “two hybrid prey pooling approach” (MI:1112) by one PPI database but to the parent term “two hybrid” (MI:0018) by another database; a coimmunoprecipitation (co-IP) experiment might be annotated to the deeper term “anti-tag coimmunoprecipitation” (MI:0007) by one database but to the parent term “affinity chromatography technology” (MI:0004) by another. To compensate for variability in the annotated methods, when the same paper with the same PMID had different MI terms in two databases, we reassigned the deeper term “up” to the corresponding parent binary or nonbinary term on the same PSI-MI branch. In the examples given, the two Y2H annotations collapse to the single ID MI:0018, while the two co-IP annotations collapse to the single ID MI:0004.</p>\r\n\r\n<p>The binary human dataset was next separated into “binary multiple” (Lit-BM-10) (Table S1A), containing all interactions supported by two or more pieces of experimental evidence, at least one of which was binary (4,906 PPIs); versus “binary single”, containing all interactions supported by exactly one piece of binary experimental evidence (Lit-BS-10) (9,056 PPIs).</p>\r\n\r\n<p><strong>Updating the Lit dataset to 2013:</strong> To construct Lit-2013 (Figure S1B and Table S1B) we downloaded, on August 5, 2013, the updated curated PPI content of the same seven PPI databases used for Lit-2010.</p>",
      "body_md": "# Human Interaction Database\r\n\r\nThe Human Interaction Database (HID) 2014 [release](http://interactome.dfci.harvard.edu/H_sapiens/index.php?page=download) [@10.1016/j.cell.2014.10.050] contained two PPI datasets:\r\n\r\n+ `HI-II-14` -- 13,944 interactions -- proteome-scale map of the human binary interactome network generated by systematically screening Space-II\r\n+ `Lit-BM-13` -- 11,045 interactions -- high-quality recurated literature binary interactions extracted from 7 public repository in 2013\r\n\r\nWe [compared](https://github.com/dhimmel/ppi/blob/77862798448c4272c55e9c718323a3ec5d8db571/compile-PPIs.ipynb) interactions from HID to interactions from the Incomplete Interactome (II). We found that `HI-II-14` was a subset of `II_binary`. However, only 78.2% of `Lit-BM-13` was included in `II_literature`.\r\n\r\n*To better understand `Lit-BM-13`, we have added the relevant sections of the paper [supplement](http://www.sciencedirect.com/science/MiamiMultiMediaURL/1-s2.0-S0092867414014226/1-s2.0-S0092867414014226-mmc1.pdf/272196/html/S0092867414014226/2b892ecda8f249667d75023be6d13c7b/mmc1.pdf) below, omitting the \"assignment of experimental method\" sections [@10.1016/j.cell.2014.10.050]:*\r\n\r\n**Literature datasets:** We generated two datasets from literature-curated protein-protein interactions. A first dataset was generated in 2010 and used for all experiments, concomitantly with our mapping experiment, and a second dataset was extracted in 2013 to provide an updated version for all computational analyses.\r\n\r\n**Obtaining the Lit-2010 dataset:** The Lit-2010 dataset extracts human protein-protein interactions (PPIs), annotated through December 2010, from seven primary source databases: BIND [@10.1093/nar/gkg056], BioGRID [@10.1093/nar/gks1158], DIP [@10.1093/nar/gkh086], HPRD [@10.1093/nar/gkn892], MINT [@10.1093/nar/gkr930], IntAct [@10.1093/nar/gkr1088] and PDB [@10.1093/nar/28.1.235]. For each reported PPI the interacting proteins were mapped to UniProt protein identifiers and then converted to NCBI Entrez gene ID pairs using an ID mapping table downloaded on January 12, 2012 from uniprot.org. Information about the specific publications reporting each interaction was retained and reported interactions that did not have an associated PubMed ID (PMID) were not included in the Lit-2010 dataset.\r\n\r\n**Identification of binary interactions:** We divided Lit-2010 into the PPIs reported by\r\nsystematic high-throughput binary human interactome mapping efforts [@10.1038/nature04209 @10.1016/j.cell.2005.08.029 @10.1038/nmeth.1280] and those detected in small- or medium-scale experiments. A small number of PPIs that had been detected in both systematic and other studies could appear in both datasets. Removing the PPIs only seen in systematic studies resulted in a dataset of 56,743 human PPIs.\r\n\r\nNext we attempted to distinguish binary interactions (direct biophysical contact between two proteins) [@10.1002/pmic.201100598 @10.1002/pmic.201100563] from indirect associations (associations between two proteins that are in the same complex, but may or may not directly interact) [@10.1016/j.sbi.2013.02.008]. We evaluated each experimental interaction detection method in the PSI-MI 2.5 and classified them as binary, that is, primarily detects binary interactions, versus indirect, that is, primarily detects association of proteins within a complex (Table S1C). Where an experimental method could be viewed as either, depending on the specific experimental implementation then the method was conservatively classified as indirect. Fewer methods were classified as binary here than in previous [@10.1038/nmeth.1284 @10.1126/science.1158684] or parallel [@10.1186/1752-0509-6-92] efforts to ensure the highest confidence binary Lit dataset possible.\r\n\r\nAfter parsing all PPI data from the source databases we obtained a binary human\r\ndataset of 13,962 PPIs that contained at least one piece of binary evidence supporting each PPI (there could be other pieces of experimental evidences that were either binary or indirect) and a non-binary dataset containing 42,781 PPIs for which none of the experimental methods are binary (Lit-NB-10).\r\n\r\nA paper curated independently by two or more different PPI databases is commonly annotated to different PSI-MI terms, generally to terms of different depth on the same branch of the PSI-MI ontology tree [@10.1093/database/baq026 @10.1038/nbt.1867]. If not corrected for, these annotations would count as two or more pieces of evidence for the PPI, when actually there is only one piece of supporting evidence. For example, a yeast two-hybrid experiment might be annotated to the deeper term “two hybrid prey pooling approach” (MI:1112) by one PPI database but to the parent term “two hybrid” (MI:0018) by another database; a coimmunoprecipitation (co-IP) experiment might be annotated to the deeper term “anti-tag coimmunoprecipitation” (MI:0007) by one database but to the parent term “affinity chromatography technology” (MI:0004) by another. To compensate for variability in the annotated methods, when the same paper with the same PMID had different MI terms in two databases, we reassigned the deeper term “up” to the corresponding parent binary or nonbinary term on the same PSI-MI branch. In the examples given, the two Y2H annotations collapse to the single ID MI:0018, while the two co-IP annotations collapse to the single ID MI:0004.\r\n\r\nThe binary human dataset was next separated into “binary multiple” (Lit-BM-10) (Table S1A), containing all interactions supported by two or more pieces of experimental evidence, at least one of which was binary (4,906 PPIs); versus “binary single”, containing all interactions supported by exactly one piece of binary experimental evidence (Lit-BS-10) (9,056 PPIs).\r\n\r\n**Updating the Lit dataset to 2013:** To construct Lit-2013 (Figure S1B and Table S1B) we downloaded, on August 5, 2013, the updated curated PPI content of the same seven PPI databases used for Lit-2010.",
      "profile": 17,
      "published": "2015-07-13T21:36:11.923036Z",
      "thread": 85,
      "url": "/discussion/creating-a-catalog-of-protein-interactions/85#3"
    },
    {
      "body_html": "<h1><em>hetio</em> interactions</h1>\r\n\r\n<p>Previously, we used the following method to catalog protein interactions <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1004259\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1004259\">1</a>]</span>:</p>\r\n\r\n<blockquote><p>Physical protein-protein interactions (<a href=\"http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004259#pcbi.1004259.s020\">S8 Data</a>) were extracted from iRefIndex 12.0, a compilation of 15 primary interaction databases <span class=\"citation\">[<a href=\"/doi/10.1186/1471-2105-9-405\" class=\"citation\" data-key=\"10.1186/1471-2105-9-405\">2</a>]</span>. The iRefIndex was processed with ppiTrim to convert proteins to genes, remove protein complexes, and condense duplicated entries <span class=\"citation\">[<a href=\"/doi/10.1093/database/bar036\" class=\"citation\" data-key=\"10.1093/database/bar036\">3</a>]</span>.</p></blockquote>\r\n\r\n<p>The method contributed 97,938 interactions to our network of protein-coding genes. For this project, we converted these interactions to entrez genes. Prior to filtering for coding genes, 98,119 interactions were in the hetio dataset.</p>\r\n\r\n<p>The hetio interactions overlapped most with <code>Lit-BM-13</code>, <code>II_literature</code>, and <code>II_signaling</code> (<a href=\"https://github.com/dhimmel/ppi/blob/919264e834a95bf8724c7177b8657af9de8622fd/compile-PPIs.ipynb\">notebook</a>).</p>",
      "body_md": "# *hetio* interactions\r\n\r\nPreviously, we used the following method to catalog protein interactions [@10.1371/journal.pcbi.1004259]:\r\n\r\n> Physical protein-protein interactions ([S8 Data](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004259#pcbi.1004259.s020)) were extracted from iRefIndex 12.0, a compilation of 15 primary interaction databases [@10.1186/1471-2105-9-405]. The iRefIndex was processed with ppiTrim to convert proteins to genes, remove protein complexes, and condense duplicated entries [@10.1093/database/bar036].\r\n\r\nThe method contributed 97,938 interactions to our network of protein-coding genes. For this project, we converted these interactions to entrez genes. Prior to filtering for coding genes, 98,119 interactions were in the hetio dataset.\r\n\r\nThe hetio interactions overlapped most with `Lit-BM-13`, `II_literature`, and `II_signaling` ([notebook](https://github.com/dhimmel/ppi/blob/919264e834a95bf8724c7177b8657af9de8622fd/compile-PPIs.ipynb)).",
      "profile": 17,
      "published": "2015-07-13T23:21:46.024878Z",
      "thread": 85,
      "url": "/discussion/creating-a-catalog-of-protein-interactions/85#4"
    },
    {
      "body_html": "<p>We have <a href=\"http://thinklab.com/d/67\">adopted a literature mining scheme</a> for term relations based on cooccurrence of MeSH topics in MEDLINE. Of <a href=\"http://thinklab.com/discussion/unifying-disease-vocabularies/44#144\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d44\">DO Slim</a> diseases, 133 out of 137 have a MeSH cross-reference. For each disease, we identified all studies assigned that topic and then computed pairwise cooccurrences (<a href=\"https://github.com/dhimmel/medline/blob/54a621d7fc360a2ad623d2cb3180f60485aff37f/diseases.ipynb\">notebook</a>, <a href=\"https://github.com/dhimmel/medline/blob/54a621d7fc360a2ad623d2cb3180f60485aff37f/data/disease-disease-cooccurrence.tsv\">tsv</a>).</p>\r\n\r\n<p>Since disease topics seemed averse to cooccurrence, we did not enforce the major topic filter. Even so, diseases cooccurred less than would be expected by chance. This makes sense given that papers often focus on a single disease only but means that our p-values are likely conservative.</p>",
      "body_md": "We have [adopted a literature mining scheme](http://thinklab.com/d/67) for term relations based on cooccurrence of MeSH topics in MEDLINE. Of [DO Slim](http://thinklab.com/discussion/unifying-disease-vocabularies/44#144) diseases, 133 out of 137 have a MeSH cross-reference. For each disease, we identified all studies assigned that topic and then computed pairwise cooccurrences ([notebook](https://github.com/dhimmel/medline/blob/54a621d7fc360a2ad623d2cb3180f60485aff37f/diseases.ipynb), [tsv](https://github.com/dhimmel/medline/blob/54a621d7fc360a2ad623d2cb3180f60485aff37f/data/disease-disease-cooccurrence.tsv)).\r\n\r\nSince disease topics seemed averse to cooccurrence, we did not enforce the major topic filter. Even so, diseases cooccurred less than would be expected by chance. This makes sense given that papers often focus on a single disease only but means that our p-values are likely conservative.",
      "profile": 17,
      "published": "2015-07-14T19:30:52.174618Z",
      "thread": 93,
      "url": "/discussion/disease-similarity-from-medline-topic-cooccurrence/93"
    },
    {
      "body_html": "<h1>Disease–Disease Relationships</h1>\r\n\r\n<p>We computed disease similarities based on MEDLINE cooccurrences. Refer to <a href=\"http://thinklab.com/discussion/disease-similarity-from-medline-topic-cooccurrence/93\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d93\">this discussion</a> for more information.</p>",
      "body_md": "# Disease--Disease Relationships\r\n\r\nWe computed disease similarities based on MEDLINE cooccurrences. Refer to [this discussion](http://thinklab.com/discussion/disease-similarity-from-medline-topic-cooccurrence/93) for more information.",
      "profile": 17,
      "published": "2015-07-14T19:32:41.490015Z",
      "thread": 67,
      "url": "/discussion/mining-knowledge-from-medline-articles-and-their-indexed-mesh-terms/67#6"
    },
    {
      "body_html": "<p><a href=\"http://doa.nubic.northwestern.edu/pages/search.php\">DOAF</a>, the Disease Ontology Annotation Framework <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pone.0049686\" class=\"citation\" data-key=\"10.1371/journal.pone.0049686\">1</a>]</span>, provides gene–disease relationships extracted from GeneRIF. <a href=\"http://www.ncbi.nlm.nih.gov/gene/about-generif\">GeneRIF</a> is a crowdsourced database of functional gene annotations that is integrated into NCBI's Entrez Gene. DOAF describes the resources <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pone.0049686\" class=\"citation\" data-key=\"10.1371/journal.pone.0049686\">1</a>]</span>:</p>\r\n\r\n<blockquote><p>GeneRIF contains brief textual descriptions of genes (up to 250 characters) and are available from the NCBI <span class=\"citation\">[<a href=\"/doi/10.1186/1471-2105-9-s3-s9\" class=\"citation\" data-key=\"10.1186/1471-2105-9-s3-s9\">2</a>]</span>, <span class=\"citation\">[<a href=\"/doi/10.1142/9789812772435_0026\" class=\"citation\" data-key=\"10.1142/9789812772435_0026\">3</a>]</span>. Every GeneRIF entry is associated with a PubMed ID, providing published evidence for each description.</p></blockquote>\r\n\r\n<h2>Processing</h2>\r\n\r\n<p>We converted the <code>IDMappings.txt</code> <a href=\"http://doa.nubic.northwestern.edu/pages/download.php\">data release</a> into a tsv with a single row per disease–gene pair (<a href=\"https://github.com/dhimmel/doaf/blob/bbe1c326aa385416e36d02b144e89e2b99e700b6/doaf.ipynb\">notebook</a>, <a href=\"https://github.com/dhimmel/doaf/blob/bbe1c326aa385416e36d02b144e89e2b99e700b6/data/doaf.tsv\">tsv</a>). Genes are in entrez identifiers and diseases are DO terms.</p>\r\n\r\n<p>Overall, we identified 50,863 gene–disease functional annotations.</p>",
      "body_md": "[DOAF](http://doa.nubic.northwestern.edu/pages/search.php), the Disease Ontology Annotation Framework [@10.1371/journal.pone.0049686], provides gene--disease relationships extracted from GeneRIF. [GeneRIF](http://www.ncbi.nlm.nih.gov/gene/about-generif) is a crowdsourced database of functional gene annotations that is integrated into NCBI's Entrez Gene. DOAF describes the resources [@10.1371/journal.pone.0049686]:\r\n\r\n> GeneRIF contains brief textual descriptions of genes (up to 250 characters) and are available from the NCBI [@10.1186/1471-2105-9-s3-s9], [@10.1142/9789812772435_0026]. Every GeneRIF entry is associated with a PubMed ID, providing published evidence for each description.\r\n\r\n## Processing\r\n\r\nWe converted the `IDMappings.txt` [data release](http://doa.nubic.northwestern.edu/pages/download.php) into a tsv with a single row per disease--gene pair ([notebook](https://github.com/dhimmel/doaf/blob/bbe1c326aa385416e36d02b144e89e2b99e700b6/doaf.ipynb), [tsv](https://github.com/dhimmel/doaf/blob/bbe1c326aa385416e36d02b144e89e2b99e700b6/data/doaf.tsv)). Genes are in entrez identifiers and diseases are DO terms.\r\n\r\nOverall, we identified 50,863 gene--disease functional annotations.",
      "profile": 17,
      "published": "2015-07-14T19:55:24.483842Z",
      "thread": 94,
      "url": "/discussion/functional-disease-annotations-for-genes-using-doaf/94"
    },
    {
      "body_html": "<h2>Latest indication catalog</h2>\r\n\r\n<p>We have <a href=\"http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#191\">completed</a> a first-draft of our indication catalog. Recently, we updated the catalog with some <a href=\"http://thinklab.com/discussion/disease-ontology-feature-requests/68#221\">fresh</a> disease cross-references. The latest catalog is now online (<a href=\"https://cdn.rawgit.com/dhimmel/indications/7c2b17f463babafcf4ec441e720b831340b186fe/merge.html#indication-table\">webpage</a>, <a href=\"https://github.com/dhimmel/indications/blob/7c2b17f463babafcf4ec441e720b831340b186fe/data/indications.tsv\">tsv</a>, <a href=\"https://github.com/dhimmel/indications/tree/7c2b17f463babafcf4ec441e720b831340b186fe\">repository</a>).</p>\r\n\r\n<p>The catalog includes 1,388 high-confidence indications from four resources — MEDI-HPS <span class=\"citation\">[<a href=\"/doi/10.1136/amiajnl-2012-001431\" class=\"citation\" data-key=\"10.1136/amiajnl-2012-001431\">1</a>]</span>, ehrlink <span class=\"citation\">[<a href=\"/doi/10.1136/amiajnl-2012-000852\" class=\"citation\" data-key=\"10.1136/amiajnl-2012-000852\">2</a>]</span>, LabeledIn <span class=\"citation\">[<a href=\"/doi/10.1016/j.jbi.2014.08.004\" class=\"citation\" data-key=\"10.1016/j.jbi.2014.08.004\">3</a>, <a href=\"/doi/10.1093/database/bav016\" class=\"citation\" data-key=\"10.1093/database/bav016\">4</a>]</span>, and PREDICT <span class=\"citation\">[<a href=\"/doi/10.1038/msb.2011.26\" class=\"citation\" data-key=\"10.1038/msb.2011.26\">5</a>]</span> — and 1,114 low-confidence indications from MEDI-LPS <span class=\"citation\">[<a href=\"/doi/10.1136/amiajnl-2012-001431\" class=\"citation\" data-key=\"10.1136/amiajnl-2012-001431\">1</a>]</span>. We are primarily concerned about the high-confidence associations which connect 108 diseases from <a href=\"http://thinklab.com/discussion/unifying-disease-vocabularies/44#144\">DO Slim</a> and 744 small molecules from <a href=\"http://thinklab.com/discussion/unifying-drug-vocabularies/40#192\">Drugbank Slim</a>.</p>\r\n\r\n<h2>Problematic indications</h2>\r\n\r\n<p>The source databases have a loose definition of indication — symptomatic treatments are often considered indications. For example, a narcolepsy drug approved for MS-induced fatigue is indicated for MS in our catalog. However, we are primarily interested in disease-modifying therapies. Since our method trains itself from this catalog, our predictions will recapitulate the types of indications included.</p>\r\n\r\n<h2>Seeking an expert</h2>\r\n\r\n<p>We are <strong>seeking an expert physician</strong> to manually review our 1,388 high-confidence indications and identify the disease-modifying subset. Since the classification is <em>unlikely</em> to be straightforward, we are looking for someone who can conceptualize the task from a high-throughput systems pharmacology perspective. We are happy to offer project authorship for a job well done.</p>",
      "body_md": "## Latest indication catalog\r\n\r\nWe have [completed](http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#191) a first-draft of our indication catalog. Recently, we updated the catalog with some [fresh](http://thinklab.com/discussion/disease-ontology-feature-requests/68#221) disease cross-references. The latest catalog is now online ([webpage](https://cdn.rawgit.com/dhimmel/indications/7c2b17f463babafcf4ec441e720b831340b186fe/merge.html#indication-table), [tsv](https://github.com/dhimmel/indications/blob/7c2b17f463babafcf4ec441e720b831340b186fe/data/indications.tsv), [repository](https://github.com/dhimmel/indications/tree/7c2b17f463babafcf4ec441e720b831340b186fe)).\r\n\r\nThe catalog includes 1,388 high-confidence indications from four resources -- MEDI-HPS [@10.1136/amiajnl-2012-001431], ehrlink [@10.1136/amiajnl-2012-000852], LabeledIn [@10.1016/j.jbi.2014.08.004 @10.1093/database/bav016], and PREDICT [@10.1038/msb.2011.26] -- and 1,114 low-confidence indications from MEDI-LPS [@10.1136/amiajnl-2012-001431]. We are primarily concerned about the high-confidence associations which connect 108 diseases from [DO Slim](http://thinklab.com/discussion/unifying-disease-vocabularies/44#144) and 744 small molecules from [Drugbank Slim](http://thinklab.com/discussion/unifying-drug-vocabularies/40#192).\r\n\r\n## Problematic indications\r\n\r\nThe source databases have a loose definition of indication -- symptomatic treatments are often considered indications. For example, a narcolepsy drug approved for MS-induced fatigue is indicated for MS in our catalog. However, we are primarily interested in disease-modifying therapies. Since our method trains itself from this catalog, our predictions will recapitulate the types of indications included.\r\n\r\n## Seeking an expert\r\n\r\nWe are **seeking an expert physician** to manually review our 1,388 high-confidence indications and identify the disease-modifying subset. Since the classification is *unlikely* to be straightforward, we are looking for someone who can conceptualize the task from a high-throughput systems pharmacology perspective. We are happy to offer project authorship for a job well done.",
      "profile": 17,
      "published": "2015-07-14T21:45:53.225564Z",
      "thread": 95,
      "url": "/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95"
    },
    {
      "body_html": "<h1>Expert curation of the indication catalog</h1>\r\n\r\n<p>We have decided to filter our catalog for disease-modifying indications and are seeking an expert curator to assist with this task. We <a href=\"http://thinklab.com/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d95\">started a new discussion</a> for this next step.</p>\r\n\r\n<p><a href=\"/u/allisonmccoy\" class=\"username\">@allisonmccoy</a>, have you thought more about releasing the data from your recent publication <span class=\"citation\">[<a href=\"/doi/10.4338/ACI-2015-01-RA-0010\" class=\"citation\" data-key=\"10.4338/ACI-2015-01-RA-0010\">1</a>]</span>? If you can do this in the next week or two, we would be thrilled to include this data. Otherwise we will have to move ahead with only the <a href=\"http://thinklab.com/discussion/extracting-indications-from-the-ehrlink-resource/62\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d62\">ehrlink data</a> from your initial study <span class=\"citation\">[<a href=\"/doi/10.1136/amiajnl-2012-000852\" class=\"citation\" data-key=\"10.1136/amiajnl-2012-000852\">2</a>]</span>.</p>",
      "body_md": "# Expert curation of the indication catalog\r\n\r\nWe have decided to filter our catalog for disease-modifying indications and are seeking an expert curator to assist with this task. We [started a new discussion](http://thinklab.com/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95) for this next step.\r\n\r\n@allisonmccoy, have you thought more about releasing the data from your recent publication [@10.4338/ACI-2015-01-RA-0010]? If you can do this in the next week or two, we would be thrilled to include this data. Otherwise we will have to move ahead with only the [ehrlink data](http://thinklab.com/discussion/extracting-indications-from-the-ehrlink-resource/62) from your initial study [@10.1136/amiajnl-2012-000852].",
      "profile": 17,
      "published": "2015-07-14T21:55:43.335809Z",
      "thread": 21,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#23"
    },
    {
      "body_html": "<h1>Signaling PPI</h1>\r\n\r\n<p><em>The incomplete interactome paper includes interactions from a signaling study <span class=\"citation\">[<a href=\"/doi/10.1126/scisignal.2001699\" class=\"citation\" data-key=\"10.1126/scisignal.2001699\">1</a>]</span>. We report the sources of this <code>HPPI1</code> network from the original paper's <a href=\"http://stke.sciencemag.org/content/sigtrans/suppl/2011/09/01/4.189.rs8.DC1/4_rs8_SM.pdf\">supplement</a>:</em></p>\r\n\r\n<p><strong>Table S3</strong>: Human PPI interaction data sets used to construct the HPPI1 network. A comprehensive HPPI1 network was created by unifying the data sets listed. Name of the data set, publication reference, number of proteins and number of interactions in the data set (after mapping to NCBI Entrez GeneID) are given. </p>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th>PPI Data sets</th><th>Reference</th><th>Proteins</th><th>Interactions</th></tr></thead><tbody><tr><td>Human Protein Reference Database V7.0</td><td><span class=\"citation\">[<a href=\"/doi/10.1007/978-1-60761-232-2_6\" class=\"citation\" data-key=\"10.1007/978-1-60761-232-2_6\">2</a>]</span></td><td>9305</td><td>35021</td></tr><tr><td>Genome-wide Y2H screen and literature-derived PPI data</td><td><span class=\"citation\">[<a href=\"/doi/10.1038/nature04209\" class=\"citation\" data-key=\"10.1038/nature04209\">3</a>]</span></td><td>3024</td><td>6221</td></tr><tr><td>Y2H screen for inherited ataxia and literature-derived PPI data</td><td><span class=\"citation\">[<a href=\"/doi/10.1016/j.cell.2006.03.032\" class=\"citation\" data-key=\"10.1016/j.cell.2006.03.032\">4</a>]</span></td><td>2909</td><td>5440</td></tr><tr><td>Genome-wide Y2H screen</td><td><span class=\"citation\">[<a href=\"/doi/10.1016/j.cell.2005.08.029\" class=\"citation\" data-key=\"10.1016/j.cell.2005.08.029\">5</a>]</span></td><td>1699</td><td>3150</td></tr><tr><td>Y2H PPIs</td><td>This study</td><td>1126</td><td>2626</td></tr><tr><td>Mouse signaling PPI data from AfCS</td><td><a href=\"http://www.signalinggateway.org/\">http://www.signalinggateway.org/</a></td><td>857</td><td>1004</td></tr><tr><td>Network for Smad signaling</td><td><span class=\"citation\">[<a href=\"/doi/10.1101/gr.2334104\" class=\"citation\" data-key=\"10.1101/gr.2334104\">6</a>]</span></td><td>623</td><td>874</td></tr><tr><td>PPIs of proteins in MHC class III region and mRNA decay</td><td><span class=\"citation\">[<a href=\"/doi/10.1101/gr.2122004\" class=\"citation\" data-key=\"10.1101/gr.2122004\">7</a>, <a href=\"/doi/10.1016/S0888-7543%2803%2900235-0\" class=\"citation\" data-key=\"10.1016/S0888-7543(03)00235-0\">8</a>]</span></td><td>300</td><td>376</td></tr><tr><td>Network of nuclear receptors</td><td><span class=\"citation\">[<a href=\"/doi/10.1074/mcp.M400169-MCP200\" class=\"citation\" data-key=\"10.1074/mcp.M400169-MCP200\">9</a>]</span></td><td>134</td><td>288</td></tr><tr><td>Huntingtin’s disease PPI network</td><td><span class=\"citation\">[<a href=\"/doi/10.1016/j.molcel.2004.09.016\" class=\"citation\" data-key=\"10.1016/j.molcel.2004.09.016\">10</a>]</span></td><td>64</td><td>156</td></tr><tr><td>PPIs between KIAA proteins</td><td><span class=\"citation\">[<a href=\"/doi/10.1101/gr.406902\" class=\"citation\" data-key=\"10.1101/gr.406902\">11</a>]</span></td><td>94</td><td>84</td></tr><tr><td>Total</td><td></td><td>9832</td><td>39641</td></tr></tbody></table>",
      "body_md": "# Signaling PPI\r\n\r\n*The incomplete interactome paper includes interactions from a signaling study [@10.1126/scisignal.2001699]. We report the sources of this `HPPI1` network from the original paper's [supplement](http://stke.sciencemag.org/content/sigtrans/suppl/2011/09/01/4.189.rs8.DC1/4_rs8_SM.pdf):*\r\n\r\n\r\n**Table S3**: Human PPI interaction data sets used to construct the HPPI1 network. A comprehensive HPPI1 network was created by unifying the data sets listed. Name of the data set, publication reference, number of proteins and number of interactions in the data set (after mapping to NCBI Entrez GeneID) are given. \r\n\r\n| PPI Data sets | Reference | Proteins | Interactions |\r\n|--------------------|---------------|----------|--------------|\r\n| Human Protein Reference Database V7.0 | [@10.1007/978-1-60761-232-2_6] | 9305 | 35021 |\r\n| Genome-wide Y2H screen and literature-derived PPI data | [@10.1038/nature04209] | 3024 | 6221 |\r\n| Y2H screen for inherited ataxia and literature-derived PPI data | [@10.1016/j.cell.2006.03.032] | 2909 | 5440 |\r\n| Genome-wide Y2H screen | [@10.1016/j.cell.2005.08.029] | 1699 | 3150 |\r\n| Y2H PPIs | This study | 1126 | 2626 |\r\n| Mouse signaling PPI data from AfCS | http://www.signalinggateway.org/ | 857 | 1004 |\r\n| Network for Smad signaling | [@10.1101/gr.2334104] | 623 | 874 |\r\n| PPIs of proteins in MHC class III region and mRNA decay | [@10.1101/gr.2122004 @10.1016/S0888-7543(03)00235-0] | 300 | 376 |\r\n| Network of nuclear receptors | [@10.1074/mcp.M400169-MCP200] | 134 | 288 |\r\n| Huntingtin’s disease PPI network | [@10.1016/j.molcel.2004.09.016] | 64 | 156 |\r\n| PPIs between KIAA proteins | [@10.1101/gr.406902] | 94 | 84 |\r\n| Total |  | 9832 | 39641 |",
      "profile": 17,
      "published": "2015-07-15T00:58:47.066661Z",
      "thread": 85,
      "url": "/discussion/creating-a-catalog-of-protein-interactions/85#5"
    },
    {
      "body_html": "<h1>Migrating to owltools</h1>\r\n\r\n<p>We are in the decade of the ontology: the pace of development and growth of this field is incredible. Therefore, I would like to outsource the ontology reasoning and inference to established software projects, namely <a href=\"https://github.com/owlcollab/owltools\">owltools</a>.</p>\r\n\r\n<p>The first step will be to load GO with <code>owltools http://purl.obolibrary.org/obo/go.owl</code>. Beyond this step I am stuck and haven't found sufficient documentation. <a href=\"/u/chrismungall\" class=\"username\">@chrismungall</a> or <a href=\"/u/fbastian\" class=\"username\">@fbastian</a> perhaps you could help me with the below queries or point me to a good tutorial:</p>\r\n\r\n<ol><li><strong>Adding annotations</strong>: we would like to add human gene annotations to GO terms.</li><li><strong>Propagating annotations</strong>: we would like to propagate annotations up <code>is_a</code> and <code>part_of</code> edges. Negative (<code>NOT</code>) annotations should short-circuit annotation propagation. </li><li><strong>Filter overly broad terms</strong>: Remove the \"<a href=\"http://geneontology.org/ontology/subsets/gocheck_do_not_annotate.obo\">do not annotate</a>\" terms for GO.</li><li><strong>Output:</strong> Write the propagated annotations to a text file</li></ol>\r\n\r\n<p>If it's not possible to perform all of these steps in one command, then we can work on a piecemeal approach.</p>",
      "body_md": "# Migrating to owltools\r\n\r\nWe are in the decade of the ontology: the pace of development and growth of this field is incredible. Therefore, I would like to outsource the ontology reasoning and inference to established software projects, namely [owltools](https://github.com/owlcollab/owltools).\r\n\r\nThe first step will be to load GO with `owltools http://purl.obolibrary.org/obo/go.owl`. Beyond this step I am stuck and haven't found sufficient documentation. @chrismungall or @fbastian perhaps you could help me with the below queries or point me to a good tutorial:\r\n\r\n1. **Adding annotations**: we would like to add human gene annotations to GO terms.\r\n2. **Propagating annotations**: we would like to propagate annotations up `is_a` and `part_of` edges. Negative (`NOT`) annotations should short-circuit annotation propagation. \r\n3. **Filter overly broad terms**: Remove the \"[do not annotate](http://geneontology.org/ontology/subsets/gocheck_do_not_annotate.obo)\" terms for GO.\r\n4. **Output:** Write the propagated annotations to a text file\r\n\r\nIf it's not possible to perform all of these steps in one command, then we can work on a piecemeal approach.",
      "profile": 17,
      "published": "2015-07-15T18:31:30.951908Z",
      "thread": 39,
      "url": "/discussion/compiling-gene-ontology-annotations-into-an-easy-to-use-format/39#6"
    },
    {
      "body_html": "<p>You should definitely check with Chris, GO annotation/propagation is not really my area of expertise. And it is indeed possible to do a lot of things in a single command using owltools. </p>",
      "body_md": "You should definitely check with Chris, GO annotation/propagation is not really my area of expertise. And it is indeed possible to do a lot of things in a single command using owltools. ",
      "profile": 111,
      "published": "2015-07-17T00:15:58.471609Z",
      "thread": 39,
      "url": "/discussion/compiling-gene-ontology-annotations-into-an-easy-to-use-format/39#7"
    },
    {
      "body_html": "<p>Actually, for some mappings we needed to request for new terms in Uberon, e.g.: <a href=\"https://github.com/obophenotype/uberon/issues/725\">https://github.com/obophenotype/uberon/issues/725</a></p>\r\n\r\n<p>Things will be slow until mid-August on our side. </p>",
      "body_md": "Actually, for some mappings we needed to request for new terms in Uberon, e.g.: https://github.com/obophenotype/uberon/issues/725\r\n\r\nThings will be slow until mid-August on our side. ",
      "profile": 111,
      "published": "2015-07-17T00:21:40.270939Z",
      "thread": 82,
      "url": "/discussion/processing-gtex-for-tissue-specific-gene-expression/82#14"
    },
    {
      "body_html": "<p>Have you seen <a href=\"http://interactome.dfci.harvard.edu/H_sapiens/\">Human Interactome</a> out of Harvard.  I used them for my <a href=\"http://www.nature.com/ncomms/2014/140606/ncomms5074/full/ncomms5074.html\">PPI work for CNVs in GWAS for Autism</a> and worked out quite well.  They seem to have at least some of the ones you have already listed, although I think your list looks more extensive.  You want to be careful of integrating genome-wide PPI (i.e. Y2H) vs targeted PPI datasets.  It may mess up interpreting the statistics if the evidence for PPI is biased vs genome-wide.</p>",
      "body_md": "Have you seen [Human Interactome](http://interactome.dfci.harvard.edu/H_sapiens/) out of Harvard.  I used them for my [PPI work for CNVs in GWAS for Autism](http://www.nature.com/ncomms/2014/140606/ncomms5074/full/ncomms5074.html) and worked out quite well.  They seem to have at least some of the ones you have already listed, although I think your list looks more extensive.  You want to be careful of integrating genome-wide PPI (i.e. Y2H) vs targeted PPI datasets.  It may mess up interpreting the statistics if the evidence for PPI is biased vs genome-wide.",
      "profile": 121,
      "published": "2015-07-28T19:29:02.651491Z",
      "thread": 85,
      "url": "/discussion/creating-a-catalog-of-protein-interactions/85#6"
    },
    {
      "body_html": "<p>A 2011 study <span class=\"citation\">[<a href=\"/doi/10.1126/scitranslmed.3001318\" class=\"citation\" data-key=\"10.1126/scitranslmed.3001318\">1</a>]</span> introduced the idea of large-scale drug repurposing based disease expression profiles. However, the field has faced a great impediment: results from differential expression experiments are only available on a <em>per study</em> basis. Our project requires a consensus signature (that aggregates many experiments) for <em>each of 137</em> diseases.</p>\r\n\r\n<p>A forthcoming project called <a href=\"http://dev.stargeo.io/\">STARGEO</a> aims to provide disease-specific expression signatures on a broad scale. The webapp crowdsources <a href=\"http://www.ncbi.nlm.nih.gov/geo/\">GEO</a> <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gks1193\" class=\"citation\" data-key=\"10.1093/nar/gks1193\">2</a>, <a href=\"/doi/10.1093/nar/30.1.207\" class=\"citation\" data-key=\"10.1093/nar/30.1.207\">3</a>]</span> annotation and performs case-control analyses based on user queries. The following video introduces the project:</p>\r\n\r\n<p></p><div class=\"iframe-container\"><iframe src=\"https://www.youtube.com/embed/61lw_d6Eoik\" frameborder=\"0\" allowfullscreen=\"true\"></iframe></div>\r\n\r\n<p>We now join forces with STARGEO and welcome its creator <a href=\"/u/idrdex\" class=\"username\">@idrdex</a> to the team! The first stage will be tagging all GEO datasets containing <a href=\"http://thinklab.com/discussion/unifying-disease-vocabularies/44#144\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d44\">DO Slim</a> diseases. STARGEO's current DO Slim coverage is <a href=\"https://github.com/dhimmel/stargeo/blob/master/data/DO-tag-mapping.tsv\">available here</a>.</p>",
      "body_md": "A 2011 study [@10.1126/scitranslmed.3001318] introduced the idea of large-scale drug repurposing based disease expression profiles. However, the field has faced a great impediment: results from differential expression experiments are only available on a *per study* basis. Our project requires a consensus signature (that aggregates many experiments) for *each of 137* diseases.\r\n\r\nA forthcoming project called [STARGEO](http://dev.stargeo.io/) aims to provide disease-specific expression signatures on a broad scale. The webapp crowdsources [GEO](http://www.ncbi.nlm.nih.gov/geo/) [@10.1093/nar/gks1193 @10.1093/nar/30.1.207] annotation and performs case-control analyses based on user queries. The following video introduces the project:\r\n\r\n![:youtube](61lw_d6Eoik)\r\n\r\nWe now join forces with STARGEO and welcome its creator @idrdex to the team! The first stage will be tagging all GEO datasets containing [DO Slim](http://thinklab.com/discussion/unifying-disease-vocabularies/44#144) diseases. STARGEO's current DO Slim coverage is [available here](https://github.com/dhimmel/stargeo/blob/master/data/DO-tag-mapping.tsv).",
      "profile": 17,
      "published": "2015-07-28T21:14:07.077429Z",
      "thread": 96,
      "url": "/discussion/stargeo-expression-signatures-for-disease-using-crowdsourced-geo-annotation/96"
    },
    {
      "body_html": "<p><a href=\"/u/idrdex\" class=\"username\">@idrdex</a>, we're referring to this data as the Human Interaction Database (HID) and found that the systematic datasets were wholly included in the Incomplete Interactome (<code>II</code>) data (<a href=\"#323\">comment</a>, <a href=\"https://github.com/dhimmel/ppi/blob/77862798448c4272c55e9c718323a3ec5d8db571/compile-PPIs.ipynb\">notebook</a>).</p>\r\n\r\n<p>You are right that once we introduce PPIs from targeted or curated analyses, we have introduce knowledge bias. While we prefer systematic data, we <a href=\"http://thinklab.com/discussion/text-as-a-resource-for-network-population/48#120\">decided to incorporate</a> biased knowledge. We will likely perform a parallel analysis on an network built from only unbiased data sources. For this analysis, we'll use the Y2H datasets used in your autism study <span class=\"citation\">[<a href=\"/doi/10.1038/ncomms5074\" class=\"citation\" data-key=\"10.1038/ncomms5074\">1</a>]</span>.</p>",
      "body_md": "@idrdex, we're referring to this data as the Human Interaction Database (HID) and found that the systematic datasets were wholly included in the Incomplete Interactome (`II`) data ([comment](#323), [notebook](https://github.com/dhimmel/ppi/blob/77862798448c4272c55e9c718323a3ec5d8db571/compile-PPIs.ipynb)).\r\n\r\nYou are right that once we introduce PPIs from targeted or curated analyses, we have introduce knowledge bias. While we prefer systematic data, we [decided to incorporate](http://thinklab.com/discussion/text-as-a-resource-for-network-population/48#120) biased knowledge. We will likely perform a parallel analysis on an network built from only unbiased data sources. For this analysis, we'll use the Y2H datasets used in your autism study [@10.1038/ncomms5074].",
      "profile": 17,
      "published": "2015-07-28T21:32:56.762735Z",
      "thread": 85,
      "url": "/discussion/creating-a-catalog-of-protein-interactions/85#7"
    },
    {
      "body_html": "<h1>Similarities between associated genes in drug compounds</h1>\r\n\r\n<p><a href=\"https://github.com/sabrinalchen/drugbank-similarity/blob/912848e1d13fc0648ae33e022308d1da719f5a1a/similarities.ipynb\">Notebook</a></p>\r\n\r\n<h2>Objective:</h2>\r\n\r\n<p>We wanted to visualize the similarities among the associated genes for drug pairs in each of the four types of drug-bank interaction categories. To do so we extracted data from various sources to compile drugs with associated genes and the compound similarities between pairs of drugs compounds. </p>\r\n\r\n<h2>The Data:</h2>\r\n\r\n<p>We extracted <a href=\"https://raw.githubusercontent.com/dhimmel/drugbank/3e87872db5fca5ac427ce27464ab945c0ceb4ec6/data/proteins.tsv\">DrugBank-protein relationships</a> which lists drug types and associated genes, as well as <a href=\"http://nbviewer.ipython.org/github/dhimmel/drugbank/blob/9eb1e15ada5e6579e5aea1cae784c10602037b05/similarity.ipynb\">compound similarity data</a> which gives a value between zero and one based on the chemical similarity of a pair of drugs. </p>\r\n\r\n<h2>Jaccard Values and Initial Visualization:</h2>\r\n\r\n<p>Our first step was to create a dataframe with combination of compound pairs. Each compound is associated with a certain number of genes and we were able to define a Jaccard function to calculate the Jaccard value of the overlapping genes in each compound pair. It was noted – as expected — that the wide majority of the compound pairs had no similar genes, with a Jaccard value of zero. The drug-protein interactions were then categorized into four subgroups: carrier, enzyme, target and transporter. The similarity data was added for each compound pair. All five categories were graphed on a Seaborn PairGrid using a histogram on the univariate level and a hexbin scatterplot on the bivariate level.</p>\r\n\r\n<h2>Analysis of PairGrid Jaccard Value Visualization:</h2>\r\n\r\n<p>The data for each of the graphs did indeed center around zero, meaning that most compounds had no genes in common. In fact, the data was so skewed in the histogram that we needed to use logarithmic bins. Though the data was skewed right towards zero and dipped around 0.9 for each category, the histograms showed that there was also significant data for the Jaccard value of one, so that the graphs had U-shaped figures. This means that there are some compound pairs with all genes in common. In terms of the hexbin scatterplots, the darkest areas were zero and one, which reflected what was observed in the histograms. One other interesting trend to note are that for carrier and transporter, the data also concentrated around 0.5 and 0.33. </p>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/sabrinalchen/drugbank-similarity/ad3edac94b5c8d00e81395095468c7cce621e9d6/figure/similarity.png\" alt=\"\"></p>\r\n\r\n<h5>Figure 1: Compares Jaccard values for each drug-protein interaction category and chemical similarity</h5>\r\n\r\n<h2>Mean Jaccard Pointplot:</h2>\r\n\r\n<p>We used the Seaborn <a href=\"https://github.com/sabrinalchen/drugbank-similarity/blob/22bf54916d14b933be846bf52ad93237d54394c9/figure/similarity_mean.png\">Pointplot</a> to visualize the data in a different way. The means of Jaccard values were calculated for each of the four protein-interaction categories. With the similarity on the x-axis and the mean of Jaccard on the y-axis, it was concluded that the mean Jaccard peaked when the similarity was about 0.8 to 0.9. The general upwards trend was expected, as increased compound similarity should indicate an increase of gene overlap. By far, the target category had the most dramatic increase in mean Jaccard value. Though it started with a pretty flat mean Jaccard value at zero, it increased rapidly to hit a mean Jaccard value of nearly 0.5 at similarity=0.8. </p>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/sabrinalchen/drugbank-similarity/912848e1d13fc0648ae33e022308d1da719f5a1a/figure/similarity_mean.png\" alt=\"\"></p>\r\n\r\n<h5>Figure 2: Demonstrates relationship between chemical similarity of compounds and the mean Jaccard values</h5>\r\n\r\n<h2>Similarity Threshold:</h2>\r\n\r\n<p>The final visualization we created was a series of complex barplots to compare a similarity threshold among the four category groups. The similarity values were replaced with zero or one depending on whether the original value was greater or less than 0.5. Next, contingency tables were created for each category so that relative frequencies could be calculated. </p>\r\n\r\n<p>The visualization showed that similar compounds are likely to have common targets. For example, if two compounds shared a transporter, they also shared a carrier 25% of the time, compared to 5% of the time if they did not.  This trend continued throughout each of the bar graphs and was particularly notable in the comparisons with similarity. Note that the y-axis’s are labeled differently for easier reading. </p>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/sabrinalchen/drugbank-similarity/ad3edac94b5c8d00e81395095468c7cce621e9d6/figure/output.png\" alt=\"\"></p>\r\n\r\n<h5>Figure 3: Illustrates similarity threshold among the drug-protein interaction categories and chemical similarity</h5>",
      "body_md": "#Similarities between associated genes in drug compounds\r\n\r\n[Notebook](https://github.com/sabrinalchen/drugbank-similarity/blob/912848e1d13fc0648ae33e022308d1da719f5a1a/similarities.ipynb)\r\n\r\n##Objective: \r\nWe wanted to visualize the similarities among the associated genes for drug pairs in each of the four types of drug-bank interaction categories. To do so we extracted data from various sources to compile drugs with associated genes and the compound similarities between pairs of drugs compounds. \r\n\r\n##The Data: \r\nWe extracted [DrugBank-protein relationships](https://raw.githubusercontent.com/dhimmel/drugbank/3e87872db5fca5ac427ce27464ab945c0ceb4ec6/data/proteins.tsv) which lists drug types and associated genes, as well as [compound similarity data](http://nbviewer.ipython.org/github/dhimmel/drugbank/blob/9eb1e15ada5e6579e5aea1cae784c10602037b05/similarity.ipynb) which gives a value between zero and one based on the chemical similarity of a pair of drugs. \r\n\r\n##Jaccard Values and Initial Visualization:\r\nOur first step was to create a dataframe with combination of compound pairs. Each compound is associated with a certain number of genes and we were able to define a Jaccard function to calculate the Jaccard value of the overlapping genes in each compound pair. It was noted – as expected -- that the wide majority of the compound pairs had no similar genes, with a Jaccard value of zero. The drug-protein interactions were then categorized into four subgroups: carrier, enzyme, target and transporter. The similarity data was added for each compound pair. All five categories were graphed on a Seaborn PairGrid using a histogram on the univariate level and a hexbin scatterplot on the bivariate level.\r\n\r\n##Analysis of PairGrid Jaccard Value Visualization:\r\nThe data for each of the graphs did indeed center around zero, meaning that most compounds had no genes in common. In fact, the data was so skewed in the histogram that we needed to use logarithmic bins. Though the data was skewed right towards zero and dipped around 0.9 for each category, the histograms showed that there was also significant data for the Jaccard value of one, so that the graphs had U-shaped figures. This means that there are some compound pairs with all genes in common. In terms of the hexbin scatterplots, the darkest areas were zero and one, which reflected what was observed in the histograms. One other interesting trend to note are that for carrier and transporter, the data also concentrated around 0.5 and 0.33. \r\n\r\n![](https://raw.githubusercontent.com/sabrinalchen/drugbank-similarity/ad3edac94b5c8d00e81395095468c7cce621e9d6/figure/similarity.png)\r\n#####Figure 1: Compares Jaccard values for each drug-protein interaction category and chemical similarity\r\n\r\n##Mean Jaccard Pointplot:\r\nWe used the Seaborn [Pointplot](https://github.com/sabrinalchen/drugbank-similarity/blob/22bf54916d14b933be846bf52ad93237d54394c9/figure/similarity_mean.png) to visualize the data in a different way. The means of Jaccard values were calculated for each of the four protein-interaction categories. With the similarity on the x-axis and the mean of Jaccard on the y-axis, it was concluded that the mean Jaccard peaked when the similarity was about 0.8 to 0.9. The general upwards trend was expected, as increased compound similarity should indicate an increase of gene overlap. By far, the target category had the most dramatic increase in mean Jaccard value. Though it started with a pretty flat mean Jaccard value at zero, it increased rapidly to hit a mean Jaccard value of nearly 0.5 at similarity=0.8. \r\n\r\n![](https://raw.githubusercontent.com/sabrinalchen/drugbank-similarity/912848e1d13fc0648ae33e022308d1da719f5a1a/figure/similarity_mean.png)\r\n#####Figure 2: Demonstrates relationship between chemical similarity of compounds and the mean Jaccard values\r\n\r\n##Similarity Threshold:\r\nThe final visualization we created was a series of complex barplots to compare a similarity threshold among the four category groups. The similarity values were replaced with zero or one depending on whether the original value was greater or less than 0.5. Next, contingency tables were created for each category so that relative frequencies could be calculated. \r\n\r\nThe visualization showed that similar compounds are likely to have common targets. For example, if two compounds shared a transporter, they also shared a carrier 25% of the time, compared to 5% of the time if they did not.  This trend continued throughout each of the bar graphs and was particularly notable in the comparisons with similarity. Note that the y-axis’s are labeled differently for easier reading. \r\n\r\n![](https://raw.githubusercontent.com/sabrinalchen/drugbank-similarity/ad3edac94b5c8d00e81395095468c7cce621e9d6/figure/output.png)\r\n#####Figure 3: Illustrates similarity threshold among the drug-protein interaction categories and chemical similarity",
      "profile": 112,
      "published": "2015-07-28T22:03:29.420605Z",
      "thread": 65,
      "url": "/discussion/protein-target-carrier-transporter-and-enzyme-interactions-in-drugbank/65#2"
    },
    {
      "body_html": "<h1>Migration cancelled</h1>\r\n\r\n<p>In the interest of time, we did not switch to owltools. The OWL ecosystem codebases rely on a completely different stack than our current python workflow, and the documentation is often incomplete. We <a href=\"https://github.com/owlcollab/owltools/issues/129\">asked</a> our usage questions on GitHub and will consider migrating in the future with clearer guidance.</p>\r\n\r\n<h1>Updated annotations framework</h1>\r\n\r\n<p>We revamped the analysis behind our <a href=\"http://git.dhimmel.com/gene-ontology/\">user-friendly GO annotation utility</a> <span class=\"citation\">[<a href=\"/doi/10.5281/zenodo.21711\" class=\"citation\" data-key=\"10.5281/zenodo.21711\">1</a>]</span>.</p>\r\n\r\n<p>We made the following changes:</p>\r\n\r\n<ul><li>an option to discard annotations without experimental evidence</li><li>propagation along <code>part_of</code> (as well as <code>is_a</code>) relationships</li><li>direct annotations short-circuit the propagation of conflicting annotations. This occurs only when negative (<code>NOT</code>) and positive annotations conflict.</li><li>exclude terms in the <code>goantislim_grouping</code>, <code>gocheck_do_not_annotate</code>, or <code>gocheck_do_not_manually_annotate</code> subsets</li></ul>\r\n\r\n<p>We removed the \"protein-coding genes only\" option and made a single download with gene identifiers and symbols.</p>",
      "body_md": "# Migration cancelled\r\n\r\nIn the interest of time, we did not switch to owltools. The OWL ecosystem codebases rely on a completely different stack than our current python workflow, and the documentation is often incomplete. We [asked](https://github.com/owlcollab/owltools/issues/129) our usage questions on GitHub and will consider migrating in the future with clearer guidance.\r\n\r\n# Updated annotations framework\r\n\r\nWe revamped the analysis behind our [user-friendly GO annotation utility](http://git.dhimmel.com/gene-ontology/) [@10.5281/zenodo.21711].\r\n\r\nWe made the following changes:\r\n\r\n+ an option to discard annotations without experimental evidence\r\n+ propagation along `part_of` (as well as `is_a`) relationships\r\n+ direct annotations short-circuit the propagation of conflicting annotations. This occurs only when negative (`NOT`) and positive annotations conflict.\r\n+ exclude terms in the `goantislim_grouping`, `gocheck_do_not_annotate`, or `gocheck_do_not_manually_annotate` subsets\r\n\r\nWe removed the \"protein-coding genes only\" option and made a single download with gene identifiers and symbols.",
      "profile": 17,
      "published": "2015-07-29T17:06:14.274108Z",
      "thread": 39,
      "url": "/discussion/compiling-gene-ontology-annotations-into-an-easy-to-use-format/39#8"
    },
    {
      "body_html": "<h1>Phenotype–Disease Associations</h1>\r\n\r\n<p>A recent paper titled \"The Human Phenotype Ontology: Semantic Unification of Common and Rare Disease\" <span class=\"citation\">[<a href=\"/doi/10.1016/j.ajhg.2015.05.020\" class=\"citation\" data-key=\"10.1016/j.ajhg.2015.05.020\">1</a>]</span> constructed a catalog of 132,006 phenotypic annotations for common diseases.</p>\r\n\r\n<p>The approach relied on text mining of PubMed abstracts. Abstracts were annotated with diseases using MEDLINE topics. Phenotype annotation, however, relied on concept recognition. Thus the method appears to produce similar results to our <a href=\"http://thinklab.com/discussion/mining-knowledge-from-medline-articles-and-their-indexed-mesh-terms/67#2\">disease–symptom MEDLINE approach</a>. The difference being that their approach achieves greater phenotype/symptom coverage by substituting manual topic annotation with concept recognition.</p>\r\n\r\n<p>The data is <a href=\"http://pubmed-browser.human-phenotype-ontology.org/hp_common_annotations_all.tab\">online</a>. The column names for the dataset, provided by Tudor Groza in personal communication, are:</p>\r\n\r\n<ul><li>MeSH ID</li><li>MeSH descriptor</li><li>Disease Ontology ID</li><li>HPO ID</li><li>HPO label</li><li>Ranking score of the HPO concept in the context of the MeSH term - this is a modified version of TF-IDF (as per the paper)</li><li>Number of Publications containing this association</li><li>5 PMIDs (of the total number listed above) referring to this association</li></ul>\r\n\r\n<p>I am still slightly unclear on what a phenotype means in the context of human disease, but we will keep this dataset on hand.</p>",
      "body_md": "# Phenotype--Disease Associations\r\n\r\nA recent paper titled \"The Human Phenotype Ontology: Semantic Unification of Common and Rare Disease\" [@10.1016/j.ajhg.2015.05.020] constructed a catalog of 132,006 phenotypic annotations for common diseases.\r\n\r\nThe approach relied on text mining of PubMed abstracts. Abstracts were annotated with diseases using MEDLINE topics. Phenotype annotation, however, relied on concept recognition. Thus the method appears to produce similar results to our [disease--symptom MEDLINE approach](http://thinklab.com/discussion/mining-knowledge-from-medline-articles-and-their-indexed-mesh-terms/67#2). The difference being that their approach achieves greater phenotype/symptom coverage by substituting manual topic annotation with concept recognition.\r\n\r\nThe data is [online](http://pubmed-browser.human-phenotype-ontology.org/hp_common_annotations_all.tab). The column names for the dataset, provided by Tudor Groza in personal communication, are:\r\n\r\n* MeSH ID\r\n* MeSH descriptor\r\n* Disease Ontology ID\r\n* HPO ID\r\n* HPO label\r\n* Ranking score of the HPO concept in the context of the MeSH term - this is a modified version of TF-IDF (as per the paper)\r\n* Number of Publications containing this association\r\n* 5 PMIDs (of the total number listed above) referring to this association\r\n\r\nI am still slightly unclear on what a phenotype means in the context of human disease, but we will keep this dataset on hand.",
      "profile": 17,
      "published": "2015-07-30T20:42:18.764501Z",
      "thread": 22,
      "url": "/discussion/suggestions-for-additional-information-types/22#3"
    },
    {
      "body_html": "<p>A recent study <span class=\"citation\">[<a href=\"/doi/10.1038/ng.3314\" class=\"citation\" data-key=\"10.1038/ng.3314\">1</a>]</span> found disease-associated genes are mildly predictive of effective drug targets, which is in line with the findings of a preceding but less rigorous study <span class=\"citation\">[<a href=\"/doi/10.1038/nbt.3183\" class=\"citation\" data-key=\"10.1038/nbt.3183\">2</a>]</span>.</p>\r\n\r\n<p>The data supplement for  \"The support of human genetic evidence for approved drug indications\" contains two potentially useful resources:</p>\r\n\r\n<p><strong>Gene-disease associations</strong> from GWAS and OMIM. Compared to <a href=\"http://dx.doi.org/10.15363/thinklab.d80\">our approach</a> for converting from SNP to gene, their method incorporates experimental genomic evidence.</p>\r\n\r\n<p><strong>Disease-target combinations</strong> extracted from <a href=\"https://citeline.com/products/pharmaprojects/\">Pharmaprojects</a>, a commercial database. As per the paper: </p>\r\n\r\n<blockquote><p>A target was defined as successful in treating an indication if a drug targeting that gene product was approved for the corresponding indication in the United States or the European Union, as annotated in Pharmaprojects.</p></blockquote>\r\n\r\n<p>Unfortunately, this dataset is a step abstracted from the <em>real deal</em> (separate databases of drug targets and drug indications).</p>\r\n\r\n<p>We performed some basic manipulation of their data, which is <a href=\"https://github.com/dhimmel/nelson/blob/af1066df8f8d2b599869864a2a5d7935cf67c1ba/process.ipynb\">available here</a>.</p>",
      "body_md": "A recent study [@10.1038/ng.3314] found disease-associated genes are mildly predictive of effective drug targets, which is in line with the findings of a preceding but less rigorous study [@10.1038/nbt.3183].\r\n\r\nThe data supplement for  \"The support of human genetic evidence for approved drug indications\" contains two potentially useful resources:\r\n\r\n**Gene-disease associations** from GWAS and OMIM. Compared to [our approach](http://dx.doi.org/10.15363/thinklab.d80) for converting from SNP to gene, their method incorporates experimental genomic evidence.\r\n\r\n**Disease-target combinations** extracted from [Pharmaprojects](https://citeline.com/products/pharmaprojects/), a commercial database. As per the paper: \r\n> A target was defined as successful in treating an indication if a drug targeting that gene product was approved for the corresponding indication in the United States or the European Union, as annotated in Pharmaprojects.\r\n\r\nUnfortunately, this dataset is a step abstracted from the *real deal* (separate databases of drug targets and drug indications).\r\n\r\nWe performed some basic manipulation of their data, which is [available here](https://github.com/dhimmel/nelson/blob/af1066df8f8d2b599869864a2a5d7935cf67c1ba/process.ipynb).",
      "profile": 17,
      "published": "2015-07-31T18:00:29.433790Z",
      "thread": 22,
      "url": "/discussion/suggestions-for-additional-information-types/22#4"
    },
    {
      "body_html": "<p>What do you think about implementing high-throughput data from RNA interference screenings? RNAi is an alternative, more precise way to control gene expression and it should have less off-target effects compared to pharmacological inhibition.</p>",
      "body_md": "What do you think about implementing high-throughput data from RNA interference screenings? RNAi is an alternative, more precise way to control gene expression and it should have less off-target effects compared to pharmacological inhibition.",
      "profile": 82,
      "published": "2015-07-31T20:43:01.711330Z",
      "thread": 22,
      "url": "/discussion/suggestions-for-additional-information-types/22#5"
    },
    {
      "body_html": "<h1>Set of anatomy nodes</h1>\r\n\r\n<p>We have settled on 402 Uberon terms to use as our anatomy vocabulary (<a href=\"https://github.com/dhimmel/uberon/blob/134f23479186abba03ba340fc6dc90e16c781920/data/hetio-slim.tsv\">tsv</a>, <a href=\"https://github.com/dhimmel/uberon/blob/134f23479186abba03ba340fc6dc90e16c781920/process.ipynb\">notebook</a>).</p>\r\n\r\n<p>We included terms that met the following conditions:</p>\r\n\r\n<ul><li>were in the <code>uberon_slim</code> subset.</li><li>were not in the <code>non_informative</code>, <code>upper_level</code>, or <code>grouping_class</code> subsets. See this related <a href=\"https://github.com/obophenotype/uberon/issues/1133\">GitHub issue</a>.</li><li>contained a MeSH cross-reference</li><li>were human-relevant based on the <a href=\"#12\">no negative evidence</a> standard.</li></ul>\r\n\r\n<p>We chose a restrictive subset of Uberon terms because the vast extent of tissue-specific gene expression edges can become computationally troubling. We did not include cell types from the Cell Ontology because this ontology lags behind Uberon in terms of subset assignments, cross-references, and documentation.</p>",
      "body_md": "# Set of anatomy nodes\r\n\r\nWe have settled on 402 Uberon terms to use as our anatomy vocabulary ([tsv](https://github.com/dhimmel/uberon/blob/134f23479186abba03ba340fc6dc90e16c781920/data/hetio-slim.tsv), [notebook](https://github.com/dhimmel/uberon/blob/134f23479186abba03ba340fc6dc90e16c781920/process.ipynb)).\r\n\r\nWe included terms that met the following conditions:\r\n\r\n+ were in the `uberon_slim` subset.\r\n+ were not in the `non_informative`, `upper_level`, or `grouping_class` subsets. See this related [GitHub issue](https://github.com/obophenotype/uberon/issues/1133).\r\n+ contained a MeSH cross-reference\r\n+ were human-relevant based on the [no negative evidence](#12) standard.\r\n\r\nWe chose a restrictive subset of Uberon terms because the vast extent of tissue-specific gene expression edges can become computationally troubling. We did not include cell types from the Cell Ontology because this ontology lags behind Uberon in terms of subset assignments, cross-references, and documentation.",
      "profile": 17,
      "published": "2015-08-03T23:26:00.170473Z",
      "thread": 41,
      "url": "/discussion/tissue-node/41#16"
    },
    {
      "body_html": "<h1>Relationship between transcriptional and chemical similarity</h1>\r\n\r\n<p><a href=\"https://github.com/sabrinalchen/drugbank-similarity/blob/master/L1000.ipynb\">Notebook</a></p>\r\n\r\n<h2>Objective:</h2>\r\n\r\n<p>Determine the correlation and visualize the relationship between L1000 transcriptional and chemical compound similarity.</p>\r\n\r\n<h2>The data:</h2>\r\n\r\n<p>We extracted <a href=\"http://files.figshare.com/2166122/consensus_drugbank.tsv.gz\">L1000 perturbation data</a> which lists drug types with perturbation IDs and <a href=\"http://nbviewer.ipython.org/github/dhimmel/drugbank/blob/9eb1e15ada5e6579e5aea1cae784c10602037b05/similarity.ipynb\">similarity data</a> which gives a value between zero and one based on the chemical similarity of a pair of drugs. </p>\r\n\r\n<h2>Jointplot comparing chemical and transcriptional similarities:</h2>\r\n\r\n<p>From the imported L1000 perturbation data, we calculated the spearman correlation values for each combination of pair of drugs. This correlation value was labeled as the \"transcriptional similarity.\" This value was graphed against the imported similarity data for each drug pair. From the Jointplot, (which plots both bivariate data on a hexbin plot and univariate data on a histogram,) we concluded that there was no strong correlation visually. The darkest parts of the hexbin graph were grouped at the bottom left of the grid and the graph showed no real positive pattern. Nevertheless, the graph had an extremely small p-value, indicating significance, and leading to a small effect. </p>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/sabrinalchen/drugbank-similarity/master/figure/similarity2.png\" alt=\"\"></p>\r\n\r\n<h5>Figure 1: Compares transcriptional similarity for each drug pair and chemical similarity</h5>\r\n\r\n<h2>Rounded chemical similarity pointplot:</h2>\r\n\r\n<p>To visualize the correlation a different way, the chemical similarity data was rounded off to the nearest tenth and the mean transcriptional value was found for each subset. This data was graphed on a pointplot which demonstrated a clear positive correlation, especially when the chemical similarity reached 0.5. After this point, the correlation was even stronger, increasing steadily as the chemical similarity reached 1.0. It should be noted that as the chemical similarity increased, there were less and less data points to be used in graphing. In fact, the last data point where chemical similarity was 1.0 had only a single data point (as seen in the table of rounded values shown in the cell before the pointplot.)</p>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/sabrinalchen/drugbank-similarity/master/figure/similarity_mean2.png\" alt=\"\"></p>\r\n\r\n<h5>Figure 2: Demonstrates relationship between chemical similarity of compounds and the mean transcriptional similarity values</h5>",
      "body_md": "#Relationship between transcriptional and chemical similarity\r\n\r\n[Notebook](https://github.com/sabrinalchen/drugbank-similarity/blob/master/L1000.ipynb)\r\n\r\n##Objective:\r\nDetermine the correlation and visualize the relationship between L1000 transcriptional and chemical compound similarity.\r\n\r\n##The data:\r\nWe extracted [L1000 perturbation data](\r\nhttp://files.figshare.com/2166122/consensus_drugbank.tsv.gz) which lists drug types with perturbation IDs and [similarity data](http://nbviewer.ipython.org/github/dhimmel/drugbank/blob/9eb1e15ada5e6579e5aea1cae784c10602037b05/similarity.ipynb) which gives a value between zero and one based on the chemical similarity of a pair of drugs. \r\n\r\n##Jointplot comparing chemical and transcriptional similarities:\r\nFrom the imported L1000 perturbation data, we calculated the spearman correlation values for each combination of pair of drugs. This correlation value was labeled as the \"transcriptional similarity.\" This value was graphed against the imported similarity data for each drug pair. From the Jointplot, (which plots both bivariate data on a hexbin plot and univariate data on a histogram,) we concluded that there was no strong correlation visually. The darkest parts of the hexbin graph were grouped at the bottom left of the grid and the graph showed no real positive pattern. Nevertheless, the graph had an extremely small p-value, indicating significance, and leading to a small effect. \r\n\r\n![](https://raw.githubusercontent.com/sabrinalchen/drugbank-similarity/master/figure/similarity2.png)\r\n#####Figure 1: Compares transcriptional similarity for each drug pair and chemical similarity\r\n\r\n##Rounded chemical similarity pointplot:\r\nTo visualize the correlation a different way, the chemical similarity data was rounded off to the nearest tenth and the mean transcriptional value was found for each subset. This data was graphed on a pointplot which demonstrated a clear positive correlation, especially when the chemical similarity reached 0.5. After this point, the correlation was even stronger, increasing steadily as the chemical similarity reached 1.0. It should be noted that as the chemical similarity increased, there were less and less data points to be used in graphing. In fact, the last data point where chemical similarity was 1.0 had only a single data point (as seen in the table of rounded values shown in the cell before the pointplot.)\r\n\r\n![](https://raw.githubusercontent.com/sabrinalchen/drugbank-similarity/master/figure/similarity_mean2.png)\r\n#####Figure 2: Demonstrates relationship between chemical similarity of compounds and the mean transcriptional similarity values",
      "profile": 112,
      "published": "2015-08-03T23:33:50.805103Z",
      "thread": 70,
      "url": "/discussion/calculating-molecular-similarities-between-drugbank-compounds/70#2"
    },
    {
      "body_html": "<p>Great work <a href=\"/u/sabrinachen\" class=\"username\">@sabrinachen</a>! Your analysis reveals many interesting findings.</p>\r\n\r\n<p>First, chemical similarity is a strong indicator that two compounds share a target (Figure 2). One of the most successful target prediction algorithms, named the Similarity Ensemble Approach (SEA) <span class=\"citation\">[<a href=\"/doi/10.1038/nature08506\" class=\"citation\" data-key=\"10.1038/nature08506\">1</a>]</span>, is based on this very observation. Our data shows an enrichment of shared protein interactions above a chemical similarity threshold of 0.5. Interestingly, when binarizing chemical similarity scores, SEA also chose a cutoff of 0.5 (<a href=\"http://salilab.org/~nkhuri/files/systems-pharmacology-lecture4-01262015.pdf#page=19\">source</a>).</p>\r\n\r\n<p>Second, when two proteins share proteins of a specific category, they are more likely to share proteins of other categories (Figure 3). For example, when two compounds share a transporter, they have a 14% chance of sharing an enzyme, compared to 3% otherwise. This trend applies to all categories but is most pronounced between chemical similarity and target similarity.</p>\r\n\r\n<p>Finally, it would be interesting to know how many compound pairs were included for each chemical similarity bin in Figure 2.</p>",
      "body_md": "Great work @sabrinachen! Your analysis reveals many interesting findings.\r\n\r\nFirst, chemical similarity is a strong indicator that two compounds share a target (Figure 2). One of the most successful target prediction algorithms, named the Similarity Ensemble Approach (SEA) [@10.1038/nature08506], is based on this very observation. Our data shows an enrichment of shared protein interactions above a chemical similarity threshold of 0.5. Interestingly, when binarizing chemical similarity scores, SEA also chose a cutoff of 0.5 ([source](http://salilab.org/~nkhuri/files/systems-pharmacology-lecture4-01262015.pdf#page=19)).\r\n\r\nSecond, when two proteins share proteins of a specific category, they are more likely to share proteins of other categories (Figure 3). For example, when two compounds share a transporter, they have a 14% chance of sharing an enzyme, compared to 3% otherwise. This trend applies to all categories but is most pronounced between chemical similarity and target similarity.\r\n\r\nFinally, it would be interesting to know how many compound pairs were included for each chemical similarity bin in Figure 2.",
      "profile": 17,
      "published": "2015-08-04T18:40:15.210241Z",
      "thread": 65,
      "url": "/discussion/protein-target-carrier-transporter-and-enzyme-interactions-in-drugbank/65#3"
    },
    {
      "body_html": "<p>Nice analysis. Chemical similarity appears to be a <em>weak</em> predictor of transcriptional similarity (Figure 1, <span class=\"math\">$$\\rho = 0.02, p = 10 ^ {-66}$$</span>). However, this correlation is highly influenced by the majority of compound pairs where chemical similarity is less than 0.5. As we have <a href=\"http://thinklab.com/discussion/protein-target-carrier-transporter-and-enzyme-interactions-in-drugbank/65#3\">previously noticed</a>, chemical similarity becomes predictive of other types of similarity above 0.5. As seen in Figure 2, the same trend applies to transcriptional similarity. Therefore, within the meaningful range of chemical similarity values, the association looks stronger.</p>\r\n\r\n<p>Nice catch that the highest bin (chemical similarity ≥ 0.95) only has a single compound pair. This is due to our <a href=\"http://thinklab.com/discussion/unifying-drug-vocabularies/40#5\">selection criteria</a> for compounds which aims to avoid redundancy. We have computed chemical similarities for the entire LINCS L1000 perturbation set, so we could rerun this analysis with all perturbagens.</p>",
      "body_md": "Nice analysis. Chemical similarity appears to be a *weak* predictor of transcriptional similarity (Figure 1, $$\\rho = 0.02, p = 10 ^ {-66}$$). However, this correlation is highly influenced by the majority of compound pairs where chemical similarity is less than 0.5. As we have [previously noticed](http://thinklab.com/discussion/protein-target-carrier-transporter-and-enzyme-interactions-in-drugbank/65#3), chemical similarity becomes predictive of other types of similarity above 0.5. As seen in Figure 2, the same trend applies to transcriptional similarity. Therefore, within the meaningful range of chemical similarity values, the association looks stronger.\r\n\r\nNice catch that the highest bin (chemical similarity ≥ 0.95) only has a single compound pair. This is due to our [selection criteria](http://thinklab.com/discussion/unifying-drug-vocabularies/40#5) for compounds which aims to avoid redundancy. We have computed chemical similarities for the entire LINCS L1000 perturbation set, so we could rerun this analysis with all perturbagens.",
      "profile": 17,
      "published": "2015-08-04T19:13:17.098008Z",
      "thread": 70,
      "url": "/discussion/calculating-molecular-similarities-between-drugbank-compounds/70#3"
    },
    {
      "body_html": "<h1>Chemical similarity association with side effect and indication similarity</h1>\r\n\r\n<p><a href=\"https://github.com/sabrinalchen/drugbank-similarity/blob/master/side-effect.ipynb\">Notebook</a></p>\r\n\r\n<h2>Objective:</h2>\r\n\r\n<p>To determine the relationship between side effect similarity and chemical similarity as well as drug indication and chemical similarity</p>\r\n\r\n<h2>Data:</h2>\r\n\r\n<p>We extracted <a href=\"https://github.com/dhimmel/SIDER2/blob/9d585685dbeaba3bbac58024c814ac87521122ad/data/similarities.txt.gz\">side effect and indication similarity data</a> which lists drug pairs (pubchem ID,) and their associated side effect similarity and indication similarity. Side effect similarity deals with the similarity of the side effect when a drug is used to treat a protein. Indication is the term doctors use when a drug treats a disease.In addition, <a href=\"https://github.com/dhimmel/drugbank/blob/e8567eed2dd48ae0694a0960c518763a777845ff/data/mapping/pubchem.tsv\">drugbank data</a> was extracted to convert pubchem ID into drugbank ID. Finally, we extracted <a href=\"http://nbviewer.ipython.org/github/dhimmel/drugbank/blob/9eb1e15ada5e6579e5aea1cae784c10602037b05/similarity.ipynb\">chemical similarity data</a> which gives a value between zero and one based on the chemical similarity of a pair of drugs. </p>\r\n\r\n<h2>Chemical similarity vs substructure jointplot</h2>\r\n\r\n<p>As predicted, a positive correlation was found between these two variables. It seemed from the graph that the two were strongly correlated, especially since the p-value was zero. An interesting trend to note was that the data seemed to be banded and centered around certain values. The smaller bands could be due to the fact that substructure data was rounded off to the nearest hundredth place. However, we are not sure of what the large horizontal bands indicate. </p>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/sabrinalchen/drugbank-similarity/master/figure/substructure.png\" alt=\"\"></p>\r\n\r\n<h5>Figure 1: Compares chemical similarity to substructure and demonstrates a positive correlation</h5>\r\n\r\n<h2>Chemical similarity vs side effect similarity jointplot</h2>\r\n\r\n<p>The graph demonstrated a positive correlation between chemical compound similarity and side effect similarity. To make it easier to read, we took the square root of side effect similarity values and also used logarithmic bins. With the data transformed this way, the correlation was clearer.  Though the correlation coefficient was less than that of the previous comparison, the p-value was zero. </p>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/sabrinalchen/drugbank-similarity/master/figure/side_effect.png\" alt=\"\"></p>\r\n\r\n<h5>Figure 2: Compares chemical similarity to side effect similarity</h5>\r\n\r\n<h2>Chemical similarity vs indication similarity jointplot</h2>\r\n\r\n<p>Though this graph was slightly harder to read than the previous two because of the skewed indication similarity data, the zero p-value showed some degree of significance. Most of the indication values were zero, so even by taking the square root of the values, it was difficult to visualize the positive correlation. We decided to use a different visualization for clearer analysis (see pointplot below.)</p>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/sabrinalchen/drugbank-similarity/master/figure/indication.png\" alt=\"\"></p>\r\n\r\n<h5>Figure 3: Compares chemical similarity to indication similarity</h5>\r\n\r\n<h2>Pointplot</h2>\r\n\r\n<p>Because the correlation was a bit difficult to read in previous graphs (especially in the indication jointplot,) we used a pointplot for a different visualization. The chemical similarity data was rounded off to the nearest tenth and the mean values for both side effect similarity and indication similarity was found for each subset. This data was graphed on a pointplot which demonstrated a clear positive correlation, especially when the chemical similarity passed 0.4. For the side effect similarity, it was easy to see a steady positive correlation, apart from the fall between 0.9 and 1.0 on the chemical similarity axis. This could be attributed to the small number of data points, however. For the indication similarity, the graph showed no real correlation between 0.0 and 0.3, but a steady upward trend began starting from 0.4. From our graphs we concluded that increased chemical compound similarity did indeed indicate an increase for side effect similarity and indication similarity.</p>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/sabrinalchen/drugbank-similarity/master/figure/combined_similarity_mean.png\" alt=\"\"></p>\r\n\r\n<h5>Figure 4: A clearer representation of the association between chemical compound similarity and side effect and indication similarity</h5>",
      "body_md": "#Chemical similarity association with side effect and indication similarity\r\n\r\n[Notebook](https://github.com/sabrinalchen/drugbank-similarity/blob/master/side-effect.ipynb)\r\n\r\n##Objective:\r\nTo determine the relationship between side effect similarity and chemical similarity as well as drug indication and chemical similarity\r\n\r\n##Data:\r\nWe extracted [side effect and indication similarity data](https://github.com/dhimmel/SIDER2/blob/9d585685dbeaba3bbac58024c814ac87521122ad/data/similarities.txt.gz) which lists drug pairs (pubchem ID,) and their associated side effect similarity and indication similarity. Side effect similarity deals with the similarity of the side effect when a drug is used to treat a protein. Indication is the term doctors use when a drug treats a disease.In addition, [drugbank data](https://github.com/dhimmel/drugbank/blob/e8567eed2dd48ae0694a0960c518763a777845ff/data/mapping/pubchem.tsv) was extracted to convert pubchem ID into drugbank ID. Finally, we extracted [chemical similarity data](http://nbviewer.ipython.org/github/dhimmel/drugbank/blob/9eb1e15ada5e6579e5aea1cae784c10602037b05/similarity.ipynb) which gives a value between zero and one based on the chemical similarity of a pair of drugs. \r\n\r\n##Chemical similarity vs substructure jointplot\r\nAs predicted, a positive correlation was found between these two variables. It seemed from the graph that the two were strongly correlated, especially since the p-value was zero. An interesting trend to note was that the data seemed to be banded and centered around certain values. The smaller bands could be due to the fact that substructure data was rounded off to the nearest hundredth place. However, we are not sure of what the large horizontal bands indicate. \r\n\r\n![](https://raw.githubusercontent.com/sabrinalchen/drugbank-similarity/master/figure/substructure.png)\r\n#####Figure 1: Compares chemical similarity to substructure and demonstrates a positive correlation\r\n\r\n##Chemical similarity vs side effect similarity jointplot\r\nThe graph demonstrated a positive correlation between chemical compound similarity and side effect similarity. To make it easier to read, we took the square root of side effect similarity values and also used logarithmic bins. With the data transformed this way, the correlation was clearer.  Though the correlation coefficient was less than that of the previous comparison, the p-value was zero. \r\n\r\n![](https://raw.githubusercontent.com/sabrinalchen/drugbank-similarity/master/figure/side_effect.png)\r\n#####Figure 2: Compares chemical similarity to side effect similarity\r\n\r\n##Chemical similarity vs indication similarity jointplot\r\nThough this graph was slightly harder to read than the previous two because of the skewed indication similarity data, the zero p-value showed some degree of significance. Most of the indication values were zero, so even by taking the square root of the values, it was difficult to visualize the positive correlation. We decided to use a different visualization for clearer analysis (see pointplot below.)\r\n\r\n![](https://raw.githubusercontent.com/sabrinalchen/drugbank-similarity/master/figure/indication.png)\r\n#####Figure 3: Compares chemical similarity to indication similarity\r\n\r\n##Pointplot \r\nBecause the correlation was a bit difficult to read in previous graphs (especially in the indication jointplot,) we used a pointplot for a different visualization. The chemical similarity data was rounded off to the nearest tenth and the mean values for both side effect similarity and indication similarity was found for each subset. This data was graphed on a pointplot which demonstrated a clear positive correlation, especially when the chemical similarity passed 0.4. For the side effect similarity, it was easy to see a steady positive correlation, apart from the fall between 0.9 and 1.0 on the chemical similarity axis. This could be attributed to the small number of data points, however. For the indication similarity, the graph showed no real correlation between 0.0 and 0.3, but a steady upward trend began starting from 0.4. From our graphs we concluded that increased chemical compound similarity did indeed indicate an increase for side effect similarity and indication similarity.\r\n\r\n![](https://raw.githubusercontent.com/sabrinalchen/drugbank-similarity/master/figure/combined_similarity_mean.png)\r\n#####Figure 4: A clearer representation of the association between chemical compound similarity and side effect and indication similarity",
      "profile": 112,
      "published": "2015-08-04T21:19:36.253694Z",
      "thread": 30,
      "url": "/discussion/assessing-the-quality-and-applicability-of-the-sider-2-resource/30#3"
    },
    {
      "body_html": "<p>I contacted Mike Keiser, the human intellect behind SEA, regarding chemical similarity thresholding. Below and with permission, I've posted his reply to our <a href=\"#3\">above</a> comment:</p>\r\n\r\n<blockquote><p>The <a href=\"http://salilab.org/~nkhuri/files/systems-pharmacology-lecture4-01262015.pdf#page=19\">0.5 measure</a> refers to a tanimoto coefficient on daylight path-based fingerprints. It'd be in the supplemental materials and/or methods of Keiser et al, Nat Biotechnol, 2007 <span class=\"citation\">[<a href=\"/doi/10.1038/nbt1284\" class=\"citation\" data-key=\"10.1038/nbt1284\">1</a>]</span>. So I think the closest equivalent in rdkit would be tanimoto instead of dice coefficient, or rdkit-path fingerprints. Using ECFP4 (i.e., Morgan with radius 2 in rdkit) and a tanimoto coefficient, we found cutoffs more around 0.28 (the range can vary pretty substantially depending on fingerprint type used). In general, 0.5 is considered pretty high similarity for ECFP/Morgan fingerprints at least with tanimoto coefficients (I'm less sure of the Dice coefficient equivalents, off-hand).</p></blockquote>",
      "body_md": "I contacted Mike Keiser, the human intellect behind SEA, regarding chemical similarity thresholding. Below and with permission, I've posted his reply to our [above](#3) comment:\r\n\r\n> The [0.5 measure](http://salilab.org/~nkhuri/files/systems-pharmacology-lecture4-01262015.pdf#page=19) refers to a tanimoto coefficient on daylight path-based fingerprints. It'd be in the supplemental materials and/or methods of Keiser et al, Nat Biotechnol, 2007 [@10.1038/nbt1284]. So I think the closest equivalent in rdkit would be tanimoto instead of dice coefficient, or rdkit-path fingerprints. Using ECFP4 (i.e., Morgan with radius 2 in rdkit) and a tanimoto coefficient, we found cutoffs more around 0.28 (the range can vary pretty substantially depending on fingerprint type used). In general, 0.5 is considered pretty high similarity for ECFP/Morgan fingerprints at least with tanimoto coefficients (I'm less sure of the Dice coefficient equivalents, off-hand).",
      "profile": 17,
      "published": "2015-08-05T20:35:16.257715Z",
      "thread": 65,
      "url": "/discussion/protein-target-carrier-transporter-and-enzyme-interactions-in-drugbank/65#4"
    },
    {
      "body_html": "<p>Please note that <a href=\"http://sider-beta.embl.de/\">SIDER 4</a> has just been released.</p>\r\n\r\n<p>Before anyone asks: no there has never been a SIDER 3. We decided to jump to version 4 to make SIDER version numbers consistent with STITCH. This means that compound IDs of SIDER 4 are consistent with those of STITCH 4, and that SIDER 5 will be consistent with STITCH 5 etc.</p>\r\n\r\n<p>I am a bit surprised to see that you use SIDER as a source of drug indications. As is hopefully clear, the focus of SIDER is very much on side effect information. It should thus be no surprise that the quality of the drug indication information is presumably lower than the side effect information.</p>",
      "body_md": "Please note that [SIDER 4](http://sider-beta.embl.de/) has just been released.\r\n\r\nBefore anyone asks: no there has never been a SIDER 3. We decided to jump to version 4 to make SIDER version numbers consistent with STITCH. This means that compound IDs of SIDER 4 are consistent with those of STITCH 4, and that SIDER 5 will be consistent with STITCH 5 etc.\r\n\r\nI am a bit surprised to see that you use SIDER as a source of drug indications. As is hopefully clear, the focus of SIDER is very much on side effect information. It should thus be no surprise that the quality of the drug indication information is presumably lower than the side effect information.",
      "profile": 125,
      "published": "2015-08-08T16:15:09.942557Z",
      "thread": 30,
      "url": "/discussion/assessing-the-quality-and-applicability-of-the-sider-2-resource/30#4"
    },
    {
      "body_html": "<p>It may be worth noting that the work on SIDER actually started as part of a drug-repurposing / off-target-prediction project at EMBL <span class=\"citation\">[<a href=\"/doi/10.1126/science.1158140\" class=\"citation\" data-key=\"10.1126/science.1158140\">1</a>]</span>.</p>\r\n\r\n<p>I do not want to be overly negative, but there is a reason why we only made use of side effects and not indications to calculate drug similarity: it seems very unlikely to me that you would be able to make non-obvious predictions based on the known indications. If drug X is approved for indications A, B, C and D, and drug Y is approved for indications A, B and C, I would consider the prediction that drug Y might also work for indication D to be trivial. Especially if drugs X and Y are similar chemical compounds.</p>\r\n\r\n<p>I believe this is the biggest challenge in computational drug repurposing: how do you predict something that is correct and not obvious? In my experience, this turns out to be much, much harder than to predict something that is just correct.</p>",
      "body_md": "It may be worth noting that the work on SIDER actually started as part of a drug-repurposing / off-target-prediction project at EMBL [@10.1126/science.1158140].\r\n\r\nI do not want to be overly negative, but there is a reason why we only made use of side effects and not indications to calculate drug similarity: it seems very unlikely to me that you would be able to make non-obvious predictions based on the known indications. If drug X is approved for indications A, B, C and D, and drug Y is approved for indications A, B and C, I would consider the prediction that drug Y might also work for indication D to be trivial. Especially if drugs X and Y are similar chemical compounds.\r\n\r\nI believe this is the biggest challenge in computational drug repurposing: how do you predict something that is correct and not obvious? In my experience, this turns out to be much, much harder than to predict something that is just correct.",
      "profile": 125,
      "published": "2015-08-08T16:18:22.874851Z",
      "thread": 30,
      "url": "/discussion/assessing-the-quality-and-applicability-of-the-sider-2-resource/30#5"
    },
    {
      "body_html": "<p>The TISSUES resource uses the proteins in the latest version of <a href=\"http://string-db.org/\">STRING</a> as baseline. If you need to map the <a href=\"http://www.ensembl.org/\">Ensembl</a> protein identifiers to other database identifiers, the best thing to do is thus to use either the <a href=\"http://string-db.org/newstring_download/protein.aliases.v10.txt.gz\">STRING alias file</a> or one of the specific mapping files available <a href=\"ftp://string-db.org/STRING/10.0/mapping_files/\">here</a>.</p>",
      "body_md": "The TISSUES resource uses the proteins in the latest version of [STRING](http://string-db.org/) as baseline. If you need to map the [Ensembl](http://www.ensembl.org/) protein identifiers to other database identifiers, the best thing to do is thus to use either the [STRING alias file](http://string-db.org/newstring_download/protein.aliases.v10.txt.gz) or one of the specific mapping files available [here](ftp://string-db.org/STRING/10.0/mapping_files/).",
      "profile": 125,
      "published": "2015-08-08T16:28:14.150286Z",
      "thread": 91,
      "url": "/discussion/the-tissues-resource-for-the-tissue-specificity-of-genes/91#2"
    },
    {
      "body_html": "<p>Introducing a score cutoff does not sound like the right way to go about it to me. The problem is that there is no right cutoff; wherever you put it, what scores just above the cutoff is almost exactly as reliable as what scores just below the cutoff.</p>\r\n\r\n<p>I know that life is much simpler if you do not have to deal with confidence scores. However, the moment you take associations with confidence scores and make them binary by applying an arbitrary cutoff, you are throwing away information. For this reason, you will almost always be better off having a method that can deal with confidence scores in a sensible manner and only apply a cutoff on your predictions in the very end after all available evidence has been combined.</p>",
      "body_md": "Introducing a score cutoff does not sound like the right way to go about it to me. The problem is that there is no right cutoff; wherever you put it, what scores just above the cutoff is almost exactly as reliable as what scores just below the cutoff.\r\n\r\nI know that life is much simpler if you do not have to deal with confidence scores. However, the moment you take associations with confidence scores and make them binary by applying an arbitrary cutoff, you are throwing away information. For this reason, you will almost always be better off having a method that can deal with confidence scores in a sensible manner and only apply a cutoff on your predictions in the very end after all available evidence has been combined.",
      "profile": 125,
      "published": "2015-08-08T16:28:42.632510Z",
      "thread": 91,
      "url": "/discussion/the-tissues-resource-for-the-tissue-specificity-of-genes/91#3"
    },
    {
      "body_html": "<p><a href=\"/u/larsjuhljensen\" class=\"username\">@larsjuhljensen</a>, great timing and thanks for the heads up!</p>\r\n\r\n<blockquote><p>I am a bit surprised to see that you use SIDER as a source of drug indications.</p></blockquote>\r\n\r\n<p>SIDER was one of the first resources we played with for this project. At the time, we decided to investigate the indications because 1) they were there and 2) we were unaware of other indication databases.</p>\r\n\r\n<p>Since then we've spent considerable time on <a href=\"http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d21\">creating a catalog of indications</a>. We ended up combining four indication databases, one of which (<a href=\"http://knowledgemap.mc.vanderbilt.edu/research/content/MEDI\">MEDI</a> <span class=\"citation\">[<a href=\"/doi/10.1136/amiajnl-2014-002954\" class=\"citation\" data-key=\"10.1136/amiajnl-2014-002954\">1</a>]</span>) uses SIDER 2 as an input. We have now moved on to a final stage of <a href=\"http://thinklab.com/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d95\">expert curation</a>.</p>\r\n\r\n<blockquote><p>it seems very unlikely to me that you would be able to make non-obvious predictions based on the known indications.</p></blockquote>\r\n\r\n<p>Our heterogeneous network edge prediction <a href=\"http://het.io/hnep/\">method</a> is a supervised method. Therefore, we need efficacious indications to train our model. However, I do have hope for some metapaths containing an indication metaedge to produce non-obvious predictions. For example, </p>\r\n\r\n<ul><li>disease A has 3 indicated drugs (X, Y, Z)</li><li>X, Y, Z elicit similar transcriptional responses (in <a href=\"http://thinklab.com/discussion/computing-consensus-transcriptional-profiles-for-compounds-using-lincs/43\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d43\">LINCS L1000 data</a>)</li><li>W elicits a similar transcriptional response to X, Y, and Z</li><li>drug W may treat disease A</li></ul>\r\n\r\n<p>I agree that repurposing using only the bipartite indication network will produce mostly obvious predictions. However, our approach is capable of much more!</p>",
      "body_md": "@larsjuhljensen, great timing and thanks for the heads up!\r\n\r\n> I am a bit surprised to see that you use SIDER as a source of drug indications.\r\n\r\nSIDER was one of the first resources we played with for this project. At the time, we decided to investigate the indications because 1) they were there and 2) we were unaware of other indication databases.\r\n\r\nSince then we've spent considerable time on [creating a catalog of indications](http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21). We ended up combining four indication databases, one of which ([MEDI](http://knowledgemap.mc.vanderbilt.edu/research/content/MEDI) [@10.1136/amiajnl-2014-002954]) uses SIDER 2 as an input. We have now moved on to a final stage of [expert curation](http://thinklab.com/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95).\r\n\r\n> it seems very unlikely to me that you would be able to make non-obvious predictions based on the known indications.\r\n\r\nOur heterogeneous network edge prediction [method](http://het.io/hnep/) is a supervised method. Therefore, we need efficacious indications to train our model. However, I do have hope for some metapaths containing an indication metaedge to produce non-obvious predictions. For example, \r\n\r\n+ disease A has 3 indicated drugs (X, Y, Z)\r\n+ X, Y, Z elicit similar transcriptional responses (in [LINCS L1000 data](http://thinklab.com/discussion/computing-consensus-transcriptional-profiles-for-compounds-using-lincs/43))\r\n+ W elicits a similar transcriptional response to X, Y, and Z\r\n+ drug W may treat disease A\r\n\r\nI agree that repurposing using only the bipartite indication network will produce mostly obvious predictions. However, our approach is capable of much more!",
      "profile": 17,
      "published": "2015-08-08T16:39:10.962107Z",
      "thread": 30,
      "url": "/discussion/assessing-the-quality-and-applicability-of-the-sider-2-resource/30#6"
    },
    {
      "body_html": "<blockquote><p>Introducing a score cutoff does not sound like the right way to go about it to me.</p></blockquote>\r\n\r\n<p>I totally agree that cutoffs are suboptimal because they require arbitrary decision-making, conflate levels of confidence, and throw away information. In the long term, we hope to modify our method to enable weighted edge and to investigate other methods that allow weights (such as data fusion <span class=\"citation\">[<a href=\"/doi/10.1109/TPAMI.2014.2343973\" class=\"citation\" data-key=\"10.1109/TPAMI.2014.2343973\">1</a>, <a href=\"/doi/10.1038/srep03202\" class=\"citation\" data-key=\"10.1038/srep03202\">2</a>]</span>). However, in the short term, I want to proceed with unweighted edges and understand the sacrifice.</p>\r\n\r\n<p><a href=\"/u/larsjuhljensen\" class=\"username\">@larsjuhljensen</a>, you may disagree with implementing cutoffs, but by providing \"confidence scores that are comparable between datasets\" <span class=\"citation\">[<a href=\"/doi/10.7717/peerj.1054\" class=\"citation\" data-key=\"10.7717/peerj.1054\">3</a>]</span>, you have made the life of binners like me much more pleasant (:</p>\r\n\r\n<p>For the experimental dataset from TISSUES, one cutoff I envision is 2 or more sources reporting scores ≥ 3 per gene–tissue relation. Does this sound reasonable?</p>\r\n\r\n<blockquote><p>The TISSUES resource uses the proteins in the latest version of STRING as baseline.</p></blockquote>\r\n\r\n<p>We have begun processing the TISSUES datasets (<a href=\"https://github.com/dhimmel/tissues/blob/b7711d18e51ff1a8e91837354415d271bf975907/tissues.ipynb\">notebook</a>). I was mapping Ensembl proteins using <a href=\"https://github.com/hammerlab/pyensembl\"><code>pyensembl</code></a>, but will switch to the <a href=\"ftp://string-db.org/STRING/10.0/mapping_files/entrez_mappings/\">mapping</a> suggested by <a href=\"/u/larsjuhljensen\" class=\"username\">@larsjuhljensen</a>.</p>",
      "body_md": "> Introducing a score cutoff does not sound like the right way to go about it to me.\r\n\r\nI totally agree that cutoffs are suboptimal because they require arbitrary decision-making, conflate levels of confidence, and throw away information. In the long term, we hope to modify our method to enable weighted edge and to investigate other methods that allow weights (such as data fusion [@10.1109/TPAMI.2014.2343973 @10.1038/srep03202]). However, in the short term, I want to proceed with unweighted edges and understand the sacrifice.\r\n\r\n@larsjuhljensen, you may disagree with implementing cutoffs, but by providing \"confidence scores that are comparable between datasets\" [@10.7717/peerj.1054], you have made the life of binners like me much more pleasant (:\r\n\r\nFor the experimental dataset from TISSUES, one cutoff I envision is 2 or more sources reporting scores ≥ 3 per gene--tissue relation. Does this sound reasonable?\r\n\r\n> The TISSUES resource uses the proteins in the latest version of STRING as baseline.\r\n\r\nWe have begun processing the TISSUES datasets ([notebook](https://github.com/dhimmel/tissues/blob/b7711d18e51ff1a8e91837354415d271bf975907/tissues.ipynb)). I was mapping Ensembl proteins using [`pyensembl`](https://github.com/hammerlab/pyensembl), but will switch to the [mapping](ftp://string-db.org/STRING/10.0/mapping_files/entrez_mappings/) suggested by @larsjuhljensen.",
      "profile": 17,
      "published": "2015-08-08T17:15:00.985960Z",
      "thread": 91,
      "url": "/discussion/the-tissues-resource-for-the-tissue-specificity-of-genes/91#4"
    },
    {
      "body_html": "<p>My gut feeling is that your cutoff is too stringent. If you want support from at least two different experimental datasets, I would not put the cutoff at 3. I would at most put it at 2.</p>\r\n\r\n<p>However, I think there are more fundamental problems with that approach than just the numeric cutoff. Not all tissues were included in all datasets. This means that some tissues will be entirely lost if you require support from two datasets. Also, I do not understand why you would want to exclude the other channels (knowledge and text mining). Having, for example, text mining to support something that would otherwise be based on only a single dataset is very valuable.</p>\r\n\r\n<p>If you want to enforce a hard cutoff to make things binary, I would urge you to at least take the integrated scores that takes everything into account and apply the cutoff to that. In this case a score of 3 might be appropriate. Applying cutoffs to the scores of individual datasets before combining them is in my opinion a fundamentally bad idea.</p>",
      "body_md": "My gut feeling is that your cutoff is too stringent. If you want support from at least two different experimental datasets, I would not put the cutoff at 3. I would at most put it at 2.\r\n\r\nHowever, I think there are more fundamental problems with that approach than just the numeric cutoff. Not all tissues were included in all datasets. This means that some tissues will be entirely lost if you require support from two datasets. Also, I do not understand why you would want to exclude the other channels (knowledge and text mining). Having, for example, text mining to support something that would otherwise be based on only a single dataset is very valuable.\r\n\r\nIf you want to enforce a hard cutoff to make things binary, I would urge you to at least take the integrated scores that takes everything into account and apply the cutoff to that. In this case a score of 3 might be appropriate. Applying cutoffs to the scores of individual datasets before combining them is in my opinion a fundamentally bad idea.",
      "profile": 125,
      "published": "2015-08-08T17:41:25.920080Z",
      "thread": 91,
      "url": "/discussion/the-tissues-resource-for-the-tissue-specificity-of-genes/91#5"
    },
    {
      "body_html": "<h1>Initial release of consensus signatures</h1>\r\n\r\n<p>We have computed consensus transcriptional signatures for LINCS L1000 perturbations. We have released <a href=\"https://github.com/dhimmel/lincs/tree/b36dfd82b6b1aa0b0c45ec905cb2548ddf7dc53e/data/consensi\">datasets</a> <span class=\"citation\">[<a href=\"/doi/10.5281/zenodo.27229\" class=\"citation\" data-key=\"10.5281/zenodo.27229\">1</a>]</span> for the following pertubation sets:</p>\r\n\r\n<ul><li>LINCS pert_ids: 38,327 consensi (<a href=\"https://github.com/dhimmel/lincs/blob/b36dfd82b6b1aa0b0c45ec905cb2548ddf7dc53e/data/consensi/consensi-pert_id.tsv.gz\">download</a>)</li><li>DrugBank compounds: 1,170 consensi (<a href=\"https://github.com/dhimmel/lincs/blob/b36dfd82b6b1aa0b0c45ec905cb2548ddf7dc53e/data/consensi/consensi-drugbank.tsv.gz\">download</a>)</li><li>Gene knockdowns: 4,363 consensi (<a href=\"https://github.com/dhimmel/lincs/blob/b36dfd82b6b1aa0b0c45ec905cb2548ddf7dc53e/data/consensi/consensi-knockdown.tsv.gz\">download</a>)</li><li>Gene over-expressions: 2,471 consensi (<a href=\"https://github.com/dhimmel/lincs/blob/b36dfd82b6b1aa0b0c45ec905cb2548ddf7dc53e/data/consensi/consensi-overexpression.tsv.gz\">download</a>)</li></ul>\r\n\r\n<p>The datasets are tsv-formatted with perturbations as rows and genes as columns. We only report expression values for the 978 assayed genes. Non-gold signatures were omitted. We set a <a href=\"#4\">minimum signature weight</a> of 0.05 and combined z-scores using <a href=\"#5\">Stouffer's method</a>.</p>",
      "body_md": "# Initial release of consensus signatures\r\n\r\nWe have computed consensus transcriptional signatures for LINCS L1000 perturbations. We have released [datasets](https://github.com/dhimmel/lincs/tree/b36dfd82b6b1aa0b0c45ec905cb2548ddf7dc53e/data/consensi) [@10.5281/zenodo.27229] for the following pertubation sets:\r\n\r\n+ LINCS pert_ids: 38,327 consensi ([download](https://github.com/dhimmel/lincs/blob/b36dfd82b6b1aa0b0c45ec905cb2548ddf7dc53e/data/consensi/consensi-pert_id.tsv.gz))\r\n+ DrugBank compounds: 1,170 consensi ([download](https://github.com/dhimmel/lincs/blob/b36dfd82b6b1aa0b0c45ec905cb2548ddf7dc53e/data/consensi/consensi-drugbank.tsv.gz))\r\n+ Gene knockdowns: 4,363 consensi ([download](https://github.com/dhimmel/lincs/blob/b36dfd82b6b1aa0b0c45ec905cb2548ddf7dc53e/data/consensi/consensi-knockdown.tsv.gz))\r\n+ Gene over-expressions: 2,471 consensi ([download](https://github.com/dhimmel/lincs/blob/b36dfd82b6b1aa0b0c45ec905cb2548ddf7dc53e/data/consensi/consensi-overexpression.tsv.gz))\r\n\r\nThe datasets are tsv-formatted with perturbations as rows and genes as columns. We only report expression values for the 978 assayed genes. Non-gold signatures were omitted. We set a [minimum signature weight](#4) of 0.05 and combined z-scores using [Stouffer's method](#5).",
      "profile": 17,
      "published": "2015-08-08T20:12:51.051081Z",
      "thread": 43,
      "url": "/discussion/computing-consensus-transcriptional-profiles-for-lincs-l1000-perturbations/43#6"
    },
    {
      "body_html": "<h1>Genetic perturbation edges</h1>\r\n\r\n<p><a href=\"/u/alessandrodidonna\" class=\"username\">@alessandrodidonna</a>, thanks for the recommendation. You have motivated to us to add four new gene–gene metaedges:</p>\r\n\r\n<ul><li>Gene → knockdown downregulates → Gene</li><li>Gene → knockdown upregulates → Gene</li><li>Gene → overexpression downregulates → Gene</li><li>Gene → overexpression upregulates → Gene</li></ul>\r\n\r\n<p>These will be our first directed edges, so it will be exciting to stress test our support for directed edges, a feature that we <a href=\"https://github.com/dhimmel/hetio/blob/340b5f3572e29a766cb103b0883796323f983e97/hetio/graph.py#L322\">designed</a> our implementation to support.</p>\r\n\r\n<p>We'll be taking the data from <a href=\"http://www.lincscloud.org/\">LINCS L1000</a> which contains a large number of genetic perturbation experiments:</p>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/dhimmel/rephetio/b4ccfe08be839a4caa4b4a0e2b918b03d50cde65/figure/lincs-l1000-synopsys.png\" alt=\"\"></p>\r\n\r\n<p><a href=\"http://thinklab.com/discussion/computing-consensus-transcriptional-profiles-for-compounds-using-lincs/43#6\">Read more</a> about our consensus signatures for gene knockdowns (of 4,363 genes) and overexpressions (of 2,471 genes).</p>",
      "body_md": "# Genetic perturbation edges\r\n\r\n@alessandrodidonna, thanks for the recommendation. You have motivated to us to add four new gene--gene metaedges:\r\n\r\n+ Gene → knockdown downregulates → Gene\r\n+ Gene → knockdown upregulates → Gene\r\n+ Gene → overexpression downregulates → Gene\r\n+ Gene → overexpression upregulates → Gene\r\n\r\nThese will be our first directed edges, so it will be exciting to stress test our support for directed edges, a feature that we [designed](https://github.com/dhimmel/hetio/blob/340b5f3572e29a766cb103b0883796323f983e97/hetio/graph.py#L322) our implementation to support.\r\n\r\nWe'll be taking the data from [LINCS L1000](http://www.lincscloud.org/) which contains a large number of genetic perturbation experiments:\r\n\r\n![](https://raw.githubusercontent.com/dhimmel/rephetio/b4ccfe08be839a4caa4b4a0e2b918b03d50cde65/figure/lincs-l1000-synopsys.png)\r\n\r\n[Read more](http://thinklab.com/discussion/computing-consensus-transcriptional-profiles-for-compounds-using-lincs/43#6) about our consensus signatures for gene knockdowns (of 4,363 genes) and overexpressions (of 2,471 genes).",
      "profile": 17,
      "published": "2015-08-08T20:37:06.568177Z",
      "thread": 22,
      "url": "/discussion/suggestions-for-additional-information-types/22#6"
    },
    {
      "body_html": "<h1>Knowledge biased and unbiased edges</h1>\r\n\r\n<p>Thanks to <a href=\"/u/b_good\" class=\"username\">@b_good</a>'s suggestion for text mining and curated databases, we have incorporated several edges that are subject to knowledge bias.</p>\r\n\r\n<p>Text mining edges include:</p>\r\n\r\n<ul><li><em>Disease — causation — Symptom</em> edges from MEDLINE <a href=\"http://thinklab.com/discussion/mining-knowledge-from-medline-articles-and-their-indexed-mesh-terms/67#4\">topic cooccurrence</a></li><li><em>Disease — similarity — Disease</em> edges from MEDLINE <a href=\"http://thinklab.com/discussion/disease-similarity-from-medline-topic-cooccurrence/93\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d93\">topic cooccurrence</a></li><li><em>Disease — localization — Anatomy</em> edges from MEDLINE <a href=\"http://thinklab.com/discussion/mining-knowledge-from-medline-articles-and-their-indexed-mesh-terms/67#5\">topic cooccurrence</a></li></ul>\r\n\r\n<p>Literature curation edges include:</p>\r\n\r\n<ul><li><em>Compound — target — Gene</em> edges from <a href=\"http://thinklab.com/discussion/protein-target-carrier-transporter-and-enzyme-interactions-in-drugbank/65\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d65\">DrugBank</a> and <a href=\"http://thinklab.com/discussion/integrating-drug-target-information-from-bindingdb/53\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d53\">BindingDB</a></li><li><em>Gene — interaction — Gene</em> edges from <a href=\"http://thinklab.com/discussion/creating-a-catalog-of-protein-interactions/85\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d85\">curated databases</a></li><li><em>Gene — membership — Pathway</em> edges from <a href=\"http://thinklab.com/discussion/adding-pathway-resources-to-your-network/72\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d72\">WikiPathways and MSigDB</a></li><li><em>Gene — membership — GO Domain</em> edges from the <a href=\"http://thinklab.com/discussion/compiling-gene-ontology-annotations-into-an-easy-to-use-format/39#8\">Gene Ontology</a> annotations</li><li><em>Gene — function — Disease</em> edges from the <a href=\"http://thinklab.com/discussion/functional-disease-annotations-for-genes-using-doaf/94\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d94\">DOAF</a></li></ul>\r\n\r\n<p>And we have several edges from systematic technologies that are not subject to knowledge biases.</p>\r\n\r\n<ul><li><em>Disease — expression — Anatomy</em> edges from <a href=\"http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#8\">Bgee</a> and <a href=\"http://thinklab.com/discussion/the-tissues-resource-for-the-tissue-specificity-of-genes/91\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d91\">TISSUES</a></li><li><em>Disease — up/down-regulation — Gene</em> edges from <a href=\"http://thinklab.com/discussion/stargeo-expression-signatures-for-disease-using-crowdsourced-geo-annotation/96\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d96\">STARGEO</a></li><li><em>Gene — membership — Perturbation Gene Set</em> edges from MSigDB</li><li><em>Gene — interaction — Gene</em> edges from <a href=\"http://thinklab.com/discussion/creating-a-catalog-of-protein-interactions/85#7\">Y2H experiments</a></li><li><em>Gene — evolution — Gene</em> edges from <a href=\"http://thinklab.com/discussion/selecting-informative-erc-evolutionary-rate-covariation-values-between-genes/57\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d57\">evolutionary rate covariation</a></li><li><em>Gene — knowdown up/down-reglulation — Gene</em> edges from <a href=\"http://thinklab.com/discussion/suggestions-for-additional-information-types/22#6\">LINCS L1000</a></li><li><em>Compound — up/down-regulation — Gene</em> from <a href=\"http://thinklab.com/discussion/computing-consensus-transcriptional-profiles-for-compounds-using-lincs/43#6\">LINCS L1000</a></li></ul>\r\n\r\n<p>Thus, for each edge we will create an <code>unbiased</code> attribute which takes a <code>True</code> or <code>False</code> value. <strong>Using our network masking feature, we can easily switch between using the whole network or only the knowledge-unbiased portion.</strong></p>\r\n\r\n<p>Some metaedges will contain a mix of biased and unbiased edges. For example, protein interactions based on their source database. When both a biased and unbiased source contribute an edge, we will give precedence to the unbiased designation.</p>",
      "body_md": "# Knowledge biased and unbiased edges\r\n\r\nThanks to @b_good's suggestion for text mining and curated databases, we have incorporated several edges that are subject to knowledge bias.\r\n\r\nText mining edges include:\r\n\r\n+ *Disease -- causation -- Symptom* edges from MEDLINE [topic cooccurrence](http://thinklab.com/discussion/mining-knowledge-from-medline-articles-and-their-indexed-mesh-terms/67#4)\r\n+ *Disease -- similarity -- Disease* edges from MEDLINE [topic cooccurrence](http://thinklab.com/discussion/disease-similarity-from-medline-topic-cooccurrence/93)\r\n+ *Disease -- localization -- Anatomy* edges from MEDLINE [topic cooccurrence](http://thinklab.com/discussion/mining-knowledge-from-medline-articles-and-their-indexed-mesh-terms/67#5)\r\n\r\nLiterature curation edges include:\r\n\r\n+  *Compound -- target -- Gene* edges from [DrugBank](http://thinklab.com/discussion/protein-target-carrier-transporter-and-enzyme-interactions-in-drugbank/65) and [BindingDB](http://thinklab.com/discussion/integrating-drug-target-information-from-bindingdb/53)\r\n+ *Gene -- interaction -- Gene* edges from [curated databases](http://thinklab.com/discussion/creating-a-catalog-of-protein-interactions/85)\r\n+ *Gene -- membership -- Pathway* edges from [WikiPathways and MSigDB](http://thinklab.com/discussion/adding-pathway-resources-to-your-network/72)\r\n+ *Gene -- membership -- GO Domain* edges from the [Gene Ontology](http://thinklab.com/discussion/compiling-gene-ontology-annotations-into-an-easy-to-use-format/39#8) annotations\r\n+ *Gene -- function -- Disease* edges from the [DOAF](http://thinklab.com/discussion/functional-disease-annotations-for-genes-using-doaf/94)\r\n\r\nAnd we have several edges from systematic technologies that are not subject to knowledge biases.\r\n\r\n+ *Disease -- expression -- Anatomy* edges from [Bgee](http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#8) and [TISSUES](http://thinklab.com/discussion/the-tissues-resource-for-the-tissue-specificity-of-genes/91)\r\n+ *Disease -- up/down-regulation -- Gene* edges from [STARGEO](http://thinklab.com/discussion/stargeo-expression-signatures-for-disease-using-crowdsourced-geo-annotation/96)\r\n+ *Gene -- membership -- Perturbation Gene Set* edges from MSigDB\r\n+ *Gene -- interaction -- Gene* edges from [Y2H experiments](http://thinklab.com/discussion/creating-a-catalog-of-protein-interactions/85#7)\r\n+ *Gene -- evolution -- Gene* edges from [evolutionary rate covariation](http://thinklab.com/discussion/selecting-informative-erc-evolutionary-rate-covariation-values-between-genes/57)\r\n+ *Gene -- knowdown up/down-reglulation -- Gene* edges from [LINCS L1000](http://thinklab.com/discussion/suggestions-for-additional-information-types/22#6)\r\n+ *Compound -- up/down-regulation -- Gene* from [LINCS L1000](http://thinklab.com/discussion/computing-consensus-transcriptional-profiles-for-compounds-using-lincs/43#6)\r\n\r\nThus, for each edge we will create an `unbiased` attribute which takes a `True` or `False` value. **Using our network masking feature, we can easily switch between using the whole network or only the knowledge-unbiased portion.**\r\n\r\nSome metaedges will contain a mix of biased and unbiased edges. For example, protein interactions based on their source database. When both a biased and unbiased source contribute an edge, we will give precedence to the unbiased designation.",
      "profile": 17,
      "published": "2015-08-08T22:27:20.277476Z",
      "thread": 48,
      "url": "/discussion/text-as-a-resource-for-network-population/48#3"
    },
    {
      "body_html": "<p><a href=\"/u/larsjuhljensen\" class=\"username\">@larsjuhljensen</a>, we will use the integrated dataset as the primary resource. However, the integrated dataset is subject to knowledge biases (stemming from text mining and UniProtKB). Since we <a href=\"http://thinklab.com/discussion/text-as-a-resource-for-network-population/48#3\">want the option</a> to subset the network to only include knowledge-unbaised edges, we would also like a consolidated score using only the experimental dataset.</p>\r\n\r\n<p>In other words, if a gene–tissue edge scores above the cutoff in the consolidated experimental dataset, it's considered unbiased. However, if it only passes the cutoff in the integrated dataset, it's considered biased.</p>\r\n\r\n<p>So that leaves one remaining question: <strong>how to create an integrated score using only experimental evidence?</strong></p>\r\n\r\n<blockquote><p>some tissues will be entirely lost if you require support from two datasets.</p></blockquote>\r\n\r\n<p>Let's not worry to much about uniform coverage of tissues. Our approach can handle nonuniform network sparsity and uniform coverage is unfeasible in most cases.</p>",
      "body_md": "@larsjuhljensen, we will use the integrated dataset as the primary resource. However, the integrated dataset is subject to knowledge biases (stemming from text mining and UniProtKB). Since we [want the option](http://thinklab.com/discussion/text-as-a-resource-for-network-population/48#3) to subset the network to only include knowledge-unbaised edges, we would also like a consolidated score using only the experimental dataset.\r\n\r\nIn other words, if a gene--tissue edge scores above the cutoff in the consolidated experimental dataset, it's considered unbiased. However, if it only passes the cutoff in the integrated dataset, it's considered biased.\r\n\r\nSo that leaves one remaining question: **how to create an integrated score using only experimental evidence?**\r\n\r\n> some tissues will be entirely lost if you require support from two datasets.\r\n\r\nLet's not worry to much about uniform coverage of tissues. Our approach can handle nonuniform network sparsity and uniform coverage is unfeasible in most cases.",
      "profile": 17,
      "published": "2015-08-08T22:39:34.502949Z",
      "thread": 91,
      "url": "/discussion/the-tissues-resource-for-the-tissue-specificity-of-genes/91#6"
    },
    {
      "body_html": "<p>SIDER is a project to extract side effects from drug labels <span class=\"citation\">[<a href=\"/doi/10.1038/msb.2009.98\" class=\"citation\" data-key=\"10.1038/msb.2009.98\">1</a>]</span>, originally motivated by off-target prediction <span class=\"citation\">[<a href=\"/doi/10.1126/science.1158140\" class=\"citation\" data-key=\"10.1126/science.1158140\">2</a>]</span>. We <a href=\"http://thinklab.com/discussion/assessing-the-quality-and-applicability-of-the-sider-2-resource/30#1\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d30\">evaluated version 2</a> and produced an <a href=\"http://git.dhimmel.com/SIDER2/\">online tutorial</a>. We found that side effect similarity was a <a href=\"http://thinklab.com/discussion/assessing-the-quality-and-applicability-of-the-sider-2-resource/30#3\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d30\">weak predictor</a> of chemical and indication similarity.</p>\r\n\r\n<p>Just two days ago, <a href=\"http://sideeffects.embl.de/\">version 4</a> was released. Here, we will detail our extraction of side effects from SIDER4.</p>",
      "body_md": "SIDER is a project to extract side effects from drug labels [@10.1038/msb.2009.98], originally motivated by off-target prediction [@10.1126/science.1158140]. We [evaluated version 2](http://thinklab.com/discussion/assessing-the-quality-and-applicability-of-the-sider-2-resource/30#1) and produced an [online tutorial](http://git.dhimmel.com/SIDER2/). We found that side effect similarity was a [weak predictor](http://thinklab.com/discussion/assessing-the-quality-and-applicability-of-the-sider-2-resource/30#3) of chemical and indication similarity.\r\n\r\nJust two days ago, [version 4](http://sideeffects.embl.de/) was released. Here, we will detail our extraction of side effects from SIDER4.",
      "profile": 17,
      "published": "2015-08-08T23:36:57.882948Z",
      "thread": 97,
      "url": "/discussion/extracting-side-effects-from-sider-4/97"
    },
    {
      "body_html": "<h1>Data release formatting</h1>\r\n\r\n<p>We ran into some issues when parsing the SIDER4 datasets. In defense of the creators, version 4 is still in beta and hasn't become the default version (the url was <a href=\"http://thinklab.com/discussion/assessing-the-quality-and-applicability-of-the-sider-2-resource/30#4\">provided to us</a> by a project member).</p>\r\n\r\n<p>The remainder of the post refers to <a href=\"https://github.com/dhimmel/SIDER4/blob/master/SIDER4.ipynb\">this notebook</a>. I ran into the following issues:</p>\r\n\r\n<ul><li><code>label_mapping.tsv.gz</code> is strangely encoded and/or is improperly tab-delimited</li><li><code>meddra_all_indications.tsv.gz</code> is not documented in the README</li></ul>\r\n\r\n<p><a href=\"/u/larsjuhljensen\" class=\"username\">@larsjuhljensen</a>, are you the right contact for this project?</p>",
      "body_md": "# Data release formatting\r\n\r\nWe ran into some issues when parsing the SIDER4 datasets. In defense of the creators, version 4 is still in beta and hasn't become the default version (the url was [provided to us](http://thinklab.com/discussion/assessing-the-quality-and-applicability-of-the-sider-2-resource/30#4) by a project member).\r\n\r\nThe remainder of the post refers to [this notebook](https://github.com/dhimmel/SIDER4/blob/master/SIDER4.ipynb). I ran into the following issues:\r\n\r\n+ `label_mapping.tsv.gz` is strangely encoded and/or is improperly tab-delimited\r\n+ `meddra_all_indications.tsv.gz` is not documented in the README\r\n\r\n@larsjuhljensen, are you the right contact for this project?",
      "profile": 17,
      "published": "2015-08-08T23:49:37.750250Z",
      "thread": 97,
      "url": "/discussion/extracting-side-effects-from-sider-4/97#2"
    },
    {
      "body_html": "<p>The way we currently calculate the integrated score can obviously be applied to any subset of evidence channels and sources (e.g. all sources in the experiments channel, or all sources in the experiments channel except HPA-IHC).</p>\r\n\r\n<p>First, we convert all the confidence scores (<span class=\"math\">$$s_{ijk}$$</span>) between 0 and 5 to pseudo-probabilities (<span class=\"math\">$$p_{ijk}$$</span>) between 0 and 1 by simply dividing with 5. Here <span class=\"math\">$$i$$</span> and <span class=\"math\">$$j$$</span> are the two entities (proteins, tissues, diseases, etc.) and <span class=\"math\">$$k$$</span> is the channel or source. Next, assuming independence between the different types of evidence, we define the combined pseudo-probability for two entities as:</p>\r\n\r\n<div class=\"math\">$$$p_{ij} = 1-\\prod_{k}{(1-p_{ijk})}$$$</div>\r\n\r\n<p>Finally, we convert <span class=\"math\">$$p_{ij}$$</span> back to <span class=\"math\">$$s_{ij}$$</span> by simply multiplying with 5.</p>\r\n\r\n<p>This is admitted <em>ad hoc</em> and has not yet been benchmarked or otherwise compared to other alternatives. The major assumption here is that you can convert confidence scores to some sort of probabilities by simply dividing with 5, which is obviously an oversimplification. There is also the assumption of independence, but I believe this is less of a problem. The formula for combining probabilities is very similar to the <a href=\"http://string-db.org/\">STRING</a> scoring scheme, which has been extensively tested.</p>",
      "body_md": "The way we currently calculate the integrated score can obviously be applied to any subset of evidence channels and sources (e.g. all sources in the experiments channel, or all sources in the experiments channel except HPA-IHC).\r\n\r\nFirst, we convert all the confidence scores ($$s_{ijk}$$) between 0 and 5 to pseudo-probabilities ($$p_{ijk}$$) between 0 and 1 by simply dividing with 5. Here $$i$$ and $$j$$ are the two entities (proteins, tissues, diseases, etc.) and $$k$$ is the channel or source. Next, assuming independence between the different types of evidence, we define the combined pseudo-probability for two entities as:\r\n\r\n$$$p_{ij} = 1-\\prod_{k}{(1-p_{ijk})}$$$\r\n\r\nFinally, we convert $$p_{ij}$$ back to $$s_{ij}$$ by simply multiplying with 5.\r\n\r\nThis is admitted *ad hoc* and has not yet been benchmarked or otherwise compared to other alternatives. The major assumption here is that you can convert confidence scores to some sort of probabilities by simply dividing with 5, which is obviously an oversimplification. There is also the assumption of independence, but I believe this is less of a problem. The formula for combining probabilities is very similar to the [STRING](http://string-db.org/) scoring scheme, which has been extensively tested.",
      "profile": 125,
      "published": "2015-08-09T08:47:44.404612Z",
      "thread": 91,
      "url": "/discussion/the-tissues-resource-for-the-tissue-specificity-of-genes/91#7"
    },
    {
      "body_html": "<h1>Initial release of processed TISSUES data</h1>\r\n\r\n<p>We have completed an initial processing of the TISSUES data (<a href=\"https://github.com/dhimmel/tissues/tree/93cd71464ee9673661bc4ddadfc6a8e0e219cd7b\">repository</a>, <a href=\"https://github.com/dhimmel/tissues/blob/93cd71464ee9673661bc4ddadfc6a8e0e219cd7b/tissues.ipynb\">notebook</a>) <span class=\"citation\">[<a href=\"/doi/10.5281/zenodo.27244\" class=\"citation\" data-key=\"10.5281/zenodo.27244\">1</a>]</span>. The main output of our analysis is <a href=\"https://github.com/dhimmel/tissues/blob/93cd71464ee9673661bc4ddadfc6a8e0e219cd7b/data/merged.tsv.gz\"><code>merged.tsv.gz</code></a>, a table where each row is a tissue (Uberon)–gene (Entrez) pair. For each pair, we provide 5 scores:</p>\r\n\r\n<ul><li><code>score_text</code>: score from the text mining channel</li><li><code>score_knowledge</code>: score from the UniProtKB/knowledge channel</li><li><code>score_experiment</code>: integrated score from the experimental channel</li><li><code>score_experiment_unbiased</code>: integrated score from the experimental channel without immunohistochemical staining data from the Human Protein Atlas</li><li><code>score_integrated</code>:  integrated score combining everything</li></ul>\r\n\r\n<p>Integrations (<code>score_experiment</code> and <code>score_experiment_unbiased</code>) were calculated using the <a href=\"#7\">above formula</a>.</p>\r\n\r\n<h2>Visualizing channel concordance</h2>\r\n\r\n<p>We visualized the relationships between scores. The off-diagonal plots show a 2D histogram, using hexagonal bins. The diagonal of the grid contains 1D histograms for the x-variable. Bin counts for all panels are log-transformed.</p>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/dhimmel/tissues/93cd71464ee9673661bc4ddadfc6a8e0e219cd7b/figure/channel-histograms.png\" alt=\"\"></p>\r\n\r\n<p>You may be surprised to see points where <code>y &lt; x</code> for the integrated 2D histograms. This occurs because Uberon and Entrez Gene mappings are not always one-to-one.</p>",
      "body_md": "# Initial release of processed TISSUES data\r\n\r\nWe have completed an initial processing of the TISSUES data ([repository](https://github.com/dhimmel/tissues/tree/93cd71464ee9673661bc4ddadfc6a8e0e219cd7b), [notebook](https://github.com/dhimmel/tissues/blob/93cd71464ee9673661bc4ddadfc6a8e0e219cd7b/tissues.ipynb)) [@10.5281/zenodo.27244]. The main output of our analysis is [`merged.tsv.gz`](https://github.com/dhimmel/tissues/blob/93cd71464ee9673661bc4ddadfc6a8e0e219cd7b/data/merged.tsv.gz), a table where each row is a tissue (Uberon)--gene (Entrez) pair. For each pair, we provide 5 scores:\r\n\r\n+ `score_text`: score from the text mining channel\r\n+ `score_knowledge`: score from the UniProtKB/knowledge channel\r\n+ `score_experiment`: integrated score from the experimental channel\r\n+ `score_experiment_unbiased`: integrated score from the experimental channel without immunohistochemical staining data from the Human Protein Atlas\r\n+ `score_integrated`:  integrated score combining everything\r\n\r\nIntegrations (`score_experiment` and `score_experiment_unbiased`) were calculated using the [above formula](#7).\r\n\r\n## Visualizing channel concordance\r\n\r\nWe visualized the relationships between scores. The off-diagonal plots show a 2D histogram, using hexagonal bins. The diagonal of the grid contains 1D histograms for the x-variable. Bin counts for all panels are log-transformed.\r\n\r\n![](https://raw.githubusercontent.com/dhimmel/tissues/93cd71464ee9673661bc4ddadfc6a8e0e219cd7b/figure/channel-histograms.png)\r\n\r\nYou may be surprised to see points where `y < x` for the integrated 2D histograms. This occurs because Uberon and Entrez Gene mappings are not always one-to-one.",
      "profile": 17,
      "published": "2015-08-09T19:42:47.394553Z",
      "thread": 91,
      "url": "/discussion/the-tissues-resource-for-the-tissue-specificity-of-genes/91#8"
    },
    {
      "body_html": "<h2>Updated datasets</h2>\r\n\r\n<p>We modified the code and formats for our merged indication datasets (<a href=\"https://cdn.rawgit.com/dhimmel/indications/6375b195df61b6e0d44c4690abfa2ac0710bc690//merge.html#indication-table\">website</a>, <a href=\"https://github.com/dhimmel/indications/tree/6375b195df61b6e0d44c4690abfa2ac0710bc690/data\">downloads</a>). The underlying indications have not changed from <a href=\"#1\">above</a>.</p>\r\n\r\n<h2>Pilot on 50 indications</h2>\r\n\r\n<p>We created a <a href=\"https://github.com/dhimmel/indications/blob/6375b195df61b6e0d44c4690abfa2ac0710bc690/data/curation.tsv\">simpler tsv file</a> as a curation template. As a start, we are going to have two UCSF physicians each classify a <a href=\"https://github.com/dhimmel/indications/blob/6375b195df61b6e0d44c4690abfa2ac0710bc690/data/curation-subset.tsv\">random subset</a> of 50 indications. These indications will be a pilot to see if the task is well-defined or needs revision.</p>\r\n\r\n<p>The curators will independently do a first pass. Then they will come to a consensus on conflicting classifications. We'll report back with our experience on the pilot. </p>",
      "body_md": "## Updated datasets\r\n\r\nWe modified the code and formats for our merged indication datasets ([website](https://cdn.rawgit.com/dhimmel/indications/6375b195df61b6e0d44c4690abfa2ac0710bc690//merge.html#indication-table), [downloads](https://github.com/dhimmel/indications/tree/6375b195df61b6e0d44c4690abfa2ac0710bc690/data)). The underlying indications have not changed from [above](#1).\r\n\r\n## Pilot on 50 indications\r\n\r\nWe created a [simpler tsv file](https://github.com/dhimmel/indications/blob/6375b195df61b6e0d44c4690abfa2ac0710bc690/data/curation.tsv) as a curation template. As a start, we are going to have two UCSF physicians each classify a [random subset](https://github.com/dhimmel/indications/blob/6375b195df61b6e0d44c4690abfa2ac0710bc690/data/curation-subset.tsv) of 50 indications. These indications will be a pilot to see if the task is well-defined or needs revision.\r\n\r\nThe curators will independently do a first pass. Then they will come to a consensus on conflicting classifications. We'll report back with our experience on the pilot. ",
      "profile": 17,
      "published": "2015-08-09T21:05:19.849811Z",
      "thread": 95,
      "url": "/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95#2"
    },
    {
      "body_html": "<h1>SIDER 4</h1>\r\n\r\n<p>We have begun working with SIDER 4. See <a href=\"http://thinklab.com/discussion/extracting-side-effects-from-sider4/97\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d97\">the dedicated discussion</a> for more information.</p>",
      "body_md": "# SIDER 4\r\n\r\nWe have begun working with SIDER 4. See [the dedicated discussion](http://thinklab.com/discussion/extracting-side-effects-from-sider4/97) for more information.",
      "profile": 17,
      "published": "2015-08-10T18:14:36.201349Z",
      "thread": 30,
      "url": "/discussion/assessing-the-quality-and-applicability-of-the-sider-2-resource/30#7"
    },
    {
      "body_html": "<h1>Initial processing complete</h1>\r\n\r\n<p>We've completed a first pass off the SIDER 4 data processing (<a href=\"https://github.com/dhimmel/SIDER4/blob/2acca0b065e736bc99702906024efd4718e502ee/SIDER4.ipynb\">notebook</a>, <a href=\"https://github.com/dhimmel/SIDER4/tree/2acca0b065e736bc99702906024efd4718e502ee/data\">downloads</a>). Our analysis consisted of mapping <a href=\"http://stitch.embl.de/\">STICH</a> <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkt1207\" class=\"citation\" data-key=\"10.1093/nar/gkt1207\">1</a>, <a href=\"/doi/10.1093/nar/gkm795\" class=\"citation\" data-key=\"10.1093/nar/gkm795\">2</a>]</span> compounds to DrugBank and consolidating duplicate rows. </p>\r\n\r\n<p>We added the side effects extracted from <code>meddra_all_se.tsv.gz</code> to our network. Overall, the resource <a href=\"https://github.com/dhimmel/integrate/blob/9986ecb2ad62f0e08044334d74d63a9590e4eafd/integrate.ipynb\">contributed</a> 139,235 compound-side effect relationships for 5,745 side effects.</p>\r\n\r\n<h2>Data quality</h2>\r\n\r\n<p>Compared to version 2, I subjectively noticed a considerable quality improvement. However, many of the <a href=\"http://thinklab.com/discussion/assessing-the-quality-and-applicability-of-the-sider-2-resource/30#2\">problems</a> inherent to label based NLP extraction remain. I think there are two potential methods for extracting higher confidence side effects:</p>\r\n\r\n<ol><li><strong>Number of labels approach</strong>: Most drugs have multiple labels. Side effects reported by more labels may be of higher quality. <a href=\"http://sideeffects.embl.de/drugs/3007/\">Amphetamine</a> is a good example.</li><li><strong>Frequency approach</strong>: Some side effects have associated frequency information. Placebo comparisons are also sometimes present. Thus enrichment in frequency compared to placebo, other drugs, or a cutoff is feasible. <a href=\"http://sideeffects.embl.de/drugs/3672/\">Ibuprofen</a> is a good example. </li></ol>\r\n\r\n<p>The current data release may be insufficient to apply these methods. More documentation is needed. Judging from the webapp the underlying database would support both methods. </p>",
      "body_md": "# Initial processing complete\r\n\r\nWe've completed a first pass off the SIDER 4 data processing ([notebook](https://github.com/dhimmel/SIDER4/blob/2acca0b065e736bc99702906024efd4718e502ee/SIDER4.ipynb), [downloads](https://github.com/dhimmel/SIDER4/tree/2acca0b065e736bc99702906024efd4718e502ee/data)). Our analysis consisted of mapping [STICH](http://stitch.embl.de/) [@10.1093/nar/gkt1207 @10.1093/nar/gkm795] compounds to DrugBank and consolidating duplicate rows. \r\n\r\nWe added the side effects extracted from `meddra_all_se.tsv.gz` to our network. Overall, the resource [contributed](https://github.com/dhimmel/integrate/blob/9986ecb2ad62f0e08044334d74d63a9590e4eafd/integrate.ipynb) 139,235 compound-side effect relationships for 5,745 side effects.\r\n\r\n## Data quality\r\n\r\nCompared to version 2, I subjectively noticed a considerable quality improvement. However, many of the [problems](http://thinklab.com/discussion/assessing-the-quality-and-applicability-of-the-sider-2-resource/30#2) inherent to label based NLP extraction remain. I think there are two potential methods for extracting higher confidence side effects:\r\n\r\n1. **Number of labels approach**: Most drugs have multiple labels. Side effects reported by more labels may be of higher quality. [Amphetamine](http://sideeffects.embl.de/drugs/3007/) is a good example.\r\n2. **Frequency approach**: Some side effects have associated frequency information. Placebo comparisons are also sometimes present. Thus enrichment in frequency compared to placebo, other drugs, or a cutoff is feasible. [Ibuprofen](http://sideeffects.embl.de/drugs/3672/) is a good example. \r\n\r\nThe current data release may be insufficient to apply these methods. More documentation is needed. Judging from the webapp the underlying database would support both methods. \r\n\r\n\r\n",
      "profile": 17,
      "published": "2015-08-11T06:05:43.190878Z",
      "thread": 97,
      "url": "/discussion/extracting-side-effects-from-sider-4/97#3"
    },
    {
      "body_html": "<p>A recently published study <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkv810\" class=\"citation\" data-key=\"10.1093/nar/gkv810\">1</a>]</span>, which calls itself <a href=\"http://acgt.cs.tau.ac.il/adeptus/download.html\">ADEPTUS</a>, calculated differential expression profiles for 14 diseases.</p>\r\n\r\n<p>Their disease concepts are broad, so only 3 match a <a href=\"http://thinklab.com/discussion/unifying-disease-vocabularies/44#6\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d44\">DO Slim</a> disease (<a href=\"https://github.com/dhimmel/adeptus/blob/4169d25bbc99177d88d0cbd428ae02e886a2d2f9/adeptus.ipynb\">notebook</a>, <a href=\"https://github.com/dhimmel/adeptus/tree/4169d25bbc99177d88d0cbd428ae02e886a2d2f9/data\">downloads</a>). Those diseases along with the corresponding number of up and down-regulated genes are:</p>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th>disease id</th><th>disease name</th><th>genes down</th><th>genes up</th></tr></thead><tbody><tr><td>DOID:1324</td><td>lung cancer</td><td>101</td><td>211</td></tr><tr><td>DOID:1612</td><td>breast cancer</td><td>61</td><td>68</td></tr><tr><td>DOID:2531</td><td>hematologic cancer</td><td>512</td><td>631</td></tr></tbody></table>\r\n\r\n<p>We're included these edges in our network as a placeholder until <a href=\"http://thinklab.com/discussion/stargeo-expression-signatures-for-disease-using-crowdsourced-geo-annotation/96\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d96\">STARGEO is ready</a>. STARGEO currently <a href=\"http://dev.stargeo.io/\">contains</a> 463,824 sample annotations (tags) whereas ADEPTUS contains only 14,840.</p>",
      "body_md": "A recently published study [@10.1093/nar/gkv810], which calls itself [ADEPTUS](http://acgt.cs.tau.ac.il/adeptus/download.html), calculated differential expression profiles for 14 diseases.\r\n\r\nTheir disease concepts are broad, so only 3 match a [DO Slim](http://thinklab.com/discussion/unifying-disease-vocabularies/44#6) disease ([notebook](https://github.com/dhimmel/adeptus/blob/4169d25bbc99177d88d0cbd428ae02e886a2d2f9/adeptus.ipynb), [downloads](https://github.com/dhimmel/adeptus/tree/4169d25bbc99177d88d0cbd428ae02e886a2d2f9/data)). Those diseases along with the corresponding number of up and down-regulated genes are:\r\n\r\n| disease id | disease name | genes down | genes up |\r\n|-----------|--------------------|------|-----|\r\n| DOID:1324 | lung cancer | 101 | 211 |\r\n| DOID:1612 | breast cancer | 61 | 68 |\r\n| DOID:2531 | hematologic cancer | 512 | 631 |\r\n\r\nWe're included these edges in our network as a placeholder until [STARGEO is ready](http://thinklab.com/discussion/stargeo-expression-signatures-for-disease-using-crowdsourced-geo-annotation/96). STARGEO currently [contains](http://dev.stargeo.io/) 463,824 sample annotations (tags) whereas ADEPTUS contains only 14,840.",
      "profile": 17,
      "published": "2015-08-12T23:40:14.883645Z",
      "thread": 101,
      "url": "/discussion/the-adeptus-resource-for-expression-signatures-of-disease/101"
    },
    {
      "body_html": "<h1>Ontologies for disease-centric GEO annotation</h1>\r\n\r\n<p>STARGEO allows users to define arbitrary \"tags\" for sample annotation. We have been adding Disease Ontology (DO) IDs to our tag descriptions. This suffices for simple case-control comparisons, but is insufficient for more complex comparisons.</p>\r\n\r\n<p>For example, many cancer studies, will compare tumors to healthy tissue but all samples are from cases. Therefore the contrast is not case versus control, but healthy versus diseased tissue. In general, we will want to incorporate these contrasts into our disease-specific expression signatures.</p>\r\n\r\n<p><a href=\"/u/fbastian\" class=\"username\">@fbastian</a> and <a href=\"/u/chrismungall\" class=\"username\">@chrismungall</a>, do you know of any ontologies that could help with our annotation task? I know that Bgee focuses on healthy tissue, but I though you may be able to direct us in the right direction.</p>\r\n\r\n<p><strong>Summary</strong>: we are gathering disease-specific expression signatures. What terminologies should we use to create contrast between samples within a study (GEO Series)?</p>",
      "body_md": "# Ontologies for disease-centric GEO annotation\r\n\r\nSTARGEO allows users to define arbitrary \"tags\" for sample annotation. We have been adding Disease Ontology (DO) IDs to our tag descriptions. This suffices for simple case-control comparisons, but is insufficient for more complex comparisons.\r\n\r\nFor example, many cancer studies, will compare tumors to healthy tissue but all samples are from cases. Therefore the contrast is not case versus control, but healthy versus diseased tissue. In general, we will want to incorporate these contrasts into our disease-specific expression signatures.\r\n\r\n@fbastian and @chrismungall, do you know of any ontologies that could help with our annotation task? I know that Bgee focuses on healthy tissue, but I though you may be able to direct us in the right direction.\r\n\r\n**Summary**: we are gathering disease-specific expression signatures. What terminologies should we use to create contrast between samples within a study (GEO Series)?",
      "profile": 17,
      "published": "2015-08-12T03:41:52.524912Z",
      "thread": 96,
      "url": "/discussion/stargeo-expression-signatures-for-disease-using-crowdsourced-geo-annotation/96#2"
    },
    {
      "body_html": "<h1>Unbiased PPI datasets</h1>\r\n\r\n<p>Since we now include a <a href=\"http://thinklab.com/d/48#3\">edge attribute for bias</a> in our network, we need to identify a subset of our PPIs that are derived from hypothesis free, i.e. unbiased, experiments.</p>\r\n\r\n<p>The <em>Incomplete Interactome</em> <span class=\"citation\">[<a href=\"/doi/10.1126/science.1257601\" class=\"citation\" data-key=\"10.1126/science.1257601\">1</a>]</span> <a href=\"http://www.sciencemag.org/content/suppl/2015/02/18/347.6224.1257601.DC1/Menche_SM.pdf#page=5\">describes</a> their creation of an unbiased interactome:</p>\r\n\r\n<blockquote><p>Since our interactome includes data from literature curation, it is inherently biased towards much studied disease-associated proteins and their interactions. We, therefore, complement our analysis using only interactions from well controlled and completely unbiased high-throughput yeast two-hybrid (y2h) datasets <span class=\"citation\">[<a href=\"/doi/10.1016/j.cell.2014.10.050\" class=\"citation\" data-key=\"10.1016/j.cell.2014.10.050\">2</a>, <a href=\"/doi/10.1038/nmeth.1280\" class=\"citation\" data-key=\"10.1038/nmeth.1280\">3</a>, <a href=\"/doi/10.1038/nmeth.1597\" class=\"citation\" data-key=\"10.1038/nmeth.1597\">4</a>, <a href=\"/doi/10.1038/nature04209\" class=\"citation\" data-key=\"10.1038/nature04209\">5</a>, <a href=\"\" class=\"citation\" data-key=\"55\">6</a>]</span>.</p></blockquote>\r\n\r\n<p>Minus the last citation <span class=\"citation\">[<a href=\"\" class=\"citation\" data-key=\"55\">6</a>]</span> which is prepublication, we can use the other four resources <span class=\"citation\">[<a href=\"/doi/10.1016/j.cell.2014.10.050\" class=\"citation\" data-key=\"10.1016/j.cell.2014.10.050\">2</a>, <a href=\"/doi/10.1038/nmeth.1280\" class=\"citation\" data-key=\"10.1038/nmeth.1280\">3</a>, <a href=\"/doi/10.1038/nmeth.1597\" class=\"citation\" data-key=\"10.1038/nmeth.1597\">4</a>, <a href=\"/doi/10.1038/nature04209\" class=\"citation\" data-key=\"10.1038/nature04209\">5</a>]</span>, two of which were used by <a href=\"/u/idrdex\" class=\"username\">@idrdex</a> in his study <span class=\"citation\">[<a href=\"/doi/10.1038/ncomms5074\" class=\"citation\" data-key=\"10.1038/ncomms5074\">7</a>]</span>.</p>\r\n\r\n<p></p>",
      "body_md": "# Unbiased PPI datasets\r\n\r\nSince we now include a [edge attribute for bias](http://thinklab.com/d/48#3) in our network, we need to identify a subset of our PPIs that are derived from hypothesis free, i.e. unbiased, experiments.\r\n\r\nThe *Incomplete Interactome* [@10.1126/science.1257601] [describes](http://www.sciencemag.org/content/suppl/2015/02/18/347.6224.1257601.DC1/Menche_SM.pdf#page=5) their creation of an unbiased interactome:\r\n\r\n> Since our interactome includes data from literature curation, it is inherently biased towards much studied disease-associated proteins and their interactions. We, therefore, complement our analysis using only interactions from well controlled and completely unbiased high-throughput yeast two-hybrid (y2h) datasets [@10.1016/j.cell.2014.10.050 @10.1038/nmeth.1280 @10.1038/nmeth.1597 @10.1038/nature04209 @55].\r\n\r\nMinus the last citation [@55] which is prepublication, we can use the other four resources [@10.1016/j.cell.2014.10.050 @10.1038/nmeth.1280 @10.1038/nmeth.1597 @10.1038/nature04209], two of which were used by @idrdex in his study [@10.1038/ncomms5074].\r\n \r\n[@55]: \"Center for Cancer Systems Biology, Hi-2012 prepublication\"",
      "profile": 17,
      "published": "2015-08-12T18:23:34.548120Z",
      "thread": 85,
      "url": "/discussion/creating-a-catalog-of-protein-interactions/85#8"
    },
    {
      "body_html": "<h1>Completed PPI catalog</h1>\r\n\r\n<p>We have completed an initial version of our protein interaction catalog for this project <span class=\"citation\">[<a href=\"/doi/10.5281/zenodo.48443\" class=\"citation\" data-key=\"10.5281/zenodo.48443\">1</a>]</span>, named <code>hetio-ind</code>. We defined interaction as <em>two genes whose protein products physically interact</em>. Physical associations from protein complexes were minimized.</p>\r\n\r\n<p>Interactions were taken from the following sources:</p>\r\n\r\n<ul><li><strong>Human Interactome Database</strong> (<a href=\"http://interactome.dfci.harvard.edu/H_sapiens/index.php?page=download\">HID</a>): specifically the <code>HI-I-05</code> <span class=\"citation\">[<a href=\"/doi/10.1038/nature04209\" class=\"citation\" data-key=\"10.1038/nature04209\">2</a>]</span>, <code>Venkatesan-09</code> <span class=\"citation\">[<a href=\"/doi/10.1038/nmeth.1280\" class=\"citation\" data-key=\"10.1038/nmeth.1280\">3</a>]</span>, <code>Yu-11</code> <span class=\"citation\">[<a href=\"/doi/10.1038/nmeth.1597\" class=\"citation\" data-key=\"10.1038/nmeth.1597\">4</a>]</span>, <code>HI-II-14</code> <span class=\"citation\">[<a href=\"/doi/10.1016/j.cell.2014.10.050\" class=\"citation\" data-key=\"10.1016/j.cell.2014.10.050\">5</a>]</span>, <code>Lit-BM-13</code> <span class=\"citation\">[<a href=\"/doi/10.1016/j.cell.2014.10.050\" class=\"citation\" data-key=\"10.1016/j.cell.2014.10.050\">5</a>]</span> datasets.</li><li><strong>Incomplete Interactome</strong> <span class=\"citation\">[<a href=\"/doi/10.1126/science.1257601\" class=\"citation\" data-key=\"10.1126/science.1257601\">6</a>]</span>: specifically the <code>II_binary</code> and <code>II_literature</code> <a href=\"#2\">subsets</a>.</li><li><strong>hetio-dag</strong> <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1004259\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1004259\">7</a>]</span>: our previous project (<a href=\"#4\">details</a>). We removed all interactions that were not physical associations (<a href=\"http://www.ebi.ac.uk/ontology-lookup/browse.do?ontName=MI&amp;termId=MI%3A0915&amp;termName=physical%20association\"><code>MI:0195</code></a>). This step excluded genetic and colocalization interactions.</li></ul>\r\n\r\n<p>16,526 interactions reported by <code>HI-I-05</code> <span class=\"citation\">[<a href=\"/doi/10.1038/nature04209\" class=\"citation\" data-key=\"10.1038/nature04209\">2</a>]</span>, <code>Venkatesan-09</code> <span class=\"citation\">[<a href=\"/doi/10.1038/nmeth.1280\" class=\"citation\" data-key=\"10.1038/nmeth.1280\">3</a>]</span>, <code>Yu-11</code> <span class=\"citation\">[<a href=\"/doi/10.1038/nmeth.1597\" class=\"citation\" data-key=\"10.1038/nmeth.1597\">4</a>]</span>, or <code>HI-II-14</code> <span class=\"citation\">[<a href=\"/doi/10.1016/j.cell.2014.10.050\" class=\"citation\" data-key=\"10.1016/j.cell.2014.10.050\">5</a>]</span> were considered unbiased. The 135,203 other interactions were considered biased.</p>\r\n\r\n<p>In total our dataset contains 151,729 protein interactions (<a href=\"https://github.com/dhimmel/ppi/blob/4012fe7af1699222539844256e3639782ae72695/compile-PPIs.ipynb\">notebook</a>, <a href=\"https://github.com/dhimmel/ppi/tree/4012fe7af1699222539844256e3639782ae72695/data\">downloads</a>).</p>",
      "body_md": "# Completed PPI catalog\r\n\r\nWe have completed an initial version of our protein interaction catalog for this project [@10.5281/zenodo.48443], named `hetio-ind`. We defined interaction as *two genes whose protein products physically interact*. Physical associations from protein complexes were minimized.\r\n\r\nInteractions were taken from the following sources:\r\n\r\n+ **Human Interactome Database** ([HID](http://interactome.dfci.harvard.edu/H_sapiens/index.php?page=download)): specifically the `HI-I-05` [@10.1038/nature04209], `Venkatesan-09` [@10.1038/nmeth.1280], `Yu-11` [@10.1038/nmeth.1597], `HI-II-14` [@10.1016/j.cell.2014.10.050], `Lit-BM-13` [@10.1016/j.cell.2014.10.050] datasets.\r\n+ **Incomplete Interactome** [@10.1126/science.1257601]: specifically the `II_binary` and `II_literature` [subsets](#2).\r\n+ **hetio-dag** [@10.1371/journal.pcbi.1004259]: our previous project ([details](#4)). We removed all interactions that were not physical associations ([`MI:0195`](http://www.ebi.ac.uk/ontology-lookup/browse.do?ontName=MI&termId=MI%3A0915&termName=physical%20association)). This step excluded genetic and colocalization interactions.\r\n\r\n16,526 interactions reported by `HI-I-05` [@10.1038/nature04209], `Venkatesan-09` [@10.1038/nmeth.1280], `Yu-11` [@10.1038/nmeth.1597], or `HI-II-14` [@10.1016/j.cell.2014.10.050] were considered unbiased. The 135,203 other interactions were considered biased.\r\n\r\nIn total our dataset contains 151,729 protein interactions ([notebook](https://github.com/dhimmel/ppi/blob/4012fe7af1699222539844256e3639782ae72695/compile-PPIs.ipynb), [downloads](https://github.com/dhimmel/ppi/tree/4012fe7af1699222539844256e3639782ae72695/data)).",
      "profile": 17,
      "published": "2015-08-12T21:01:31.686816Z",
      "thread": 85,
      "url": "/discussion/creating-a-catalog-of-protein-interactions/85#9"
    },
    {
      "body_html": "<h1>Jupyter 1.0.0 released</h1>\r\n\r\n<p>Last Wednesday <a href=\"http://blog.jupyter.org/2015/08/12/first-release-of-jupyter/\">marked a historic day</a> for biodata science. The language agnostic parts of IPython, including the notebook, have been <a href=\"https://blog.jupyter.org/2015/04/15/the-big-split/\">repackaged</a> as Jupyter. The big split was necessary because the project now supports <a href=\"https://github.com/ipython/ipython/wiki/IPython-kernels-for-other-languages\">many languages</a> not just python.</p>\r\n\r\n<p>I am updating the <a href=\"#1\">above</a> guide, by replacing <code>ipython</code> with <code>jupyter</code> in code snippets. The revised guidance will apply to new installations. If you have an existing Anaconda installation, you can <a href=\"http://jupyter.readthedocs.org/en/latest/install.html\">install Jupyter</a> with <code>conda install jupyter</code>.</p>\r\n\r\n<p>Now who is excited for <a href=\"https://www.python.org/dev/peps/pep-0478/\">September 13th</a> and the <a href=\"https://github.com/takluyver/talks/blob/master/Python%203.5%20lightning%20talk.ipynb\">features</a> this day will bring!</p>",
      "body_md": "# Jupyter 1.0.0 released\r\n\r\nLast Wednesday [marked a historic day](http://blog.jupyter.org/2015/08/12/first-release-of-jupyter/) for biodata science. The language agnostic parts of IPython, including the notebook, have been [repackaged](https://blog.jupyter.org/2015/04/15/the-big-split/) as Jupyter. The big split was necessary because the project now supports [many languages](https://github.com/ipython/ipython/wiki/IPython-kernels-for-other-languages) not just python.\r\n\r\nI am updating the [above](#1) guide, by replacing `ipython` with `jupyter` in code snippets. The revised guidance will apply to new installations. If you have an existing Anaconda installation, you can [install Jupyter](http://jupyter.readthedocs.org/en/latest/install.html) with `conda install jupyter`.\r\n\r\nNow who is excited for [September 13th](https://www.python.org/dev/peps/pep-0478/) and the [features](https://github.com/takluyver/talks/blob/master/Python%203.5%20lightning%20talk.ipynb) this day will bring!",
      "profile": 17,
      "published": "2015-08-16T23:53:59.216980Z",
      "thread": 84,
      "url": "/discussion/python-for-the-modern-biodata-scientist/84#5"
    },
    {
      "body_html": "<p>To annotate the condition of a sample, you can use the Experimental Factor Ontology (EFO). But not sure what you mean by \"contrast between samples\" (do you want to annotate each sample, or have terms directly representing, e.g. \"healthy vs. diseased contrast\")</p>",
      "body_md": "To annotate the condition of a sample, you can use the Experimental Factor Ontology (EFO). But not sure what you mean by \"contrast between samples\" (do you want to annotate each sample, or have terms directly representing, e.g. \"healthy vs. diseased contrast\")\r\n\r\n",
      "profile": 111,
      "published": "2015-08-14T11:05:20.654050Z",
      "thread": 96,
      "url": "/discussion/stargeo-expression-signatures-for-disease-using-crowdsourced-geo-annotation/96#3"
    },
    {
      "body_html": "<h1>Network version 1.0</h1>\r\n\r\n<p>We have completed an initial version of our network (<a href=\"https://github.com/dhimmel/integrate/blob/2256f1d6d01758c8bab59212a68d890ecb42bb7f/integrate.ipynb\">notebook</a>, <a href=\"https://github.com/dhimmel/integrate/blob/2256f1d6d01758c8bab59212a68d890ecb42bb7f/data/graph.json.gz\">download</a>) <span class=\"citation\">[<a href=\"/doi/10.5281/zenodo.28040\" class=\"citation\" data-key=\"10.5281/zenodo.28040\">1</a>]</span>.</p>\r\n\r\n<p>The network consists of 10 types of nodes (metanodes) and 27 types of edges (metaedges). It contains 49,427 nodes and 2,997,892 edges (1,488,312 of which are <a href=\"http://thinklab.com/discussion/text-as-a-resource-for-network-population/48#3\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d48\">unbiased</a>). The network is visualized below, laid out by metanode and colored by metaedge (only a subset of edges are drawn for efficiency):</p>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/dhimmel/rephetio/eaad6455815c3886a47aeddf76931fcf1779e090/figure/network-v1.0-labeled.png\" alt=\"\"></p>\r\n\r\n<p>For additional information, see the <a href=\"https://github.com/dhimmel/integrate/blob/2256f1d6d01758c8bab59212a68d890ecb42bb7f/data/summary/metanodes.tsv\">summary of nodes</a>, <a href=\"https://github.com/dhimmel/integrate/blob/2256f1d6d01758c8bab59212a68d890ecb42bb7f/data/summary/metaedges.tsv\">summary of edges</a>, or <a href=\"https://github.com/dhimmel/integrate/blob/2256f1d6d01758c8bab59212a68d890ecb42bb7f/viz/degrees.pdf\">visualization of degree distributions</a>. Network existence (SHA256 checksum for <code>graph.json.gz</code>) is <a href=\"https://blockchain.info/tx/092f81abd7bb5c59e52e2d8e794de6cee4a1cd701f7a87d2bc11cfefe97d4923?show_adv=true\">proven</a> in Bitcoin block 369,898.</p>\r\n\r\n<h2>Future changes</h2>\r\n\r\n<p>There are a few changes we hope to make in the near future. First, replacing <a href=\"http://thinklab.com/discussion/the-adeptus-resource-for-expression-signatures-of-disease/101\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d101\">ADEPTUS</a> with <a href=\"http://thinklab.com/discussion/stargeo-expression-signatures-for-disease-using-crowdsourced-geo-annotation/96\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d96\">STARGEO</a> for expression signatures of disease. Second, updating our indications with a <a href=\"http://thinklab.com/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d95\">manually curated</a> subset. As always, suggestions for additional information types are <a href=\"http://thinklab.com/discussion/suggestions-for-additional-information-types/22\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d22\">welcome here</a>.</p>",
      "body_md": "# Network version 1.0\r\n\r\nWe have completed an initial version of our network ([notebook](https://github.com/dhimmel/integrate/blob/2256f1d6d01758c8bab59212a68d890ecb42bb7f/integrate.ipynb), [download](https://github.com/dhimmel/integrate/blob/2256f1d6d01758c8bab59212a68d890ecb42bb7f/data/graph.json.gz)) [@10.5281/zenodo.28040].\r\n\r\nThe network consists of 10 types of nodes (metanodes) and 27 types of edges (metaedges). It contains 49,427 nodes and 2,997,892 edges (1,488,312 of which are [unbiased](http://thinklab.com/discussion/text-as-a-resource-for-network-population/48#3)). The network is visualized below, laid out by metanode and colored by metaedge (only a subset of edges are drawn for efficiency):\r\n\r\n![](https://raw.githubusercontent.com/dhimmel/rephetio/eaad6455815c3886a47aeddf76931fcf1779e090/figure/network-v1.0-labeled.png)\r\n\r\nFor additional information, see the [summary of nodes](https://github.com/dhimmel/integrate/blob/2256f1d6d01758c8bab59212a68d890ecb42bb7f/data/summary/metanodes.tsv), [summary of edges](https://github.com/dhimmel/integrate/blob/2256f1d6d01758c8bab59212a68d890ecb42bb7f/data/summary/metaedges.tsv), or [visualization of degree distributions](https://github.com/dhimmel/integrate/blob/2256f1d6d01758c8bab59212a68d890ecb42bb7f/viz/degrees.pdf). Network existence (SHA256 checksum for `graph.json.gz`) is [proven](https://blockchain.info/tx/092f81abd7bb5c59e52e2d8e794de6cee4a1cd701f7a87d2bc11cfefe97d4923?show_adv=true) in Bitcoin block 369,898.\r\n\r\n## Future changes\r\n\r\nThere are a few changes we hope to make in the near future. First, replacing [ADEPTUS](http://thinklab.com/discussion/the-adeptus-resource-for-expression-signatures-of-disease/101) with [STARGEO](http://thinklab.com/discussion/stargeo-expression-signatures-for-disease-using-crowdsourced-geo-annotation/96) for expression signatures of disease. Second, updating our indications with a [manually curated](http://thinklab.com/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95) subset. As always, suggestions for additional information types are [welcome here](http://thinklab.com/discussion/suggestions-for-additional-information-types/22).",
      "profile": 17,
      "published": "2015-08-14T22:24:08.088663Z",
      "thread": 102,
      "url": "/discussion/one-network-to-rule-them-all/102"
    },
    {
      "body_html": "<p>Nice of you to share this big network with everyone; however, I think you need to take care not to get yourself into legal trouble here.</p>\r\n\r\n<p>I looked into the JSON network file and found the following:<br>- Gene membership of all KEGG maps. If you look at the <a href=\"http://www.kegg.jp/kegg/legal.html\">KEGG license</a>, it is questionable if you can do that at all, and very clear that you cannot allow commercial use.<br>- Side effect data from (I assume) SIDER. The SIDER download files are distributed under the <a href=\"http://creativecommons.org/licenses/by-nc-sa/3.0/\">CC-BY-NC-SA license</a>, which means that you are only allowed to redistribute if you give attribution and attach the same license to the derived work.</p>\r\n\r\n<p>Given earlier posts, I assume that you also import associations from the <a href=\"http://tissues.jensenlab.org/\">TISSUES database</a>. Even though I distribute this resource under the very permissive <a href=\"http://creativecommons.org/licenses/by/4.0/\">CC-BY license</a>, you are still required to give attribution. This could, for example, be done by including relevant linkouts under \"data\" : { \"url\" : \"...\" }.</p>\r\n\r\n<p>I am not trying to cause trouble here - just the contrary. When making a meta-resource, licenses and copyright law are not something you can afford to ignore. I regularly leave out certain data sources from my resources for legal reasons. For example, <a href=\"http://www.omim.org/\">OMIM</a> is not included in <a href=\"http://diseases.jensenlab.org/\">DISEASES</a> due to <a href=\"http://www.omim.org/help/agreement\">its restrictive license</a>.</p>",
      "body_md": "Nice of you to share this big network with everyone; however, I think you need to take care not to get yourself into legal trouble here.\r\n\r\nI looked into the JSON network file and found the following:\r\n- Gene membership of all KEGG maps. If you look at the [KEGG license](http://www.kegg.jp/kegg/legal.html), it is questionable if you can do that at all, and very clear that you cannot allow commercial use.\r\n- Side effect data from (I assume) SIDER. The SIDER download files are distributed under the [CC-BY-NC-SA license](http://creativecommons.org/licenses/by-nc-sa/3.0/), which means that you are only allowed to redistribute if you give attribution and attach the same license to the derived work.\r\n\r\nGiven earlier posts, I assume that you also import associations from the [TISSUES database](http://tissues.jensenlab.org/). Even though I distribute this resource under the very permissive [CC-BY license](http://creativecommons.org/licenses/by/4.0/), you are still required to give attribution. This could, for example, be done by including relevant linkouts under \"data\" : { \"url\" : \"...\" }.\r\n\r\nI am not trying to cause trouble here - just the contrary. When making a meta-resource, licenses and copyright law are not something you can afford to ignore. I regularly leave out certain data sources from my resources for legal reasons. For example, [OMIM](http://www.omim.org/) is not included in [DISEASES](http://diseases.jensenlab.org/) due to [its restrictive license](http://www.omim.org/help/agreement).",
      "profile": 125,
      "published": "2015-08-16T07:35:00.051806Z",
      "thread": 102,
      "url": "/discussion/one-network-to-rule-them-all/102#2"
    },
    {
      "body_html": "<p>Related to the license issues, it is wise that you solicit advice from legal experts. However, as long as you are dealing with databases created by researchers in academia, the risk of actually getting sued is pretty minimal. The most important question that you should be asking yourself is thus not \"what can I technically do without risking to get sued?\", but rather \"what was the intent of the license?\". If you frequently do things that may be technically legal but clearly go against the intent of other researchers, you will quickly make many enemies.</p>\r\n\r\n<p>Just friendly words of advise :-)</p>",
      "body_md": "Related to the license issues, it is wise that you solicit advice from legal experts. However, as long as you are dealing with databases created by researchers in academia, the risk of actually getting sued is pretty minimal. The most important question that you should be asking yourself is thus not \"what can I technically do without risking to get sued?\", but rather \"what was the intent of the license?\". If you frequently do things that may be technically legal but clearly go against the intent of other researchers, you will quickly make many enemies.\r\n\r\nJust friendly words of advise :-)",
      "profile": 125,
      "published": "2015-08-16T09:28:46.591169Z",
      "thread": 102,
      "url": "/discussion/one-network-to-rule-them-all/102#3"
    },
    {
      "body_html": "<p>We have been developing <a href=\"https://github.com/dhimmel/hetio\">tools</a> and <a href=\"http://het.io\">applications</a> <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1004259\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1004259\">1</a>, <a href=\"/p/rephetio\" class=\"citation\" data-key=\"10.15363/thinklab.4\">2</a>]</span> for <em>graphs with multiple node and edge types with optional directionality declared by the edge type</em>. Now, we would like to choose the best term for this definition.</p>\r\n\r\n<p>We have adopted a <a href=\"https://dx.doi.org/10.1371/journal.pcbi.1004259#article1.body1.sec4.sec2.p1\">nomenclature</a> where graph elements (nodes, edges, paths) are prepended with meta when referring to their type (metanodes, metaedges, metapaths). Support for directionality is necessitated by certain metaedges that connect a metanode to itself (<a href=\"http://thinklab.com/discussion/suggestions-for-additional-information-types/22#6\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d22\">example</a>). Whether our conception of directionality should be mandated by the definition is open for discussion.</p>\r\n\r\n<h2>Potential names</h2>\r\n\r\n<ul><li><strong>heterogeneous information network</strong> — the term used by the foundational works in social network analysis <span class=\"citation\">[<a href=\"/doi/10.2200/S00433ED1V01Y201207DMK005\" class=\"citation\" data-key=\"10.2200/S00433ED1V01Y201207DMK005\">3</a>, <a href=\"/doi/10.1109/ASONAM.2011.112\" class=\"citation\" data-key=\"10.1109/ASONAM.2011.112\">4</a>, <a href=\"/doi/10.1109/ASONAM.2011.107\" class=\"citation\" data-key=\"10.1109/ASONAM.2011.107\">5</a>]</span> to describe a graph with typed nodes and edges. The 397 <a href=\"https://scholar.google.com/scholar?hl=en&amp;q=%22heterogeneous+information+network%22\">occurrences</a> in Google Scholar are of high precision. My major complaints with this term are its verbosity and drabness.</li><li><strong>heterogeneous network</strong> — the term we are currently using but that has a <a href=\"https://en.wikipedia.org/wiki/Heterogeneous_network\">preexisting meaning</a> in computer networking. The term retrieves 2,930 Google Scholar <a href=\"https://scholar.google.com/scholar?q=%22heterogeneous+network%22&amp;as_ylo=2015\">occurrences</a> since 2015 with precision below 50%. I dislike this term's ambiguity and use of \"heterogeneous\" which is lengthy and esoteric.</li><li><strong>other options</strong>: we would like suggestions for other names. The following criteria are important: brevity, precision (once adopted), intuitiveness, cheer, and accessibility.</li></ul>\r\n\r\n<h2>Related terms</h2>\r\n\r\n<p>Below, I list several graph types (out of many <span class=\"citation\">[<a href=\"/doi/10.1002/bult.2010.1720360610\" class=\"citation\" data-key=\"10.1002/bult.2010.1720360610\">6</a>]</span>) that are related to but distinct from our definition:</p>\r\n\r\n<ul><li><a href=\"https://en.wikipedia.org/wiki/Multipartite_graph\">multipartite graphs</a> — graphs with typed nodes but without typed edges. Sometimes <span class=\"citation\">[<a href=\"/doi/10.1103/physreve.79.036113\" class=\"citation\" data-key=\"10.1103/physreve.79.036113\">7</a>]</span> referred to as <em>multitype networks</em>.</li><li><a href=\"https://en.wikipedia.org/wiki/Multigraph\">multigraph</a> — graphs allowing multiple edges between the same source and target nodes but without typed edges. </li><li><a href=\"http://neo4j.com/docs/stable/cypher-cookbook-multirelational-social-network.html\">multi-relational graphs</a> — graphs with multiple edge types  </li><li><a href=\"https://github.com/tinkerpop/gremlin/wiki/Defining-a-Property-Graph\">property graphs</a> — directed multi-relational graphs with edge attributes.</li></ul>\r\n\r\n<h2>Seeking input</h2>\r\n\r\n<p>We would like input and suggestions. Some possibilities are polynets, hetnets, multigraphs, typednets, typedgraphs, and multitype networks.</p>\r\n\r\n<p>Collectively, we are pioneering a branch of network analysis that will play a prominent role going forward. We do not want to be hindered by vocabulary.</p>",
      "body_md": "We have been developing [tools](https://github.com/dhimmel/hetio) and [applications](http://het.io) [@10.1371/journal.pcbi.1004259 @10.15363/thinklab.4] for *graphs with multiple node and edge types with optional directionality declared by the edge type*. Now, we would like to choose the best term for this definition.\r\n\r\nWe have adopted a [nomenclature](https://dx.doi.org/10.1371/journal.pcbi.1004259#article1.body1.sec4.sec2.p1) where graph elements (nodes, edges, paths) are prepended with meta when referring to their type (metanodes, metaedges, metapaths). Support for directionality is necessitated by certain metaedges that connect a metanode to itself ([example](http://thinklab.com/discussion/suggestions-for-additional-information-types/22#6)). Whether our conception of directionality should be mandated by the definition is open for discussion.\r\n\r\n## Potential names\r\n\r\n+ **heterogeneous information network** -- the term used by the foundational works in social network analysis [@10.2200/S00433ED1V01Y201207DMK005 @10.1109/ASONAM.2011.112 @10.1109/ASONAM.2011.107] to describe a graph with typed nodes and edges. The 397 [occurrences](https://scholar.google.com/scholar?hl=en&q=%22heterogeneous+information+network%22) in Google Scholar are of high precision. My major complaints with this term are its verbosity and drabness.\r\n+ **heterogeneous network** -- the term we are currently using but that has a [preexisting meaning](https://en.wikipedia.org/wiki/Heterogeneous_network) in computer networking. The term retrieves 2,930 Google Scholar [occurrences](https://scholar.google.com/scholar?q=%22heterogeneous+network%22&as_ylo=2015) since 2015 with precision below 50%. I dislike this term's ambiguity and use of \"heterogeneous\" which is lengthy and esoteric.\r\n+ **other options**: we would like suggestions for other names. The following criteria are important: brevity, precision (once adopted), intuitiveness, cheer, and accessibility.\r\n\r\n## Related terms\r\n\r\nBelow, I list several graph types (out of many [@10.1002/bult.2010.1720360610]) that are related to but distinct from our definition:\r\n\r\n+ [multipartite graphs](https://en.wikipedia.org/wiki/Multipartite_graph) -- graphs with typed nodes but without typed edges. Sometimes [@10.1103/physreve.79.036113] referred to as *multitype networks*.\r\n+ [multigraph](https://en.wikipedia.org/wiki/Multigraph) -- graphs allowing multiple edges between the same source and target nodes but without typed edges. \r\n+ [multi-relational graphs](http://neo4j.com/docs/stable/cypher-cookbook-multirelational-social-network.html) -- graphs with multiple edge types  \r\n+ [property graphs](https://github.com/tinkerpop/gremlin/wiki/Defining-a-Property-Graph) -- directed multi-relational graphs with edge attributes.\r\n\r\n## Seeking input\r\n\r\nWe would like input and suggestions. Some possibilities are polynets, hetnets, multigraphs, typednets, typedgraphs, and multitype networks.\r\n\r\nCollectively, we are pioneering a branch of network analysis that will play a prominent role going forward. We do not want to be hindered by vocabulary.",
      "profile": 17,
      "published": "2015-08-17T02:56:46.391930Z",
      "thread": 104,
      "url": "/discussion/renaming-heterogeneous-networks-to-a-more-concise-and-catchy-term/104"
    },
    {
      "body_html": "<p>Thanks to a suggestion by <a href=\"/u/janispi\" class=\"username\">@janispi</a>, we have begun processing <a href=\"http://www.disgenet.org/web/DisGeNET/menu/home\">DisGeNET</a> <span class=\"citation\">[<a href=\"/doi/10.1093/database/bav028\" class=\"citation\" data-key=\"10.1093/database/bav028\">1</a>, <a href=\"/doi/10.1371/journal.pone.0020284\" class=\"citation\" data-key=\"10.1371/journal.pone.0020284\">2</a>, <a href=\"/doi/10.1093/bioinformatics/btq538\" class=\"citation\" data-key=\"10.1093/bioinformatics/btq538\">3</a>]</span> to include as a disease–gene edge in our network.</p>\r\n\r\n<p>DisGeNET integrates <a href=\"http://www.disgenet.org/web/DisGeNET/menu/dbinfo#ontology\">associations</a> from many <a href=\"http://www.disgenet.org/web/DisGeNET/menu/dbinfo#sources\">sources</a> and provides a unified <a href=\"http://www.disgenet.org/web/DisGeNET/menu/dbinfo#score\">score</a> for each gene–disease pair.</p>\r\n\r\n<p>We will likely replace or merge the function edge we <a href=\"http://thinklab.com/discussion/functional-disease-annotations-for-genes-using-doaf/94\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d94\">extracted from DOAF</a> with DisGeNET.</p>\r\n\r\n<p>Diseases in DisGeNET are identified with UMLS identifiers. We were <a href=\"https://github.com/dhimmel/disgenet/blob/2154cd74307f92d00d2ed192c584cfb33733d7da/disgenet.ipynb\">able to map</a> 125 out of 137 of our <a href=\"http://thinklab.com/discussion/unifying-disease-vocabularies/44#6\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d44\">DO Slim</a> diseases.</p>",
      "body_md": "Thanks to a suggestion by @janispi, we have begun processing [DisGeNET](http://www.disgenet.org/web/DisGeNET/menu/home) [@10.1093/database/bav028 @10.1371/journal.pone.0020284 @10.1093/bioinformatics/btq538] to include as a disease--gene edge in our network.\r\n\r\nDisGeNET integrates [associations](http://www.disgenet.org/web/DisGeNET/menu/dbinfo#ontology) from many [sources](http://www.disgenet.org/web/DisGeNET/menu/dbinfo#sources) and provides a unified [score](http://www.disgenet.org/web/DisGeNET/menu/dbinfo#score) for each gene--disease pair.\r\n\r\nWe will likely replace or merge the function edge we [extracted from DOAF](http://thinklab.com/discussion/functional-disease-annotations-for-genes-using-doaf/94) with DisGeNET.\r\n\r\nDiseases in DisGeNET are identified with UMLS identifiers. We were [able to map](https://github.com/dhimmel/disgenet/blob/2154cd74307f92d00d2ed192c584cfb33733d7da/disgenet.ipynb) 125 out of 137 of our [DO Slim](http://thinklab.com/discussion/unifying-disease-vocabularies/44#6) diseases.",
      "profile": 17,
      "published": "2015-08-18T03:46:33.049097Z",
      "thread": 105,
      "url": "/discussion/processing-disgenet-for-disease-gene-relationships/105"
    },
    {
      "body_html": "<h1>Data format suggestion</h1>\r\n\r\n<p>The datasets on the <a href=\"http://www.disgenet.org/web/DisGeNET/menu/downloads\">download page</a> are gzipped tarballs but only contain a single text file. Using <code>zless</code> or <code>pandas.read_table()</code> on the <code>tar.gz</code> file led to strange behavior. I ended up extracting the file from the tarball and then gzipping it again to reduce filesize (<a href=\"https://github.com/dhimmel/disgenet/blob/2154cd74307f92d00d2ed192c584cfb33733d7da/download/all_gene_disease_associations.txt.gz\">new file</a>).</p>\r\n\r\n<p><a href=\"/u/janispi\" class=\"username\">@janispi</a>, would it make sense to remove the tarball at your end and go with a plain <code>.txt.gz</code> or <code>.tsv.gz</code> extension?</p>",
      "body_md": "# Data format suggestion\r\n\r\nThe datasets on the [download page](http://www.disgenet.org/web/DisGeNET/menu/downloads) are gzipped tarballs but only contain a single text file. Using `zless` or `pandas.read_table()` on the `tar.gz` file led to strange behavior. I ended up extracting the file from the tarball and then gzipping it again to reduce filesize ([new file](https://github.com/dhimmel/disgenet/blob/2154cd74307f92d00d2ed192c584cfb33733d7da/download/all_gene_disease_associations.txt.gz)).\r\n\r\n@janispi, would it make sense to remove the tarball at your end and go with a plain `.txt.gz` or `.tsv.gz` extension?",
      "profile": 17,
      "published": "2015-08-18T03:55:14.132455Z",
      "thread": 105,
      "url": "/discussion/processing-disgenet-for-disease-gene-relationships/105#2"
    },
    {
      "body_html": "<p>you should not be having problems, but maybe you should just untar the file and then load it? <br>Let me know how it goes, so if there is any issue, we will take care of it. </p>",
      "body_md": "you should not be having problems, but maybe you should just untar the file and then load it? \r\nLet me know how it goes, so if there is any issue, we will take care of it. ",
      "profile": 129,
      "published": "2015-08-18T08:08:19.392686Z",
      "thread": 105,
      "url": "/discussion/processing-disgenet-for-disease-gene-relationships/105#3"
    },
    {
      "body_html": "<p>I ended up doing the following steps to strip out the tarball:</p>\r\n\r\n<pre><code class=\"no-highlight hljs\">tar -xzf all_gene_disease_associations.tar.gz\r\ngzip all_gene_disease_associations.txt\r\nrm all_gene_disease_associations.tar.gz</code></pre>\r\n\r\n<p>The procedure isn't difficult, but you could save users some time by doing away with the tarball, since it only contains a single file.</p>",
      "body_md": "I ended up doing the following steps to strip out the tarball:\r\n\r\n```shell\r\ntar -xzf all_gene_disease_associations.tar.gz\r\ngzip all_gene_disease_associations.txt\r\nrm all_gene_disease_associations.tar.gz\r\n```\r\n\r\nThe procedure isn't difficult, but you could save users some time by doing away with the tarball, since it only contains a single file.",
      "profile": 17,
      "published": "2015-08-18T17:15:29.535230Z",
      "thread": 105,
      "url": "/discussion/processing-disgenet-for-disease-gene-relationships/105#4"
    },
    {
      "body_html": "<h1>Choosing a score threshold</h1>\r\n\r\n<p>DisGeNET includes a <a href=\"http://www.disgenet.org/web/DisGeNET/menu/dbinfo#score\">score</a> for reported gene–disease relationships, described as <span class=\"citation\">[<a href=\"/doi/10.1093/database/bav028\" class=\"citation\" data-key=\"10.1093/database/bav028\">1</a>]</span>:</p>\r\n\r\n<blockquote><p>The score ranges from 0 to 1 and is computed according to the formula described in ‘Methods’ section. The DisGeNET score allows obtaining a ranking of GDAs and a straightforward classification of curated vs predicted vs literature-based associations since it stratifies the associations based on their level of evidence. For instance, associations only reported by UniProt or CTD, which have been curated by experts, have higher scores (i.e. associations with S ≥ 0.3) than those only supported by animal models or text-mining based sources.</p></blockquote>\r\n\r\n<p>We will need to choose a <a href=\"http://thinklab.com/discussion/the-tissues-resource-for-the-tissue-specificity-of-genes/91#4\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d91\">minumum threshold</a> for edge inclusion in our network. <a href=\"/u/janispi\" class=\"username\">@janispi</a>, can you give us some more information regarding scores? Specifically,</p>\r\n\r\n<ul><li>how do scores correspond to precision (the probability of the relationship being real)?</li><li>what is a reasonable cutoff to eliminate junk? Does any relationship with score &gt; 0 already have acceptable confidence?</li></ul>\r\n\r\n<p>We would like a permissive threshold, allowing up to a ~30% false discovery rate.</p>",
      "body_md": "# Choosing a score threshold\r\n\r\nDisGeNET includes a [score](http://www.disgenet.org/web/DisGeNET/menu/dbinfo#score) for reported gene--disease relationships, described as [@10.1093/database/bav028]:\r\n\r\n> The score ranges from 0 to 1 and is computed according to the formula described in ‘Methods’ section. The DisGeNET score allows obtaining a ranking of GDAs and a straightforward classification of curated vs predicted vs literature-based associations since it stratifies the associations based on their level of evidence. For instance, associations only reported by UniProt or CTD, which have been curated by experts, have higher scores (i.e. associations with S ≥ 0.3) than those only supported by animal models or text-mining based sources.\r\n\r\nWe will need to choose a [minumum threshold](http://thinklab.com/discussion/the-tissues-resource-for-the-tissue-specificity-of-genes/91#4) for edge inclusion in our network. @janispi, can you give us some more information regarding scores? Specifically,\r\n\r\n+ how do scores correspond to precision (the probability of the relationship being real)?\r\n+ what is a reasonable cutoff to eliminate junk? Does any relationship with score > 0 already have acceptable confidence?\r\n\r\nWe would like a permissive threshold, allowing up to a ~30% false discovery rate.",
      "profile": 17,
      "published": "2015-08-18T17:44:36.017043Z",
      "thread": 105,
      "url": "/discussion/processing-disgenet-for-disease-gene-relationships/105#5"
    },
    {
      "body_html": "<h1>Naming disease–gene metaedges</h1>\r\n\r\n<p>Up till now, we have been calling our <a href=\"http://thinklab.com/discussion/extracting-disease-gene-associations-from-the-gwas-catalog/80\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d80\">edges from GWAS</a> \"associations\" and our <a href=\"http://thinklab.com/discussion/functional-disease-annotations-for-genes-using-doaf/94\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d94\">edges from DOAF</a> \"functions\".</p>\r\n\r\n<p>DisGeNET uses a different <a href=\"http://www.disgenet.org/web/DisGeNET/menu/dbinfo#ontology\">nomenclature</a>. 'Association' refers to all disease–gene relationships while 'genetic variation' is more in line with what we call 'association'.</p>\r\n\r\n<p>Should we continue to call our GWAS edge 'association' and put DisGeNET into our 'function' edge? Or we could rename 'function' to 'relationship' to be more general? Or we could switch our GWAS edge to 'variation'.</p>",
      "body_md": "# Naming disease--gene metaedges\r\n\r\nUp till now, we have been calling our [edges from GWAS](http://thinklab.com/discussion/extracting-disease-gene-associations-from-the-gwas-catalog/80) \"associations\" and our [edges from DOAF](http://thinklab.com/discussion/functional-disease-annotations-for-genes-using-doaf/94) \"functions\".\r\n\r\nDisGeNET uses a different [nomenclature](http://www.disgenet.org/web/DisGeNET/menu/dbinfo#ontology). 'Association' refers to all disease--gene relationships while 'genetic variation' is more in line with what we call 'association'.\r\n\r\nShould we continue to call our GWAS edge 'association' and put DisGeNET into our 'function' edge? Or we could rename 'function' to 'relationship' to be more general? Or we could switch our GWAS edge to 'variation'.",
      "profile": 17,
      "published": "2015-08-18T17:56:25.861851Z",
      "thread": 105,
      "url": "/discussion/processing-disgenet-for-disease-gene-relationships/105#6"
    },
    {
      "body_html": "<p>I'm a fan of HetNets. It's short and catchy. It does seem to be used for wireless networking but if it's relatively unique in bioinformatics I think the short and catchy part is a big advantage.</p>",
      "body_md": "I'm a fan of HetNets. It's short and catchy. It does seem to be used for wireless networking but if it's relatively unique in bioinformatics I think the short and catchy part is a big advantage.",
      "profile": 22,
      "published": "2015-08-18T19:01:30.203903Z",
      "thread": 104,
      "url": "/discussion/renaming-heterogeneous-networks-to-a-more-concise-and-catchy-term/104#2"
    },
    {
      "body_html": "<p>I would recommend using the same criteria as in DisGeNET. \"Genetic Variation\" would be equivalent to GWAS.</p>",
      "body_md": "I would recommend using the same criteria as in DisGeNET. \"Genetic Variation\" would be equivalent to GWAS.",
      "profile": 129,
      "published": "2015-08-19T09:22:09.808177Z",
      "thread": 105,
      "url": "/discussion/processing-disgenet-for-disease-gene-relationships/105#7"
    },
    {
      "body_html": "<p><a href=\"/u/fbastian\" class=\"username\">@fbastian</a></p>\r\n\r\n<p>For now STARGEO annotations really funnel towards performing classical meta-analysis across studies given an standardized set of \"cases\" vs \"controls\".  This kind of fits with micro-array experimental design which usually \"contrast\" some type of case vs control.  So there is a concept of a control for a disease which may or may not be a \"healthy\" control.</p>",
      "body_md": "@fbastian\r\n\r\nFor now STARGEO annotations really funnel towards performing classical meta-analysis across studies given an standardized set of \"cases\" vs \"controls\".  This kind of fits with micro-array experimental design which usually \"contrast\" some type of case vs control.  So there is a concept of a control for a disease which may or may not be a \"healthy\" control.",
      "profile": 121,
      "published": "2015-08-19T18:11:25.554602Z",
      "thread": 96,
      "url": "/discussion/stargeo-expression-signatures-for-disease-using-crowdsourced-geo-annotation/96#4"
    },
    {
      "body_html": "<p><a href=\"/u/dhimmel\" class=\"username\">@dhimmel</a> </p>\r\n\r\n<p>We will soon have analyses working on the dev site: <a href=\"http://dev.stargeo.io\">http://dev.stargeo.io</a>.  We will probably shut down the old site by the end of the month.</p>",
      "body_md": "@dhimmel \r\n\r\nWe will soon have analyses working on the dev site: http://dev.stargeo.io.  We will probably shut down the old site by the end of the month.",
      "profile": 121,
      "published": "2015-08-19T18:12:42.999473Z",
      "thread": 96,
      "url": "/discussion/stargeo-expression-signatures-for-disease-using-crowdsourced-geo-annotation/96#5"
    },
    {
      "body_html": "<p>The Experimental Factor Ontology (<a href=\"http://www.ebi.ac.uk/efo/\">EFO</a>) <span class=\"citation\">[<a href=\"/doi/10.1093/bioinformatics/btq099\" class=\"citation\" data-key=\"10.1093/bioinformatics/btq099\">1</a>]</span> has a few terms related to what we need. For example,</p>\r\n\r\n<ul><li>case control design (<a href=\"http://www.ebi.ac.uk/efo/EFO_0001427\"><code>EFO:0001427</code></a>)</li><li>control (<a href=\"http://www.ebi.ac.uk/efo/EFO_0001461\"><code>EFO:0001461</code></a>)</li><li>individual (<a href=\"http://www.ebi.ac.uk/efo/EFO_0000542\"><code>EFO:0000542</code></a>)</li><li>tissue specimen (<a href=\"http://purl.obolibrary.org/obo/OBI_0001479\"><code>OBI:0001479</code></a>)</li></ul>\r\n\r\n<p>Ultimately, for a given disease, we want to be able to differentiate the following:</p>\r\n\r\n<ul><li>a healthy sample from healthy individual</li><li>a healthy sample from diseased individual</li><li>a disease sample from diseased individual</li></ul>\r\n\r\n<p>Essentially, we want to support two types of case-control analyses based on a single set of annotations:</p>\r\n\r\n<ol><li>samples from healthy individuals versus diseased individuals</li><li>samples from healthy tissue versus diseased tissue, where all samples may come from diseased individuals</li></ol>\r\n\r\n<p>It seems that most existing ontologies are good at describing the characteristics of a single sample — for example, its tissue of origin, the developmental stage of the donor, the phenotypes/diseases of the donor — but they are not good at allowing tagging for the sole purpose of contrast.</p>\r\n\r\n<p><a href=\"/u/idrdex\" class=\"username\">@idrdex</a> suggested that we could use \"qualifiers for the tags: like <code>PC_individual_case</code> vs <code>PC_individual_control</code> or <code>PC_tissue_case</code> vs <code>PC_tissue_control</code>\" I like this idea and think it is a good immediate solution.</p>",
      "body_md": "The Experimental Factor Ontology ([EFO](http://www.ebi.ac.uk/efo/)) [@10.1093/bioinformatics/btq099] has a few terms related to what we need. For example,\r\n\r\n+ case control design ([`EFO:0001427`](http://www.ebi.ac.uk/efo/EFO_0001427))\r\n+ control ([`EFO:0001461`](http://www.ebi.ac.uk/efo/EFO_0001461))\r\n+ individual ([`EFO:0000542`](http://www.ebi.ac.uk/efo/EFO_0000542))\r\n+ tissue specimen ([`OBI:0001479`](http://purl.obolibrary.org/obo/OBI_0001479))\r\n\r\nUltimately, for a given disease, we want to be able to differentiate the following:\r\n\r\n+ a healthy sample from healthy individual\r\n+ a healthy sample from diseased individual\r\n+ a disease sample from diseased individual\r\n\r\nEssentially, we want to support two types of case-control analyses based on a single set of annotations:\r\n\r\n1. samples from healthy individuals versus diseased individuals\r\n2. samples from healthy tissue versus diseased tissue, where all samples may come from diseased individuals\r\n\r\nIt seems that most existing ontologies are good at describing the characteristics of a single sample -- for example, its tissue of origin, the developmental stage of the donor, the phenotypes/diseases of the donor -- but they are not good at allowing tagging for the sole purpose of contrast.\r\n\r\n@idrdex suggested that we could use \"qualifiers for the tags: like `PC_individual_case` vs `PC_individual_control` or `PC_tissue_case` vs `PC_tissue_control`\" I like this idea and think it is a good immediate solution.",
      "profile": 17,
      "published": "2015-08-19T18:53:33.878396Z",
      "thread": 96,
      "url": "/discussion/stargeo-expression-signatures-for-disease-using-crowdsourced-geo-annotation/96#6"
    },
    {
      "body_html": "<blockquote><p>Do the names on these edges actually matter based on how you are using the network?</p></blockquote>\r\n\r\n<p><a href=\"/u/b_good\" class=\"username\">@b_good</a>, for predicting the probability of efficacy of a compound–disease pair, the metaedge names do not matter. The <a href=\"http://het.io/hnep\">algorithm</a> only considers the structure of the network. The names are used to assist with interpretability. For example, the <em>CtGad</em> feature (capturing when a compound targets genes that are associated with the disease) may be predictive. In this case, we would conclude that disease-associated genes are informative for repurposing. If what <a href=\"http://thinklab.com/discussion/processing-disgenet-for-disease-gene-relationships/105#6\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d105\">we call</a> an 'association' is actually some other type of relationship, then the interpretation that associations are influential will be unfounded.</p>\r\n\r\n<p>When we have multiple metaedges between the same metanodes, we hope there is a difference in the type of information encoded. Otherwise, we would be better off having only a single metaedge. For example, we included <a href=\"http://thinklab.com/discussion/integrating-drug-target-information-from-bindingdb/53#5\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d53\">binding</a> and <a href=\"http://thinklab.com/discussion/protein-target-carrier-transporter-and-enzyme-interactions-in-drugbank/65#1\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d65\">target</a> edges between compounds and genes. It is unclear whether merging these edges would be beneficial, because it's difficult to know how they differ. Therefore, a good understanding what information each metaedge captures will assist with metagraph design. Accurate metaedge names can help with understanding edge content and therefore network design decisions.</p>\r\n\r\n<blockquote><p>Would also like to see how the result of other text-ming approaches would influence the outcome. e.g. would it change things if you swapped in the relations from semmedDB?</p></blockquote>\r\n\r\n<p>Currently, I am happy with the quality of our <a href=\"http://thinklab.com/discussion/mining-knowledge-from-medline-articles-and-their-indexed-mesh-terms/67\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d67\">MEDLINE topic cooccurrence</a> approach. I assume that you highlight <a href=\"http://skr3.nlm.nih.gov/SemMedDB/\">SemMedDB</a> <span class=\"citation\">[<a href=\"/doi/10.1093/bioinformatics/bts591\" class=\"citation\" data-key=\"10.1093/bioinformatics/bts591\">1</a>]</span> because it has the ability to extract the type of relationship. I agree this could be a valuable addition. However, in the interest of time, this will most likely have to wait till a successive project.</p>",
      "body_md": "> Do the names on these edges actually matter based on how you are using the network?\r\n\r\n@b_good, for predicting the probability of efficacy of a compound--disease pair, the metaedge names do not matter. The [algorithm](http://het.io/hnep) only considers the structure of the network. The names are used to assist with interpretability. For example, the *CtGad* feature (capturing when a compound targets genes that are associated with the disease) may be predictive. In this case, we would conclude that disease-associated genes are informative for repurposing. If what [we call](http://thinklab.com/discussion/processing-disgenet-for-disease-gene-relationships/105#6) an 'association' is actually some other type of relationship, then the interpretation that associations are influential will be unfounded.\r\n\r\nWhen we have multiple metaedges between the same metanodes, we hope there is a difference in the type of information encoded. Otherwise, we would be better off having only a single metaedge. For example, we included [binding](http://thinklab.com/discussion/integrating-drug-target-information-from-bindingdb/53#5) and [target](http://thinklab.com/discussion/protein-target-carrier-transporter-and-enzyme-interactions-in-drugbank/65#1) edges between compounds and genes. It is unclear whether merging these edges would be beneficial, because it's difficult to know how they differ. Therefore, a good understanding what information each metaedge captures will assist with metagraph design. Accurate metaedge names can help with understanding edge content and therefore network design decisions.\r\n\r\n> Would also like to see how the result of other text-ming approaches would influence the outcome. e.g. would it change things if you swapped in the relations from semmedDB?\r\n\r\nCurrently, I am happy with the quality of our [MEDLINE topic cooccurrence](http://thinklab.com/discussion/mining-knowledge-from-medline-articles-and-their-indexed-mesh-terms/67) approach. I assume that you highlight [SemMedDB](http://skr3.nlm.nih.gov/SemMedDB/) [@10.1093/bioinformatics/bts591] because it has the ability to extract the type of relationship. I agree this could be a valuable addition. However, in the interest of time, this will most likely have to wait till a successive project.",
      "profile": 17,
      "published": "2015-08-20T06:09:34.741046Z",
      "thread": 48,
      "url": "/discussion/text-as-a-resource-for-network-population/48#4"
    },
    {
      "body_html": "<p><a href=\"/u/idrdex\" class=\"username\">@idrdex</a>: We've been doing some extensive curation (i.e. back to the literature describing the experiments) on ~1000 samples that matter a lot to a project that we're working on. A couple questions about STARGEO:<br>1) Is it going to/does it also include samples from organisms other than human?<br>2) What's the best way to contribute these annotations? Anything programmatic and/or spreadsheet friendly?<br>3) What's the API like to extract annotations?</p>\r\n\r\n<p>Thanks!<br>Casey</p>",
      "body_md": "@idrdex: We've been doing some extensive curation (i.e. back to the literature describing the experiments) on ~1000 samples that matter a lot to a project that we're working on. A couple questions about STARGEO:\r\n1) Is it going to/does it also include samples from organisms other than human?\r\n2) What's the best way to contribute these annotations? Anything programmatic and/or spreadsheet friendly?\r\n3) What's the API like to extract annotations?\r\n\r\nThanks!\r\nCasey",
      "profile": 22,
      "published": "2015-08-20T16:26:43.227203Z",
      "thread": 96,
      "url": "/discussion/stargeo-expression-signatures-for-disease-using-crowdsourced-geo-annotation/96#7"
    },
    {
      "body_html": "<p>Analysis is working now here: <a href=\"http://dev.stargeo.io/analysis/\">http://dev.stargeo.io/analysis/</a></p>",
      "body_md": "Analysis is working now here: http://dev.stargeo.io/analysis/",
      "profile": 121,
      "published": "2015-08-20T17:07:44.875916Z",
      "thread": 96,
      "url": "/discussion/stargeo-expression-signatures-for-disease-using-crowdsourced-geo-annotation/96#8"
    },
    {
      "body_html": "<h1>Preliminary processing complete</h1>\r\n\r\n<p>We processed DisGeNET by converting to DO Slim diseases (<a href=\"https://github.com/dhimmel/disgenet/blob/b21553d7fced15c309f0a7aa15fc68e4d3edaa03/disgenet.ipynb\">notebook</a>, <a href=\"https://github.com/dhimmel/disgenet/blob/b21553d7fced15c309f0a7aa15fc68e4d3edaa03/data/consolidated.tsv\">download</a>). We used propagated mappings, so for example relationships with relapsing-remitting multiple sclerosis would be included for multiple sclerosis.</p>\r\n\r\n<p>The result was 82,833 gene–disease associations. After filtering for scores ≥ 0.06, 7,779 associations remained with large variability in the number of associations per disease. Additionally, many of the associations appear to be 'genetic variation' edges, which may be captured by our <a href=\"http://thinklab.com/discussion/extracting-disease-gene-associations-from-the-gwas-catalog/80\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d80\">GWAS edge</a>. As a reminder, the 0.06 score threshold includes the following (thanks <a href=\"/u/janispi\" class=\"username\">@janispi</a>):</p>\r\n\r\n<blockquote><p>If you choose score ≥ 0.06, then you will be including associations reporting by curated sources, or having animal models supporting them, or being reported by several papers (20–200).</p></blockquote>\r\n\r\n<p>We mapped DO Slim terms to DisGeNET using UMLS cross-references. The UMLS cross-references in the DO were often non-exact, so one DO term would reference many UMLS terms. Several <a href=\"https://github.com/dhimmel/disgenet/blob/master/data/unmapped-umls.tsv\">UMLS terms</a> referenced by the DO were not in DisGeNET.</p>",
      "body_md": "# Preliminary processing complete\r\n\r\nWe processed DisGeNET by converting to DO Slim diseases ([notebook](https://github.com/dhimmel/disgenet/blob/b21553d7fced15c309f0a7aa15fc68e4d3edaa03/disgenet.ipynb), [download](https://github.com/dhimmel/disgenet/blob/b21553d7fced15c309f0a7aa15fc68e4d3edaa03/data/consolidated.tsv)). We used propagated mappings, so for example relationships with relapsing-remitting multiple sclerosis would be included for multiple sclerosis.\r\n\r\nThe result was 82,833 gene--disease associations. After filtering for scores ≥ 0.06, 7,779 associations remained with large variability in the number of associations per disease. Additionally, many of the associations appear to be 'genetic variation' edges, which may be captured by our [GWAS edge](http://thinklab.com/discussion/extracting-disease-gene-associations-from-the-gwas-catalog/80). As a reminder, the 0.06 score threshold includes the following (thanks @janispi):\r\n\r\n> If you choose score ≥ 0.06, then you will be including associations reporting by curated sources, or having animal models supporting them, or being reported by several papers (20--200).\r\n\r\nWe mapped DO Slim terms to DisGeNET using UMLS cross-references. The UMLS cross-references in the DO were often non-exact, so one DO term would reference many UMLS terms. Several [UMLS terms](https://github.com/dhimmel/disgenet/blob/master/data/unmapped-umls.tsv) referenced by the DO were not in DisGeNET.",
      "profile": 17,
      "published": "2015-08-20T20:58:18.326439Z",
      "thread": 105,
      "url": "/discussion/processing-disgenet-for-disease-gene-relationships/105#8"
    },
    {
      "body_html": "<p>We are looking into <a href=\"http://diseases.jensenlab.org/Search\">DISEASES</a> <span class=\"citation\">[<a href=\"/doi/10.1016/j.ymeth.2014.11.020\" class=\"citation\" data-key=\"10.1016/j.ymeth.2014.11.020\">1</a>]</span> as a resource for gene–disease relationships. This database is produced by <a href=\"/u/larsjuhljensen\" class=\"username\">@larsjuhljensen</a>'s group and follows similar protocols as <a href=\"http://tissues.jensenlab.org/Search\">TISSUES</a> <span class=\"citation\">[<a href=\"/doi/10.7717/peerj.1054\" class=\"citation\" data-key=\"10.7717/peerj.1054\">2</a>]</span>, which we have <a href=\"http://thinklab.com/discussion/the-tissues-resource-for-the-tissue-specificity-of-genes/91\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d91\">already processed</a>.</p>\r\n\r\n<p>DISEASES includes three types of evidence:</p>\r\n\r\n<ul><li><strong>text mining</strong>: using named entity recognition to look for disease–protein cooccurrences in abstracts and sentences. <a href=\"/u/larsjuhljensen\" class=\"username\">@larsjuhljensen</a>, which literature corpus was used?</li><li><strong>knowledge</strong>: curated relationships from <a href=\"http://ghr.nlm.nih.gov/\">GHR</a> and <a href=\"http://www.uniprot.org/uniprot/\">UniProtKB</a> <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkt1140\" class=\"citation\" data-key=\"10.1093/nar/gkt1140\">3</a>]</span></li><li><strong>experiments</strong>: cancer mutation data from <a href=\"http://cancer.sanger.ac.uk/cosmic\">COSMIC</a> <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gku1075\" class=\"citation\" data-key=\"10.1093/nar/gku1075\">4</a>, <a href=\"/doi/10.1002/0471142905.hg1011s57\" class=\"citation\" data-key=\"10.1002/0471142905.hg1011s57\">5</a>]</span> and GWAS data from <a href=\"http://distild.jensenlab.org/about.html\">DistiLD</a> <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkr899\" class=\"citation\" data-key=\"10.1093/nar/gkr899\">6</a>]</span></li></ul>\r\n\r\n<p>We did a preliminary processing of the integrated dataset, which yielded 81,499 gene–disease relationships for <a href=\"http://thinklab.com/discussion/unifying-disease-vocabularies/44#6\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d44\">DO Slim</a> diseases (<a href=\"https://github.com/dhimmel/diseases/blob/9ffce4c46d243392a66192ec879df153b2f6830a/diseases.ipynb\">notebook</a>, <a href=\"https://github.com/dhimmel/diseases/blob/9ffce4c46d243392a66192ec879df153b2f6830a/data/integrated.tsv\">download</a>). Filtering for scores ≥ 3, resulted in 2,441 relationships.</p>\r\n\r\n<p><a href=\"/u/larsjuhljensen\" class=\"username\">@larsjuhljensen</a>, are scores in DISEASES comparable between datasets? In other words, are confidence scores standardized to a common gold standard?</p>\r\n\r\n<p>We may consider creating an <a href=\"http://thinklab.com/discussion/the-tissues-resource-for-the-tissue-specificity-of-genes/91#7\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d91\">integrated score</a> excluding DistiLD, since we have a <a href=\"http://thinklab.com/discussion/extracting-disease-gene-associations-from-the-gwas-catalog/80\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d80\">distinct GWAS edge</a>.</p>",
      "body_md": "We are looking into [DISEASES](http://diseases.jensenlab.org/Search) [@10.1016/j.ymeth.2014.11.020] as a resource for gene--disease relationships. This database is produced by @larsjuhljensen's group and follows similar protocols as [TISSUES](http://tissues.jensenlab.org/Search) [@10.7717/peerj.1054], which we have [already processed](http://thinklab.com/discussion/the-tissues-resource-for-the-tissue-specificity-of-genes/91).\r\n\r\nDISEASES includes three types of evidence:\r\n\r\n+ **text mining**: using named entity recognition to look for disease--protein cooccurrences in abstracts and sentences. @larsjuhljensen, which literature corpus was used?\r\n+ **knowledge**: curated relationships from [GHR](http://ghr.nlm.nih.gov/) and [UniProtKB](http://www.uniprot.org/uniprot/) [@10.1093/nar/gkt1140]\r\n+ **experiments**: cancer mutation data from [COSMIC](http://cancer.sanger.ac.uk/cosmic) [@10.1093/nar/gku1075 @10.1002/0471142905.hg1011s57] and GWAS data from [DistiLD](http://distild.jensenlab.org/about.html) [@10.1093/nar/gkr899]\r\n\r\nWe did a preliminary processing of the integrated dataset, which yielded 81,499 gene–disease relationships for [DO Slim](http://thinklab.com/discussion/unifying-disease-vocabularies/44#6) diseases ([notebook](https://github.com/dhimmel/diseases/blob/9ffce4c46d243392a66192ec879df153b2f6830a/diseases.ipynb), [download](https://github.com/dhimmel/diseases/blob/9ffce4c46d243392a66192ec879df153b2f6830a/data/integrated.tsv)). Filtering for scores ≥ 3, resulted in 2,441 relationships.\r\n\r\n@larsjuhljensen, are scores in DISEASES comparable between datasets? In other words, are confidence scores standardized to a common gold standard?\r\n\r\nWe may consider creating an [integrated score](http://thinklab.com/discussion/the-tissues-resource-for-the-tissue-specificity-of-genes/91#7) excluding DistiLD, since we have a [distinct GWAS edge](http://thinklab.com/discussion/extracting-disease-gene-associations-from-the-gwas-catalog/80).",
      "profile": 17,
      "published": "2015-08-20T21:45:22.268315Z",
      "thread": 106,
      "url": "/discussion/processing-the-diseases-resource-for-diseasegene-relationships/106"
    },
    {
      "body_html": "<h1>Network overview</h1>\r\n\r\n<p>We recently released the <a href=\"http://thinklab.com/discussion/one-network-to-rule-them-all/102\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d102\">first version</a> of our network containing 10 node types and 27 edge types. The network contains data (nodes and edges) extracted from <a href=\"https://github.com/dhimmel/integrate/tree/0e343d8745a98757c86e73f645b41353f52b82b5/licenses\">28 resources</a>. Many of these 28 resources have themselves compiled data from disparately-licensed resources. In addition:</p>\r\n\r\n<ul><li>12 lack any licensing information</li><li>10 use standard licenses</li><li>6 use custom licenses</li><li>3 resources are publication supplements</li><li>6 forbid commercial use</li><li>2 forbid any redistribution of the data</li></ul>\r\n\r\n<h1>Why an open network</h1>\r\n\r\n<p>We <a href=\"http://thinklab.com/discussion/enabling-reproducibility-and-reuse/23#6\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d23\">are committed</a> to performing an open project, where all code, data, analyses, and results are maximally reproducible and reusable. The foundation of our research is that datasets are more informative when placed in a broader context. Through integration, we create a resource that is more informative and versatile than the 28 separate sources.</p>\r\n\r\n<p>However, data integration is challenging and time intensive. Thus far, our integration effort consists of an 8 month time investment, 41 Thinklab <a href=\"http://thinklab.com/p/rephetio/discussion\">discussions</a>, and 35 GitHub <a href=\"https://github.com/dhimmel?tab=repositories\">repositories</a>. By making our network public and extensible, other researchers can avoid this laborious process while harnessing the benefits of integration.</p>\r\n\r\n<h1>The licensing problem</h1>\r\n\r\n<p>We initially released our network under the <a href=\"https://creativecommons.org/publicdomain/zero/1.0/\">CC0</a> (public domain) license, but <a href=\"/u/larsjuhljensen\" class=\"username\">@larsjuhljensen</a> <a href=\"http://thinklab.com/discussion/one-network-to-rule-them-all/102#2\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d102\">pointed out</a> that this may violate many sources' licensing. While we used only publicly available resources — funded primarily by the public — many resources are burdened by restrictive licenses. We now must integrate data with incompatible licenses that require legal expertise to understand and operate in jurisdiction dependent manners.</p>\r\n\r\n<h1>Compliance and caveats</h1>\r\n\r\n<p>We are seeking expert advice on how to proceed. We would like to achieve the following:</p>\r\n\r\n<ul><li>a network that is publicly available in full and maximally unrestricted</li><li>public domain findings. Foremost, unencumbered predictions of drug efficacy</li><li>legal compliance</li><li>normative compliance that respects the intent of the data creators</li><li>minimal pruning of the current network to preserve our investment</li></ul>\r\n\r\n<p>We plan to add node/edge-specific attribution and license information to our network, but will await expert advice before proceeding.</p>",
      "body_md": "# Network overview\r\n\r\nWe recently released the [first version](http://thinklab.com/discussion/one-network-to-rule-them-all/102) of our network containing 10 node types and 27 edge types. The network contains data (nodes and edges) extracted from [28 resources](https://github.com/dhimmel/integrate/tree/0e343d8745a98757c86e73f645b41353f52b82b5/licenses). Many of these 28 resources have themselves compiled data from disparately-licensed resources. In addition:\r\n\r\n+ 12 lack any licensing information\r\n+ 10 use standard licenses\r\n+ 6 use custom licenses\r\n+ 3 resources are publication supplements\r\n+ 6 forbid commercial use\r\n+ 2 forbid any redistribution of the data\r\n\r\n# Why an open network\r\n\r\nWe [are committed](http://thinklab.com/discussion/enabling-reproducibility-and-reuse/23#6) to performing an open project, where all code, data, analyses, and results are maximally reproducible and reusable. The foundation of our research is that datasets are more informative when placed in a broader context. Through integration, we create a resource that is more informative and versatile than the 28 separate sources.\r\n\r\nHowever, data integration is challenging and time intensive. Thus far, our integration effort consists of an 8 month time investment, 41 Thinklab [discussions](http://thinklab.com/p/rephetio/discussion), and 35 GitHub [repositories](https://github.com/dhimmel?tab=repositories). By making our network public and extensible, other researchers can avoid this laborious process while harnessing the benefits of integration.\r\n\r\n# The licensing problem\r\n\r\nWe initially released our network under the [CC0](https://creativecommons.org/publicdomain/zero/1.0/) (public domain) license, but @larsjuhljensen [pointed out](http://thinklab.com/discussion/one-network-to-rule-them-all/102#2) that this may violate many sources' licensing. While we used only publicly available resources -- funded primarily by the public -- many resources are burdened by restrictive licenses. We now must integrate data with incompatible licenses that require legal expertise to understand and operate in jurisdiction dependent manners.\r\n\r\n#  Compliance and caveats\r\n\r\nWe are seeking expert advice on how to proceed. We would like to achieve the following:\r\n\r\n+ a network that is publicly available in full and maximally unrestricted\r\n+ public domain findings. Foremost, unencumbered predictions of drug efficacy\r\n+ legal compliance\r\n+ normative compliance that respects the intent of the data creators\r\n+ minimal pruning of the current network to preserve our investment\r\n\r\nWe plan to add node/edge-specific attribution and license information to our network, but will await expert advice before proceeding.",
      "profile": 17,
      "published": "2015-08-28T19:10:38.921394Z",
      "thread": 107,
      "url": "/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107"
    },
    {
      "body_html": "<p>Regarding the scores, they are designed to be as comparable as we could make them; however, it was not possible to do so purely through benchmarking, since a high-quality unbiased benchmark set does not exist.</p>\r\n\r\n<p>If you already have GWAS from another source, I would exclude DistiLD too. You already import mutation data from e.g. COSMIC, I would exclude the experiments channel entirely. This also makes comparability of scores much less of an issue, since you're left with only automatically text-mined associations, which are scores the same way as tissue associations, and manually curated associations, which are inherently highly reliable.</p>",
      "body_md": "Regarding the scores, they are designed to be as comparable as we could make them; however, it was not possible to do so purely through benchmarking, since a high-quality unbiased benchmark set does not exist.\r\n\r\nIf you already have GWAS from another source, I would exclude DistiLD too. You already import mutation data from e.g. COSMIC, I would exclude the experiments channel entirely. This also makes comparability of scores much less of an issue, since you're left with only automatically text-mined associations, which are scores the same way as tissue associations, and manually curated associations, which are inherently highly reliable.",
      "profile": 125,
      "published": "2015-08-21T05:35:20.574451Z",
      "thread": 106,
      "url": "/discussion/processing-the-diseases-resource-for-diseasegene-relationships/106#2"
    },
    {
      "body_html": "<h1>Completed processing</h1>\r\n\r\n<p>We have completed an initial processing of DISEASES (<a href=\"https://github.com/dhimmel/diseases/blob/e0089ef89a56348d7d4e0684a9c51c5747b16237/diseases.ipynb\">notebook</a>). The output is a tsv of gene–disease pairs (<a href=\"https://github.com/dhimmel/diseases/blob/e0089ef89a56348d7d4e0684a9c51c5747b16237/data/merged.tsv.gz\">download</a>) with scores for following channels:</p>\r\n\r\n<ul><li>text mining</li><li>knowledge</li><li>cosmic — the COSMIC subset of the experiments channel</li><li>distild — the DistiLD subset of the experiments channel</li><li>integrated_no_distild — the integration of the four aforementioned scores</li><li>integrated — the integrated score calculated by the DISEASES team, without any exclusions</li></ul>\r\n\r\n<p>Genes were converted to Entrez identifiers using the STRING 9.1 mapping (<a href=\"ftp://string-db.org/STRING/9.1/mapping_files/Entrez_mappings/entrez_gene_id.vs.string.v9.05.28122012.txt\"><code>entrez_gene_id.vs.string.v9.05.28122012.txt</code></a>). We also created a dataset with only <a href=\"http://thinklab.com/discussion/unifying-disease-vocabularies/44#6\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d44\">DO Slim</a> diseases (<a href=\"https://github.com/dhimmel/diseases/blob/e0089ef89a56348d7d4e0684a9c51c5747b16237/data/merged-slim.tsv\">download</a>). For this file, we propagated scores from subsumed diseases and reported the max.</p>\r\n\r\n<h2>Visualizing channel concordance</h2>\r\n\r\n<p>We visualized the relationships between scores on the full dataset. The off-diagonal plots show a 2D histogram, using hexagonal bins. The diagonal of the grid contains 1D histograms for the x-variable. Bin counts for all panels are log-transformed.</p>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/dhimmel/diseases/e0089ef89a56348d7d4e0684a9c51c5747b16237/figure/channel-histograms.png\" alt=\"\"></p>",
      "body_md": "# Completed processing\r\n\r\nWe have completed an initial processing of DISEASES ([notebook](https://github.com/dhimmel/diseases/blob/e0089ef89a56348d7d4e0684a9c51c5747b16237/diseases.ipynb)). The output is a tsv of gene--disease pairs ([download](https://github.com/dhimmel/diseases/blob/e0089ef89a56348d7d4e0684a9c51c5747b16237/data/merged.tsv.gz)) with scores for following channels:\r\n\r\n+ text mining\r\n+ knowledge\r\n+ cosmic -- the COSMIC subset of the experiments channel\r\n+ distild -- the DistiLD subset of the experiments channel\r\n+ integrated_no_distild -- the integration of the four aforementioned scores\r\n+ integrated -- the integrated score calculated by the DISEASES team, without any exclusions\r\n\r\nGenes were converted to Entrez identifiers using the STRING 9.1 mapping ([`entrez_gene_id.vs.string.v9.05.28122012.txt`](ftp://string-db.org/STRING/9.1/mapping_files/Entrez_mappings/entrez_gene_id.vs.string.v9.05.28122012.txt)). We also created a dataset with only [DO Slim](http://thinklab.com/discussion/unifying-disease-vocabularies/44#6) diseases ([download](https://github.com/dhimmel/diseases/blob/e0089ef89a56348d7d4e0684a9c51c5747b16237/data/merged-slim.tsv)). For this file, we propagated scores from subsumed diseases and reported the max.\r\n\r\n## Visualizing channel concordance\r\n\r\nWe visualized the relationships between scores on the full dataset. The off-diagonal plots show a 2D histogram, using hexagonal bins. The diagonal of the grid contains 1D histograms for the x-variable. Bin counts for all panels are log-transformed.\r\n\r\n![](https://raw.githubusercontent.com/dhimmel/diseases/e0089ef89a56348d7d4e0684a9c51c5747b16237/figure/channel-histograms.png)",
      "profile": 17,
      "published": "2015-08-21T20:49:59.898726Z",
      "thread": 106,
      "url": "/discussion/processing-the-diseases-resource-for-diseasegene-relationships/106#3"
    },
    {
      "body_html": "<p>You may also want to consider splitting the network into multiple files. For example, you may have a base file that includes, for example, only public domain and CC-BY content. The edges with SA and/or NC clauses could be in \"add-on\" files. This partially avoids the problem that your complete network file becomes subject to the lowest common denominator.</p>\r\n\r\n<p>Having multiple files will allow people to \"pick their poison\", so to speak. If they need the most permissive license, they will get a less complete network. If they want the most complete network, they will have to live with a less permissive license.</p>",
      "body_md": "You may also want to consider splitting the network into multiple files. For example, you may have a base file that includes, for example, only public domain and CC-BY content. The edges with SA and/or NC clauses could be in \"add-on\" files. This partially avoids the problem that your complete network file becomes subject to the lowest common denominator.\r\n\r\nHaving multiple files will allow people to \"pick their poison\", so to speak. If they need the most permissive license, they will get a less complete network. If they want the most complete network, they will have to live with a less permissive license.",
      "profile": 125,
      "published": "2015-08-28T19:37:36.448537Z",
      "thread": 107,
      "url": "/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#2"
    },
    {
      "body_html": "<p>Regarding the 12 that completely lack any licensing information, I would contact the authors. For academic databases/datasets, this is usually due to people not knowing that when it comes to copyright, the default is \"all rights reserved\". Academics often put things on the internet, thinking that this makes it \"public domain\". If you ask them, they will likely be happy to put a CC0 waiver or CC-BY license on it.</p>",
      "body_md": "Regarding the 12 that completely lack any licensing information, I would contact the authors. For academic databases/datasets, this is usually due to people not knowing that when it comes to copyright, the default is \"all rights reserved\". Academics often put things on the internet, thinking that this makes it \"public domain\". If you ask them, they will likely be happy to put a CC0 waiver or CC-BY license on it.",
      "profile": 125,
      "published": "2015-08-28T21:19:52.274011Z",
      "thread": 107,
      "url": "/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#3"
    },
    {
      "body_html": "<h1>Conda for R</h1>\r\n\r\n<p>Conda is an awesome package manager that <a href=\"http://thinklab.com/discussion/python-for-the-modern-biodata-scientist/84\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d84\">we've been using</a> for Python. In most cases, conda alleviates the horror of installation errors and dependencies.</p>\r\n\r\n<p>Now conda is <a href=\"http://continuum.io/conda-for-R\">available for R</a>. In other words, you can install R using the following command:</p>\r\n\r\n<pre><code class=\"no-highlight hljs\">conda install --channel r r</code></pre>\r\n\r\n<p>I found many of my favorite R packages were available in conda's r channel, usually with <code>r-</code> prepended to their lowercased name. For example, I installed <code>r-ggplot2</code>, <code>r-tidyr</code>, <code>r-dplyr</code>, <code>r-caret</code>, and <code>r-glmnet</code>. For notebook support, I installed <code>rpy2</code> and <code>r-irkernel</code>.</p>\r\n\r\n<p>Installing R packages that are not included in conda's channel became more difficult after the switch to conda management. For example, <code>devtools::install_github()</code> was failing, and when doing traditional package installation, I had to specify the repos argument because the GUI popup was broken:</p>\r\n\r\n<pre><code class=\"r\">install.packages('readr', repos='http://cran.us.r-project.org')</code></pre>\r\n\r\n<p>Conda can definitely save R users lots of frustration, but I suggest the general user wait for greater maturity before adoption.</p>",
      "body_md": "# Conda for R\r\n\r\nConda is an awesome package manager that [we've been using](http://thinklab.com/discussion/python-for-the-modern-biodata-scientist/84) for Python. In most cases, conda alleviates the horror of installation errors and dependencies.\r\n\r\nNow conda is [available for R](http://continuum.io/conda-for-R). In other words, you can install R using the following command:\r\n\r\n```shell\r\nconda install --channel r r\r\n```\r\n\r\nI found many of my favorite R packages were available in conda's r channel, usually with `r-` prepended to their lowercased name. For example, I installed `r-ggplot2`, `r-tidyr`, `r-dplyr`, `r-caret`, and `r-glmnet`. For notebook support, I installed `rpy2` and `r-irkernel`.\r\n\r\nInstalling R packages that are not included in conda's channel became more difficult after the switch to conda management. For example, `devtools::install_github()` was failing, and when doing traditional package installation, I had to specify the repos argument because the GUI popup was broken:\r\n\r\n```r\r\ninstall.packages('readr', repos='http://cran.us.r-project.org')\r\n```\r\n\r\nConda can definitely save R users lots of frustration, but I suggest the general user wait for greater maturity before adoption.",
      "profile": 17,
      "published": "2015-08-29T19:43:43.418204Z",
      "thread": 83,
      "url": "/discussion/r-best-practices/83#2"
    },
    {
      "body_html": "<h1>Workflow details</h1>\r\n\r\n<p>Our data workflow consists of three major stages. Each stage invokes various <a href=\"http://www.smashingmagazine.com/2011/06/understanding-copyright-and-licenses/\">aspects of copyright</a> as described below</p>\r\n\r\n<h2>1) Resource processing</h2>\r\n\r\n<p>Most resources require processing before they can be added to the network. Common steps include terminology conversion, quality control, subsetting, and record merging.</p>\r\n\r\n<p>Our general procedure is to create a public GitHub repository for each resource (examples <a href=\"https://github.com/dhimmel/diseases\">1</a>, <a href=\"https://github.com/dhimmel/SIDER4\">2</a>, <a href=\"https://github.com/dhimmel/lincs\">3</a>, <a href=\"https://github.com/dhimmel/uberon\">4</a>, <a href=\"https://github.com/dhimmel/drugbank\">5</a>). Separate repositories help keep the project modular and reusable. Each repository contains a <code>download</code> directory where we store the unmodified input. Having the local copy is important for reproducibility because the original download location may become unavailable or serve an updated dataset. Therefore, our <code>download</code> directory redistributes unmodified data.</p>\r\n\r\n<p>Next, we process data from the <code>download</code> directory and save the resulting datasets in the <code>data</code> directory. The processing steps generally change the database model and field names and include a substantial portion of the original data. However, the original data has usually been transformed in some regard.</p>\r\n\r\n<p><strong>Proposed action</strong>: apply the source's license to the contents of <code>download</code>. For the contents of <code>data</code>, apply either the source's license or CC0 if the underlying data is not <a href=\"http://www.bitlaw.com/copyright/database.html\">subject to copyright</a> or the derivative work qualifies as fair use. Resources without a license or that explicitly forbid redistribution are problematic. We propose contacting the creators of these resources for permission or licensing clarification. Components in these repositories that do not derive from protected resources will be released as CC0.</p>\r\n\r\n<h2>2) Integrative network</h2>\r\n\r\n<p>Our <a href=\"https://github.com/dhimmel/integrate\"><code>integrate</code></a> repository combines the resource-specific data from stage 1 into a single network. The <a href=\"https://github.com/dhimmel/integrate/tree/master/compile\"><code>compile</code></a> directory merges resources with the same type of information. The creation of the network is performed by <a href=\"https://github.com/dhimmel/integrate/blob/master/integrate.ipynb\"><code>integrate.ipynb</code></a>. We have <a href=\"https://github.com/dhimmel/integrate/tree/master/licenses\">compiled</a> the licenses for each resource. The network is saved as text files in the <a href=\"https://github.com/dhimmel/integrate/tree/master/data\"><code>data</code></a> directory with <a href=\"https://github.com/dhimmel/integrate/blob/master/data/hetnet.json.gz\"><code>hetnet.json.gz</code></a> as the main release. In this integrated network, the database model and field names from the original resource are not present, just derived data.</p>\r\n\r\n<p><strong>Proposed action</strong>: Adopt a per node/edge licensing framework. Identify which nodes and edges, if any, are eligible for CC0 release. CC0 release may be possible if the creators chose a permissive license or give us permission, the network is fair use, or if the underlying content is not subject to copyright.</p>\r\n\r\n<h2>3) Network analyses</h2>\r\n\r\n<p>Next, we use the integrated network from stage 2 for data mining. The purpose of the data mining is to evaluate methods, extract insights, and make predictions. As an example, see <a href=\"https://github.com/dhimmel/snplentiful\">this analysis</a> <span class=\"citation\">[<a href=\"/doi/10.5281/zenodo.30105\" class=\"citation\" data-key=\"10.5281/zenodo.30105\">1</a>]</span> of the network that <a href=\"/u/caseygreene\" class=\"username\">@caseygreene</a> and I recently did for a separate project.</p>\r\n\r\n<p>Here, it is crucial that findings from analyses on the network are fair use and can be placed in the public domain. Since, the network contains data with <a href=\"https://wiki.creativecommons.org/wiki/Wiki/cc_license_compatibility\">incompatible</a> licenses such as CC-BY-SA and CC-BY-NC, data mining will be impossible if not considered fair use. In the US, <a href=\"http://www.arl.org/storage/documents/TDM-5JUNE2015.pdf\">precedent</a> implies our network analyses qualify as fair use. </p>\r\n\r\n<p><strong>Proposed action</strong>: Identify whether our network analyses qualify as fair use and whether our results can be released as CC0. Evaluate when and if we are subject to European copyright laws, which are less favorable for content users.</p>\r\n\r\n<h1>Expert feedback requested</h1>\r\n\r\n<p>We are seeking expert advice. Specifically, are the proposed actions compliant with copyright law? Regarding the three stages, are we on the right track? Will network analyses count as fair use?</p>",
      "body_md": "# Workflow details\r\n\r\nOur data workflow consists of three major stages. Each stage invokes various [aspects of copyright](http://www.smashingmagazine.com/2011/06/understanding-copyright-and-licenses/) as described below\r\n\r\n## 1) Resource processing\r\n\r\nMost resources require processing before they can be added to the network. Common steps include terminology conversion, quality control, subsetting, and record merging.\r\n\r\nOur general procedure is to create a public GitHub repository for each resource (examples [1](https://github.com/dhimmel/diseases), [2](https://github.com/dhimmel/SIDER4), [3](https://github.com/dhimmel/lincs), [4](https://github.com/dhimmel/uberon), [5](https://github.com/dhimmel/drugbank)). Separate repositories help keep the project modular and reusable. Each repository contains a `download` directory where we store the unmodified input. Having the local copy is important for reproducibility because the original download location may become unavailable or serve an updated dataset. Therefore, our `download` directory redistributes unmodified data.\r\n\r\nNext, we process data from the `download` directory and save the resulting datasets in the `data` directory. The processing steps generally change the database model and field names and include a substantial portion of the original data. However, the original data has usually been transformed in some regard.\r\n\r\n**Proposed action**: apply the source's license to the contents of `download`. For the contents of `data`, apply either the source's license or CC0 if the underlying data is not [subject to copyright](http://www.bitlaw.com/copyright/database.html) or the derivative work qualifies as fair use. Resources without a license or that explicitly forbid redistribution are problematic. We propose contacting the creators of these resources for permission or licensing clarification. Components in these repositories that do not derive from protected resources will be released as CC0.\r\n\r\n## 2) Integrative network\r\n\r\nOur [`integrate`](https://github.com/dhimmel/integrate) repository combines the resource-specific data from stage 1 into a single network. The [`compile`](https://github.com/dhimmel/integrate/tree/master/compile) directory merges resources with the same type of information. The creation of the network is performed by [`integrate.ipynb`](https://github.com/dhimmel/integrate/blob/master/integrate.ipynb). We have [compiled](https://github.com/dhimmel/integrate/tree/master/licenses) the licenses for each resource. The network is saved as text files in the [`data`](https://github.com/dhimmel/integrate/tree/master/data) directory with [`hetnet.json.gz`](https://github.com/dhimmel/integrate/blob/master/data/hetnet.json.gz) as the main release. In this integrated network, the database model and field names from the original resource are not present, just derived data.\r\n\r\n**Proposed action**: Adopt a per node/edge licensing framework. Identify which nodes and edges, if any, are eligible for CC0 release. CC0 release may be possible if the creators chose a permissive license or give us permission, the network is fair use, or if the underlying content is not subject to copyright.\r\n\r\n## 3) Network analyses\r\n\r\nNext, we use the integrated network from stage 2 for data mining. The purpose of the data mining is to evaluate methods, extract insights, and make predictions. As an example, see [this analysis](https://github.com/dhimmel/snplentiful) [@10.5281/zenodo.30105] of the network that @caseygreene and I recently did for a separate project.\r\n\r\nHere, it is crucial that findings from analyses on the network are fair use and can be placed in the public domain. Since, the network contains data with [incompatible](https://wiki.creativecommons.org/wiki/Wiki/cc_license_compatibility) licenses such as CC-BY-SA and CC-BY-NC, data mining will be impossible if not considered fair use. In the US, [precedent](http://www.arl.org/storage/documents/TDM-5JUNE2015.pdf) implies our network analyses qualify as fair use. \r\n\r\n**Proposed action**: Identify whether our network analyses qualify as fair use and whether our results can be released as CC0. Evaluate when and if we are subject to European copyright laws, which are less favorable for content users.\r\n\r\n# Expert feedback requested\r\n\r\nWe are seeking expert advice. Specifically, are the proposed actions compliant with copyright law? Regarding the three stages, are we on the right track? Will network analyses count as fair use?",
      "profile": 17,
      "published": "2015-09-05T00:49:45.249465Z",
      "thread": 107,
      "url": "/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#4"
    },
    {
      "body_html": "<h1>Licensing and copyright</h1>\r\n\r\n<p>Thanks <a href=\"/u/larsjuhljensen\" class=\"username\">@larsjuhljensen</a> for your helpful advice. We have created a <a href=\"http://thinklab.com/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#4\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d107\">separate discussion</a> for licensing and copyright to continue the conversation.</p>",
      "body_md": "# Licensing and copyright\r\n\r\nThanks @larsjuhljensen for your helpful advice. We have created a [separate discussion](http://thinklab.com/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#4) for licensing and copyright to continue the conversation.",
      "profile": 17,
      "published": "2015-09-05T00:58:52.840593Z",
      "thread": 102,
      "url": "/discussion/one-network-to-rule-them-all/102#4"
    },
    {
      "body_html": "<h1>Network name</h1>\r\n\r\n<p>We've realized it's a major impediment to not have a name for the network. We are tentatively calling the network <code>hetio-ind</code>. However, <code>rephetio</code> — the current Thinklab handle for the project — is also an option. Will update when the name is finalized.</p>",
      "body_md": "# Network name\r\n\r\nWe've realized it's a major impediment to not have a name for the network. We are tentatively calling the network `hetio-ind`. However, `rephetio` -- the current Thinklab handle for the project -- is also an option. Will update when the name is finalized.",
      "profile": 17,
      "published": "2015-09-05T01:01:19.214381Z",
      "thread": 102,
      "url": "/discussion/one-network-to-rule-them-all/102#5"
    },
    {
      "body_html": "<p>To keep you posted: we were recently given access to the GTEx data, so we have started annotating/analyzing the data. We hope to have a new release of Bgee including these data in about 2 months.</p>",
      "body_md": "To keep you posted: we were recently given access to the GTEx data, so we have started annotating/analyzing the data. We hope to have a new release of Bgee including these data in about 2 months.",
      "profile": 111,
      "published": "2015-09-07T11:10:57.149929Z",
      "thread": 82,
      "url": "/discussion/processing-gtex-for-tissue-specific-gene-expression/82#15"
    },
    {
      "body_html": "<p>Your analysis of the situation looks great — you've correctly described the difficulty of combining incompatible licenses and the data they cover, and the potential of fair use (for extracting data subsets and data mining) for what you're trying to do. And for the datasets that lack a license, you know that in many cases they aren't protected by copyright so you're free to do what you want. Federal government agencies are notorious for refusing to assign licenses or rights waivers to the data they release, claiming that everything they have and do is in the public domain and we should all just know that, so sometimes no license means you're fine. Your goal of making it clear to users what rights and licenses apply to which datasets is laudable.</p>\r\n\r\n<p>The one thing I didn't see you covering is liability. I can't figure out who actually owns the work that you're doing — you want to put it in the public domain, which is great, but do you personally have the right to do that? Are you working on a grant project or employed by a university that might claim \"ownership\" of your results? This is usually dealt with by the licenses. Apache open source software licenses include the language \"Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\" Even CC licenses include language like \"No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.\" So if you're using CC0 wherever you can, you might want a separate statement of warranty (or lack thereof) unless you want to be liable, or implicate your institution, if you do accidentally screw up (easy enough to do, in such a complex project, even when you've done everything you can). </p>",
      "body_md": "Your analysis of the situation looks great -- you've correctly described the difficulty of combining incompatible licenses and the data they cover, and the potential of fair use (for extracting data subsets and data mining) for what you're trying to do. And for the datasets that lack a license, you know that in many cases they aren't protected by copyright so you're free to do what you want. Federal government agencies are notorious for refusing to assign licenses or rights waivers to the data they release, claiming that everything they have and do is in the public domain and we should all just know that, so sometimes no license means you're fine. Your goal of making it clear to users what rights and licenses apply to which datasets is laudable.\r\n\r\nThe one thing I didn't see you covering is liability. I can't figure out who actually owns the work that you're doing -- you want to put it in the public domain, which is great, but do you personally have the right to do that? Are you working on a grant project or employed by a university that might claim \"ownership\" of your results? This is usually dealt with by the licenses. Apache open source software licenses include the language \"Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\" Even CC licenses include language like \"No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.\" So if you're using CC0 wherever you can, you might want a separate statement of warranty (or lack thereof) unless you want to be liable, or implicate your institution, if you do accidentally screw up (easy enough to do, in such a complex project, even when you've done everything you can). ",
      "profile": 135,
      "published": "2015-09-08T02:16:42.932187Z",
      "thread": 107,
      "url": "/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#5"
    },
    {
      "body_html": "<h1>Who owns the created work</h1>\r\n\r\n<p>After some background <a href=\"http://www.d.umn.edu/~pschoff/documents/ElliotR05WhoOwnsScientificDatapdf.pdf\">reading</a> <span class=\"citation\">[<a href=\"/doi/10.1087/0953151053584984\" class=\"citation\" data-key=\"10.1087/0953151053584984\">1</a>]</span> and <a href=\"https://www.youtube.com/playlist?list=PLYTiwx6hV33tXEfueCk5k7xyLePcqj4XK\">video watching</a>, who owns the work we're creating is not straightforward.</p>\r\n\r\n<p>I am a graduate student at UCSF and my PI, <a href=\"/u/sergiobaranzini\" class=\"username\">@sergiobaranzini</a>, is a professor at UCSF. I am largely, but not completely, funded by an NSF Graduate Research Fellowship whose conditions <a href=\"http://www.nsf.gov/pubs/2015/nsf15597/nsf15597.htm\">state</a>:</p>\r\n\r\n<blockquote><p>The National Science Foundation claims no rights to any inventions or writings that might result from its fellowship or traineeship grants.</p></blockquote>\r\n\r\n<h2>Copyright and the UC</h2>\r\n\r\n<p>The UC's 1992 <a href=\"http://copyright.universityofcalifornia.edu/resources/copyright-ownership.html\">'copyright ownership' policy</a> stipulates ownership by category of work. Several of these categories may apply:</p>\r\n\r\n<ul><li><p>academic appointee originator ownership of \"scholarly/aesthetic work\"</p><blockquote><p>A scholarly/aesthetic work is a work originated by a designated academic appointee resulting from independent academic effort. Ownership of copyrights to scholarly/aesthetic works shall reside with the designated academic appointee originator, unless they are also sponsored works or contracted facilities works, or unless the designated academic appointee agrees to participate in a project which has special provisions on copyright ownership pursuant to Section V.C. of this Policy.</p></blockquote></li><li><p>originator ownership of \"personal work\"</p><blockquote><p>A personal work is a work that is prepared outside the course and scope of University employment (except for permissible non-University consulting activities) without the use of University Resources. Ownership of copyrights to Personal works shall reside with the originator.</p></blockquote></li><li><p>originator ownership of \"student work\":</p><blockquote><p>A student work is a work produced by a registered student without the use of University funds (other than Student Financial Aid), that is produced outside any University employment, and is not a sponsored, contracted facilities, or commissioned work. Ownership of copyrights to student works shall reside with the originator.</p></blockquote></li><li><p>university ownership of \"institutional work\":</p><blockquote><p>Except as otherwise provided in this Policy, the University shall own all copyrights to works made by University employees in the course and scope of their employment and shall own all copyrights to works made with the use of University resources.</p></blockquote></li></ul>\r\n\r\n<p>Therefore, UC's asserted ownership is dependent on which categories our work falls under. Additional guidance <a href=\"http://copyright.universityofcalifornia.edu/ownership/works-created-at-uc.html\">states</a>:</p>\r\n\r\n<blockquote><p>University staff who create works within the scope of their employment generally do not own the copyright to the work. A work prepared by an employee within the scope of his or her employment is considered a \"work made for hire.\"  When a work qualifies as a work made for hire, the employer or commissioning party is considered its author. Under UC policies, some written works created by certain categories of UC faculty, graduate students, and staff are considered works made for hire. </p></blockquote>\r\n\r\n<p>Thus, the University's assertion of ownership may be contradicted by the <a href=\"http://chronicle.com/article/Employees-or-Not-/145573/\">strong argument</a> that graduate students, such as myself, are not employees and do not produce \"work made for hire\". Furthermore, the policies and guidelines are outdated and not well tailored towards the collaborative, digital, online, and open approach our project takes. The work I perform goes beyond the sole purposes of studentship, employment, and institutional work. And the academic community has established norms and precedent for allowing creators to transfer copyright and choose licensing — the foremost examples being academic publishing and open source software contribution.</p>\r\n\r\n<h2>Data and the UC</h2>\r\n\r\n<p>The UC <a href=\"http://copyright.universityofcalifornia.edu/resources/copyright-ownership.html\">'copyright ownership' policy</a> explicitly states that it only:</p>\r\n\r\n<blockquote><p>addresses ownership of copyright; it does not address ownership or access to the underlying research results or data, as covered in Academic Personnel Manual Section 020. </p></blockquote>\r\n\r\n<p>The Academic Personnel Manual <a href=\"http://www.ucop.edu/academic-personnel-programs/_files/apm/apm-020.pdf\">Section 020</a>, dated in 1953, provides little clarification:</p>\r\n\r\n<blockquote><p>All such research shall be conducted so as to be as generally useful as possible. To this end, the right of publication is reserved by the University. The University may itself publish the material or may authorize, in any specific case, a member or members of the faculty to publish it through some recognized scientific or professional medium of publication. A report detailing the essential data and presenting the final results must be filed with the University. Notebooks and other original records of the research are the property of the University.</p></blockquote>\r\n\r\n<p>Outside of official policy, UC appears to claim ownership of results and data. Quoting from a <a href=\"https://youtu.be/QQOEG_PyRWY\">talk</a> by <a href=\"/u/mackenziesmith\" class=\"username\">@mackenziesmith</a>:</p>\r\n\r\n<blockquote><p>The University of California posits that it actually has a contractual obligation to maintain the ownership of all research data produced from grant funded projects by any researcher at UC, especially federally funded grants. So they claim that the university owns the data.</p></blockquote>\r\n\r\n<p>Additionally, a <a href=\"http://ucsd.libguides.com/c.php?g=90957&amp;p=585144\">UCSD guide</a> states:</p>\r\n\r\n<blockquote><ul><li>Data produced by UC researchers belong to the Regents of the University of California.</li><li>To promote sharing and unlimited use of your data, make your data available under a Creative Commons <a href=\"http://creativecommons.org/choose/zero\">CC0 Declaration</a>.</li></ul></blockquote>\r\n\r\n<p>These seemingly contradictory statements imply that UC may own the data but that its creators are free to release it into the public domain.</p>\r\n\r\n<h2>Resolutions</h2>\r\n\r\n<p>We are looking for suggested courses of action to address the ambiguity and potential multiplicity of claims regarding ownership. Two possible actions are:</p>\r\n\r\n<ul><li>applying a <em>without warranty</em> clause to our licensing to limit our liability.</li><li>identifying all potential parties that may claim ownership and request permission to release the work as freely as possible given the <a href=\"#4\">aforementioned considerations</a>.</li></ul>",
      "body_md": "# Who owns the created work\r\n\r\nAfter some background [reading](http://www.d.umn.edu/~pschoff/documents/ElliotR05WhoOwnsScientificDatapdf.pdf) [@10.1087/0953151053584984] and [video watching](https://www.youtube.com/playlist?list=PLYTiwx6hV33tXEfueCk5k7xyLePcqj4XK), who owns the work we're creating is not straightforward.\r\n\r\nI am a graduate student at UCSF and my PI, @sergiobaranzini, is a professor at UCSF. I am largely, but not completely, funded by an NSF Graduate Research Fellowship whose conditions [state](http://www.nsf.gov/pubs/2015/nsf15597/nsf15597.htm):\r\n\r\n> The National Science Foundation claims no rights to any inventions or writings that might result from its fellowship or traineeship grants.\r\n\r\n## Copyright and the UC\r\n\r\nThe UC's 1992 ['copyright ownership' policy](http://copyright.universityofcalifornia.edu/resources/copyright-ownership.html) stipulates ownership by category of work. Several of these categories may apply:\r\n\r\n+ academic appointee originator ownership of \"scholarly/aesthetic work\"\r\n> A scholarly/aesthetic work is a work originated by a designated academic appointee resulting from independent academic effort. Ownership of copyrights to scholarly/aesthetic works shall reside with the designated academic appointee originator, unless they are also sponsored works or contracted facilities works, or unless the designated academic appointee agrees to participate in a project which has special provisions on copyright ownership pursuant to Section V.C. of this Policy.\r\n\r\n+ originator ownership of \"personal work\"\r\n> A personal work is a work that is prepared outside the course and scope of University employment (except for permissible non-University consulting activities) without the use of University Resources. Ownership of copyrights to Personal works shall reside with the originator.\r\n\r\n+ originator ownership of \"student work\":\r\n> A student work is a work produced by a registered student without the use of University funds (other than Student Financial Aid), that is produced outside any University employment, and is not a sponsored, contracted facilities, or commissioned work. Ownership of copyrights to student works shall reside with the originator.\r\n\r\n+ university ownership of \"institutional work\":\r\n> Except as otherwise provided in this Policy, the University shall own all copyrights to works made by University employees in the course and scope of their employment and shall own all copyrights to works made with the use of University resources.\r\n\r\nTherefore, UC's asserted ownership is dependent on which categories our work falls under. Additional guidance [states](http://copyright.universityofcalifornia.edu/ownership/works-created-at-uc.html):\r\n\r\n> University staff who create works within the scope of their employment generally do not own the copyright to the work. A work prepared by an employee within the scope of his or her employment is considered a \"work made for hire.\"  When a work qualifies as a work made for hire, the employer or commissioning party is considered its author. Under UC policies, some written works created by certain categories of UC faculty, graduate students, and staff are considered works made for hire. \r\n\r\nThus, the University's assertion of ownership may be contradicted by the [strong argument](http://chronicle.com/article/Employees-or-Not-/145573/) that graduate students, such as myself, are not employees and do not produce \"work made for hire\". Furthermore, the policies and guidelines are outdated and not well tailored towards the collaborative, digital, online, and open approach our project takes. The work I perform goes beyond the sole purposes of studentship, employment, and institutional work. And the academic community has established norms and precedent for allowing creators to transfer copyright and choose licensing -- the foremost examples being academic publishing and open source software contribution.\r\n\r\n## Data and the UC\r\n\r\nThe UC ['copyright ownership' policy](http://copyright.universityofcalifornia.edu/resources/copyright-ownership.html) explicitly states that it only:\r\n\r\n> addresses ownership of copyright; it does not address ownership or access to the underlying research results or data, as covered in Academic Personnel Manual Section 020. \r\n\r\nThe Academic Personnel Manual [Section 020](http://www.ucop.edu/academic-personnel-programs/_files/apm/apm-020.pdf), dated in 1953, provides little clarification:\r\n\r\n> All such research shall be conducted so as to be as generally useful as possible. To this end, the right of publication is reserved by the University. The University may itself publish the material or may authorize, in any specific case, a member or members of the faculty to publish it through some recognized scientific or professional medium of publication. A report detailing the essential data and presenting the final results must be filed with the University. Notebooks and other original records of the research are the property of the University.\r\n\r\nOutside of official policy, UC appears to claim ownership of results and data. Quoting from a [talk](https://youtu.be/QQOEG_PyRWY) by @mackenziesmith:\r\n\r\n> The University of California posits that it actually has a contractual obligation to maintain the ownership of all research data produced from grant funded projects by any researcher at UC, especially federally funded grants. So they claim that the university owns the data.\r\n\r\nAdditionally, a [UCSD guide](http://ucsd.libguides.com/c.php?g=90957&p=585144) states:\r\n\r\n> + Data produced by UC researchers belong to the Regents of the University of California.\r\n> + To promote sharing and unlimited use of your data, make your data available under a Creative Commons [CC0 Declaration](http://creativecommons.org/choose/zero).\r\n\r\nThese seemingly contradictory statements imply that UC may own the data but that its creators are free to release it into the public domain.\r\n\r\n## Resolutions\r\n\r\nWe are looking for suggested courses of action to address the ambiguity and potential multiplicity of claims regarding ownership. Two possible actions are:\r\n\r\n+ applying a *without warranty* clause to our licensing to limit our liability.\r\n+ identifying all potential parties that may claim ownership and request permission to release the work as freely as possible given the [aforementioned considerations](#4).",
      "profile": 17,
      "published": "2015-09-09T05:39:13.885633Z",
      "thread": 107,
      "url": "/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#8"
    },
    {
      "body_html": "<p>Regarding the problem of incompatible licenses, it is very important that you are clear on the difference between redistribution and data mining.</p>\r\n\r\n<p>You write that \"Since, the network contains data with incompatible licenses such as CC-BY-SA and CC-BY-NC, data mining will be impossible if not considered fair use\". This is to my knowledge simply not true. There is no problem whatsoever in combining material from these incompatible licenses and mining it in any way that you want. The reason is that copyright purely has to do with how you are allow to redistribute things. And if the data mining leads to some results that are substantially different and not effectively a copy of the original material, there is also no problem in redistributing the results.</p>\r\n\r\n<p>The problem comes when you want to make what is effectively a meta-resource that combines material from a lot of databases and redistributes it. In this case, you are redistributing something that is effectively a reformatted version of the material. In my opinion, your network falls squarely in that category</p>\r\n\r\n<p>However, the solution is very simple. As I have suggested before, you can split the network into subnetworks, that are all redistributed under their respective licenses. You can bundle everything CC-BY-SA in one file and redistribute it under CC-BY-SA. You can bundle everything CC-BY-NC in another file and redistribute it under CC-BY-NC. And as described above, nothing prevents anyone in the world from legally downloading both files, combining them, and mining the data as they please.</p>\r\n\r\n<p>To make it simpler, let me make an analogy from the world of text mining, where the situation is somewhat more clearcut, since there is no doubt that articles are subject to copyright law. I can download some articles under CC-BY-SA and some others under CC-BY-NC. I can run text mining on all of them despite the licenses being incompatible, and I can redistribute the results of my efforts under any license I please, because the results are my results, which are not simply a reformatting of the original text. However, I cannot take all the articles, combine them into a text corpus, and release it under CC0.</p>\r\n\r\n<p>Caveats: I am not a lawyer, this does constitute legal advice etc.</p>",
      "body_md": "Regarding the problem of incompatible licenses, it is very important that you are clear on the difference between redistribution and data mining.\r\n\r\nYou write that \"Since, the network contains data with incompatible licenses such as CC-BY-SA and CC-BY-NC, data mining will be impossible if not considered fair use\". This is to my knowledge simply not true. There is no problem whatsoever in combining material from these incompatible licenses and mining it in any way that you want. The reason is that copyright purely has to do with how you are allow to redistribute things. And if the data mining leads to some results that are substantially different and not effectively a copy of the original material, there is also no problem in redistributing the results.\r\n\r\nThe problem comes when you want to make what is effectively a meta-resource that combines material from a lot of databases and redistributes it. In this case, you are redistributing something that is effectively a reformatted version of the material. In my opinion, your network falls squarely in that category\r\n\r\nHowever, the solution is very simple. As I have suggested before, you can split the network into subnetworks, that are all redistributed under their respective licenses. You can bundle everything CC-BY-SA in one file and redistribute it under CC-BY-SA. You can bundle everything CC-BY-NC in another file and redistribute it under CC-BY-NC. And as described above, nothing prevents anyone in the world from legally downloading both files, combining them, and mining the data as they please.\r\n\r\nTo make it simpler, let me make an analogy from the world of text mining, where the situation is somewhat more clearcut, since there is no doubt that articles are subject to copyright law. I can download some articles under CC-BY-SA and some others under CC-BY-NC. I can run text mining on all of them despite the licenses being incompatible, and I can redistribute the results of my efforts under any license I please, because the results are my results, which are not simply a reformatting of the original text. However, I cannot take all the articles, combine them into a text corpus, and release it under CC0.\r\n\r\nCaveats: I am not a lawyer, this does constitute legal advice etc.",
      "profile": 125,
      "published": "2015-09-08T05:32:04.663666Z",
      "thread": 107,
      "url": "/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#6"
    },
    {
      "body_html": "<p><a href=\"/u/mackenziesmith\" class=\"username\">@mackenziesmith</a> makes a very good point about liability, which in my opinion is why you should not attempt to take copyrighted material, claim fair use under US law, and slap a CC0 waiver on it.</p>\r\n\r\n<p>Imagine someone in Europe were to download your network, assume that everything was free of copyright (which is what CC0 effectively promises), take all the SIDER data, and redistribute it under CC0. Since SIDER is covered by European <em>sui generis</em> database rights, they could get sued and would likely lose. Subsequently, they could choose to sue you for liabilities.</p>\r\n\r\n<p>Caveats: I am not a lawyer, this does constitute legal advice etc.</p>",
      "body_md": "@mackenziesmith makes a very good point about liability, which in my opinion is why you should not attempt to take copyrighted material, claim fair use under US law, and slap a CC0 waiver on it.\r\n\r\nImagine someone in Europe were to download your network, assume that everything was free of copyright (which is what CC0 effectively promises), take all the SIDER data, and redistribute it under CC0. Since SIDER is covered by European *sui generis* database rights, they could get sued and would likely lose. Subsequently, they could choose to sue you for liabilities.\r\n\r\nCaveats: I am not a lawyer, this does constitute legal advice etc.",
      "profile": 125,
      "published": "2015-09-08T05:32:43.986923Z",
      "thread": 107,
      "url": "/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#7"
    },
    {
      "body_html": "<p>At a workshop I organized at UC Davis last year — Data Rights and Data Wrongs — senior counsel from the UC Office of General Counsel (i.e., the university's lawyers) was very clear that UC retains ownership rights to original data as the official 'grantee' and to insure compliance with federal laws for research conduct, etc. I think the relevant policy is here <a href=\"http://www.ucop.edu/raohome/cgmemos/84-31.html\">http://www.ucop.edu/raohome/cgmemos/84-31.html</a> (old but still in effect). So I think your assessment is right that UC asserts ownership but allows you to release the data under reasonable terms, including CC0. However most of the data you're working with isn't original to you, so what UC 'owns' is your own findings and if you did something wrong, the university is liable to some extent. </p>\r\n\r\n<p>Of course, finding all the rights holders and getting their explicit permission to do what you're doing would be ideal, but is that practical? Do you even know who holds the rights to all the data sources? I disagree with the point that you can't rely on Fair Use and release your results under a CC0 waiver — I believe that's what Fair Use is for, if it's truly transformative — but you might want to be explicit about the waiver of liability. Especially given how gray the area you're working in is, legally speaking.</p>",
      "body_md": "At a workshop I organized at UC Davis last year -- Data Rights and Data Wrongs -- senior counsel from the UC Office of General Counsel (i.e., the university's lawyers) was very clear that UC retains ownership rights to original data as the official 'grantee' and to insure compliance with federal laws for research conduct, etc. I think the relevant policy is here http://www.ucop.edu/raohome/cgmemos/84-31.html (old but still in effect). So I think your assessment is right that UC asserts ownership but allows you to release the data under reasonable terms, including CC0. However most of the data you're working with isn't original to you, so what UC 'owns' is your own findings and if you did something wrong, the university is liable to some extent. \r\n\r\nOf course, finding all the rights holders and getting their explicit permission to do what you're doing would be ideal, but is that practical? Do you even know who holds the rights to all the data sources? I disagree with the point that you can't rely on Fair Use and release your results under a CC0 waiver -- I believe that's what Fair Use is for, if it's truly transformative -- but you might want to be explicit about the waiver of liability. Especially given how gray the area you're working in is, legally speaking.",
      "profile": 135,
      "published": "2015-09-09T14:49:37.330926Z",
      "thread": 107,
      "url": "/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#9"
    },
    {
      "body_html": "<p>Most people I have discussed this with, would understand what heterogeneous networks mean without much ambiguity. I realize that it may not be completely specific to the kind of work we are doing here, but a balance between specificity and name simplicity needs to be reached. In my view, HetNets does it. </p>",
      "body_md": "Most people I have discussed this with, would understand what heterogeneous networks mean without much ambiguity. I realize that it may not be completely specific to the kind of work we are doing here, but a balance between specificity and name simplicity needs to be reached. In my view, HetNets does it. ",
      "profile": 20,
      "published": "2015-09-16T20:27:18.620784Z",
      "thread": 104,
      "url": "/discussion/renaming-heterogeneous-networks-to-a-more-concise-and-catchy-term/104#3"
    },
    {
      "body_html": "<h1>Preliminary adoption of 'hetnet'</h1>\r\n\r\n<p>I agree with the previous two comments that \"hetnet\" is a good term. The term transforms \"heterogeneous network\" into a <a href=\"https://en.wikipedia.org/wiki/English_compound#Types_of_compound_nouns\">solid compound noun</a>.</p>\r\n\r\n<p>However, I prefer <a href=\"https://en.wikipedia.org/wiki/Letter_case#Sentence_case\">sentence case</a> (hetnet) to <a href=\"https://en.wikipedia.org/wiki/CamelCase\">camel case</a> (HetNet). Removing the camel case improves the aesthetics and differentiates the term from its <a href=\"https://en.wikipedia.org/w/index.php?title=Heterogeneous_network&amp;oldid=629383185#HetNet\">computer networking usage</a>.</p>\r\n\r\n<p>The term respects its lineage through compatibility with \"heterogeneous network\" and \"heterogeneous information network\".</p>\r\n\r\n<p>I have begun publicly using the term hetnet. For example, I <a href=\"https://github.com/dhimmel/hetio/commit/869a67858086c9168ae50d693393ade2308f51ce\">renamed</a>  the<code>hetio.graph</code> module to <code>hetio.hetnet</code> to better describe the content. I also <a href=\"https://github.com/dhimmel/hetio/commit/a006a862d1501ec322bd172dc55cb6f3fe83301a\">now</a> describe the hetio <a href=\"https://github.com/dhimmel/hetio\">package</a> as \"Hetnets in Python\".</p>",
      "body_md": "# Preliminary adoption of 'hetnet'\r\n\r\nI agree with the previous two comments that \"hetnet\" is a good term. The term transforms \"heterogeneous network\" into a [solid compound noun](https://en.wikipedia.org/wiki/English_compound#Types_of_compound_nouns).\r\n\r\nHowever, I prefer [sentence case](https://en.wikipedia.org/wiki/Letter_case#Sentence_case) (hetnet) to [camel case](https://en.wikipedia.org/wiki/CamelCase) (HetNet). Removing the camel case improves the aesthetics and differentiates the term from its [computer networking usage](https://en.wikipedia.org/w/index.php?title=Heterogeneous_network&oldid=629383185#HetNet).\r\n\r\nThe term respects its lineage through compatibility with \"heterogeneous network\" and \"heterogeneous information network\".\r\n\r\nI have begun publicly using the term hetnet. For example, I [renamed](https://github.com/dhimmel/hetio/commit/869a67858086c9168ae50d693393ade2308f51ce)  the`hetio.graph` module to `hetio.hetnet` to better describe the content. I also [now](https://github.com/dhimmel/hetio/commit/a006a862d1501ec322bd172dc55cb6f3fe83301a) describe the hetio [package](https://github.com/dhimmel/hetio) as \"Hetnets in Python\".",
      "profile": 17,
      "published": "2015-09-16T21:29:39.630331Z",
      "thread": 104,
      "url": "/discussion/renaming-heterogeneous-networks-to-a-more-concise-and-catchy-term/104#4"
    },
    {
      "body_html": "<h1>Curation pilot results</h1>\r\n\r\n<p><a href=\"/u/sergiobaranzini\" class=\"username\">@sergiobaranzini</a> recruited two UCSF physicians — <a href=\"http://greenlab.ucsf.edu/ari-j-green-md\">Ari Green</a> (AJG) and <a href=\"http://profiles.ucsf.edu/christine.hessler\">Christine Hessler</a> (CSH) — for the curation task. We asked the curators to independently classify 50 random indications as disease modifying or symptomatic. See the raw results for <a href=\"https://github.com/dhimmel/indications/blob/49efba793216dcaf739fe22a32f60c4549aa3f7a/curation/pilot/ajg.tsv\">AJG</a> and <a href=\"https://github.com/dhimmel/indications/blob/49efba793216dcaf739fe22a32f60c4549aa3f7a/curation/pilot/csh.tsv\">CSH</a>.</p>\r\n\r\n<h2>Combined results</h2>\r\n\r\n<p>While we did not specify that \"not an indication\" was an option, both curators identified these instances. While our indication dataset derives from high precision datasets, non-indications will be present. Thus, going forward we will include \"not an indication\" as a classification.</p>\r\n\r\n<p>I cleaned the curators free text into 3 classifications: <code>DM</code> for disease modifying, <code>SYM</code> for symptomatic, and <code>NOT</code> for not an indication. This cleaning step required some inference on my part and thus could have introduced a minor bias. The <a href=\"https://github.com/dhimmel/indications/blob/49efba793216dcaf739fe22a32f60c4549aa3f7a/curation/pilot/combined.tsv\">combined results</a> show 66% agreement, with the following breakdown by curator:</p>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th>class</th><th>ajg</th><th>csh</th></tr></thead><tbody><tr><td>DM</td><td>26</td><td>32</td></tr><tr><td>SYM</td><td>20</td><td>17</td></tr><tr><td>NOT</td><td>4</td><td>1</td></tr></tbody></table>\r\n\r\n<p>The pilot suggests that the compiled indications are ~58% disease modifying, ~37% symptomatic, and ~5% non-indications.</p>\r\n\r\n<h2>Definitions</h2>\r\n\r\n<p>We did not provide the curators with clear definitions of disease modifying and symptomatic. Our plan is to use the pilot experience to help define the categories. Preferably, these definitions should be crafted by the physicians.</p>\r\n\r\n<p>Informally, CSH defined disease modifying as:</p>\r\n\r\n<blockquote><p>any agent that changes the course of the illness or complications of the illness (not necessarily curing the disease, but preventing \"flares\" or complications). I'd ask myself, \"is it poor form not to prescribe this medication to my patient with disease y and could I be sued for it?\"</p></blockquote>\r\n\r\n<p>And symptomatic as:</p>\r\n\r\n<blockquote><p>agents that are used purely for patient's comfort and don't alter the course of the disease at all. </p></blockquote>\r\n\r\n<h2>Cocaine and cavities</h2>\r\n\r\n<p>One bizarre indication was <a href=\"http://www.drugbank.ca/drugs/DB00907\">cocaine</a> and <a href=\"http://www.disease-ontology.org/term/DOID%3A216\">dental carries</a>, which was contributed <a href=\"https://cdn.rawgit.com/dhimmel/indications/6375b195df61b6e0d44c4690abfa2ac0710bc690//merge.html#indication-table\">by</a> MEDI-HPS <span class=\"citation\">[<a href=\"/doi/10.1136/amiajnl-2012-001431\" class=\"citation\" data-key=\"10.1136/amiajnl-2012-001431\">1</a>]</span>. Since MEDI doesn't report its sources for individual indications and the authors <a href=\"http://thinklab.com/discussion/medi-indications-data-discrepancy-in-resource-specific-counts/31#2\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d31\">did not release</a> this data upon request, tracking down the provenance of this indication is difficult. However, MEDI does <a href=\"https://cdn.rawgit.com/dhimmel/indications/6375b195df61b6e0d44c4690abfa2ac0710bc690/medi/\">specify</a> that this indication existed in SIDER 2 <span class=\"citation\">[<a href=\"/doi/10.1038/msb.2009.98\" class=\"citation\" data-key=\"10.1038/msb.2009.98\">2</a>]</span>, which text mined drug labels. Presumably, <a href=\"http://www.drugs.com/pro/cocaine-hydrochloride-topical-solution.html\">this label</a> for Cocaine Hydrochloride Topical Solution (brand name C-Topical) was the source for the cocaine–cavities indication. The label states the solution \"is indicated for introduction of local (topical) anesthesia of accessible mucous membranes of the oral, laryngeal and nasal cavities.\" This suggests a non-indication or at most a symptomatic indication between cocaine and dental caries. However, for a SIDER 2 indication to be included in MEDI-HPS, at least one other source must report the indication. AJG informally suggested that the \"reason for the hit is that cocaine accelerates dental caries and might therefore be considered disease modifying (but in a bad way).\"</p>\r\n\r\n<h2>Next steps</h2>\r\n\r\n<p>The pilot experience proved the importance of expert curation of our compiled indication catalog. Before proceeding with the remaining indications, we should formally define the three classifications (DM, SYM, NOT). Clear definitions may increase the agreement between curators, but a final resolution stage for conflicts will be necessary.</p>",
      "body_md": "# Curation pilot results\r\n\r\n@sergiobaranzini recruited two UCSF physicians -- [Ari Green](http://greenlab.ucsf.edu/ari-j-green-md) (AJG) and [Christine Hessler](http://profiles.ucsf.edu/christine.hessler) (CSH) -- for the curation task. We asked the curators to independently classify 50 random indications as disease modifying or symptomatic. See the raw results for [AJG](https://github.com/dhimmel/indications/blob/49efba793216dcaf739fe22a32f60c4549aa3f7a/curation/pilot/ajg.tsv) and [CSH](https://github.com/dhimmel/indications/blob/49efba793216dcaf739fe22a32f60c4549aa3f7a/curation/pilot/csh.tsv).\r\n\r\n## Combined results\r\n\r\nWhile we did not specify that \"not an indication\" was an option, both curators identified these instances. While our indication dataset derives from high precision datasets, non-indications will be present. Thus, going forward we will include \"not an indication\" as a classification.\r\n\r\nI cleaned the curators free text into 3 classifications: `DM` for disease modifying, `SYM` for symptomatic, and `NOT` for not an indication. This cleaning step required some inference on my part and thus could have introduced a minor bias. The [combined results](https://github.com/dhimmel/indications/blob/49efba793216dcaf739fe22a32f60c4549aa3f7a/curation/pilot/combined.tsv) show 66% agreement, with the following breakdown by curator:\r\n\r\n| class | ajg | csh |\r\n|-------|-----|-----|\r\n| DM | 26 | 32 |\r\n| SYM | 20 | 17 |\r\n| NOT | 4 | 1 |\r\n\r\nThe pilot suggests that the compiled indications are ~58% disease modifying, ~37% symptomatic, and ~5% non-indications.\r\n\r\n## Definitions\r\n\r\nWe did not provide the curators with clear definitions of disease modifying and symptomatic. Our plan is to use the pilot experience to help define the categories. Preferably, these definitions should be crafted by the physicians.\r\n\r\nInformally, CSH defined disease modifying as:\r\n\r\n> any agent that changes the course of the illness or complications of the illness (not necessarily curing the disease, but preventing \"flares\" or complications). I'd ask myself, \"is it poor form not to prescribe this medication to my patient with disease y and could I be sued for it?\"\r\n\r\nAnd symptomatic as:\r\n\r\n> agents that are used purely for patient's comfort and don't alter the course of the disease at all. \r\n\r\n## Cocaine and cavities\r\n\r\nOne bizarre indication was [cocaine](http://www.drugbank.ca/drugs/DB00907) and [dental carries](http://www.disease-ontology.org/term/DOID%3A216), which was contributed [by](https://cdn.rawgit.com/dhimmel/indications/6375b195df61b6e0d44c4690abfa2ac0710bc690//merge.html#indication-table) MEDI-HPS [@10.1136/amiajnl-2012-001431]. Since MEDI doesn't report its sources for individual indications and the authors [did not release](http://thinklab.com/discussion/medi-indications-data-discrepancy-in-resource-specific-counts/31#2) this data upon request, tracking down the provenance of this indication is difficult. However, MEDI does [specify](https://cdn.rawgit.com/dhimmel/indications/6375b195df61b6e0d44c4690abfa2ac0710bc690/medi/) that this indication existed in SIDER 2 [@10.1038/msb.2009.98], which text mined drug labels. Presumably, [this label](http://www.drugs.com/pro/cocaine-hydrochloride-topical-solution.html) for Cocaine Hydrochloride Topical Solution (brand name C-Topical) was the source for the cocaine--cavities indication. The label states the solution \"is indicated for introduction of local (topical) anesthesia of accessible mucous membranes of the oral, laryngeal and nasal cavities.\" This suggests a non-indication or at most a symptomatic indication between cocaine and dental caries. However, for a SIDER 2 indication to be included in MEDI-HPS, at least one other source must report the indication. AJG informally suggested that the \"reason for the hit is that cocaine accelerates dental caries and might therefore be considered disease modifying (but in a bad way).\"\r\n\r\n## Next steps\r\n\r\nThe pilot experience proved the importance of expert curation of our compiled indication catalog. Before proceeding with the remaining indications, we should formally define the three classifications (DM, SYM, NOT). Clear definitions may increase the agreement between curators, but a final resolution stage for conflicts will be necessary.",
      "profile": 17,
      "published": "2015-09-19T17:41:55.442799Z",
      "thread": 95,
      "url": "/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95#3"
    },
    {
      "body_html": "<p>Recently, I went a two-part meetup series on the graph database <a href=\"http://neo4j.com/\">neo4j</a>. <a href=\"http://nicolewhite.github.io/\">Nicole White</a> led the meetups and her materials are online:</p>\r\n\r\n<ol><li>neo4j: Intro to Graphs (<a href=\"https://www.dropbox.com/s/zv0s4lwc6gvwxjy/Galvanize.pptx?dl=0\">slides</a>, <a href=\"http://www.meetup.com/SF-Data-Science/events/224956828\">meetup</a>)</li><li>Data Science with Python and Neo4j (<a href=\"http://nicolewhite.github.io/neo4j-jupyter/main.html\">tutorial</a>, <a href=\"https://github.com/nicolewhite/neo4j-jupyter\">repository</a>, <a href=\"http://www.meetup.com/SF-Data-Science/events/224406352\">meetup</a>)</li></ol>\r\n\r\n<p>Currently, we store our hetnets in compressed json text files. To perform any computation or graph analyses, we must load the network into memory, a process that takes from 2–5 minutes for version one of our <a href=\"http://thinklab.com/discussion/one-network-to-rule-them-all/102#1\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d102\">network</a>. In contrast neo4j provides persistent storage with immediate access.</p>\r\n\r\n<p>Additional benefits of neo4j include a mature <a href=\"https://github.com/GraphGeeks/awesome-neo4j\">ecosystem</a> offering broad functionality. The <a href=\"http://neo4j.com/developer/cypher-query-language/\">Cyper</a> query language is especially exciting. Cypher uses an ASCII-art based syntax to enable advanced graph lookups and traversals with little boilerplate.</p>\r\n\r\n<h2>Comparison to hetio</h2>\r\n\r\n<p>There are a few differences between neo4j and our python package <a href=\"https://github.com/dhimmel/hetio\">hetio</a> <span class=\"citation\">[<a href=\"/doi/10.5281/zenodo.31763\" class=\"citation\" data-key=\"10.5281/zenodo.31763\">1</a>]</span> with regards to hetnets:</p>\r\n\r\n<ul><li><strong>nomenclature</strong> — an edge in hetio is called a relationship in neo4j. hetio calls itself a <a href=\"http://thinklab.com/discussion/renaming-heterogeneous-networks-to-a-more-concise-and-catchy-term/104\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d104\">hetnet</a>, while neo4j calls itself a <a href=\"http://neo4j.com/developer/graph-database/#property-graph\">property graph</a></li><li><strong>node type</strong> — in hetio each node belongs to one metanode representing its type. in neo4j node are annotated with a label to indicate type, and a node can have ≥ 0 labels</li><li><strong>directionality</strong> — in hetio metaedges are either directed or undirected and edges conform to their metaedge's directionality. neo4j doesn't support undirected edges. The <a href=\"http://graphaware.com/neo4j/2013/10/11/neo4j-bidirectional-relationships.html\">suggested workaround</a> is to arbitrarily choose a direction upon creation and ignore the direction when querying</li><li><strong>type graph</strong> — hetio requires a predefined graph of types called a metagraph. neo4j does not enforce or explicitly support a graph of type definitions</li><li><strong>inverted edges</strong> — hetio internally stores two copies of each edge (inverses of each other)</li></ul>\r\n\r\n<p>We plan to create export functionality from hetio to neo4j, so we can leverage the strengths of neo4j.</p>",
      "body_md": "Recently, I went a two-part meetup series on the graph database [neo4j](http://neo4j.com/). [Nicole White](http://nicolewhite.github.io/) led the meetups and her materials are online:\r\n\r\n1.  neo4j: Intro to Graphs ([slides](https://www.dropbox.com/s/zv0s4lwc6gvwxjy/Galvanize.pptx?dl=0), [meetup](http://www.meetup.com/SF-Data-Science/events/224956828))\r\n2. Data Science with Python and Neo4j ([tutorial](http://nicolewhite.github.io/neo4j-jupyter/main.html), [repository](https://github.com/nicolewhite/neo4j-jupyter), [meetup](http://www.meetup.com/SF-Data-Science/events/224406352))\r\n\r\nCurrently, we store our hetnets in compressed json text files. To perform any computation or graph analyses, we must load the network into memory, a process that takes from 2--5 minutes for version one of our [network](http://thinklab.com/discussion/one-network-to-rule-them-all/102#1). In contrast neo4j provides persistent storage with immediate access.\r\n\r\nAdditional benefits of neo4j include a mature [ecosystem](https://github.com/GraphGeeks/awesome-neo4j) offering broad functionality. The [Cyper](http://neo4j.com/developer/cypher-query-language/) query language is especially exciting. Cypher uses an ASCII-art based syntax to enable advanced graph lookups and traversals with little boilerplate.\r\n\r\n## Comparison to hetio\r\n\r\nThere are a few differences between neo4j and our python package [hetio](https://github.com/dhimmel/hetio) [@10.5281/zenodo.31763] with regards to hetnets:\r\n\r\n+ **nomenclature** -- an edge in hetio is called a relationship in neo4j. hetio calls itself a [hetnet](http://thinklab.com/discussion/renaming-heterogeneous-networks-to-a-more-concise-and-catchy-term/104), while neo4j calls itself a [property graph](http://neo4j.com/developer/graph-database/#property-graph)\r\n+ **node type** -- in hetio each node belongs to one metanode representing its type. in neo4j node are annotated with a label to indicate type, and a node can have ≥ 0 labels\r\n+ **directionality** -- in hetio metaedges are either directed or undirected and edges conform to their metaedge's directionality. neo4j doesn't support undirected edges. The [suggested workaround](http://graphaware.com/neo4j/2013/10/11/neo4j-bidirectional-relationships.html) is to arbitrarily choose a direction upon creation and ignore the direction when querying\r\n+ **type graph** -- hetio requires a predefined graph of types called a metagraph. neo4j does not enforce or explicitly support a graph of type definitions\r\n+ **inverted edges** -- hetio internally stores two copies of each edge (inverses of each other)\r\n\r\nWe plan to create export functionality from hetio to neo4j, so we can leverage the strengths of neo4j.",
      "profile": 17,
      "published": "2015-10-02T22:20:16.909046Z",
      "thread": 112,
      "url": "/discussion/using-the-neo4j-graph-database-for-hetnets/112"
    },
    {
      "body_html": "<p>Hi all,<br>I am <strong>a</strong> lawyer, but not <strong>your</strong> lawyer (or UC’s lawyer), and this isn’t legal advice. Also, I’m not yet familiar with the data sources or the project at a high level of detail - but here’s what I can say about the general issues.</p>\r\n\r\n<h1>I. U.S. law</h1>\r\n\r\n<h2>A. Layers of copyright</h2>\r\n\r\n<ol><li>I notice that you’ve generally got a single assessment of copyright/licensing issues associated with each data source. I could see each one having up to three. For instance, you could have facts that both the original distributor and the downstream user agree are in the public domain - layer 1, you can do anything with those if you’ve extracted them and rearranged them. They could be collected and shared in a database that’s licensed under something like CC BY-SA, and the terms of that license would need to be followed when distributing the whole database, or parts of it, in such a way that you were copying &amp; distributing the licensor’s copyrightable arrangement/selection/original authorship. The database is layer 2. Then you might have special software created by the data distributor to access and manipulate the data and/or the database, and that might be licensed separately, either with a CC license or with an open source software license like MIT or BSD. That’s layer 3. Without being an expert on these particular databases, I’m guessing 2 and 3 are often going to be the same thing, but it’s best not to just assume that.</li><li>If no license terms are posted, the underlying facts are in the public domain, and any copyrightable expression like software, or creative arrangement, is copyright default’s “all rights reserved.”</li><li>Why am I bothering to spell this out? Depending on the terms of these things and how you want to use/redistribute them, it’s possible that something like the GSEA/MIT terms that look really restrictive may not be a hurdle. I read that one to limit what you can do with layers 2 (“the DATABASE”) and 3 (“the PROGRAM”), but less so layer 1. If you’re committed to redistribution of layer 2 wholesale, then yeah, we’ve got barriers.</li></ol>\r\n\r\n<h2>B. Particular licenses</h2>\r\n\r\n<ol><li>Software licenses are more commonly used for software than CC licenses are, although either would theoretically work. UC recommends BSD and MIT licenses in particular, because they don’t say anything about patents.</li><li>There’s some interesting stuff in the fine print of the CC licenses that might be helpful. For instance, the SA requirement has to be retained by the original material, and has to be attached to any “Adapted Material.” But not every use of a work is “Adapted Material.” Compilations generally aren’t an adaptation, so maybe there’s some creative thinking to be done around that. Attribution requirements can be satisfied in “any reasonable manner based on the medium, means, and context,” and maybe we could do some thinking about what’s a reasonable manner in <em>this</em> context.</li></ol>\r\n\r\n<h2>C. The CC0 dedication</h2>\r\n\r\n<ol><li>Like the CC licenses, the CC0 dedication only applies to … what it <em>can</em> apply to. Just the things the licensor has the ability to waive rights to. Here’s the language:<br><em>Affirmer disclaims responsibility for clearing rights of other persons that may apply to the Work or any use thereof, including without limitation any person's Copyright and Related Rights in the Work. Further, Affirmer disclaims responsibility for obtaining any necessary consents, permissions or other rights required for any use of the Work.</em><br>On the bright side this means that theoretically, you can just release your own layer/contributions/authorship as CC0, without affecting the things you reference or incorporate. Unfortunately, this isn’t so helpful for downstream users who have to try to figure out what the CC0 applies to and what other rights are lurking there. Lots of explanation, labeling, help pages, etc. can be useful if people read them.</li></ol>\r\n\r\n<h2>D. UC and data “ownership”</h2>\r\n\r\n<ol><li>I’m going to keep putting “ownership” in quotes until something official explains to me, to my satisfaction, exactly what UC is claiming to own. The APM policy they seem to rely on from the 50s talks about records, like notebooks. Data can only be owned to the extent there’s intellectual property involved, like patent, copyright, or trade secrets. If none of those are present, there’s nothing to own. There may be contractual restrictions about what you can or must do with something, that you’ve agreed to as part of an employment agreement or a grant agreement, but that’s a different animal, and will be more explicit than the automatic rights involved in copyright.</li><li>Depending on how this project is funded I think any copyrightable work here - the software, for instance - could be student work, personal work, or institutional work, under the 1992 Copyright Ownership Policy. It’s unlikely to be a scholarly/aesthetic work because of the definition of “designated academic appointee,” but I don’t know who the co-authors are.</li><li>UC’s lawyers - OGC and general counsel - will generally weigh in to assess legal risk to the university or disposition of university intellectual property. They will not/cannot provide advice about liability to an individual, or assessment of their personal intellectual property.</li><li>Each campus has a designated authority who is authorized to approve licensing decisions and the like on that campus. I believe UCSF’s is Karin Immergluck. In my experience, if we get to a place where we decide “well, this project includes copyrights owned by UCSF, but we want to license them CC BY or dedicate them to the public domain,” an email to the relevant campus person explaining the rationale (and preferably why this isn’t something the university would make money off of) results in a quick approval.</li></ol>\r\n\r\n<h2>E. Contracts</h2>\r\n\r\n<ol><li>U.S. copyright law includes all kinds of rights for users, including fair use, and the fact that certain things are in the public domain. But you can sign a contract giving away any of these rights. To the extent that you have to agree to restrictive terms to get access to a data set, those terms may effectively limit your rights to reuse even factual data. It’s like when libraries sign a license for a ProQuest database and promise not to make any copies of newspaper articles from the 1800s.</li></ol>\r\n\r\n<h1>II. International law</h1>\r\n\r\n<h2>A. Database protection generally</h2>\r\n\r\n<p>Lots of countries protect a database, but not the underlying facts, with copyright law. I see you found the Bitlaw page on this, which is where I would have sent you.</p>\r\n\r\n<h2>B. European database directive</h2>\r\n\r\n<p>I’ve never had occasion to deal with this before, but there’s a parallel thing in some countries like Italy for, e.g. digitizing old manuscripts. Limited protection as an incentive to create the thing or make it accessible. It sounds like enough of a pain that it’s probably worth figuring out which of the proposed sources are covered. That may be time consuming and difficult - so, something for further discussion/research.</p>\r\n\r\n<h2>C. International liability for potential copyright infringement</h2>\r\n\r\n<p>This is a tricky issue, and a fun subject for law review articles. Most of them revolve around <em>selling</em> things internationally, for a couple reasons. First, that’s when you’re likely to make people mad enough to bother with suing you. Second, there are jurisdictional issues about how much you have to do in a country to subject yourself to a lawsuit there. All I can say is that internet plus free distribution doesn’t automatically equal global legal risk. But that may not matter much because...</p>\r\n\r\n<h1>II. There’s law, and then there’s politics.</h1>\r\n\r\n<p>If we were looking at hundreds of sources, contacting them individually would be a horrible thing to contemplate. With a couple dozen, it might be worth it to put together a form letter to let people know about the project, to avoid burning bridges with current colleagues and potential future collaborators. This could address the things these folks are most likely to be concerned about: what is this project doing with the data sources? How will downstream users be able to tell the source of the data? What things will facilitate or burden commercial use? And there could be a few different versions depending on the legal assessment of the underlying rights and which ones the project implicates - maybe a letter to US sources is more of an FYI, and one to European sources asks them to reply granting permission. Or maybe if the project really wants <em>everything</em> to be as open as possible, you just actually get permission from everyone to a release of some version of their data, in this context, under your chosen license. Just because they make it available to the world under, e.g., CC BY-SA doesn’t mean they can’t make it available to you under different terms. There are options. None of them are as easy as “just use it,” but if people have tried to restrict how their stuff is used you have to decide the relative value you place on maximizing your rights under the law vs. maintaining goodwill.</p>",
      "body_md": "Hi all,\r\nI am **a** lawyer, but not **your** lawyer (or UC’s lawyer), and this isn’t legal advice. Also, I’m not yet familiar with the data sources or the project at a high level of detail - but here’s what I can say about the general issues.\r\n\r\n#I. U.S. law\r\n##   A. Layers of copyright\r\n1. I notice that you’ve generally got a single assessment of copyright/licensing issues associated with each data source. I could see each one having up to three. For instance, you could have facts that both the original distributor and the downstream user agree are in the public domain - layer 1, you can do anything with those if you’ve extracted them and rearranged them. They could be collected and shared in a database that’s licensed under something like CC BY-SA, and the terms of that license would need to be followed when distributing the whole database, or parts of it, in such a way that you were copying & distributing the licensor’s copyrightable arrangement/selection/original authorship. The database is layer 2. Then you might have special software created by the data distributor to access and manipulate the data and/or the database, and that might be licensed separately, either with a CC license or with an open source software license like MIT or BSD. That’s layer 3. Without being an expert on these particular databases, I’m guessing 2 and 3 are often going to be the same thing, but it’s best not to just assume that.\r\n2. If no license terms are posted, the underlying facts are in the public domain, and any copyrightable expression like software, or creative arrangement, is copyright default’s “all rights reserved.”\r\n3. Why am I bothering to spell this out? Depending on the terms of these things and how you want to use/redistribute them, it’s possible that something like the GSEA/MIT terms that look really restrictive may not be a hurdle. I read that one to limit what you can do with layers 2 (“the DATABASE”) and 3 (“the PROGRAM”), but less so layer 1. If you’re committed to redistribution of layer 2 wholesale, then yeah, we’ve got barriers.\r\n##   B. Particular licenses\r\n1.  Software licenses are more commonly used for software than CC licenses are, although either would theoretically work. UC recommends BSD and MIT licenses in particular, because they don’t say anything about patents.\r\n2. There’s some interesting stuff in the fine print of the CC licenses that might be helpful. For instance, the SA requirement has to be retained by the original material, and has to be attached to any “Adapted Material.” But not every use of a work is “Adapted Material.” Compilations generally aren’t an adaptation, so maybe there’s some creative thinking to be done around that. Attribution requirements can be satisfied in “any reasonable manner based on the medium, means, and context,” and maybe we could do some thinking about what’s a reasonable manner in *this* context.\r\n##   C. The CC0 dedication\r\n1. Like the CC licenses, the CC0 dedication only applies to … what it *can* apply to. Just the things the licensor has the ability to waive rights to. Here’s the language:\r\n*Affirmer disclaims responsibility for clearing rights of other persons that may apply to the Work or any use thereof, including without limitation any person's Copyright and Related Rights in the Work. Further, Affirmer disclaims responsibility for obtaining any necessary consents, permissions or other rights required for any use of the Work.*\r\nOn the bright side this means that theoretically, you can just release your own layer/contributions/authorship as CC0, without affecting the things you reference or incorporate. Unfortunately, this isn’t so helpful for downstream users who have to try to figure out what the CC0 applies to and what other rights are lurking there. Lots of explanation, labeling, help pages, etc. can be useful if people read them.\r\n##   D. UC and data “ownership”\r\n1. I’m going to keep putting “ownership” in quotes until something official explains to me, to my satisfaction, exactly what UC is claiming to own. The APM policy they seem to rely on from the 50s talks about records, like notebooks. Data can only be owned to the extent there’s intellectual property involved, like patent, copyright, or trade secrets. If none of those are present, there’s nothing to own. There may be contractual restrictions about what you can or must do with something, that you’ve agreed to as part of an employment agreement or a grant agreement, but that’s a different animal, and will be more explicit than the automatic rights involved in copyright.\r\n2. Depending on how this project is funded I think any copyrightable work here - the software, for instance - could be student work, personal work, or institutional work, under the 1992 Copyright Ownership Policy. It’s unlikely to be a scholarly/aesthetic work because of the definition of “designated academic appointee,” but I don’t know who the co-authors are.\r\n3. UC’s lawyers - OGC and general counsel - will generally weigh in to assess legal risk to the university or disposition of university intellectual property. They will not/cannot provide advice about liability to an individual, or assessment of their personal intellectual property.\r\n4. Each campus has a designated authority who is authorized to approve licensing decisions and the like on that campus. I believe UCSF’s is Karin Immergluck. In my experience, if we get to a place where we decide “well, this project includes copyrights owned by UCSF, but we want to license them CC BY or dedicate them to the public domain,” an email to the relevant campus person explaining the rationale (and preferably why this isn’t something the university would make money off of) results in a quick approval.\r\n##   E. Contracts\r\n1. U.S. copyright law includes all kinds of rights for users, including fair use, and the fact that certain things are in the public domain. But you can sign a contract giving away any of these rights. To the extent that you have to agree to restrictive terms to get access to a data set, those terms may effectively limit your rights to reuse even factual data. It’s like when libraries sign a license for a ProQuest database and promise not to make any copies of newspaper articles from the 1800s.\r\n#II. International law\r\n##   A. Database protection generally\r\nLots of countries protect a database, but not the underlying facts, with copyright law. I see you found the Bitlaw page on this, which is where I would have sent you.\r\n##   B. European database directive\r\nI’ve never had occasion to deal with this before, but there’s a parallel thing in some countries like Italy for, e.g. digitizing old manuscripts. Limited protection as an incentive to create the thing or make it accessible. It sounds like enough of a pain that it’s probably worth figuring out which of the proposed sources are covered. That may be time consuming and difficult - so, something for further discussion/research.\r\n##   C. International liability for potential copyright infringement\r\nThis is a tricky issue, and a fun subject for law review articles. Most of them revolve around *selling* things internationally, for a couple reasons. First, that’s when you’re likely to make people mad enough to bother with suing you. Second, there are jurisdictional issues about how much you have to do in a country to subject yourself to a lawsuit there. All I can say is that internet plus free distribution doesn’t automatically equal global legal risk. But that may not matter much because...\r\n#II. There’s law, and then there’s politics.\r\nIf we were looking at hundreds of sources, contacting them individually would be a horrible thing to contemplate. With a couple dozen, it might be worth it to put together a form letter to let people know about the project, to avoid burning bridges with current colleagues and potential future collaborators. This could address the things these folks are most likely to be concerned about: what is this project doing with the data sources? How will downstream users be able to tell the source of the data? What things will facilitate or burden commercial use? And there could be a few different versions depending on the legal assessment of the underlying rights and which ones the project implicates - maybe a letter to US sources is more of an FYI, and one to European sources asks them to reply granting permission. Or maybe if the project really wants *everything* to be as open as possible, you just actually get permission from everyone to a release of some version of their data, in this context, under your chosen license. Just because they make it available to the world under, e.g., CC BY-SA doesn’t mean they can’t make it available to you under different terms. There are options. None of them are as easy as “just use it,” but if people have tried to restrict how their stuff is used you have to decide the relative value you place on maximizing your rights under the law vs. maintaining goodwill.\r\n\r\n",
      "profile": 137,
      "published": "2015-09-22T20:42:41.586272Z",
      "thread": 107,
      "url": "/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#10"
    },
    {
      "body_html": "<p>We currently rely on <a href=\"http://www.broadinstitute.org/gsea/msigdb/index.jsp\">MSigDB</a> <span class=\"citation\">[<a href=\"/doi/10.1073/pnas.0506580102\" class=\"citation\" data-key=\"10.1073/pnas.0506580102\">1</a>, <a href=\"/doi/10.1093/bioinformatics/btr260\" class=\"citation\" data-key=\"10.1093/bioinformatics/btr260\">2</a>]</span>, the Molecular Signatures Database, for perturbation gene sets and pathways. Since the <a href=\"https://github.com/dhimmel/integrate/blob/afe1c049e4a088bc266533332e6e00499e3d2e20/licenses/custom/MSigDB.asciidoc\">license</a> is highly restrictive, we have emailed the creators with the below message. We will post any updates regarding MSigDB licensing or permissions on this discussion.</p>\r\n\r\n<hr>\r\n\r\n<p>Greetings MSigDB Team,</p>\r\n\r\n<p>I am a graduate student at UCSF, and I have been using <a href=\"http://www.broadinstitute.org/gsea/msigdb/index.jsp\">MSigDB</a> for my research. My project aims to predict new uses for existing drugs by integrating many different types of biomedical information. Recently, the issue of database copyright and licensing <a href=\"http://thinklab.com/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d107\">came up</a>, and we are now trying to ensure that we have sufficient permissions for each of the 28 databases we're integrating.</p>\r\n\r\n<p>I was surprised to learn of MSigDB's restrictive <a href=\"http://www.broadinstitute.org/cancer/software/gsea/wiki/index.php/License_info\">license</a> that forbids redistribution, especially given the projects <a href=\"http://grantome.com/grant/NIH/R01-CA121941-06A1\">public funding</a>. Currently, several resources I have created may be non-compliant with the license:</p>\r\n\r\n<p>My GitHub repository (<a href=\"https://github.com/dhimmel/msigdb\"><code>dhimmel/msigdb</code></a>) for parsing the MSigDB database contains:</p>\r\n\r\n<ul><li>unmodified MSigDB downloads</li><li>a reformatted version of the underlying data</li></ul>\r\n\r\n<p>My GitHub repository (<a href=\"https://github.com/dhimmel/pathways\"><code>dhimmel/pathways</code></a>) for combining pathway databases contains:</p>\r\n\r\n<ul><li>the reformatted version of C2:CP from <code>dhimmel/msigdb</code> and a derived dataset containing pathways from other resources</li></ul>\r\n\r\n<p>My GitHub repository (<a href=\"https://github.com/dhimmel/integrate\"><code>dhimmel/integrate</code></a>) for integrating many resources into a single network contains:</p>\r\n\r\n<ul><li>the majority of MSigDB 5.0 C2:CP and C2:CGP data stored as network nodes and edges.</li></ul>\r\n\r\n<p>My <a href=\"http://het.io/disease-genes/downloads/\">website</a> for a previous project <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1004259\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1004259\">3</a>]</span> contains:</p>\r\n\r\n<ul><li>a network download formatted as in <code>dhimmel/integrate</code>, but containing data from most collections in MSigDB version 3.0.</li></ul>\r\n\r\n<p>The public availability of the aforementioned resources is important so others can reproduce and build off of our work. Thus, we request permission for our current redistribution and derivative works of MSigDB. Ideally, we could be granted permission to release MSigDB data under a <a href=\"https://creativecommons.org/licenses/\">Creative Commons license</a> without a No Derivatives restriction. Applying a CC license would lessen the burden on downstream users.</p>\r\n\r\n<p>Thanks for your consideration. Our research is academic in nature, and we suspect it falls under the intended use of MSigDB but perhaps not the license.</p>\r\n\r\n<p>Finally, we're performing our project using an open science platform called Thinklab. I've <a href=\"#1\">posted a copy</a> of this email on Thinklab and will update the discussion with our progress. Alternatively, feel free to respond via Thinklab rather than email. By detailing each step of our research process publicly, we're hoping to create a valuable resource and explore a more holistic and collaborative medium of publication.</p>\r\n\r\n<p>Sincerely,<br>Daniel</p>",
      "body_md": "We currently rely on [MSigDB](http://www.broadinstitute.org/gsea/msigdb/index.jsp) [@10.1073/pnas.0506580102 @10.1093/bioinformatics/btr260], the Molecular Signatures Database, for perturbation gene sets and pathways. Since the [license](https://github.com/dhimmel/integrate/blob/afe1c049e4a088bc266533332e6e00499e3d2e20/licenses/custom/MSigDB.asciidoc) is highly restrictive, we have emailed the creators with the below message. We will post any updates regarding MSigDB licensing or permissions on this discussion.\r\n\r\n***\r\n\r\nGreetings MSigDB Team,\r\n\r\nI am a graduate student at UCSF, and I have been using [MSigDB](http://www.broadinstitute.org/gsea/msigdb/index.jsp) for my research. My project aims to predict new uses for existing drugs by integrating many different types of biomedical information. Recently, the issue of database copyright and licensing [came up](http://thinklab.com/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107), and we are now trying to ensure that we have sufficient permissions for each of the 28 databases we're integrating.\r\n\r\nI was surprised to learn of MSigDB's restrictive [license](http://www.broadinstitute.org/cancer/software/gsea/wiki/index.php/License_info) that forbids redistribution, especially given the projects [public funding](http://grantome.com/grant/NIH/R01-CA121941-06A1). Currently, several resources I have created may be non-compliant with the license:\r\n\r\nMy GitHub repository ([`dhimmel/msigdb`](https://github.com/dhimmel/msigdb)) for parsing the MSigDB database contains:\r\n\r\n+ unmodified MSigDB downloads\r\n+ a reformatted version of the underlying data\r\n\r\nMy GitHub repository ([`dhimmel/pathways`](https://github.com/dhimmel/pathways)) for combining pathway databases contains:\r\n\r\n+ the reformatted version of C2:CP from `dhimmel/msigdb` and a derived dataset containing pathways from other resources\r\n\r\nMy GitHub repository ([`dhimmel/integrate`](https://github.com/dhimmel/integrate)) for integrating many resources into a single network contains:\r\n\r\n+ the majority of MSigDB 5.0 C2:CP and C2:CGP data stored as network nodes and edges.\r\n\r\nMy [website](http://het.io/disease-genes/downloads/) for a previous project [@10.1371/journal.pcbi.1004259] contains:\r\n\r\n+ a network download formatted as in `dhimmel/integrate`, but containing data from most collections in MSigDB version 3.0.\r\n\r\nThe public availability of the aforementioned resources is important so others can reproduce and build off of our work. Thus, we request permission for our current redistribution and derivative works of MSigDB. Ideally, we could be granted permission to release MSigDB data under a [Creative Commons license](https://creativecommons.org/licenses/) without a No Derivatives restriction. Applying a CC license would lessen the burden on downstream users.\r\n\r\nThanks for your consideration. Our research is academic in nature, and we suspect it falls under the intended use of MSigDB but perhaps not the license.\r\n\r\nFinally, we're performing our project using an open science platform called Thinklab. I've [posted a copy](#1) of this email on Thinklab and will update the discussion with our progress. Alternatively, feel free to respond via Thinklab rather than email. By detailing each step of our research process publicly, we're hoping to create a valuable resource and explore a more holistic and collaborative medium of publication.\r\n\r\nSincerely,\r\nDaniel",
      "profile": 17,
      "published": "2015-09-28T18:33:52.869612Z",
      "thread": 108,
      "url": "/discussion/msigdb-licensing/108"
    },
    {
      "body_html": "<h1>Mixed copyright licensing</h1>\r\n\r\n<p>As explained <a href=\"#4\">above</a>, we have created resources (mostly GitHub repositories) that contain content with varying licenses and restrictions. Therefore, we need to:</p>\r\n\r\n<ul><li>license different files from the same repository under different licenses</li><li>license different portions within a single file under different licenses</li></ul>\r\n\r\n<p>It appears that there is not a rigid formula for how to specify mixed copyright. I found a few examples including the <a href=\"https://github.com/neo4j/neo4j/blob/5f991933fa531f7dd901d4b6570fe78d73f8bb3c/README.asciidoc\">neo4j source code</a> and a <a href=\"https://github.com/lucidv01d/samasy/blob/32148fdd01c1993a991787d23f4c851ff26f7c01/LICENSE\">license</a> the <a href=\"https://ita.ucsf.edu/\">UCSF Office of Innovation, Technology &amp; Alliances</a> created for my classmate.</p>\r\n\r\n<p>In the later case, my classmate asked the ITA to assist him in creating an open source license. As <a href=\"/u/mackenziesmith\" class=\"username\">@mackenziesmith</a> predicted, UC asserted ownership of the content and forbid any for-profit usage. As an aside, I am highly confident that UC does not own my work, because it is not <em>work made for hire</em>, and I never agreed to any transfer of ownership.</p>\r\n\r\n<h2>Proposed license for the SIDER4 repository</h2>\r\n\r\n<p>SIDER 4 is a resource <a href=\"http://thinklab.com/discussion/extracting-side-effects-from-sider-4/97\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d97\">we're using</a> for drug side effects. I propose the <a href=\"https://github.com/dhimmel/SIDER4/blob/45d0ba626e406ae3ce6f8f503f09d5af9b4b7b63/LICENSE.md\">following license</a> for the repository:</p>\r\n\r\n<blockquote><p>SIDER 4 data is <a href=\"http://sideeffects.embl.de/download/\">released</a> under a <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">CC-BY-NC-SA</a> license. Therefore, all redistributed and derived content from SIDER 4 is <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">CC-BY-NC-SA</a>. All original content is released under <a href=\"https://creativecommons.org/publicdomain/zero/1.0/\">CC0</a>.</p><p>Accordingly, the following files are <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">CC-BY-NC-SA</a>:</p><ul><li><code>download/meddra_all_indications.tsv.gz</code></li><li><code>download/meddra_all_se.tsv.gz</code></li><li><code>download/meddra_freq.tsv.gz</code></li><li><code>data/indication.tsv</code></li><li><code>data/side-effects.tsv</code></li></ul><p><strong>Disclaimer</strong>: The repository is provided \"as is\", without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and noninfringement. In no event shall the authors or copyright holders be liable for any claim, damages or other liability, whether in an action of contract, tort or otherwise, arising from, out of or in connection with the repository or the use or other dealings in the repository.</p></blockquote>\r\n\r\n<p>We added the disclaimer to limit our liability as <a href=\"#4\">suggested</a> by <a href=\"/u/mackenziesmith\" class=\"username\">@mackenziesmith</a>. Does the proposed license seem adequate? Is it clear? <a href=\"/u/katiefortney\" class=\"username\">@katiefortney</a>, any suggestions?</p>",
      "body_md": "# Mixed copyright licensing\r\n\r\nAs explained [above](#4), we have created resources (mostly GitHub repositories) that contain content with varying licenses and restrictions. Therefore, we need to:\r\n\r\n+ license different files from the same repository under different licenses\r\n+ license different portions within a single file under different licenses\r\n\r\nIt appears that there is not a rigid formula for how to specify mixed copyright. I found a few examples including the [neo4j source code](https://github.com/neo4j/neo4j/blob/5f991933fa531f7dd901d4b6570fe78d73f8bb3c/README.asciidoc) and a [license](https://github.com/lucidv01d/samasy/blob/32148fdd01c1993a991787d23f4c851ff26f7c01/LICENSE) the [UCSF Office of Innovation, Technology & Alliances](https://ita.ucsf.edu/) created for my classmate.\r\n\r\nIn the later case, my classmate asked the ITA to assist him in creating an open source license. As @mackenziesmith predicted, UC asserted ownership of the content and forbid any for-profit usage. As an aside, I am highly confident that UC does not own my work, because it is not *work made for hire*, and I never agreed to any transfer of ownership.\r\n\r\n## Proposed license for the SIDER4 repository\r\n\r\nSIDER 4 is a resource [we're using](http://thinklab.com/discussion/extracting-side-effects-from-sider-4/97) for drug side effects. I propose the [following license](https://github.com/dhimmel/SIDER4/blob/45d0ba626e406ae3ce6f8f503f09d5af9b4b7b63/LICENSE.md) for the repository:\r\n\r\n> SIDER 4 data is [released](http://sideeffects.embl.de/download/) under a [CC-BY-NC-SA](http://creativecommons.org/licenses/by-nc-sa/4.0/) license. Therefore, all redistributed and derived content from SIDER 4 is [CC-BY-NC-SA](http://creativecommons.org/licenses/by-nc-sa/4.0/). All original content is released under [CC0](https://creativecommons.org/publicdomain/zero/1.0/).\r\n\r\n> Accordingly, the following files are [CC-BY-NC-SA](http://creativecommons.org/licenses/by-nc-sa/4.0/):\r\n\r\n>\r\n+ `download/meddra_all_indications.tsv.gz`\r\n+ `download/meddra_all_se.tsv.gz`\r\n+ `download/meddra_freq.tsv.gz`\r\n+ `data/indication.tsv`\r\n+ `data/side-effects.tsv`\r\n\r\n> **Disclaimer**: The repository is provided \"as is\", without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and noninfringement. In no event shall the authors or copyright holders be liable for any claim, damages or other liability, whether in an action of contract, tort or otherwise, arising from, out of or in connection with the repository or the use or other dealings in the repository.\r\n\r\nWe added the disclaimer to limit our liability as [suggested](#4) by @mackenziesmith. Does the proposed license seem adequate? Is it clear? @katiefortney, any suggestions?",
      "profile": 17,
      "published": "2015-09-28T18:06:50.053385Z",
      "thread": 107,
      "url": "/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#11"
    },
    {
      "body_html": "<p>We're currently using LINCS L1000 data for compound–gene and gene–gene edges in our <a href=\"http://thinklab.com/discussion/one-network-to-rule-them-all/102\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d102\">network</a>. Thus far we have developed methods for computing <a href=\"http://thinklab.com/discussion/computing-consensus-transcriptional-profiles-for-compounds-using-lincs/43\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d43\">consensus expression signatures</a> and <a href=\"http://thinklab.com/discussion/unichem-mapping-to-lincs-small-molecules/51\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d51\">mapping LINCS compounds</a> to other identifier systems. However, <a href=\"/u/larsjuhljensen\" class=\"username\">@larsjuhljensen</a> <a href=\"http://thinklab.com/discussion/one-network-to-rule-them-all/102#2\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d102\">pointed out</a> that the <a href=\"https://github.com/dhimmel/integrate/blob/afe1c049e4a088bc266533332e6e00499e3d2e20/licenses/custom/L1000.md\">license</a> requires permission for redistribution:</p>\r\n\r\n<blockquote><p>If you have a derivative work that is significantly different from what we provide and you would like to distribute it, please contact us with the details. Our goal is to encourage significant improvements while maintaining provenance and reproducible research standards.</p></blockquote>\r\n\r\n<p>Therefore, we have emailed the LINCS L1000 team with the following permission request. We will post any updates regarding licensing or permissions on this discussion.</p>\r\n\r\n<hr>\r\n\r\n<p>Greetings LINCS L1000 Team,</p>\r\n\r\n<p>I am a graduate student at UCSF, and I have been using <a href=\"http://www.lincscloud.org/\">LINCS L1000</a> for my research. My project aims to predict new uses for existing drugs by integrating many different types of biomedical information. Recently, the issue of database copyright and licensing <a href=\"http://thinklab.com/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d107\">came up</a>, and we are now trying to ensure that we have sufficient permissions for each of the 28 databases we're integrating.</p>\r\n\r\n<p>Currently, several resources I have created may be non-compliant with the license.</p>\r\n\r\n<p>My GitHub repository (<a href=\"https://github.com/dhimmel/lincs\"><code>dhimmel/lincs</code></a>) contains:</p>\r\n\r\n<ul><li>Python code from <a href=\"https://github.com/cmap/l1ktools/tree/7f1752e87bbaeeedfce18c68f84c4e1feb331e9e/python/cmap\"><code>cmap/l1ktools/python/cmap</code></a></li><li><a href=\"https://github.com/dhimmel/lincs/tree/69956dec590ce4caace9df31f5b60c978f321fdc/data\">Data</a> retrieved from the <a href=\"http://api.lincscloud.org/\">API</a> in an unmodified json format and a condensed tsv format.</li><li><a href=\"https://github.com/dhimmel/lincs/tree/69956dec590ce4caace9df31f5b60c978f321fdc/data/consensi\">Consensus signatures</a> for DrugBank compounds, gene overexpressions, gene knockdowns, and perturbations. Our consensus signatures combine <em>z</em>-scores from multiple signatures. We <a href=\"http://thinklab.com/discussion/computing-consensus-transcriptional-profiles-for-compounds-using-lincs/43#6\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d43\">computed</a> our signatures using a method suggested to us during LINCS office hours, with some modifications.</li><li>Our <a href=\"https://github.com/dhimmel/lincs/blob/69956dec590ce4caace9df31f5b60c978f321fdc/.gitignore\"><code>.gitignore</code></a> file prevents the following items from being uploaded to the repository: our private API key, <code>modzs.gctx</code>, and a local database (<code>l1000.db</code>) that is too large for GitHub.</li><li>An archived version of this repository is <a href=\"https://doi.org/10.5281/zenodo.27229\">hosted</a> on Zenodo <span class=\"citation\">[<a href=\"/doi/10.5281/zenodo.27229\" class=\"citation\" data-key=\"10.5281/zenodo.27229\">1</a>]</span>.</li></ul>\r\n\r\n<p>My GitHub repository (<a href=\"https://github.com/dhimmel/integrate\"><code>dhimmel/integrate</code></a>) for integrating many resources into a single network contains:</p>\r\n\r\n<ul><li>Consensus signatures for DrugBank compounds and genetic perturbations (gene overexpressions and knowdowns) encoded as network nodes and edges.</li></ul>\r\n\r\n<p><a href=\"/u/leobrueggeman\" class=\"username\">@leobrueggeman</a> assisted with the LINCS analysis. His GitHub repository (<a href=\"https://github.com/LABrueggs/L1000/tree/8720f12c25bdc46ef789785c474b8f0af9200fcf\"><code>LABrueggs/L1000</code></a>) contains elements similar to <code>dhimmel/lincs</code> discussed above. Two files of consensus signatures from his repository are <a href=\"https://doi.org/10.6084/m9.figshare.1476293\">posted</a> to figshare <span class=\"citation\">[<a href=\"/doi/10.6084/m9.figshare.1476293\" class=\"citation\" data-key=\"10.6084/m9.figshare.1476293\">2</a>]</span>.</p>\r\n\r\n<p>The public availability of the aforementioned resources is important so others can reproduce and build off of our work. We have attempted to provide sufficient information for provenance and reproducibility but are happy to make any modifications to assist in these regards.</p>\r\n\r\n<p>Thus, we request permission for our current usage of LINCS L1000 data. Ideally, we could be granted permission to release the data under a <a href=\"https://creativecommons.org/licenses/\">Creative Commons license</a> without a No Derivatives restriction. Applying a CC license would lessen the burden on downstream users.</p>\r\n\r\n<p>Thanks for your consideration. Our research is academic in nature, and we suspect it is in line with the intended use of LINCS.</p>\r\n\r\n<p>Finally, we're performing our project using an open science platform called Thinklab. I've <a href=\"#1\">posted a copy</a> of this email on Thinklab and will update the discussion with our progress. Alternatively, feel free to respond via Thinklab rather than email. By detailing each step of our research process publicly, we're hoping to create a valuable resource and explore a more holistic and collaborative medium of publication.</p>\r\n\r\n<p>Sincerely,<br>Daniel</p>",
      "body_md": "We're currently using LINCS L1000 data for compound--gene and gene--gene edges in our [network](http://thinklab.com/discussion/one-network-to-rule-them-all/102). Thus far we have developed methods for computing [consensus expression signatures](http://thinklab.com/discussion/computing-consensus-transcriptional-profiles-for-compounds-using-lincs/43) and [mapping LINCS compounds](http://thinklab.com/discussion/unichem-mapping-to-lincs-small-molecules/51) to other identifier systems. However, @larsjuhljensen [pointed out](http://thinklab.com/discussion/one-network-to-rule-them-all/102#2) that the [license](https://github.com/dhimmel/integrate/blob/afe1c049e4a088bc266533332e6e00499e3d2e20/licenses/custom/L1000.md) requires permission for redistribution:\r\n\r\n> If you have a derivative work that is significantly different from what we provide and you would like to distribute it, please contact us with the details. Our goal is to encourage significant improvements while maintaining provenance and reproducible research standards.\r\n\r\nTherefore, we have emailed the LINCS L1000 team with the following permission request. We will post any updates regarding licensing or permissions on this discussion.\r\n\r\n***\r\n\r\nGreetings LINCS L1000 Team,\r\n\r\nI am a graduate student at UCSF, and I have been using [LINCS L1000](http://www.lincscloud.org/) for my research. My project aims to predict new uses for existing drugs by integrating many different types of biomedical information. Recently, the issue of database copyright and licensing [came up](http://thinklab.com/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107), and we are now trying to ensure that we have sufficient permissions for each of the 28 databases we're integrating.\r\n\r\nCurrently, several resources I have created may be non-compliant with the license.\r\n\r\nMy GitHub repository ([`dhimmel/lincs`](https://github.com/dhimmel/lincs)) contains:\r\n\r\n+ Python code from [`cmap/l1ktools/python/cmap`](https://github.com/cmap/l1ktools/tree/7f1752e87bbaeeedfce18c68f84c4e1feb331e9e/python/cmap)\r\n+ [Data](https://github.com/dhimmel/lincs/tree/69956dec590ce4caace9df31f5b60c978f321fdc/data) retrieved from the [API](http://api.lincscloud.org/) in an unmodified json format and a condensed tsv format.\r\n+ [Consensus signatures](https://github.com/dhimmel/lincs/tree/69956dec590ce4caace9df31f5b60c978f321fdc/data/consensi) for DrugBank compounds, gene overexpressions, gene knockdowns, and perturbations. Our consensus signatures combine *z*-scores from multiple signatures. We [computed](http://thinklab.com/discussion/computing-consensus-transcriptional-profiles-for-compounds-using-lincs/43#6) our signatures using a method suggested to us during LINCS office hours, with some modifications.\r\n+ Our [`.gitignore`](https://github.com/dhimmel/lincs/blob/69956dec590ce4caace9df31f5b60c978f321fdc/.gitignore) file prevents the following items from being uploaded to the repository: our private API key, `modzs.gctx`, and a local database (`l1000.db`) that is too large for GitHub.\r\n+ An archived version of this repository is [hosted](https://doi.org/10.5281/zenodo.27229) on Zenodo [@10.5281/zenodo.27229].\r\n\r\nMy GitHub repository ([`dhimmel/integrate`](https://github.com/dhimmel/integrate)) for integrating many resources into a single network contains:\r\n\r\n+ Consensus signatures for DrugBank compounds and genetic perturbations (gene overexpressions and knowdowns) encoded as network nodes and edges.\r\n\r\n@leobrueggeman assisted with the LINCS analysis. His GitHub repository ([`LABrueggs/L1000`](https://github.com/LABrueggs/L1000/tree/8720f12c25bdc46ef789785c474b8f0af9200fcf)) contains elements similar to `dhimmel/lincs` discussed above. Two files of consensus signatures from his repository are [posted](https://doi.org/10.6084/m9.figshare.1476293) to figshare [@10.6084/m9.figshare.1476293].\r\n\r\nThe public availability of the aforementioned resources is important so others can reproduce and build off of our work. We have attempted to provide sufficient information for provenance and reproducibility but are happy to make any modifications to assist in these regards.\r\n\r\nThus, we request permission for our current usage of LINCS L1000 data. Ideally, we could be granted permission to release the data under a [Creative Commons license](https://creativecommons.org/licenses/) without a No Derivatives restriction. Applying a CC license would lessen the burden on downstream users.\r\n\r\nThanks for your consideration. Our research is academic in nature, and we suspect it is in line with the intended use of LINCS.\r\n\r\nFinally, we're performing our project using an open science platform called Thinklab. I've [posted a copy](#1) of this email on Thinklab and will update the discussion with our progress. Alternatively, feel free to respond via Thinklab rather than email. By detailing each step of our research process publicly, we're hoping to create a valuable resource and explore a more holistic and collaborative medium of publication.\r\n\r\nSincerely,\r\nDaniel",
      "profile": 17,
      "published": "2015-09-28T22:59:26.416850Z",
      "thread": 110,
      "url": "/discussion/lincs-l1000-licensing/110"
    },
    {
      "body_html": "<h1>Entrez Gene <em>Homo sapiens</em> gotcha</h1>\r\n\r\n<p>The <code>Homo_sapiens.gene_info.gz</code> <a href=\"ftp://ftp.ncbi.nih.gov/gene/DATA/GENE_INFO/Mammalia/\">download</a> from Entrez Gene contains a potential <a href=\"https://en.wikipedia.org/wiki/Gotcha_%28programming%29\">gotcha</a>. A small number of records at the end of the file are for:</p>\r\n\r\n<ul><li>Neanderthal (<a href=\"http://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?mode=Info&amp;id=63221\"><code>tax_id = 63221</code></a>)</li><li>Denisovan (<a href=\"http://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?mode=Info&amp;id=741158\"><code>tax_id = 741158</code></a>)</li></ul>\r\n\r\n<p>We only want genes for non-extinct <em>Homo sapiens</em> (<a href=\"http://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?mode=Info&amp;id=9606\"><code>tax_id = 9606</code></a>). We've <a href=\"https://github.com/dhimmel/entrez-gene/commit/1ff24cce1cbabfb7029704426b5dc4b654e484a4\">updated</a> our Entrez Gene processing to filter for a 9606 tax_id.</p>\r\n\r\n<p>The downstream effects of this update should be minimal, since only 73 genes were removed (all mitochondrial). However, we may rebuild some of our resources if necessary. The inclusion of these genes should only present problems when matching by symbol rather than GeneID. We avoid matching by symbol whenever possible.</p>",
      "body_md": "# Entrez Gene *Homo sapiens* gotcha\r\n\r\nThe `Homo_sapiens.gene_info.gz` [download](ftp://ftp.ncbi.nih.gov/gene/DATA/GENE_INFO/Mammalia/) from Entrez Gene contains a potential [gotcha](https://en.wikipedia.org/wiki/Gotcha_%28programming%29). A small number of records at the end of the file are for:\r\n\r\n+ Neanderthal ([`tax_id = 63221`](http://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?mode=Info&id=63221))\r\n+ Denisovan ([`tax_id = 741158`](http://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?mode=Info&id=741158))\r\n\r\nWe only want genes for non-extinct *Homo sapiens* ([`tax_id = 9606`](http://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?mode=Info&id=9606)). We've [updated](https://github.com/dhimmel/entrez-gene/commit/1ff24cce1cbabfb7029704426b5dc4b654e484a4) our Entrez Gene processing to filter for a 9606 tax_id.\r\n\r\nThe downstream effects of this update should be minimal, since only 73 genes were removed (all mitochondrial). However, we may rebuild some of our resources if necessary. The inclusion of these genes should only present problems when matching by symbol rather than GeneID. We avoid matching by symbol whenever possible.",
      "profile": 17,
      "published": "2015-09-29T14:19:20.369515Z",
      "thread": 34,
      "url": "/discussion/using-entrez-gene-as-our-gene-vocabulary/34#9"
    },
    {
      "body_html": "<h1>Chronicling licensing and permission requests</h1>\r\n\r\n<p>Inspired by the story of Max Haeussler <span class=\"citation\">[<a href=\"/doi/10.1038/483134a\" class=\"citation\" data-key=\"10.1038/483134a\">1</a>]</span> who <a href=\"http://text.soe.ucsc.edu/progress.html\">publicly documented</a> his permission requests to publishers to text mine their corpora, I will be chronicling our licensing efforts that require contact. We will therefore release summaries and statistics pertaining to three types of requests:</p>\r\n\r\n<ul><li>permission requests to resources with licenses that <strong>forbid redistribution or derivatives</strong>. We have begun by posting our requests to <a href=\"http://thinklab.com/discussion/msigdb-licensing/108\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d108\">MSigDB</a> and <a href=\"http://thinklab.com/discussion/lincs-l1000-licensing/110\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d110\">LINCS L1000</a>.</li><li>requests to post licenses for resources <strong>without licensing information</strong>. Resources for which we could not find license information are <a href=\"https://github.com/dhimmel/integrate/tree/0e343d8745a98757c86e73f645b41353f52b82b5/licenses\">available here</a>. We have already emailed the creators of these resources and will report back with progress.</li><li>for datasets obtained from <strong>publication supplements</strong>, license clarification or permission requests to the journal.</li></ul>",
      "body_md": "# Chronicling licensing and permission requests\r\n\r\nInspired by the story of Max Haeussler [@10.1038/483134a] who [publicly documented](http://text.soe.ucsc.edu/progress.html) his permission requests to publishers to text mine their corpora, I will be chronicling our licensing efforts that require contact. We will therefore release summaries and statistics pertaining to three types of requests:\r\n\r\n+ permission requests to resources with licenses that **forbid redistribution or derivatives**. We have begun by posting our requests to [MSigDB](http://thinklab.com/discussion/msigdb-licensing/108) and [LINCS L1000](http://thinklab.com/discussion/lincs-l1000-licensing/110).\r\n+ requests to post licenses for resources **without licensing information**. Resources for which we could not find license information are [available here](https://github.com/dhimmel/integrate/tree/0e343d8745a98757c86e73f645b41353f52b82b5/licenses). We have already emailed the creators of these resources and will report back with progress.\r\n+ for datasets obtained from **publication supplements**, license clarification or permission requests to the journal.",
      "profile": 17,
      "published": "2015-09-30T17:04:25.075087Z",
      "thread": 107,
      "url": "/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#12"
    },
    {
      "body_html": "<h2>Definitions</h2>\r\n\r\n<p>In a meeting yesterday, we (Ari Green, Christine Hessler, <a href=\"/u/dhimmel\" class=\"username\">@dhimmel</a>, <a href=\"/u/sergiobaranzini\" class=\"username\">@sergiobaranzini</a>) discussed the pilot experience and definitions. </p>\r\n\r\n<p>AJG pointed out that the term \"disease modifying\" is primarily used for rheumatology and multiple sclerosis. With this caveat in mind, we set out to identify a general definition that could apply broadly to complex disease. </p>\r\n\r\n<p>When considering possible definitions, <a href=\"/u/sergiobaranzini\" class=\"username\">@sergiobaranzini</a> and I stressed the following quality of a \"disease modifying\" indication:</p>\r\n\r\n<blockquote><p>If we predicted this indication, would the disease be considered an appropriate and precise application of the drug.</p></blockquote>\r\n\r\n<h3>We agreed on the following definitions:</h3>\r\n\r\n<ul><li><strong>disease modifying</strong> (<code>DM</code>) — a drug that therapeutically changes the underlying or downstream biology of the disease</li><li><strong>symptomatic</strong> (<code>SYM</code>) — a drug that treats a significant symptom of the disease</li><li><strong>non-indication</strong> (<code>NOT</code>) — a drug that neither therapeutically changes the underlying or downstream biology nor treats a significant symptom of the disease</li></ul>\r\n\r\n<p>We also agreed on the following <strong>guidelines</strong>:</p>\r\n\r\n<ul><li><strong>reasonable evidence</strong> of efficacy is required to be classified as disease modifying or symptomatic</li><li>if no classification accurately describes an indication, the <strong>most appropriate</strong> (although imperfect) classification should be chosen</li></ul>\r\n\r\n<h2>Next steps</h2>\r\n\r\n<p>AJG and CSH found many of their disagreements on the pilot indications resolved once a common definition was reached. With these definitions, we will now move onto the full set of indications, which AJG and CSH have agreed to curate.</p>",
      "body_md": "## Definitions\r\n\r\nIn a meeting yesterday, we (Ari Green, Christine Hessler, @dhimmel, @sergiobaranzini) discussed the pilot experience and definitions. \r\n\r\nAJG pointed out that the term \"disease modifying\" is primarily used for rheumatology and multiple sclerosis. With this caveat in mind, we set out to identify a general definition that could apply broadly to complex disease. \r\n\r\nWhen considering possible definitions, @sergiobaranzini and I stressed the following quality of a \"disease modifying\" indication:\r\n\r\n> If we predicted this indication, would the disease be considered an appropriate and precise application of the drug.\r\n\r\n### We agreed on the following definitions:\r\n\r\n+ **disease modifying** (`DM`) -- a drug that therapeutically changes the underlying or downstream biology of the disease\r\n+ **symptomatic** (`SYM`) -- a drug that treats a significant symptom of the disease\r\n+ **non-indication** (`NOT`) -- a drug that neither therapeutically changes the underlying or downstream biology nor treats a significant symptom of the disease\r\n\r\nWe also agreed on the following **guidelines**:\r\n\r\n+ **reasonable evidence** of efficacy is required to be classified as disease modifying or symptomatic\r\n+ if no classification accurately describes an indication, the **most appropriate** (although imperfect) classification should be chosen\r\n\r\n## Next steps\r\n\r\nAJG and CSH found many of their disagreements on the pilot indications resolved once a common definition was reached. With these definitions, we will now move onto the full set of indications, which AJG and CSH have agreed to curate.",
      "profile": 17,
      "published": "2015-10-01T23:23:51.066177Z",
      "thread": 95,
      "url": "/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95#4"
    },
    {
      "body_html": "<p>Our protein interaction catalog includes data from the supplementary material of the Incomplete Interactome publication <span class=\"citation\">[<a href=\"/doi/10.1126/science.1257601\" class=\"citation\" data-key=\"10.1126/science.1257601\">1</a>]</span>. Specifically, <a href=\"http://thinklab.com/discussion/creating-a-catalog-of-protein-interactions/85#9\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d85\">we incorporate</a> a subset of <code>DataS1_interactome.tsv</code>.</p>\r\n\r\n<p>The <a href=\"http://www.sciencemag.org/site/feature/contribinfo/prep/4__2014LicensetoPublish_Final_6MAR2014.pdf\">License to Publish</a> agreement for <em>Science</em>, which authors must sign, states that:</p>\r\n\r\n<blockquote><p>In consideration of publication by AAAS in one of its Science journals of the work currently titled [title] and <strong>all associated supplemental materials, data</strong>, audio and/or video files (the \"Work\") and authored by [author] (\"Author\"), the <strong>sole and exclusive, irrevocable right</strong> is hereby granted to AAAS to <strong>publish, reproduce, distribute, transmit, display, store, translate, create derivative works</strong> from and otherwise use the Work in any form, manner, format, or medium, whether now known or hereafter developed, throughout the world and in any language, for the entire duration of any such right and any renewal or extension thereof and to permit/sublicense others to do any or all of the foregoing as well.</p></blockquote>\r\n\r\n<p>I bolded the relevant phrases that lead me to believe that we require the permission of the AAAS rather than the dataset authors. <em>Science's</em> <a href=\"http://www.sciencemag.org/site/about/permissions.xhtml\">reprints and permissions page</a> suggested using the Copyright Clearance Center's Rightslink service. I made an account, but my request was not supported by Rightslink. Therefore, I emailed the AAAS Permissions Department with my special request. This is our first <a href=\"http://thinklab.com/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#12\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d107\">request to a publisher</a> regarding supplementary data. The email is below.</p>\r\n\r\n<hr>\r\n\r\n<p>Dear AAAS Permissions Department,</p>\r\n\r\n<p>I am a graduate student at UCSF, and I have been using supplementary data from <a href=\"https://doi.org/10.1126/science.1257601\">Menche et. al 2015</a> for my research. My project aims to predict new uses for existing drugs by integrating many different types of biomedical information. Recently, the issue of database copyright and licensing <a href=\"http://thinklab.com/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d107\">came up</a>, and we are now trying to ensure that we have sufficient permissions for each of the 28 databases we're integrating.</p>\r\n\r\n<p>Currently, several resources I have created may be non-compliant with your terms.</p>\r\n\r\n<p>My GitHub repository (<a href=\"https://github.com/dhimmel/ppi\"><code>dhimmel/ppi</code></a>) contains:</p>\r\n\r\n<ul><li>an <a href=\"https://github.com/dhimmel/ppi/blob/master/download/ii/Datasets_S1-S4.zip\">unmodified copy</a> of <code>Datasets_S1-S4.zip</code> download from the Additional Data section <a href=\"https://www.sciencemag.org/content/347/6224/1257601/suppl/DC1\">online</a></li><li><a href=\"https://github.com/dhimmel/ppi/tree/master/data\">reformatted versions</a> of the data from <code>DataS1_interactome.tsv</code>, a file inside <code>Datasets_S1-S4.zip</code>.</li></ul>\r\n\r\n<p>My GitHub repository (<a href=\"https://github.com/dhimmel/integrate\"><code>dhimmel/integrate</code></a>) for integrating many resources into a single network contains:</p>\r\n\r\n<ul><li>a subset of the protein interactions from the supplement stored as network nodes and edges.</li></ul>\r\n\r\n<p><a href=\"/u/leobrueggeman\" class=\"username\">@leobrueggeman</a> assisted with parts of the analysis. His GitHub repository  (<a href=\"https://github.com/LABrueggs/incomplete-interactome\"><code>LABrueggs/incomplete-interactome</code></a>) contains:</p>\r\n\r\n<ul><li>datasets of disease names derived from the supplement</li></ul>\r\n\r\n<p>The public availability of the aforementioned resources is important so others can reproduce and build off of our work. Thus, we request permission for our aforementioned redistribution and derivative works of the publication's supplemental data. Ideally, we could be granted permission to release the supplemental data under a <a href=\"https://creativecommons.org/licenses/\">Creative Commons license</a> without a No Derivatives restriction. Applying a CC license would lessen the burden on downstream users.</p>\r\n\r\n<p>Thanks for your consideration. Our research is academic in nature, and we suspect it falls under the intended use of supplemental materials but perhaps not your <a href=\"http://www.sciencemag.org/site/about/copyright.xhtml\">terms and conditions</a>.</p>\r\n\r\n<p>Finally, we're performing our project using an open science platform called Thinklab. I've <a href=\"#1\">posted a copy</a> of this email on Thinklab and will update the discussion with our progress. Alternatively, feel free to respond via Thinklab rather than email. By detailing each step of our research process publicly, we're hoping to create a valuable resource and explore a more holistic and collaborative medium of publication.</p>\r\n\r\n<p>Sincerely,<br>Daniel Himmelstein<br>Graduate Student<br>University of California, San Francisco</p>",
      "body_md": "Our protein interaction catalog includes data from the supplementary material of the Incomplete Interactome publication [@10.1126/science.1257601]. Specifically, [we incorporate](http://thinklab.com/discussion/creating-a-catalog-of-protein-interactions/85#9) a subset of `DataS1_interactome.tsv`.\r\n\r\nThe [License to Publish](http://www.sciencemag.org/site/feature/contribinfo/prep/4__2014LicensetoPublish_Final_6MAR2014.pdf) agreement for *Science*, which authors must sign, states that:\r\n\r\n> In consideration of publication by AAAS in one of its Science journals of the work currently titled [title] and **all associated supplemental materials, data**, audio and/or video files (the \"Work\") and authored by [author] (\"Author\"), the **sole and exclusive, irrevocable right** is hereby granted to AAAS to **publish, reproduce, distribute, transmit, display, store, translate, create derivative works** from and otherwise use the Work in any form, manner, format, or medium, whether now known or hereafter developed, throughout the world and in any language, for the entire duration of any such right and any renewal or extension thereof and to permit/sublicense others to do any or all of the foregoing as well.\r\n\r\nI bolded the relevant phrases that lead me to believe that we require the permission of the AAAS rather than the dataset authors. *Science's* [reprints and permissions page](http://www.sciencemag.org/site/about/permissions.xhtml) suggested using the Copyright Clearance Center's Rightslink service. I made an account, but my request was not supported by Rightslink. Therefore, I emailed the AAAS Permissions Department with my special request. This is our first [request to a publisher](http://thinklab.com/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#12) regarding supplementary data. The email is below.\r\n\r\n***\r\n\r\nDear AAAS Permissions Department,\r\n\r\nI am a graduate student at UCSF, and I have been using supplementary data from [Menche et. al 2015](https://doi.org/10.1126/science.1257601) for my research. My project aims to predict new uses for existing drugs by integrating many different types of biomedical information. Recently, the issue of database copyright and licensing [came up](http://thinklab.com/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107), and we are now trying to ensure that we have sufficient permissions for each of the 28 databases we're integrating.\r\n\r\nCurrently, several resources I have created may be non-compliant with your terms.\r\n\r\nMy GitHub repository ([`dhimmel/ppi`](https://github.com/dhimmel/ppi)) contains:\r\n\r\n+ an [unmodified copy](https://github.com/dhimmel/ppi/blob/master/download/ii/Datasets_S1-S4.zip) of `Datasets_S1-S4.zip` download from the Additional Data section [online](https://www.sciencemag.org/content/347/6224/1257601/suppl/DC1)\r\n+ [reformatted versions](https://github.com/dhimmel/ppi/tree/master/data) of the data from `DataS1_interactome.tsv`, a file inside `Datasets_S1-S4.zip`.\r\n\r\nMy GitHub repository ([`dhimmel/integrate`](https://github.com/dhimmel/integrate)) for integrating many resources into a single network contains:\r\n\r\n+ a subset of the protein interactions from the supplement stored as network nodes and edges.\r\n\r\n@leobrueggeman assisted with parts of the analysis. His GitHub repository  ([`LABrueggs/incomplete-interactome`](https://github.com/LABrueggs/incomplete-interactome)) contains:\r\n\r\n+ datasets of disease names derived from the supplement\r\n\r\nThe public availability of the aforementioned resources is important so others can reproduce and build off of our work. Thus, we request permission for our aforementioned redistribution and derivative works of the publication's supplemental data. Ideally, we could be granted permission to release the supplemental data under a [Creative Commons license](https://creativecommons.org/licenses/) without a No Derivatives restriction. Applying a CC license would lessen the burden on downstream users.\r\n\r\nThanks for your consideration. Our research is academic in nature, and we suspect it falls under the intended use of supplemental materials but perhaps not your [terms and conditions](http://www.sciencemag.org/site/about/copyright.xhtml).\r\n\r\nFinally, we're performing our project using an open science platform called Thinklab. I've [posted a copy](#1) of this email on Thinklab and will update the discussion with our progress. Alternatively, feel free to respond via Thinklab rather than email. By detailing each step of our research process publicly, we're hoping to create a valuable resource and explore a more holistic and collaborative medium of publication.\r\n\r\nSincerely,\r\nDaniel Himmelstein\r\nGraduate Student\r\nUniversity of California, San Francisco",
      "profile": 17,
      "published": "2015-10-02T01:58:42.602421Z",
      "thread": 111,
      "url": "/discussion/incomplete-interactome-licensing/111"
    },
    {
      "body_html": "<h1>Preliminary feature computation</h1>\r\n\r\n<h2>Background</h2>\r\n\r\n<p>Our method for <a href=\"http://het.io/hnep\">hetnet edge prediction</a> <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1004259\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1004259\">1</a>]</span> works by quantifying the connectivity between a source and target node. For this project, source nodes are compounds and target nodes are diseases. To extract a feature from the network, we quantify the prevalence of a specific type of paths (metapath) for each compound–disease pair. We use a metric called the <a href=\"https://doi.org/10.1371/journal.pcbi.1004259#article1.body1.sec2.sec2.p1\">degree weighted path count</a> (<em>DWPC</em>) to quantify the extent that that a path of the specified type connects a compound and disease. The <em>DWPC</em> downweights paths through high degree nodes, which are less specific and therefore likely less informative. Thus each metapath yields a feature. We evaluate the predictiveness of a feature by whether it discriminates indicated from non-indicated compound–disease pairs.</p>\r\n\r\n<h2>Methods</h2>\r\n\r\n<p>We computed features for the 261 metapaths with length ≤ 3 for all <a href=\"http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#191\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d21\">1,386 indications</a> and 4,227 non-indications. The 4,227 non-indications were randomly selected from all non-indications. We computed features for 2%, rather than 100%, of non-indications to decrease computation time. This compromise allows us to quickly assess feature-specific performance via <a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve\">AUROC</a>, but does not allow us to make comprehensive predictions or provide reliable estimates of measures that depend the balance between positives and negatives, such as <a href=\"https://en.wikipedia.org/wiki/Precision_and_recall\">AUPRC</a> and properly-scaled predicted probabilities.</p>\r\n\r\n<p>We separately assessed the performance of each of the 261 features (<a href=\"https://github.com/dhimmel/learn/blob/eabff6bdbe777cb21ad731a7b788720eb1d622f8/learn.ipynb\">notebook</a>). We used the <em>DWPC</em> with <span class=\"math\">$$w = 0.4$$</span> — the dampening exponent to control the downweighting of paths through high degree nodes. We chose <span class=\"math\">$$w = 0.4$$</span> because that was optimal in our previous study <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1004259\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1004259\">1</a>]</span> and performance <a href=\"https://doi.org/10.1371/journal.pcbi.1004259.s003\">was stable</a> for surrounding parameter choices.</p>\r\n\r\n<h2>Results</h2>\r\n\r\n<p>We created a <a href=\"https://github.com/dhimmel/learn/blob/eabff6bdbe777cb21ad731a7b788720eb1d622f8/data/auc.tsv\">table of feature performance</a>. Scroll to the bottom of <a href=\"https://github.com/dhimmel/learn/blob/eabff6bdbe777cb21ad731a7b788720eb1d622f8/learn.ipynb\">this notebook</a> for the abbreviation system used in the <code>metapath</code> column. <code>nonzero</code> indicates the proportion of compound–disease pairs that had at least one path for that metapath. <code>auroc</code> represents the chance that a random indication received a higher <em>DWPC</em> than a random non-indication. Stay tuned to this discussion for further analysis.</p>\r\n\r\n<h2>Limitations</h2>\r\n\r\n<p>There are still a few steps remaining before we can draw conclusions on the mechanisms of efficacy:</p>\r\n\r\n<ul><li>Our expert curated indication catalog is not yet ready. Therefore, an <a href=\"http://thinklab.com/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95#3\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d95\">estimated 42%</a> the 1,386 indications are symptomatic or non-indications.</li><li>We haven't yet created permuted networks to compute feature performance on. <a href=\"https://doi.org/10.1371/journal.pcbi.1004259.s003#article1.body1.sec2.sec6.p1\">Permuted networks</a> preserve degree but destroy edge specificity. Much of the current performance is likely attributable to node degree rather than edge specificity. For example, compounds that are indicated for many other diseases are more likely to be indicated for the target disease. Many of our 261 features will capture this effect.</li></ul>",
      "body_md": "# Preliminary feature computation\r\n\r\n## Background\r\n\r\nOur method for [hetnet edge prediction](http://het.io/hnep) [@10.1371/journal.pcbi.1004259] works by quantifying the connectivity between a source and target node. For this project, source nodes are compounds and target nodes are diseases. To extract a feature from the network, we quantify the prevalence of a specific type of paths (metapath) for each compound--disease pair. We use a metric called the [degree weighted path count](https://doi.org/10.1371/journal.pcbi.1004259#article1.body1.sec2.sec2.p1) (*DWPC*) to quantify the extent that that a path of the specified type connects a compound and disease. The *DWPC* downweights paths through high degree nodes, which are less specific and therefore likely less informative. Thus each metapath yields a feature. We evaluate the predictiveness of a feature by whether it discriminates indicated from non-indicated compound--disease pairs.\r\n\r\n## Methods\r\n\r\nWe computed features for the 261 metapaths with length ≤ 3 for all [1,386 indications](http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#191) and 4,227 non-indications. The 4,227 non-indications were randomly selected from all non-indications. We computed features for 2%, rather than 100%, of non-indications to decrease computation time. This compromise allows us to quickly assess feature-specific performance via [AUROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve), but does not allow us to make comprehensive predictions or provide reliable estimates of measures that depend the balance between positives and negatives, such as [AUPRC](https://en.wikipedia.org/wiki/Precision_and_recall) and properly-scaled predicted probabilities.\r\n\r\nWe separately assessed the performance of each of the 261 features ([notebook](https://github.com/dhimmel/learn/blob/eabff6bdbe777cb21ad731a7b788720eb1d622f8/learn.ipynb)). We used the *DWPC* with $$w = 0.4$$ -- the dampening exponent to control the downweighting of paths through high degree nodes. We chose $$w = 0.4$$ because that was optimal in our previous study [@10.1371/journal.pcbi.1004259] and performance [was stable](https://doi.org/10.1371/journal.pcbi.1004259.s003) for surrounding parameter choices.\r\n\r\n## Results\r\n\r\nWe created a [table of feature performance](https://github.com/dhimmel/learn/blob/eabff6bdbe777cb21ad731a7b788720eb1d622f8/data/auc.tsv). Scroll to the bottom of [this notebook](https://github.com/dhimmel/learn/blob/eabff6bdbe777cb21ad731a7b788720eb1d622f8/learn.ipynb) for the abbreviation system used in the `metapath` column. `nonzero` indicates the proportion of compound--disease pairs that had at least one path for that metapath. `auroc` represents the chance that a random indication received a higher *DWPC* than a random non-indication. Stay tuned to this discussion for further analysis.\r\n\r\n## Limitations\r\n\r\nThere are still a few steps remaining before we can draw conclusions on the mechanisms of efficacy:\r\n\r\n+ Our expert curated indication catalog is not yet ready. Therefore, an [estimated 42%](http://thinklab.com/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95#3) the 1,386 indications are symptomatic or non-indications.\r\n+ We haven't yet created permuted networks to compute feature performance on. [Permuted networks](https://doi.org/10.1371/journal.pcbi.1004259.s003#article1.body1.sec2.sec6.p1) preserve degree but destroy edge specificity. Much of the current performance is likely attributable to node degree rather than edge specificity. For example, compounds that are indicated for many other diseases are more likely to be indicated for the target disease. Many of our 261 features will capture this effect.",
      "profile": 17,
      "published": "2015-10-04T18:59:32.043511Z",
      "thread": 115,
      "url": "/discussion/assessing-the-informativeness-of-features/115"
    },
    {
      "body_html": "<p>Today is October 3, 2015. <a href=\"http://www.eventbrite.com/e/bay-area-open-access-week-event-for-generation-open-tickets-13233113599\">345 days ago</a>, I first met <a href=\"/u/jspauld\" class=\"username\">@jspauld</a> and learned of Thinklab. 330 days ago, <a href=\"/u/sergiobaranzini\" class=\"username\">@sergiobaranzini</a> and I agreed to try out the platform, and 264 days ago we posted an initial draft of our proposal.</p>\r\n\r\n<p>Since then our project <a href=\"http://slides.com/dhimmel/greene-lab-interview#/5/1\">has</a> recruited 22 reviewers and started 47 discussions containing 266 comments. Currently, our proposal has 641 views and our most highly viewed discussions have <a href=\"http://thinklab.com/discussion/python-for-the-modern-biodata-scientist/84\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d84\">175</a> and <a href=\"http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d21\">173</a> views. These view counts rely on Google Analytics and  are therefore <a href=\"https://peerj.com/blog/post/115284878007/using-big-data-tools-for-small-data-how-peerj-moved-from-google-analytics-to-emr/\">just estimates</a>.</p>\r\n\r\n<p>We are pleased with the current progress on Thinklab and expect continued growth as the platform matures. In this thread, we will also post instances of reuse, citation, and publicity received outside of Thinklab.</p>",
      "body_md": "Today is October 3, 2015. [345 days ago](http://www.eventbrite.com/e/bay-area-open-access-week-event-for-generation-open-tickets-13233113599), I first met @jspauld and learned of Thinklab. 330 days ago, @sergiobaranzini and I agreed to try out the platform, and 264 days ago we posted an initial draft of our proposal.\r\n\r\nSince then our project [has](http://slides.com/dhimmel/greene-lab-interview#/5/1) recruited 22 reviewers and started 47 discussions containing 266 comments. Currently, our proposal has 641 views and our most highly viewed discussions have [175](http://thinklab.com/discussion/python-for-the-modern-biodata-scientist/84) and [173](http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21) views. These view counts rely on Google Analytics and  are therefore [just estimates](https://peerj.com/blog/post/115284878007/using-big-data-tools-for-small-data-how-peerj-moved-from-google-analytics-to-emr/).\r\n\r\nWe are pleased with the current progress on Thinklab and expect continued growth as the platform matures. In this thread, we will also post instances of reuse, citation, and publicity received outside of Thinklab.",
      "profile": 17,
      "published": "2015-10-03T20:51:26.605713Z",
      "thread": 113,
      "url": "/discussion/tracking-project-reuse-citation-and-publicity/113"
    },
    {
      "body_html": "<h1>Initial network release covered by the <em>Drug Repurposing Portal</em></h1>\r\n\r\n<p>On August 6th 2015, the <a href=\"http://thinklab.com/discussion/one-network-to-rule-them-all/102#1\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d102\">initial release</a> <span class=\"citation\">[<a href=\"/doi/10.5281/zenodo.28040\" class=\"citation\" data-key=\"10.5281/zenodo.28040\">1</a>]</span> of our network was <a href=\"http://drugrepurposingportal.com/drug-repurposing-news.php?query=Himmelstein\">covered</a> by the <em>Drug Repurposing Portal</em>. This site <a href=\"http://drugrepurposingportal.com/\">describes</a> itself as a</p>\r\n\r\n<blockquote><p>first of its kind one-stop-shop platform for intelligent information on Drug Repurposing</p></blockquote>\r\n\r\n<p>The site also includes a <a href=\"http://drugrepurposingportal.com/repurposed-drug-database.php\">database</a> of over 300 instances of repurposing. While the database is unstructured text (so currently unsuitable for computational analyses), it provides a nice human-readable reference.</p>",
      "body_md": "# Initial network release covered by the *Drug Repurposing Portal*\r\n\r\nOn August 6th 2015, the [initial release](http://thinklab.com/discussion/one-network-to-rule-them-all/102#1) [@10.5281/zenodo.28040] of our network was [covered](http://drugrepurposingportal.com/drug-repurposing-news.php?query=Himmelstein) by the *Drug Repurposing Portal*. This site [describes](http://drugrepurposingportal.com/) itself as a\r\n\r\n> first of its kind one-stop-shop platform for intelligent information on Drug Repurposing\r\n\r\nThe site also includes a [database](http://drugrepurposingportal.com/repurposed-drug-database.php) of over 300 instances of repurposing. While the database is unstructured text (so currently unsuitable for computational analyses), it provides a nice human-readable reference.",
      "profile": 17,
      "published": "2015-10-03T21:12:10.146650Z",
      "thread": 113,
      "url": "/discussion/tracking-project-reuse-citation-and-publicity/113#2"
    },
    {
      "body_html": "<h1>Protein-coding Entrez Genes with duplicate symbols</h1>\r\n\r\n<p>After <a href=\"#9\">restricting</a> to <em>Homo sapiens</em>, we found four protein-coding genes with duplicate symbols:</p>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th>tax_id</th><th>GeneID</th><th>Symbol</th><th>chromosome</th><th>map_location</th><th>type_of_gene</th><th>description</th></tr></thead><tbody><tr><td>9606</td><td><a href=\"http://www.ncbi.nlm.nih.gov/gene/?term=266553\">266553</a></td><td>OFCC1</td><td>6</td><td>6p24.3</td><td>protein-coding</td><td>orofacial cleft 1 candidate 1</td></tr><tr><td>9606</td><td><a href=\"http://www.ncbi.nlm.nih.gov/gene/?term=105369145\">105369145</a></td><td>OFCC1</td><td>6</td><td></td><td>protein-coding</td><td>orofacial cleft 1 candidate 1</td></tr><tr><td>9606</td><td><a href=\"http://www.ncbi.nlm.nih.gov/gene/?term=2867\">2867</a></td><td>FFAR2</td><td>19</td><td>19q13.1</td><td>protein-coding</td><td>free fatty acid receptor 2</td></tr><tr><td>9606</td><td><a href=\"http://www.ncbi.nlm.nih.gov/gene/?term=105372382\">105372382</a></td><td>FFAR2</td><td>19</td><td></td><td>protein-coding</td><td>free fatty acid receptor 2</td></tr></tbody></table>\r\n\r\n<p>We will reach out to Entrez Gene to inquire about this unexpected occurrence.</p>",
      "body_md": "# Protein-coding Entrez Genes with duplicate symbols\r\n\r\nAfter [restricting](#9) to *Homo sapiens*, we found four protein-coding genes with duplicate symbols:\r\n\r\n| tax_id | GeneID | Symbol | chromosome | map_location | type_of_gene | description |\r\n|--------|-----------|--------|------------|--------------|----------------|-------------------------------|\r\n| 9606 | [266553](http://www.ncbi.nlm.nih.gov/gene/?term=266553) | OFCC1 | 6 | 6p24.3 | protein-coding | orofacial cleft 1 candidate 1 |\r\n| 9606 | [105369145](http://www.ncbi.nlm.nih.gov/gene/?term=105369145) | OFCC1 | 6 |  | protein-coding | orofacial cleft 1 candidate 1 |\r\n| 9606 | [2867](http://www.ncbi.nlm.nih.gov/gene/?term=2867) | FFAR2 | 19 | 19q13.1 | protein-coding | free fatty acid receptor 2 |\r\n| 9606 | [105372382](http://www.ncbi.nlm.nih.gov/gene/?term=105372382) | FFAR2 | 19 |  | protein-coding | free fatty acid receptor 2 |\r\n\r\nWe will reach out to Entrez Gene to inquire about this unexpected occurrence.",
      "profile": 17,
      "published": "2015-10-04T00:25:02.158277Z",
      "thread": 34,
      "url": "/discussion/using-entrez-gene-as-our-gene-vocabulary/34#10"
    },
    {
      "body_html": "<h1>Project Altmetrics</h1>\r\n\r\n<p>Our project has an <a href=\"https://www.altmetric.com/details/4273971\">Altmetric page</a>, which tracks online attention. Currently, some of the project metadata is wrong, and most mentioning content is missing. <a href=\"/u/jspauld\" class=\"username\">@jspauld</a>, perhaps you could investigate improving Altmetric integration?</p>",
      "body_md": "# Project Altmetrics\r\n\r\nOur project has an [Altmetric page](https://www.altmetric.com/details/4273971), which tracks online attention. Currently, some of the project metadata is wrong, and most mentioning content is missing. @jspauld, perhaps you could investigate improving Altmetric integration?",
      "profile": 17,
      "published": "2015-10-04T02:49:12.228632Z",
      "thread": 113,
      "url": "/discussion/tracking-project-reuse-citation-and-publicity/113#3"
    },
    {
      "body_html": "<h1>Exporting hetio hetnets to neo4j</h1>\r\n\r\n<p>We've <a href=\"https://github.com/dhimmel/hetio/commit/1860faadb455c1f20546ce7923b5b78fd74796b3\">added</a> neo4j export capability to hetio. Our implementation uses the <a href=\"http://py2neo.org/2.0/\">py2neo</a> toolkit to interact with the neo4j server. </p>\r\n\r\n<p>Adding edges is quite slow and the database size is large. However, the neo4j browser combined with cypher is great for exploratory analyses. In a short amount of time, I discovered 4 issues with our network (<a href=\"https://github.com/dhimmel/integrate/issues/4\">1</a>, <a href=\"https://github.com/dhimmel/integrate/issues/5\">2</a>, <a href=\"https://github.com/dhimmel/integrate/issues/6\">3</a>, <a href=\"https://github.com/dhimmel/integrate/issues/7\">4</a>) and created a <a href=\"https://twitter.com/dhimmel/status/650558967492444160\">sneak-preview visualization</a>.</p>",
      "body_md": "# Exporting hetio hetnets to neo4j\r\n\r\nWe've [added](https://github.com/dhimmel/hetio/commit/1860faadb455c1f20546ce7923b5b78fd74796b3) neo4j export capability to hetio. Our implementation uses the [py2neo](http://py2neo.org/2.0/) toolkit to interact with the neo4j server. \r\n\r\nAdding edges is quite slow and the database size is large. However, the neo4j browser combined with cypher is great for exploratory analyses. In a short amount of time, I discovered 4 issues with our network ([1](https://github.com/dhimmel/integrate/issues/4), [2](https://github.com/dhimmel/integrate/issues/5), [3](https://github.com/dhimmel/integrate/issues/6), [4](https://github.com/dhimmel/integrate/issues/7)) and created a [sneak-preview visualization](https://twitter.com/dhimmel/status/650558967492444160).",
      "profile": 17,
      "published": "2015-10-04T06:50:05.515658Z",
      "thread": 112,
      "url": "/discussion/using-the-neo4j-graph-database-for-hetnets/112#2"
    },
    {
      "body_html": "<h1>Version 1.0</h1>\r\n\r\n<p>Our compilation of pathway gene sets is now <a href=\"https://github.com/dhimmel/pathways/tree/1dc7c744d0d1a8fa17a079f739195e6d3c15117e\">released</a> (version 1.0) <span class=\"citation\">[<a href=\"/doi/10.5281/zenodo.31834\" class=\"citation\" data-key=\"10.5281/zenodo.31834\">1</a>]</span>. Gene sets (<a href=\"https://github.com/dhimmel/pathways/blob/1dc7c744d0d1a8fa17a079f739195e6d3c15117e/data/pathways.tsv\">download</a>) are compiled from WikiPathways and MSigDB. This updated version contains 1,617 pathways.</p>",
      "body_md": "# Version 1.0\r\n\r\nOur compilation of pathway gene sets is now [released](https://github.com/dhimmel/pathways/tree/1dc7c744d0d1a8fa17a079f739195e6d3c15117e) (version 1.0) [@10.5281/zenodo.31834]. Gene sets ([download](https://github.com/dhimmel/pathways/blob/1dc7c744d0d1a8fa17a079f739195e6d3c15117e/data/pathways.tsv)) are compiled from WikiPathways and MSigDB. This updated version contains 1,617 pathways.",
      "profile": 17,
      "published": "2015-10-06T18:45:14.620139Z",
      "thread": 72,
      "url": "/discussion/adding-pathway-resources-to-your-network/72#10"
    },
    {
      "body_html": "<h1>Resolution of Entrez Genes with duplicate symbols</h1>\r\n\r\n<p>Mike Murphy — RefSeq Curator at NCBI\\NLM\\NIH — responded to our inquiry regarding the <a href=\"#10\">duplicate symbols</a>. With permission, we've copied his response below:</p>\r\n\r\n<blockquote><p>Thank you for your notification of two cases where the same symbol is used to represent different human GeneIDs. In each case, one of the symbols is \"official\" (as determined by the Human Gene Nomenclature Committee) and the other is \"unofficial\". We consistently use official nomenclature for the gene feature, when available. Unfortunately, situations do arise where the same symbol is used in an official and unofficial capacity on different loci. It is our general policy to retain shared symbols and names on different loci for query and retrieval purposes by various users of our database. However, in both of the cases you pointed out, the two genes with the same symbol really represent the same gene. Therefore, I merged GeneID 105369145 into GeneID 266553, and I merged GeneID 105372382 into GeneID 2867. These updates should be publicly visible within a couple of days.</p></blockquote>",
      "body_md": "# Resolution of Entrez Genes with duplicate symbols\r\n\r\nMike Murphy -- RefSeq Curator at NCBI\\NLM\\NIH -- responded to our inquiry regarding the [duplicate symbols](#10). With permission, we've copied his response below:\r\n\r\n> Thank you for your notification of two cases where the same symbol is used to represent different human GeneIDs. In each case, one of the symbols is \"official\" (as determined by the Human Gene Nomenclature Committee) and the other is \"unofficial\". We consistently use official nomenclature for the gene feature, when available. Unfortunately, situations do arise where the same symbol is used in an official and unofficial capacity on different loci. It is our general policy to retain shared symbols and names on different loci for query and retrieval purposes by various users of our database. However, in both of the cases you pointed out, the two genes with the same symbol really represent the same gene. Therefore, I merged GeneID 105369145 into GeneID 266553, and I merged GeneID 105372382 into GeneID 2867. These updates should be publicly visible within a couple of days.\r\n",
      "profile": 17,
      "published": "2015-10-05T21:32:26.910271Z",
      "thread": 34,
      "url": "/discussion/using-entrez-gene-as-our-gene-vocabulary/34#11"
    },
    {
      "body_html": "<p>We <a href=\"https://github.com/dhimmel/integrate/blob/8d93f5409a5fba85ee1109b470bbf2bb2b1a67e6/neo4j.ipynb\">exported</a> the <a href=\"https://github.com/dhimmel/integrate/tree/8d93f5409a5fba85ee1109b470bbf2bb2b1a67e6\">current version</a> of our network, which contains 49,399 nodes and 2,997,246 edges, to neo4j. The export took 10 hours and resulted in a 3.04 GB database.</p>\r\n\r\n<p>The <code>data/graph.db/</code>, which stores the database, contained the following files with sizes over 1 MB:</p>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th>file</th><th>size</th></tr></thead><tbody><tr><td><code>messages.log</code></td><td>1.3 GB</td></tr><tr><td><code>data/graph.db/neostore.transaction.db.1</code></td><td>262 MB</td></tr><tr><td><code>data/graph.db/neostore.transaction.db.2</code></td><td>262 MB</td></tr><tr><td><code>data/graph.db/neostore.transaction.db.3</code></td><td>262 MB</td></tr><tr><td><code>data/graph.db/neostore.transaction.db.4</code></td><td>262 MB</td></tr><tr><td><code>data/graph.db/neostore.transaction.db.5</code></td><td>262 MB</td></tr><tr><td><code>data/graph.db/neostore.transaction.db.6</code></td><td>262 MB</td></tr><tr><td><code>data/graph.db/neostore.transaction.db.7</code></td><td>113 MB</td></tr><tr><td><code>neostore.propertystore.db</code></td><td>127.9 MB</td></tr><tr><td><code>neostore.relationshipstore.db</code></td><td>102 MB</td></tr><tr><td><code>neostore.propertystore.db.strings</code></td><td>25 MB</td></tr><tr><td><code>neostore.relationshipgroupstore.db</code></td><td>3.2 MB</td></tr><tr><td><code>rrd</code></td><td>2.0 MB</td></tr><tr><td><code>neostore.propertystore.db.arrays</code></td><td>1.5 MB</td></tr><tr><td><code>neostore.nodestore.db</code></td><td>1.5 MB</td></tr></tbody></table>\r\n\r\n<p>We will look into ways to speed up our write times and reduce storage.</p>",
      "body_md": "We [exported](https://github.com/dhimmel/integrate/blob/8d93f5409a5fba85ee1109b470bbf2bb2b1a67e6/neo4j.ipynb) the [current version](https://github.com/dhimmel/integrate/tree/8d93f5409a5fba85ee1109b470bbf2bb2b1a67e6) of our network, which contains 49,399 nodes and 2,997,246 edges, to neo4j. The export took 10 hours and resulted in a 3.04 GB database.\r\n\r\nThe `data/graph.db/`, which stores the database, contained the following files with sizes over 1 MB:\r\n\r\n| file | size |\r\n|-------------------------------------------|----------|\r\n| `messages.log` | 1.3 GB |\r\n| `data/graph.db/neostore.transaction.db.1` | 262 MB |\r\n| `data/graph.db/neostore.transaction.db.2` | 262 MB |\r\n| `data/graph.db/neostore.transaction.db.3` | 262 MB |\r\n| `data/graph.db/neostore.transaction.db.4` | 262 MB |\r\n| `data/graph.db/neostore.transaction.db.5` | 262 MB |\r\n| `data/graph.db/neostore.transaction.db.6` | 262 MB |\r\n| `data/graph.db/neostore.transaction.db.7` | 113 MB |\r\n| `neostore.propertystore.db` | 127.9 MB |\r\n| `neostore.relationshipstore.db` | 102 MB |\r\n| `neostore.propertystore.db.strings` | 25 MB |\r\n| `neostore.relationshipgroupstore.db` | 3.2 MB |\r\n| `rrd` | 2.0 MB |\r\n| `neostore.propertystore.db.arrays` | 1.5 MB |\r\n| `neostore.nodestore.db` | 1.5 MB |\r\n\r\nWe will look into ways to speed up our write times and reduce storage.",
      "profile": 17,
      "published": "2015-10-06T16:18:22.407699Z",
      "thread": 112,
      "url": "/discussion/using-the-neo4j-graph-database-for-hetnets/112#3"
    },
    {
      "body_html": "<p>Can you add into the <a href=\"https://github.com/dhimmel/integrate/blob/0e343d8745a98757c86e73f645b41353f52b82b5/licenses/README.md\">table</a> the corporate or institutional affiliation of the project and the funding agency to each of the data sources?</p>",
      "body_md": "Can you add into the [table] (https://github.com/dhimmel/integrate/blob/0e343d8745a98757c86e73f645b41353f52b82b5/licenses/README.md) the corporate or institutional affiliation of the project and the funding agency to each of the data sources?",
      "profile": 79,
      "published": "2015-10-08T19:04:14.999349Z",
      "thread": 107,
      "url": "/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#13"
    },
    {
      "body_html": "<h1>General assessment</h1>\r\n\r\n<p>We assessed general performance trends on our <a href=\"#1\">preliminary</a> set of 261 features (<a href=\"http://nbviewer.ipython.org/github/dhimmel/learn/blob/ed022ecd93f0599887a359486f3e8849ecda1a03/feature-assess.ipynb\">interactive notebook</a>). We discuss the findings below:</p>\r\n\r\n<p>All <a href=\"https://github.com/dhimmel/learn/blob/ed022ecd93f0599887a359486f3e8849ecda1a03/data/auc.tsv\">features</a> yielded AUROCs ≥ 0.5. In other words, no features were negatively associated with indication status: greater path prevalence between a compound and disease never resulted in a lower therapeutic likelihood. The lack of negatively associated features is unsurprising given that our network is primarily composed of general relationships. For example, we have a compound–gene edge for <a href=\"https://en.wikipedia.org/wiki/Biological_target#Drug_targets\">targeting</a> but not for <a href=\"https://en.wikipedia.org/wiki/Agonist\">agonism</a> or <a href=\"https://en.wikipedia.org/wiki/Receptor_antagonist\">antagonism</a>.</p>\r\n\r\n<p>The majority of features had AUROC ≤ 0.53. In other words, most features performed only slightly better than random. However, a quarter of the features had AUROC ≥ 0.60, and five features had AUROC ≥ 0.80. The strong performance of a subset of features is encouraging.</p>\r\n\r\n<p>We <a href=\"http://nbviewer.ipython.org/github/dhimmel/learn/blob/ed022ecd93f0599887a359486f3e8849ecda1a03/feature-assess.ipynb#Performance-by-path-length\">did not observe</a> major differences in the distributions of AUROCs for features with length 2 versus length 3 metapaths. However, since there are many more metapaths with length 3 than 2, the top performing features were mostly of length 3.</p>\r\n\r\n<p>Performance <a href=\"http://nbviewer.ipython.org/github/dhimmel/learn/blob/ed022ecd93f0599887a359486f3e8849ecda1a03/feature-assess.ipynb#Feature-AUROC-versus-non-zero-fraction\">strongly correlated</a> with the fraction of nonzero values per feature. Metapaths traversing sparsely connected areas of the hetnet performed poorly because they yielded <span class=\"math\">$$DWPC = 0$$</span> for almost all compound–disease pairs.</p>\r\n\r\n<p>AUROC and AUPRC <a href=\"http://nbviewer.ipython.org/github/dhimmel/learn/blob/ed022ecd93f0599887a359486f3e8849ecda1a03/feature-assess.ipynb#Feature-AUPRC-versus-AUROC\">were</a> positively correlated. However, features with AUROCs near 0.5 (the random expectation) often had AUPRCs considerably above 0.25 (the random expectation). These features were often &gt; 99% zero. Therefore, we suspect the low-AUROC features produced decent top predictions but poor comprehensive predictions due to sparsity, leading to discordance between AUROCs and AUPRCs <span class=\"citation\">[<a href=\"/doi/10.1145/1143844.1143874\" class=\"citation\" data-key=\"10.1145/1143844.1143874\">1</a>, <a href=\"/doi/10.1371/journal.pone.0009202\" class=\"citation\" data-key=\"10.1371/journal.pone.0009202\">2</a>]</span>.</p>\r\n\r\n<p>One reason we primarily rely on AUROC rather than AUPRC is to enable comparisons across different prevalences. We may consider using the AUCROC (area under the condensed ROC  <span class=\"citation\">[<a href=\"/doi/10.1093/bioinformatics/btq140\" class=\"citation\" data-key=\"10.1093/bioinformatics/btq140\">3</a>]</span>) to emphasize top predictions while remaining balance-agnostic.</p>",
      "body_md": "# General assessment\r\n\r\nWe assessed general performance trends on our [preliminary](#1) set of 261 features ([interactive notebook](http://nbviewer.ipython.org/github/dhimmel/learn/blob/ed022ecd93f0599887a359486f3e8849ecda1a03/feature-assess.ipynb)). We discuss the findings below:\r\n\r\nAll [features](https://github.com/dhimmel/learn/blob/ed022ecd93f0599887a359486f3e8849ecda1a03/data/auc.tsv) yielded AUROCs ≥ 0.5. In other words, no features were negatively associated with indication status: greater path prevalence between a compound and disease never resulted in a lower therapeutic likelihood. The lack of negatively associated features is unsurprising given that our network is primarily composed of general relationships. For example, we have a compound--gene edge for [targeting](https://en.wikipedia.org/wiki/Biological_target#Drug_targets) but not for [agonism](https://en.wikipedia.org/wiki/Agonist) or [antagonism](https://en.wikipedia.org/wiki/Receptor_antagonist).\r\n\r\nThe majority of features had AUROC ≤ 0.53. In other words, most features performed only slightly better than random. However, a quarter of the features had AUROC ≥ 0.60, and five features had AUROC ≥ 0.80. The strong performance of a subset of features is encouraging.\r\n\r\nWe [did not observe](http://nbviewer.ipython.org/github/dhimmel/learn/blob/ed022ecd93f0599887a359486f3e8849ecda1a03/feature-assess.ipynb#Performance-by-path-length) major differences in the distributions of AUROCs for features with length 2 versus length 3 metapaths. However, since there are many more metapaths with length 3 than 2, the top performing features were mostly of length 3.\r\n\r\nPerformance [strongly correlated](http://nbviewer.ipython.org/github/dhimmel/learn/blob/ed022ecd93f0599887a359486f3e8849ecda1a03/feature-assess.ipynb#Feature-AUROC-versus-non-zero-fraction) with the fraction of nonzero values per feature. Metapaths traversing sparsely connected areas of the hetnet performed poorly because they yielded $$DWPC = 0$$ for almost all compound--disease pairs.\r\n\r\nAUROC and AUPRC [were](http://nbviewer.ipython.org/github/dhimmel/learn/blob/ed022ecd93f0599887a359486f3e8849ecda1a03/feature-assess.ipynb#Feature-AUPRC-versus-AUROC) positively correlated. However, features with AUROCs near 0.5 (the random expectation) often had AUPRCs considerably above 0.25 (the random expectation). These features were often > 99% zero. Therefore, we suspect the low-AUROC features produced decent top predictions but poor comprehensive predictions due to sparsity, leading to discordance between AUROCs and AUPRCs [@10.1145/1143844.1143874 @10.1371/journal.pone.0009202].\r\n\r\nOne reason we primarily rely on AUROC rather than AUPRC is to enable comparisons across different prevalences. We may consider using the AUCROC (area under the condensed ROC  [@10.1093/bioinformatics/btq140]) to emphasize top predictions while remaining balance-agnostic.",
      "profile": 17,
      "published": "2015-10-10T23:28:06.090764Z",
      "thread": 115,
      "url": "/discussion/assessing-the-informativeness-of-features/115#2"
    },
    {
      "body_html": "<p>On October 14, Aravind Subramanian, a member of the LINCS team at the Broad, replied to our email. He wrote (posted here with permission):</p>\r\n\r\n<blockquote><p>You are free to redistribute your re-processing of the Broad LINCS data. We are working on a manuscript describing L1000 and the dataset.</p></blockquote>\r\n\r\n<p>And continued:</p>\r\n\r\n<blockquote><p>But if you believe your work would be valuable, we don't want our publication needs to hold up access for the field, so kindly proceed as you see fit</p></blockquote>\r\n\r\n<p>Aravind took the position that there is no formal license from the Broad Institute and that the LINCS L1000 licensing is determined by the NIH — the Broad and L1000 team do not apply any additional restrictions. While the <a href=\"https://github.com/dhimmel/integrate/blob/3633a6db23996f58ea1d75ada5537b53bb99597c/licenses/custom/L1000.md\">original</a> license from <a href=\"http://www.lincscloud.org/license/\">www.lincscloud.org/license/</a> suggested otherwise, the following update was <a href=\"https://github.com/dhimmel/integrate/blob/7459896115b477301af83310de667ffeeca61f66/licenses/custom/L1000.md\">added</a>: </p>\r\n\r\n<blockquote><p><strong>Update - October 14, 2015</strong></p><p>All LINCS Production Phase L1000 data generated by the Broad Institute is posted at the NCBIs Gene Expression Omnibus (GEO). Standard NIH data access rules apply - data is freely accessible by anyone (GEO BioProject ID PRJNA290347)</p><p>The website lincscloud.org, a Broad Institute developed resource for analysis of LINCS Phase 1 (2011-2014) data, will be deprecated in 2015 as the NIH has recently funded a separate LINCS Data Coordination and Integration Center (DCIC).</p><p>Our historic license is given below for reference, but the official information on access to all LINCS resources via the DCIC is available at lincsproject.org.</p></blockquote>\r\n\r\n<p>The update specifies that LINCS data will be deposited in <a href=\"http://www.ncbi.nlm.nih.gov/geo/\">GEO</a>. However, GEO availability does not grant usage rights <a href=\"https://github.com/dhimmel/integrate/blob/3b16651051ae12129ddc2250e8d3e6d4050dd349/licenses/custom/GEO.md\">since</a>,</p>\r\n\r\n<blockquote><p>some submitters may claim patent, copyright, or other intellectual property rights in all or a portion of the data they have submitted.</p></blockquote>\r\n\r\n<p>The update further specifies that the DCIC is the authoritative source for LINCS licensing. Their data release policy <a href=\"https://github.com/dhimmel/integrate/blob/3b16651051ae12129ddc2250e8d3e6d4050dd349/licenses/custom/LINCS.md\">states</a>:</p>\r\n\r\n<blockquote><p>LINCS data are released with the sole restriction that they must be correctly cited so that others can establish provenance and access the original data</p></blockquote>\r\n\r\n<h2>Conclusion</h2>\r\n\r\n<p>We have permission to distribute our L1000 datasets. The formal LINCS data policy, which covers the L1000 project, requires attribution. Therefore, we will release our LINCS datasets as <a href=\"https://creativecommons.org/licenses/by/4.0/\">CC-BY</a>.</p>\r\n\r\n<p>The LINCS project and the <a href=\"http://www.lincscloud.org/team/\">L1000 team</a> especially have done a laudable job sharing their data and providing support. Clearly and explicitly specifying the license of all public datasets will help remove any uncertainty and avoid laborious permission requests.</p>",
      "body_md": "On October 14, Aravind Subramanian, a member of the LINCS team at the Broad, replied to our email. He wrote (posted here with permission):\r\n\r\n> You are free to redistribute your re-processing of the Broad LINCS data. We are working on a manuscript describing L1000 and the dataset.\r\n\r\nAnd continued:\r\n\r\n> But if you believe your work would be valuable, we don't want our publication needs to hold up access for the field, so kindly proceed as you see fit\r\n\r\nAravind took the position that there is no formal license from the Broad Institute and that the LINCS L1000 licensing is determined by the NIH --- the Broad and L1000 team do not apply any additional restrictions. While the [original](https://github.com/dhimmel/integrate/blob/3633a6db23996f58ea1d75ada5537b53bb99597c/licenses/custom/L1000.md) license from www.lincscloud.org/license/ suggested otherwise, the following update was [added](https://github.com/dhimmel/integrate/blob/7459896115b477301af83310de667ffeeca61f66/licenses/custom/L1000.md): \r\n\r\n> **Update - October 14, 2015**\r\n\r\n> All LINCS Production Phase L1000 data generated by the Broad Institute is posted at the NCBIs Gene Expression Omnibus (GEO). Standard NIH data access rules apply - data is freely accessible by anyone (GEO BioProject ID PRJNA290347)\r\n\r\n> The website lincscloud.org, a Broad Institute developed resource for analysis of LINCS Phase 1 (2011-2014) data, will be deprecated in 2015 as the NIH has recently funded a separate LINCS Data Coordination and Integration Center (DCIC).\r\n\r\n> Our historic license is given below for reference, but the official information on access to all LINCS resources via the DCIC is available at lincsproject.org.\r\n\r\nThe update specifies that LINCS data will be deposited in [GEO](http://www.ncbi.nlm.nih.gov/geo/). However, GEO availability does not grant usage rights [since](https://github.com/dhimmel/integrate/blob/3b16651051ae12129ddc2250e8d3e6d4050dd349/licenses/custom/GEO.md),\r\n\r\n> some submitters may claim patent, copyright, or other intellectual property rights in all or a portion of the data they have submitted.\r\n\r\nThe update further specifies that the DCIC is the authoritative source for LINCS licensing. Their data release policy [states](https://github.com/dhimmel/integrate/blob/3b16651051ae12129ddc2250e8d3e6d4050dd349/licenses/custom/LINCS.md):\r\n\r\n> LINCS data are released with the sole restriction that they must be correctly cited so that others can establish provenance and access the original data\r\n\r\n## Conclusion\r\n\r\nWe have permission to distribute our L1000 datasets. The formal LINCS data policy, which covers the L1000 project, requires attribution. Therefore, we will release our LINCS datasets as [CC-BY](https://creativecommons.org/licenses/by/4.0/).\r\n\r\nThe LINCS project and the [L1000 team](http://www.lincscloud.org/team/) especially have done a laudable job sharing their data and providing support. Clearly and explicitly specifying the license of all public datasets will help remove any uncertainty and avoid laborious permission requests.",
      "profile": 17,
      "published": "2015-10-19T23:10:30.482600Z",
      "thread": 110,
      "url": "/discussion/lincs-l1000-licensing/110#2"
    },
    {
      "body_html": "<h1><em>DWPC</em> in Cypher</h1>\r\n\r\n<p>We've <a href=\"https://github.com/dhimmel/hetio/blob/3150399c8d2570c2c6c93f1e3866d6b7c6bac2f3/hetio/neo4j.py#L135\">implemented</a> the degree-weighted path count (<em>DWPC</em> <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1004259\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1004259\">1</a>]</span>) in <a href=\"http://neo4j.com/developer/cypher-query-language/\">Cypher</a>. Our implementation produces a different query for each metapath, but specifies the source node (<code>source</code>), target node (<code>target</code>), and damping exponent (<code>w</code>) as <a href=\"http://neo4j.com/docs/2.2.6/cypher-parameters.html\">parameters</a>.</p>\r\n\r\n<p>Below is the query for the <em>CsCuGod&gt;GuD</em> metapath:</p>\r\n\r\n<pre><code class=\"no-highlight hljs\">MATCH p = (n0:Compound)-[:SIMILARITY]-(n1:Compound)-[:UPREGULATION]-(n2:Gene)-[:OVEREXPRESSION_DOWNREGULATION]-&gt;(n3:Gene)-[:UPREGULATION]-(n4:Disease)\r\nWHERE n0.name = { source }\r\nAND n4.name = { target }\r\nWITH [size((n0)-[:SIMILARITY]-(:Compound)),\r\nsize((:Compound)-[:SIMILARITY]-(n1)),\r\nsize((n1)-[:UPREGULATION]-(:Gene)),\r\nsize((:Compound)-[:UPREGULATION]-(n2)),\r\nsize((n2)-[:OVEREXPRESSION_DOWNREGULATION]-&gt;(:Gene)),\r\nsize((:Gene)-[:OVEREXPRESSION_DOWNREGULATION]-&gt;(n3)),\r\nsize((n3)-[:UPREGULATION]-(:Disease)),\r\nsize((:Gene)-[:UPREGULATION]-(n4))] AS degrees\r\nRETURN sum(reduce(pdp = 1.0, d in degrees| pdp * d ^ -{ w }))</code></pre>\r\n\r\n<p>The <em>DWPC</em> for this metapath measures the extent that compounds similar to the query compound upregulate genes whose overpression downregulates genes upregulated by the query disease. The <code>MATCH</code> clause identifies paths corresponding to the metapath. The <code>WITH</code> clause computes degrees along each path and the <code>RETURN</code> clause computes path degree products (<em>PDPs</em>) and sums them to get the <em>DWPC</em>.</p>\r\n\r\n<h2>Comparison to hetio</h2>\r\n\r\n<p>We configure our hetio queries to exclude paths with duplicate nodes. However, neo4j <a href=\"http://neo4j.com/docs/2.2.6/cypherdoc-uniqueness.html\">excludes</a> duplicate relationships. Additionally, when computing features for an indicated compound–disease pair, we configure our hetio queries to ignore that indication. Our current cypher framework does not support this exclusion.</p>\r\n\r\n<p>Our preliminary experience is that <em>DWPC</em> computations in neo4j run approximately twice as quickly as <a href=\"https://github.com/dhimmel/hetio/blob/3150399c8d2570c2c6c93f1e3866d6b7c6bac2f3/hetio/pathtools.py\">in hetio</a>. However, hetio may have more room for improvement, since we haven't implemented path caching yet.</p>",
      "body_md": "# *DWPC* in Cypher\r\n\r\nWe've [implemented](https://github.com/dhimmel/hetio/blob/3150399c8d2570c2c6c93f1e3866d6b7c6bac2f3/hetio/neo4j.py#L135) the degree-weighted path count (*DWPC* [@10.1371/journal.pcbi.1004259]) in [Cypher](http://neo4j.com/developer/cypher-query-language/). Our implementation produces a different query for each metapath, but specifies the source node (`source`), target node (`target`), and damping exponent (`w`) as [parameters](http://neo4j.com/docs/2.2.6/cypher-parameters.html).\r\n\r\nBelow is the query for the *CsCuGod>GuD* metapath:\r\n\r\n```cypher\r\nMATCH p = (n0:Compound)-[:SIMILARITY]-(n1:Compound)-[:UPREGULATION]-(n2:Gene)-[:OVEREXPRESSION_DOWNREGULATION]->(n3:Gene)-[:UPREGULATION]-(n4:Disease)\r\nWHERE n0.name = { source }\r\nAND n4.name = { target }\r\nWITH [size((n0)-[:SIMILARITY]-(:Compound)),\r\nsize((:Compound)-[:SIMILARITY]-(n1)),\r\nsize((n1)-[:UPREGULATION]-(:Gene)),\r\nsize((:Compound)-[:UPREGULATION]-(n2)),\r\nsize((n2)-[:OVEREXPRESSION_DOWNREGULATION]->(:Gene)),\r\nsize((:Gene)-[:OVEREXPRESSION_DOWNREGULATION]->(n3)),\r\nsize((n3)-[:UPREGULATION]-(:Disease)),\r\nsize((:Gene)-[:UPREGULATION]-(n4))] AS degrees\r\nRETURN sum(reduce(pdp = 1.0, d in degrees| pdp * d ^ -{ w }))        \r\n```\r\n\r\nThe *DWPC* for this metapath measures the extent that compounds similar to the query compound upregulate genes whose overpression downregulates genes upregulated by the query disease. The `MATCH` clause identifies paths corresponding to the metapath. The `WITH` clause computes degrees along each path and the `RETURN` clause computes path degree products (*PDPs*) and sums them to get the *DWPC*.\r\n\r\n## Comparison to hetio\r\n\r\nWe configure our hetio queries to exclude paths with duplicate nodes. However, neo4j [excludes](http://neo4j.com/docs/2.2.6/cypherdoc-uniqueness.html) duplicate relationships. Additionally, when computing features for an indicated compound--disease pair, we configure our hetio queries to ignore that indication. Our current cypher framework does not support this exclusion.\r\n\r\nOur preliminary experience is that *DWPC* computations in neo4j run approximately twice as quickly as [in hetio](https://github.com/dhimmel/hetio/blob/3150399c8d2570c2c6c93f1e3866d6b7c6bac2f3/hetio/pathtools.py). However, hetio may have more room for improvement, since we haven't implemented path caching yet.",
      "profile": 17,
      "published": "2015-10-16T22:00:04.098141Z",
      "thread": 112,
      "url": "/discussion/using-the-neo4j-graph-database-for-hetnets/112#4"
    },
    {
      "body_html": "<h1>GraphConnect 2015</h1>\r\n\r\n<p>Today I attended <a href=\"http://graphconnect.com/\">GraphConnect</a> — a conference focused on neo4j. CEO, Emil Eifrem, kicked the event off with several exciting announcements:</p>\r\n\r\n<ul><li>Neo4j 2.3 has been released bringing speed and scalability <a href=\"http://neo4j.com/release-notes/neo4j-2-3-0/\">improvements</a>. Specifically, the caching infrastructure has been rewritten to <a href=\"http://neo4j.com/blog/new-on-neo4j-the-neo4j-2-3-0-milestone-2-release-is-here/\">provide</a> \"significant (up to 2-3x) improvements in concurrent read scaling.\"</li><li>Neo4j 3.0 is in the works and will bring unified and official drivers across languages. The initial release will include a Python but not R driver.</li><li>Cypher <a href=\"http://neo4j.com/blog/open-cypher-sql-for-graphs/\">will be</a> open sourced as <a href=\"http://www.opencypher.org/\">openCypher</a>. This will hopefully give rise to a standard query language for all graph databases.</li></ul>\r\n\r\n<h2>Select learnings</h2>\r\n\r\n<p>Neo4j is designed for <strong>deep traversals</strong>. Other graph databases preferentially support big data (networks with billions of nodes) over efficient traversal. Since our network is small but our edge prediction method requires deep traversal, neo4j is a good fit for our application.</p>\r\n\r\n<p>Neo4j doesn't enforce or specifically support a type graph (also called a schema, <strong>metagraph</strong>, or graph model). However, a metagraph can <a href=\"http://neo4j.com/blog/rvb-2-2-meta-graph/\">easily be created</a> from an already populated graph. While neo4j won't innately reason based on the created metagraph, it can be convenient from a user standpoint.</p>",
      "body_md": "# GraphConnect 2015\r\n\r\nToday I attended [GraphConnect](http://graphconnect.com/) -- a conference focused on neo4j. CEO, Emil Eifrem, kicked the event off with several exciting announcements:\r\n\r\n+ Neo4j 2.3 has been released bringing speed and scalability [improvements](http://neo4j.com/release-notes/neo4j-2-3-0/). Specifically, the caching infrastructure has been rewritten to [provide](http://neo4j.com/blog/new-on-neo4j-the-neo4j-2-3-0-milestone-2-release-is-here/) \"significant (up to 2-3x) improvements in concurrent read scaling.\"\r\n+ Neo4j 3.0 is in the works and will bring unified and official drivers across languages. The initial release will include a Python but not R driver.\r\n+ Cypher [will be](http://neo4j.com/blog/open-cypher-sql-for-graphs/) open sourced as [openCypher](http://www.opencypher.org/). This will hopefully give rise to a standard query language for all graph databases.\r\n\r\n## Select learnings\r\n\r\nNeo4j is designed for **deep traversals**. Other graph databases preferentially support big data (networks with billions of nodes) over efficient traversal. Since our network is small but our edge prediction method requires deep traversal, neo4j is a good fit for our application.\r\n\r\nNeo4j doesn't enforce or specifically support a type graph (also called a schema, **metagraph**, or graph model). However, a metagraph can [easily be created](http://neo4j.com/blog/rvb-2-2-meta-graph/) from an already populated graph. While neo4j won't innately reason based on the created metagraph, it can be convenient from a user standpoint.",
      "profile": 17,
      "published": "2015-10-21T22:04:05.279822Z",
      "thread": 112,
      "url": "/discussion/using-the-neo4j-graph-database-for-hetnets/112#5"
    },
    {
      "body_html": "<h1>LDlink</h1>\r\n\r\n<p>A recently-published <a href=\"http://analysistools.nci.nih.gov/LDlink/\">webapp called LDlink</a> calculates SNPs in LD for a given lead SNP using 1000 Genomes Phase 3 data <span class=\"citation\">[<a href=\"/doi/10.1093/bioinformatics/btv402\" class=\"citation\" data-key=\"10.1093/bioinformatics/btv402\">1</a>]</span>. The LDproxy feature allows specifying a lead SNP and reference population. The resulting table of proxy SNPs is downloadable as a tsv.</p>\r\n\r\n<p>Unfortunately, the service doesn't release a public API. Therefore, querying at scale could be difficult.</p>",
      "body_md": "# LDlink\r\n\r\nA recently-published [webapp called LDlink](http://analysistools.nci.nih.gov/LDlink/) calculates SNPs in LD for a given lead SNP using 1000 Genomes Phase 3 data [@10.1093/bioinformatics/btv402]. The LDproxy feature allows specifying a lead SNP and reference population. The resulting table of proxy SNPs is downloadable as a tsv.\r\n\r\nUnfortunately, the service doesn't release a public API. Therefore, querying at scale could be difficult.",
      "profile": 17,
      "published": "2015-11-02T15:53:59.980194Z",
      "thread": 71,
      "url": "/discussion/calculating-genomic-windows-for-gwas-lead-snps/71#7"
    },
    {
      "body_html": "<p><a href=\"http://bgee.org\">Bgee</a> is an integrative and comparative resource for gene expression <span class=\"citation\">[<a href=\"/doi/10.1007/978-3-540-69828-9_12\" class=\"citation\" data-key=\"10.1007/978-3-540-69828-9_12\">1</a>]</span>. We extract the following edges from Bgee for our network:</p>\r\n\r\n<ul><li>Gene–Anatomy <strong>expression</strong> — whether a gene is present in an anatomy</li><li>Gene–Anatomy <strong>upregulation</strong> — whether a gene is upregulated (overexpressed) in an anatomy</li><li>Gene–Anatomy <strong>downregulation</strong> — whether a gene is downregulated (underexpressed) in an anatomy</li></ul>\r\n\r\n<p>We have already substantially discussed Bgee on <em>Thinklab</em> in a more general gene expression thread <span class=\"citation\">[<a href=\"/discussion/tissue-specific-gene-expression-resources/81\" class=\"citation\" data-key=\"10.15363/thinklab.d81\">2</a>]</span>. The comments consisted of:</p>\r\n\r\n<ul><li>an <a href=\"http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#3\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d81\">introduction</a> by <a href=\"/u/dhimmel\" class=\"username\">@dhimmel</a> </li><li><a href=\"http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#4\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d81\">questions</a> on processing parameters by <a href=\"/u/dhimmel\" class=\"username\">@dhimmel</a></li><li><a href=\"http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#5\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d81\">answers</a> by <a href=\"/u/fbastian\" class=\"username\">@fbastian</a> </li><li>preliminary <a href=\"http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#6\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d81\">analysis</a> by <a href=\"/u/dhimmel\" class=\"username\">@dhimmel</a></li><li>additional <a href=\"http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#7\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d81\">guidance</a> by <a href=\"/u/fbastian\" class=\"username\">@fbastian</a> </li><li>an initial <a href=\"http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#8\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d81\">complete analysis</a> and feedback by <a href=\"/u/dhimmel\" class=\"username\">@dhimmel</a> </li><li><a href=\"http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#9\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d81\">response</a> to feedback by <a href=\"/u/fbastian\" class=\"username\">@fbastian</a> </li><li><a href=\"http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#10\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d81\">tying up</a> loose ends by <a href=\"/u/dhimmel\" class=\"username\">@dhimmel</a></li></ul>\r\n\r\n<p>We are revisiting our Bgee analysis. Since the original discussion has become cluttered and covers a general topic, we are starting a designated Bgee discussion. Stay tuned.</p>",
      "body_md": "[Bgee](http://bgee.org) is an integrative and comparative resource for gene expression [@10.1007/978-3-540-69828-9_12]. We extract the following edges from Bgee for our network:\r\n\r\n+ Gene--Anatomy **expression** -- whether a gene is present in an anatomy\r\n+ Gene--Anatomy **upregulation** -- whether a gene is upregulated (overexpressed) in an anatomy\r\n+ Gene--Anatomy **downregulation** -- whether a gene is downregulated (underexpressed) in an anatomy\r\n\r\nWe have already substantially discussed Bgee on *Thinklab* in a more general gene expression thread [@10.15363/thinklab.d81]. The comments consisted of:\r\n\r\n+ an [introduction](http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#3) by @dhimmel \r\n+ [questions](http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#4) on processing parameters by @dhimmel\r\n+ [answers](http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#5) by @fbastian \r\n+ preliminary [analysis](http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#6) by @dhimmel\r\n+ additional [guidance](http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#7) by @fbastian \r\n+ an initial [complete analysis](http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#8) and feedback by @dhimmel \r\n+ [response](http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#9) to feedback by @fbastian \r\n+ [tying up](http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#10) loose ends by @dhimmel\r\n\r\nWe are revisiting our Bgee analysis. Since the original discussion has become cluttered and covers a general topic, we are starting a designated Bgee discussion. Stay tuned.",
      "profile": 17,
      "published": "2015-11-04T02:25:25.483825Z",
      "thread": 124,
      "url": "/discussion/processing-bgee-for-tissue-specific-gene-presence-and-overunder-expression/124"
    },
    {
      "body_html": "<h1>Bgee discussion migrating</h1>\r\n\r\n<p>I started a <a href=\"http://thinklab.com/discussion/processing-bgee-for-tissue-specific-gene-presence-and-overunder-expression/124\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d124\">designated discussion</a> for Bgee. Please direct further Bgee attention there.</p>",
      "body_md": "# Bgee discussion migrating\r\n\r\nI started a [designated discussion](http://thinklab.com/discussion/processing-bgee-for-tissue-specific-gene-presence-and-overunder-expression/124) for Bgee. Please direct further Bgee attention there.",
      "profile": 17,
      "published": "2015-11-04T02:27:37.960982Z",
      "thread": 81,
      "url": "/discussion/tissue-specific-gene-expression-resources/81#11"
    },
    {
      "body_html": "<h1>Quality control filters</h1>\r\n\r\n<p>Initially, we <a href=\"http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#5\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d81\">chose permissive filters</a> for including Bgee edges based largely on the <a href=\"http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#5\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d81\">recommendations</a> of <a href=\"/u/fbastian\" class=\"username\">@fbastian</a>. Several developments are making us reevaluate our permissiveness:</p>\r\n\r\n<ol><li><em>Gene–expression–Anatomy</em> edges are the most prevalent type in the network (see last figure in <a href=\"https://github.com/dhimmel/integrate/blob/1a43ef2718283cbfc9ae869aee1856bf20e2ad6e/integrate.ipynb\">this notebook</a>). Now that we're traversing the network to extract paths, we're noticing high-degree nodes are computationally problematic. Several anatomies have 20,000 expressed genes and 5000 differentially expressed genes (see <a href=\"https://github.com/dhimmel/integrate/blob/1a43ef2718283cbfc9ae869aee1856bf20e2ad6e/viz/degrees.pdf\">page 2 here</a>). Therefore downstream constraints favor rigorous thresholds for extremely high-degree edge types.</li><li>We <a href=\"http://doi.org/10.15363/thinklab.d91\">also extract</a> <em>Gene–expression–Anatomy</em> edges from <a href=\"http://tissues.jensenlab.org/About\">TISSUES</a> <span class=\"citation\">[<a href=\"/doi/10.7717/peerj.1054\" class=\"citation\" data-key=\"10.7717/peerj.1054\">1</a>]</span>. <a href=\"https://github.com/dhimmel/tissues/blob/d6b0c99352db27469f2c3399cecb6f9fae2db547/bgee-combine.ipynb\">Currently</a>, TISSUES contributes 321,516 expression edges, while Bgee contributes 5,406,177. I suspect our TISSUES inclusion threshold (score ≥ 3) is much more stringent than our <a href=\"http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#6\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d81\">Bgee theshold</a>.</li><li>TISSUES provides a score for each tissue–gene relationship. These scores have been calibrated on a gold standard allowing evidence-based weighting of each study. In contrast, Bgee has different categories of evidence based on ambiguity and quality. These measures are codependent and have been difficult to understand. Therefore, it's difficult to conclude whether low quality or high ambiguity relationships have merit.</li></ol>\r\n\r\n<h2>Ambiguity and call quality</h2>\r\n\r\n<p>We <a href=\"https://github.com/dhimmel/bgee/blob/add20b29b8f926004ce69b9bacff2edf69cd383c/bgee.ipynb\">looked into</a> the relationship between ambiguity and call quality.</p>\r\n\r\n<p>Below, we show the contingency table for <code>Expression</code> (columns) and <code>Call quality</code> (rows) in the <code>Homo_sapiens_expr-complete</code> dataset (<a href=\"http://bgee.org/?page=doc&amp;action=call_files#single_expr_complete_col7\">documentation</a>). Each cell contains the percentage of observations in the corresponding category:</p>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th></th><th>absent</th><th>high ambiguity</th><th>low ambiguity</th><th>present</th></tr></thead><tbody><tr><td>NA</td><td>0</td><td>0.001</td><td>0.001</td><td>0</td></tr><tr><td>poor quality</td><td>0</td><td>0</td><td>0</td><td>0.23</td></tr><tr><td>high quality</td><td>0.11</td><td>0</td><td>0</td><td>0.67</td></tr></tbody></table>\r\n\r\n<p>Below, we show the contingency table for <code>Differential expression</code> (columns) and <code>Call quality</code> (rows) in the <code>Homo_sapiens_diffexpr-anatomy-simple</code> dataset (<a href=\"http://bgee.org/?page=doc&amp;action=call_files#single_diff_simple_col7\">documentation</a>). Each cell contains the percentage of observations in the corresponding category:</p>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th></th><th>high ambiguity</th><th>low ambiguity</th><th>over-expression</th><th>under-expression</th></tr></thead><tbody><tr><td>NA</td><td>0.025</td><td>0.16</td><td>0</td><td>0</td></tr><tr><td>low quality</td><td>0</td><td>0</td><td>0.18</td><td>0.24</td></tr><tr><td>high quality</td><td>0</td><td>0</td><td>0.21</td><td>0.20</td></tr></tbody></table>\r\n\r\n<h2>Conclusions</h2>\r\n\r\n<p><strong>Presence:</strong> <code>Homo_sapiens_expr-complete</code> uses the value <code>poor quality</code> while <code>Homo_sapiens_diffexpr-anatomy-simple</code> uses <code>low quality</code>. In our previous processing, we used <code>low quality</code> as the value for both datasets. Therefore, we accidentally omitted the 23% of observations that were <code>present</code> with <code>poor quality</code>. Our proposed solution is to take only <code>high quality</code> and <code>present</code> observations. The observations with ambiguous call qualities are rare, and thus I am not worried about excluding them for simplicity.</p>\r\n\r\n<p><strong>Differential expression:</strong> In <code>Homo_sapiens_diffexpr-anatomy-simple</code> differentially expressed observations are split between low and high quality. Low quality is <a href=\"http://bgee.org/?page=doc&amp;action=call_files#single_diff_simple_col8\">explained as</a>:</p>\r\n\r\n<blockquote><p>differential expression reported as low quality, or there exists a conflict for the same gene, anatomical entity and developmental stage, from different analyses of a same data type (conflicts between different data types are treated differently). For instance, an analysis showed a gene to be over-expressed in a condition, while another analysis showed the same gene to be under-expressed or not differentially expressed in the same condition. Such conflicts are resolved by a voting system based on the number of conditions compared, weighted by p-value. Note that in one case, this quality level is used to reconcile conflicting calls from different data types: when a data type produced an under-expression call, while a different data type has shown that the same gene was never seen as expressed in the same condition. In that case, the overall summary is under-expression low quality.</p></blockquote>\r\n\r\n<p>We are undecided whether to omit low quality differential expression edges.</p>",
      "body_md": "# Quality control filters\r\n\r\nInitially, we [chose permissive filters](http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#5) for including Bgee edges based largely on the [recommendations](http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#5) of @fbastian. Several developments are making us reevaluate our permissiveness:\r\n\r\n1. *Gene--expression--Anatomy* edges are the most prevalent type in the network (see last figure in [this notebook](https://github.com/dhimmel/integrate/blob/1a43ef2718283cbfc9ae869aee1856bf20e2ad6e/integrate.ipynb)). Now that we're traversing the network to extract paths, we're noticing high-degree nodes are computationally problematic. Several anatomies have 20,000 expressed genes and 5000 differentially expressed genes (see [page 2 here](https://github.com/dhimmel/integrate/blob/1a43ef2718283cbfc9ae869aee1856bf20e2ad6e/viz/degrees.pdf)). Therefore downstream constraints favor rigorous thresholds for extremely high-degree edge types.\r\n2. We [also extract](http://doi.org/10.15363/thinklab.d91) *Gene--expression--Anatomy* edges from [TISSUES](http://tissues.jensenlab.org/About) [@10.7717/peerj.1054]. [Currently](https://github.com/dhimmel/tissues/blob/d6b0c99352db27469f2c3399cecb6f9fae2db547/bgee-combine.ipynb), TISSUES contributes 321,516 expression edges, while Bgee contributes 5,406,177. I suspect our TISSUES inclusion threshold (score ≥ 3) is much more stringent than our [Bgee theshold](http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#6).\r\n3. TISSUES provides a score for each tissue--gene relationship. These scores have been calibrated on a gold standard allowing evidence-based weighting of each study. In contrast, Bgee has different categories of evidence based on ambiguity and quality. These measures are codependent and have been difficult to understand. Therefore, it's difficult to conclude whether low quality or high ambiguity relationships have merit.\r\n\r\n## Ambiguity and call quality\r\n\r\nWe [looked into](https://github.com/dhimmel/bgee/blob/add20b29b8f926004ce69b9bacff2edf69cd383c/bgee.ipynb) the relationship between ambiguity and call quality.\r\n\r\nBelow, we show the contingency table for `Expression` (columns) and `Call quality` (rows) in the `Homo_sapiens_expr-complete` dataset ([documentation](http://bgee.org/?page=doc&action=call_files#single_expr_complete_col7)). Each cell contains the percentage of observations in the corresponding category:\r\n\r\n|  | absent | high ambiguity | low ambiguity | present |\r\n|--------|--------|-----------|---------|----------|\r\n| NA | 0 | 0.001 | 0.001 | 0 |\r\n| poor quality | 0 | 0 | 0 | 0.23 |\r\n| high quality | 0.11 | 0 | 0 | 0.67 |\r\n\r\nBelow, we show the contingency table for `Differential expression` (columns) and `Call quality` (rows) in the `Homo_sapiens_diffexpr-anatomy-simple` dataset ([documentation](http://bgee.org/?page=doc&action=call_files#single_diff_simple_col7)). Each cell contains the percentage of observations in the corresponding category:\r\n\r\n|  | high ambiguity | low ambiguity | over-expression | under-expression |\r\n|-------------------------|----------------|---------------|-----------------|------------------|\r\n| NA | 0.025 | 0.16 | 0 | 0 |\r\n| low quality | 0 | 0 | 0.18 | 0.24 |\r\n| high quality | 0 | 0 | 0.21 | 0.20 |\r\n\r\n## Conclusions\r\n\r\n**Presence:** `Homo_sapiens_expr-complete` uses the value `poor quality` while `Homo_sapiens_diffexpr-anatomy-simple` uses `low quality`. In our previous processing, we used `low quality` as the value for both datasets. Therefore, we accidentally omitted the 23% of observations that were `present` with `poor quality`. Our proposed solution is to take only `high quality` and `present` observations. The observations with ambiguous call qualities are rare, and thus I am not worried about excluding them for simplicity.\r\n\r\n**Differential expression:** In `Homo_sapiens_diffexpr-anatomy-simple` differentially expressed observations are split between low and high quality. Low quality is [explained as](http://bgee.org/?page=doc&action=call_files#single_diff_simple_col8):\r\n\r\n> differential expression reported as low quality, or there exists a conflict for the same gene, anatomical entity and developmental stage, from different analyses of a same data type (conflicts between different data types are treated differently). For instance, an analysis showed a gene to be over-expressed in a condition, while another analysis showed the same gene to be under-expressed or not differentially expressed in the same condition. Such conflicts are resolved by a voting system based on the number of conditions compared, weighted by p-value. Note that in one case, this quality level is used to reconcile conflicting calls from different data types: when a data type produced an under-expression call, while a different data type has shown that the same gene was never seen as expressed in the same condition. In that case, the overall summary is under-expression low quality.\r\n\r\nWe are undecided whether to omit low quality differential expression edges.",
      "profile": 17,
      "published": "2015-11-04T03:54:30.810471Z",
      "thread": 124,
      "url": "/discussion/processing-bgee-for-tissue-specific-gene-presence-and-overunder-expression/124#2"
    },
    {
      "body_html": "<p>For expression calls, besides being more stringent, it is also possible to discard anatomical entities close to the root of the ontology, that are less informative, and that benefits from the propagation from lots of substructures. See also <a href=\"https://github.com/owlcollab/owltools/issues/145\">https://github.com/owlcollab/owltools/issues/145</a> for a related discussion.</p>\r\n\r\n<p>For differential expression, well, as said before, I'm not really surprised with this number of 5,000 differentially expressed genes in some structures. We are still willing to update our FDR computation to take into account all analyses, as mentionned <a href=\"http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#9\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d81\">here</a> (we are currently updating our differential expression pipeline).</p>",
      "body_md": "For expression calls, besides being more stringent, it is also possible to discard anatomical entities close to the root of the ontology, that are less informative, and that benefits from the propagation from lots of substructures. See also https://github.com/owlcollab/owltools/issues/145 for a related discussion.\r\n\r\nFor differential expression, well, as said before, I'm not really surprised with this number of 5,000 differentially expressed genes in some structures. We are still willing to update our FDR computation to take into account all analyses, as mentionned [here](http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#9) (we are currently updating our differential expression pipeline).",
      "profile": 111,
      "published": "2015-11-04T10:43:40.342850Z",
      "thread": 124,
      "url": "/discussion/processing-bgee-for-tissue-specific-gene-presence-and-overunder-expression/124#3"
    },
    {
      "body_html": "<p>Emilie David — Assistant Director, Copyright, Licensing and Special Projects at AAAS — responded to our request. She indicated that AAAS does not generally allow <em>Science</em> content to be republished under Creative Commons licenses. However for Supporting Online Materials, authors are able to authorize use.</p>\r\n\r\n<p>I looked deeper into the <a href=\"http://www.sciencemag.org/site/feature/contribinfo/prep/4__2014LicensetoPublish_Final_6MAR2014.pdf\">License to Publish</a> and found the relevant section:</p>\r\n\r\n<blockquote><p>Author also retains the non-exclusive right to use the Work in the following ways without further permission but only after publication of the Work by AAAS and subject to the requirement that credit be given to its first publication in the appropriate issue of the applicable Science journal:</p><p>9) Author may use or authorize use of Supporting Online Material associated with the Work for any purpose and in any format.</p></blockquote>\r\n\r\n<p>Thus, we will proceed by requesting permission from the authors.</p>",
      "body_md": "Emilie David -- Assistant Director, Copyright, Licensing and Special Projects at AAAS -- responded to our request. She indicated that AAAS does not generally allow *Science* content to be republished under Creative Commons licenses. However for Supporting Online Materials, authors are able to authorize use.\r\n\r\nI looked deeper into the [License to Publish](http://www.sciencemag.org/site/feature/contribinfo/prep/4__2014LicensetoPublish_Final_6MAR2014.pdf) and found the relevant section:\r\n\r\n> Author also retains the non-exclusive right to use the Work in the following ways without further permission but only after publication of the Work by AAAS and subject to the requirement that credit be given to its first publication in the appropriate issue of the applicable Science journal:\r\n\r\n> 9) Author may use or authorize use of Supporting Online Material associated with the Work for any purpose and in any format.\r\n\r\nThus, we will proceed by requesting permission from the authors.",
      "profile": 17,
      "published": "2015-11-11T22:43:29.686872Z",
      "thread": 111,
      "url": "/discussion/incomplete-interactome-licensing/111#2"
    },
    {
      "body_html": "<h1>Update to October 2015 release</h1>\r\n\r\n<p>We updated our analysis <span class=\"citation\">[<a href=\"/doi/10.5281/zenodo.33987\" class=\"citation\" data-key=\"10.5281/zenodo.33987\">1</a>]</span> to the latest BindingDB release (<code>BindingDB_All_2015m10.tsv</code>) and made several implementation <a href=\"https://github.com/dhimmel/bindingdb/commit/36097bc715420827ffd06dd64e05edf95e75f038\">enhancements</a>. Now, our collapsed datasets retain source and pubmed information to help with licensing and attribution.</p>\r\n\r\n<p>For more information, see the <a href=\"https://github.com/dhimmel/bindingdb/blob/28dc70275103a233a2f02024082adcea45102a96/process.ipynb\">notebook</a> for processing the BindingDB export, the rmarkdown <a href=\"https://htmlpreview.github.io/?https://github.com/dhimmel/bindingdb/blob/28dc70275103a233a2f02024082adcea45102a96/collapse.html\">output</a> for collapsing to compound–gene relationships, and the data download <a href=\"https://github.com/dhimmel/bindingdb/tree/28dc70275103a233a2f02024082adcea45102a96/data\">directory</a>.</p>\r\n\r\n<h2>Issue feedback</h2>\r\n\r\n<p>A few issues arose which were not present for <code>BindingDB_All_2015m3.tsv</code>. Paging <a href=\"/u/mkgilson\" class=\"username\">@mkgilson</a>:</p>\r\n\r\n<ul><li>Rows 192304–192473 (one indexed) start off with SMILES rather than reactant set IDs.</li><li>Numeric binding affinities could not be extracted for 19 rows.</li></ul>\r\n\r\n<p>I recommend switching from the ragged tsv to a format that can handle nested structure, such as json or xml.</p>",
      "body_md": "# Update to October 2015 release\r\n\r\nWe updated our analysis [@10.5281/zenodo.33987] to the latest BindingDB release (`BindingDB_All_2015m10.tsv`) and made several implementation [enhancements](https://github.com/dhimmel/bindingdb/commit/36097bc715420827ffd06dd64e05edf95e75f038). Now, our collapsed datasets retain source and pubmed information to help with licensing and attribution.\r\n\r\nFor more information, see the [notebook](https://github.com/dhimmel/bindingdb/blob/28dc70275103a233a2f02024082adcea45102a96/process.ipynb) for processing the BindingDB export, the rmarkdown [output](https://htmlpreview.github.io/?https://github.com/dhimmel/bindingdb/blob/28dc70275103a233a2f02024082adcea45102a96/collapse.html) for collapsing to compound--gene relationships, and the data download [directory](https://github.com/dhimmel/bindingdb/tree/28dc70275103a233a2f02024082adcea45102a96/data).\r\n\r\n## Issue feedback\r\n\r\nA few issues arose which were not present for `BindingDB_All_2015m3.tsv`. Paging @mkgilson:\r\n\r\n+ Rows 192304--192473 (one indexed) start off with SMILES rather than reactant set IDs.\r\n+ Numeric binding affinities could not be extracted for 19 rows.\r\n\r\nI recommend switching from the ragged tsv to a format that can handle nested structure, such as json or xml.",
      "profile": 17,
      "published": "2015-11-19T02:02:39.722976Z",
      "thread": 53,
      "url": "/discussion/integrating-drug-target-information-from-bindingdb/53#6"
    },
    {
      "body_html": "<h1>Author permission</h1>\r\n\r\n<p>Yesterday, I emailed the authors, and first author, Jörg Menche, promptly responded.</p>\r\n\r\n<p>He indicated that they published the supporting data with the hope that others would find it useful. As far as they're concerned, we are free to use it anyway we like.</p>\r\n\r\n<p>He also mentioned that they compiled their interactions from a variety of resources (<a href=\"http://thinklab.com/discussion/creating-a-catalog-of-protein-interactions/85#2\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d85\">as discussed here</a>). Jörg was unsure whether this placed any restrictions on the downstream use of their dataset.</p>",
      "body_md": "# Author permission\r\n\r\nYesterday, I emailed the authors, and first author, Jörg Menche, promptly responded.\r\n\r\nHe indicated that they published the supporting data with the hope that others would find it useful. As far as they're concerned, we are free to use it anyway we like.\r\n\r\nHe also mentioned that they compiled their interactions from a variety of resources ([as discussed here](http://thinklab.com/discussion/creating-a-catalog-of-protein-interactions/85#2)). Jörg was unsure whether this placed any restrictions on the downstream use of their dataset.",
      "profile": 17,
      "published": "2015-11-23T18:11:33.331297Z",
      "thread": 111,
      "url": "/discussion/incomplete-interactome-licensing/111#3"
    },
    {
      "body_html": "<h1>Query Optimization</h1>\r\n\r\n<p>Above, we <a href=\"#4\">debuted</a> <em>DWPC</em> (degree-weighted path count) computation using Cypher. I noticed that looking up the degrees along each path was a major timesink. This finding was surprising because node degree lookup should be trivial compared to path traversal.</p>\r\n\r\n<p>In a stroke of genius, <a href=\"/u/alizee\" class=\"username\">@alizee</a> <a href=\"https://twitter.com/dhimmel/status/662415810825056256\">hypothesized</a> our inclusion of node labels was to blame. For our diagnostic query, removing node labels <a href=\"https://twitter.com/dhimmel/status/662440818398007296\">reduced</a> database hits by 1339 fold and runtime by 8 fold.</p>\r\n\r\n<p>Michael Hunger, caretaker general of the neo4j community, <a href=\"https://twitter.com/mesirii/status/662463621335818240\">explained</a>:</p>\r\n\r\n<blockquote><p><code>size(pattern)</code> uses <code>node.getDegree()</code> if pattern only contains relationship type &amp; direction</p></blockquote>\r\n\r\n<p>More explanation is <a href=\"http://neo4j.com/blog/neo4j-2-2-query-tuning/\">available here</a>, but the essential insight is that by using only direction and relationship type to lookup node degree, we no longer need to lookup the label on the other end of each edge.</p>\r\n\r\n<h2>Database changes</h2>\r\n\r\n<p>To support this optimization, we need to ensure that no two metaedges that touch a common metanode have the same relationship type. Our current hetnet is noncompliant in this regard: for example, the three Gene Ontology metaedges all have kind 'participation':</p>\r\n\r\n<ol><li>Gene–participation–Biological Process</li><li>Gene–participation–Molecular Function</li><li>Gene–participation–Cellular Component</li></ol>\r\n\r\n<p>Therefore, we have implemented unique neo4j relationship types for each metaedge (<a href=\"https://github.com/dhimmel/hetio/commit/8107d783e1b86a33123b9fb3273edf51695b5e82\">primary commit</a> and bugfixes <a href=\"https://github.com/dhimmel/hetio/commit/d4b5f4ef223a26eb5bce23245a10b403cccf9fe5\">1</a> and <a href=\"https://github.com/dhimmel/hetio/commit/98121d64feeba829214652960322f00bffcc6b75\">2</a>) by appending the standardized metaedge abbreviation to its kind. With this change, the relationship types for the Gene Ontology metaedges become:</p>\r\n\r\n<ol><li><code>PARTICIPATION_BPpG</code></li><li><code>PARTICIPATION_GpMF</code></li><li><code>PARTICIPATION_CCpG</code></li></ol>\r\n\r\n<h2>Example query</h2>\r\n\r\n<p>With the updated database, the query for calculating the <em>DWPC</em> between goserelin and lung cancer for the <code>CcSEcCdGuD</code> metapath is:</p>\r\n\r\n<pre><code class=\"no-highlight hljs\">MATCH paths = (n0:Compound)-[:CAUSATION_CcSE]-(n1)-[:CAUSATION_CcSE]-(n2)-[:DOWNREGULATION_CdG]-(n3)-[:UPREGULATION_DuG]-(n4:Disease)\r\n  WHERE n0.identifier = 'DB00014' // Goserelin\r\n  AND n4.identifier = 'DOID:1324' // lung cancer\r\nWITH [\r\n  size((n0)-[:CAUSATION_CcSE]-()),\r\n  size(()-[:CAUSATION_CcSE]-(n1)),\r\n  size((n1)-[:CAUSATION_CcSE]-()),\r\n  size(()-[:CAUSATION_CcSE]-(n2)),\r\n  size((n2)-[:DOWNREGULATION_CdG]-()),\r\n  size(()-[:DOWNREGULATION_CdG]-(n3)),\r\n  size((n3)-[:UPREGULATION_DuG]-()),\r\n  size(()-[:UPREGULATION_DuG]-(n4))\r\n  ] AS degrees, paths\r\nRETURN\r\n  COUNT(paths) AS PC,\r\n  sum(reduce(pdp = 1.0, d in degrees| pdp * d ^ -0.4)) AS DWPC</code></pre>",
      "body_md": "# Query Optimization\r\n\r\nAbove, we [debuted](#4) *DWPC* (degree-weighted path count) computation using Cypher. I noticed that looking up the degrees along each path was a major timesink. This finding was surprising because node degree lookup should be trivial compared to path traversal.\r\n\r\nIn a stroke of genius, @alizee [hypothesized](https://twitter.com/dhimmel/status/662415810825056256) our inclusion of node labels was to blame. For our diagnostic query, removing node labels [reduced](https://twitter.com/dhimmel/status/662440818398007296) database hits by 1339 fold and runtime by 8 fold.\r\n\r\nMichael Hunger, caretaker general of the neo4j community, [explained](https://twitter.com/mesirii/status/662463621335818240):\r\n\r\n> `size(pattern)` uses `node.getDegree()` if pattern only contains relationship type & direction\r\n\r\nMore explanation is [available here](http://neo4j.com/blog/neo4j-2-2-query-tuning/), but the essential insight is that by using only direction and relationship type to lookup node degree, we no longer need to lookup the label on the other end of each edge.\r\n\r\n## Database changes\r\n\r\nTo support this optimization, we need to ensure that no two metaedges that touch a common metanode have the same relationship type. Our current hetnet is noncompliant in this regard: for example, the three Gene Ontology metaedges all have kind 'participation':\r\n\r\n1. Gene--participation--Biological Process\r\n2. Gene--participation--Molecular Function\r\n3. Gene--participation--Cellular Component\r\n\r\nTherefore, we have implemented unique neo4j relationship types for each metaedge ([primary commit](https://github.com/dhimmel/hetio/commit/8107d783e1b86a33123b9fb3273edf51695b5e82) and bugfixes [1](https://github.com/dhimmel/hetio/commit/d4b5f4ef223a26eb5bce23245a10b403cccf9fe5) and [2](https://github.com/dhimmel/hetio/commit/98121d64feeba829214652960322f00bffcc6b75)) by appending the standardized metaedge abbreviation to its kind. With this change, the relationship types for the Gene Ontology metaedges become:\r\n\r\n1. `PARTICIPATION_BPpG`\r\n2. `PARTICIPATION_GpMF`\r\n3. `PARTICIPATION_CCpG`\r\n\r\n## Example query\r\n\r\nWith the updated database, the query for calculating the *DWPC* between goserelin and lung cancer for the `CcSEcCdGuD` metapath is:\r\n\r\n```cypher\r\nMATCH paths = (n0:Compound)-[:CAUSATION_CcSE]-(n1)-[:CAUSATION_CcSE]-(n2)-[:DOWNREGULATION_CdG]-(n3)-[:UPREGULATION_DuG]-(n4:Disease)\r\n  WHERE n0.identifier = 'DB00014' // Goserelin\r\n  AND n4.identifier = 'DOID:1324' // lung cancer\r\nWITH [\r\n  size((n0)-[:CAUSATION_CcSE]-()),\r\n  size(()-[:CAUSATION_CcSE]-(n1)),\r\n  size((n1)-[:CAUSATION_CcSE]-()),\r\n  size(()-[:CAUSATION_CcSE]-(n2)),\r\n  size((n2)-[:DOWNREGULATION_CdG]-()),\r\n  size(()-[:DOWNREGULATION_CdG]-(n3)),\r\n  size((n3)-[:UPREGULATION_DuG]-()),\r\n  size(()-[:UPREGULATION_DuG]-(n4))\r\n  ] AS degrees, paths\r\nRETURN\r\n  COUNT(paths) AS PC,\r\n  sum(reduce(pdp = 1.0, d in degrees| pdp * d ^ -0.4)) AS DWPC\r\n```",
      "profile": 17,
      "published": "2015-11-26T17:20:29.192990Z",
      "thread": 112,
      "url": "/discussion/using-the-neo4j-graph-database-for-hetnets/112#6"
    },
    {
      "body_html": "<h1>Concurrent queries using py2neo</h1>\r\n\r\n<p>As <a href=\"https://twitter.com/darthvader42/status/670154181064400897\">explained</a> by Stefan Armbruster, a single cypher query is limited to a single core. However, multiple queries can be fulfilled in parallel:</p>\r\n\r\n<blockquote><p>With current versions of Neo4j, a Cypher query traverses the graph in single threaded mode. Since most graph applications out there are concurrently used by multiple users, this model saturates the available cores. [<a href=\"http://stackoverflow.com/a/27578860/4651668\">source</a>]</p></blockquote>\r\n\r\n<p>Currently, we perform a separate query for each compound–disease–metapath combination. Depending on the number of compound–disease pairs and metapaths considered, we will need to compute between 1 million and 1 billion <em>DWPCs</em>. </p>\r\n\r\n<p>Our software package for hetnets, <a href=\"https://github.com/dhimmel/hetio\">hetio</a> <span class=\"citation\">[<a href=\"/doi/10.5281/zenodo.31763\" class=\"citation\" data-key=\"10.5281/zenodo.31763\">1</a>]</span>, is built in python. Despite migrating to neo4j, we are still dependent on hetio for:</p>\r\n\r\n<ul><li>metagraph operations</li><li>edge directionality</li><li>metapath abbreviation</li><li>constructing cypher queries</li></ul>\r\n\r\n<p>Therefore, we're using python to construct and execute queries with the <a href=\"http://py2neo.org/2.0/\">py2neo</a> package.</p>\r\n\r\n<p>To enable concurrent queries, we initially used the <a href=\"https://docs.python.org/3.5/library/multiprocessing.html\">multiprocessing</a> module (<a href=\"https://github.com/dhimmel/learn/blob/affb391ac35cd726e1377f08557b060f4098144f/neo4j-compute.ipynb\">notebook</a>), which enables parallelism by creating subprocesses. However, subprocesses require substantial overhead. Since the majority of computation is performed outside of python by the neo4j sever, we switched to the <a href=\"https://docs.python.org/3.5/library/threading.html\">threading</a> module (<a href=\"https://github.com/dhimmel/learn/blob/bfc0b4b3adef8ee9bab73676181b84298f6b16fe/neo4j-compute.ipynb\">notebook</a>). Threading has less overhead than multiprocessing, but is limited to a single process of pure python. However, since the cypher query releases the <a href=\"https://docs.python.org/3.5/glossary.html#term-global-interpreter-lock\">global interpreter lock</a>, the restriction to a single process is not a time-limiting step.</p>\r\n\r\n<p>In the end, we used <a href=\"https://docs.python.org/3.5/library/concurrent.futures.html#threadpoolexecutor\">concurrent.futures</a> to make threading easier (<a href=\"https://github.com/dhimmel/learn/blob/0867341ac64e5875390532e1aa31bd3b6f38c0ad/neo4j-compute.ipynb\">notebook</a>). We encountered a <a href=\"https://github.com/nigelsmall/py2neo/issues/449\">small hiccup</a> where our queue of queries waiting to be executed grew large and consumed substantial memory. We addressed the issue by postponing new query submission until the queue dropped below a given size.</p>\r\n\r\n<p>Performing concurrent queries led to ~1000% processor usage by neo4j, equivalent to 10 cores at full load. This benchmark was performed on a 16 core machine running neo4j-community-2.3.1 on Ubuntu 15.10. Increasing the number of concurrent python workers above 16 did not increase the ~1000% usage figure. Let us know of any methods to increase processor saturation.</p>",
      "body_md": "# Concurrent queries using py2neo\r\n\r\nAs [explained](https://twitter.com/darthvader42/status/670154181064400897) by Stefan Armbruster, a single cypher query is limited to a single core. However, multiple queries can be fulfilled in parallel:\r\n\r\n> With current versions of Neo4j, a Cypher query traverses the graph in single threaded mode. Since most graph applications out there are concurrently used by multiple users, this model saturates the available cores. [[source](http://stackoverflow.com/a/27578860/4651668)]\r\n\r\nCurrently, we perform a separate query for each compound--disease--metapath combination. Depending on the number of compound--disease pairs and metapaths considered, we will need to compute between 1 million and 1 billion *DWPCs*. \r\n\r\nOur software package for hetnets, [hetio](https://github.com/dhimmel/hetio) [@10.5281/zenodo.31763], is built in python. Despite migrating to neo4j, we are still dependent on hetio for:\r\n\r\n+ metagraph operations\r\n+ edge directionality\r\n+ metapath abbreviation\r\n+ constructing cypher queries\r\n\r\nTherefore, we're using python to construct and execute queries with the [py2neo](http://py2neo.org/2.0/) package.\r\n\r\nTo enable concurrent queries, we initially used the [multiprocessing](https://docs.python.org/3.5/library/multiprocessing.html) module ([notebook](https://github.com/dhimmel/learn/blob/affb391ac35cd726e1377f08557b060f4098144f/neo4j-compute.ipynb)), which enables parallelism by creating subprocesses. However, subprocesses require substantial overhead. Since the majority of computation is performed outside of python by the neo4j sever, we switched to the [threading](https://docs.python.org/3.5/library/threading.html) module ([notebook](https://github.com/dhimmel/learn/blob/bfc0b4b3adef8ee9bab73676181b84298f6b16fe/neo4j-compute.ipynb)). Threading has less overhead than multiprocessing, but is limited to a single process of pure python. However, since the cypher query releases the [global interpreter lock](https://docs.python.org/3.5/glossary.html#term-global-interpreter-lock), the restriction to a single process is not a time-limiting step.\r\n\r\nIn the end, we used [concurrent.futures](https://docs.python.org/3.5/library/concurrent.futures.html#threadpoolexecutor) to make threading easier ([notebook](https://github.com/dhimmel/learn/blob/0867341ac64e5875390532e1aa31bd3b6f38c0ad/neo4j-compute.ipynb)). We encountered a [small hiccup](https://github.com/nigelsmall/py2neo/issues/449) where our queue of queries waiting to be executed grew large and consumed substantial memory. We addressed the issue by postponing new query submission until the queue dropped below a given size.\r\n\r\nPerforming concurrent queries led to ~1000% processor usage by neo4j, equivalent to 10 cores at full load. This benchmark was performed on a 16 core machine running neo4j-community-2.3.1 on Ubuntu 15.10. Increasing the number of concurrent python workers above 16 did not increase the ~1000% usage figure. Let us know of any methods to increase processor saturation.",
      "profile": 17,
      "published": "2015-11-26T22:54:11.765140Z",
      "thread": 112,
      "url": "/discussion/using-the-neo4j-graph-database-for-hetnets/112#7"
    },
    {
      "body_html": "<p>We're building an open project to predict new uses for existing drugs. The core of our project is a network with multiple types of nodes and relationships (hetnet) <span class=\"citation\">[<a href=\"/discussion/one-network-to-rule-them-all/102\" class=\"citation\" data-key=\"10.15363/thinklab.d102\">1</a>]</span>. We <a href=\"http://thinklab.com/discussion/using-the-neo4j-graph-database-for-hetnets/112\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d112\">use neo4j</a> to store, explore, and quantify this hetnet. </p>\r\n\r\n<p>With regards to licensing, there are several ways we plan to use the software: </p>\r\n\r\n<ol><li><p><strong>distributing the hetnet</strong> — we strive to make our hetnet reusable and extensible. Therefore, we need convenient and reliable formats for distributing the network. One format we'd like to use for distribution is an archive file of the database. For example, a tarball of <code>data/graph.db/</code>. In addition to an archive of just the database location, we would like to distribute an archive of the binary with the database included. So for example, a user could extract a single archive containing the neo4j server with our database and configuration already loaded.</p></li><li><p><strong>quantifying the hetnet</strong> — our method for relationship prediction relies on extracting network features, which quantify the prevalence of specific path types between two nodes. We <a href=\"http://thinklab.com/discussion/using-the-neo4j-graph-database-for-hetnets/112#4\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d112\">implemented</a> the algorithm in cypher. We <a href=\"http://neo4j.com/neo4j-scales-web-enterprise/\">read that</a> the enterprise version's high-performance cache \"can provide up to 10x the performance under concurrent load.\" However, for our application the enterprise edition did not provide a performance improvement over the community edition (<a href=\"https://github.com/dhimmel/learn/blob/6b81cd8eccaabf7d90bdedde66c28d8a88483cc6/neo4j-comparison.ipynb\">notebook</a>). Therefore, we are inclined to stick to the community version, but may consider enterprise to scale up our query throughput via the high-availability cluster.</p></li><li><p><strong>exploring the hetnet</strong> — we have been using the neo4j browser as a GUI to interact with the network. Eventually, we would like to host a publicly-accessible neo4j server which allows anyone to interact with our hetnet from their web browser.</p></li></ol>\r\n\r\n<p>We try to adhere to the following project ground-rules:</p>\r\n\r\n<ul><li>releasing all of our original data and source code as CC0.</li><li>avoiding dependencies that forbid distribution, to prevent situations where others cannot reproduce our science due to access issues.</li></ul>\r\n\r\n<p>Neo4j comes in two <a href=\"http://neo4j.com/editions/\">editions</a>: <em>community</em> which is <a href=\"http://opensource.org/licenses/GPL-3.0\">GPLv3</a> licensed and <em>enterprise</em> which is <a href=\"http://opensource.org/licenses/AGPL-3.0\">AGPLv3</a> licensed. Exactly what these licenses mean with respect to neo4j server is <a href=\"http://stackoverflow.com/q/6500925\">subject to debate</a>. A <a href=\"https://project.nordu.net/secure/attachment/12828/Fair+Trade+Software+Licensing.pdf\">guidance document</a> by neo4j states:</p>\r\n\r\n<blockquote><p>Simply, if you are open source, then Neo4j is open source; if you are closed source, then Neo4j is commercial. </p></blockquote>\r\n\r\n<p>Our project is open source, although we use CC0 rather than a copyleft license. We do not modify neo4j's source code, although we do want to distribute the compiled version (see 1 above). Finally, it's unclear exactly which aspects of our project are affected by neo4j's license.</p>\r\n\r\n<p>Given these complexities, we will reach out to neo4j for guidance. Specifically, we're interested in what restrictions neo4j's license places on our three use cases. And if there are restriction, could we obtain an educational or research license permitting our use?</p>",
      "body_md": "We're building an open project to predict new uses for existing drugs. The core of our project is a network with multiple types of nodes and relationships (hetnet) [@10.15363/thinklab.d102]. We [use neo4j](http://thinklab.com/discussion/using-the-neo4j-graph-database-for-hetnets/112) to store, explore, and quantify this hetnet. \r\n\r\nWith regards to licensing, there are several ways we plan to use the software: \r\n\r\n1. **distributing the hetnet** -- we strive to make our hetnet reusable and extensible. Therefore, we need convenient and reliable formats for distributing the network. One format we'd like to use for distribution is an archive file of the database. For example, a tarball of `data/graph.db/`. In addition to an archive of just the database location, we would like to distribute an archive of the binary with the database included. So for example, a user could extract a single archive containing the neo4j server with our database and configuration already loaded.\r\n\r\n+ **quantifying the hetnet** -- our method for relationship prediction relies on extracting network features, which quantify the prevalence of specific path types between two nodes. We [implemented](http://thinklab.com/discussion/using-the-neo4j-graph-database-for-hetnets/112#4) the algorithm in cypher. We [read that](http://neo4j.com/neo4j-scales-web-enterprise/) the enterprise version's high-performance cache \"can provide up to 10x the performance under concurrent load.\" However, for our application the enterprise edition did not provide a performance improvement over the community edition ([notebook](https://github.com/dhimmel/learn/blob/6b81cd8eccaabf7d90bdedde66c28d8a88483cc6/neo4j-comparison.ipynb)). Therefore, we are inclined to stick to the community version, but may consider enterprise to scale up our query throughput via the high-availability cluster.\r\n\r\n+ **exploring the hetnet** -- we have been using the neo4j browser as a GUI to interact with the network. Eventually, we would like to host a publicly-accessible neo4j server which allows anyone to interact with our hetnet from their web browser.\r\n\r\nWe try to adhere to the following project ground-rules:\r\n\r\n+ releasing all of our original data and source code as CC0.\r\n+ avoiding dependencies that forbid distribution, to prevent situations where others cannot reproduce our science due to access issues.\r\n\r\nNeo4j comes in two [editions](http://neo4j.com/editions/): *community* which is [GPLv3](http://opensource.org/licenses/GPL-3.0) licensed and *enterprise* which is [AGPLv3](http://opensource.org/licenses/AGPL-3.0) licensed. Exactly what these licenses mean with respect to neo4j server is [subject to debate](http://stackoverflow.com/q/6500925). A [guidance document](https://project.nordu.net/secure/attachment/12828/Fair+Trade+Software+Licensing.pdf) by neo4j states:\r\n\r\n> Simply, if you are open source, then Neo4j is open source; if you are closed source, then Neo4j is commercial. \r\n\r\nOur project is open source, although we use CC0 rather than a copyleft license. We do not modify neo4j's source code, although we do want to distribute the compiled version (see 1 above). Finally, it's unclear exactly which aspects of our project are affected by neo4j's license.\r\n\r\nGiven these complexities, we will reach out to neo4j for guidance. Specifically, we're interested in what restrictions neo4j's license places on our three use cases. And if there are restriction, could we obtain an educational or research license permitting our use?",
      "profile": 17,
      "published": "2015-11-30T20:04:57.718617Z",
      "thread": 130,
      "url": "/discussion/licensing-neo4j/130"
    },
    {
      "body_html": "<h1>Concurrency for data science</h1>\r\n\r\n<p>Processors are now multicore and our code should take advantage of this opportunity. If execution time is not an issue, don't waste time optimizing. But if you find yourself waiting for your program to finish and your computation can be parallelized, look no further than <code>concurrent.futures</code>.</p>\r\n\r\n<p><code>concurrent.futures</code> gives you easy, no <a href=\"https://en.wikipedia.org/wiki/Boilerplate_code\">boilerplate</a>, access to the two methods of concurrency in python. The methods are:</p>\r\n\r\n<h2><a href=\"https://docs.python.org/3/library/threading.html\"><code>threading</code></a></h2>\r\n\r\n<p>Threads allow multiple paths of execution within a single program. Each thread has access to the global data space, which makes threads convenient for programming. However, proceed with caution: since a single object can be altered by multiple threads simultaneously, there is danger. Avoid the danger by using <a href=\"https://docs.python.org/3/library/threading.html#lock-objects\">locks</a> (via <a href=\"https://docs.python.org/3/library/threading.html#using-locks-conditions-and-semaphores-in-the-with-statement\"><code>with</code></a> for convenience) whenever a thread writes to a communal resource.</p>\r\n\r\n<p>The main drawback of threading is the global interpreter lock (<a href=\"https://docs.python.org/3/glossary.html#term-global-interpreter-lock\">GIL</a>) meaning that \"only one thread can execute Python code at once.\" Therefore, if you want to reap the benefits of multiple cores, you need to make sure your code isn't limited by the GIL. You can escape the GIL by moving the time intensive computations outside of python by:</p>\r\n\r\n<ol><li>Using code written in other languages such as C. Many functions implement their core features outside of python. Additionally, many operations rely on external resources such as database or web queries.</li><li>Using <a href=\"http://numba.pydata.org/\"><code>numba</code></a> to automatically compile your code with the <a href=\"http://numba.pydata.org/numba-doc/0.21.0/user/jit.html#nogil\"><code>@numba.jit(nogil=True)</code></a> decorator.</li></ol>\r\n\r\n<h2><a href=\"https://docs.python.org/3/library/multiprocessing.html\"><code>multiprocessing</code></a></h2>\r\n\r\n<p>When your code isn't amenable to releasing the GIL, try multiprocessing. Multiprocessing spawns new python instances for each task. Therefore, any needed data must be serialized via pickling and dispatched to the subprocess. This creates large overhead. Try to send the minimum amount of data required for your application to each process to reduce this overhead.</p>\r\n\r\n<h2><a href=\"https://docs.python.org/3/library/concurrent.futures.html\"><code>concurrent.futures</code></a></h2>\r\n\r\n<p><code>concurrent.futures</code> provides a queue-based system for executing functions in parallel. To use, first initiate an Executor using <code>concurrent.futures.ThreadPoolExecutor()</code> for threading or <code>concurrent.futures.ProcessPoolExecutor()</code> for multiprocessing. Both constructors accept a <code>max_workers</code> argument for the maximum number of threads/processes you would like to devote to the task.</p>\r\n\r\n<p>You can interact with the Executor in the following ways:</p>\r\n\r\n<ul><li><code>Executor.submit()</code> which submits a <em>single</em> job to the queue</li><li><code>Executor.map()</code> which submits <em>many</em> jobs to the queue</li><li><code>Executor.shutdown()</code> which shuts down the executor. The default parameter <code>wait=True</code> means this method will wait for all jobs to finish before returning.</li></ul>\r\n\r\n<p>Cheers to a concurrent future!</p>",
      "body_md": "# Concurrency for data science\r\n\r\nProcessors are now multicore and our code should take advantage of this opportunity. If execution time is not an issue, don't waste time optimizing. But if you find yourself waiting for your program to finish and your computation can be parallelized, look no further than `concurrent.futures`.\r\n\r\n`concurrent.futures` gives you easy, no [boilerplate](https://en.wikipedia.org/wiki/Boilerplate_code), access to the two methods of concurrency in python. The methods are:\r\n\r\n## [`threading`](https://docs.python.org/3/library/threading.html)\r\n\r\nThreads allow multiple paths of execution within a single program. Each thread has access to the global data space, which makes threads convenient for programming. However, proceed with caution: since a single object can be altered by multiple threads simultaneously, there is danger. Avoid the danger by using [locks](https://docs.python.org/3/library/threading.html#lock-objects) (via [`with`](https://docs.python.org/3/library/threading.html#using-locks-conditions-and-semaphores-in-the-with-statement) for convenience) whenever a thread writes to a communal resource.\r\n\r\nThe main drawback of threading is the global interpreter lock ([GIL](https://docs.python.org/3/glossary.html#term-global-interpreter-lock)) meaning that \"only one thread can execute Python code at once.\" Therefore, if you want to reap the benefits of multiple cores, you need to make sure your code isn't limited by the GIL. You can escape the GIL by moving the time intensive computations outside of python by:\r\n\r\n1. Using code written in other languages such as C. Many functions implement their core features outside of python. Additionally, many operations rely on external resources such as database or web queries.\r\n2. Using [`numba`](http://numba.pydata.org/) to automatically compile your code with the [`@numba.jit(nogil=True)`](http://numba.pydata.org/numba-doc/0.21.0/user/jit.html#nogil) decorator.\r\n\r\n## [`multiprocessing`](https://docs.python.org/3/library/multiprocessing.html)\r\n\r\nWhen your code isn't amenable to releasing the GIL, try multiprocessing. Multiprocessing spawns new python instances for each task. Therefore, any needed data must be serialized via pickling and dispatched to the subprocess. This creates large overhead. Try to send the minimum amount of data required for your application to each process to reduce this overhead.\r\n\r\n## [`concurrent.futures`](https://docs.python.org/3/library/concurrent.futures.html)\r\n\r\n`concurrent.futures` provides a queue-based system for executing functions in parallel. To use, first initiate an Executor using `concurrent.futures.ThreadPoolExecutor()` for threading or `concurrent.futures.ProcessPoolExecutor()` for multiprocessing. Both constructors accept a `max_workers` argument for the maximum number of threads/processes you would like to devote to the task.\r\n\r\nYou can interact with the Executor in the following ways:\r\n\r\n+ `Executor.submit()` which submits a *single* job to the queue\r\n+ `Executor.map()` which submits *many* jobs to the queue\r\n+ `Executor.shutdown()` which shuts down the executor. The default parameter `wait=True` means this method will wait for all jobs to finish before returning.\r\n\r\nCheers to a concurrent future!",
      "profile": 17,
      "published": "2015-12-02T23:01:57.635743Z",
      "thread": 84,
      "url": "/discussion/python-for-the-modern-biodata-scientist/84#6"
    },
    {
      "body_html": "<p>Our algorithm relies on extracting paths of a certain type (metapath) between a source and target node. For our previous project where we predicted gene–disease associations, we placed two restrictions on paths <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1004259\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1004259\">1</a>]</span>:</p>\r\n\r\n<blockquote><p>paths with duplicate nodes were excluded, and, if present, the association edge between the source gene and target disease was masked.</p></blockquote>\r\n\r\n<p>However, since <a href=\"http://thinklab.com/discussion/using-the-neo4j-graph-database-for-hetnets/112\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d112\">migrating to neo4j</a>, we have not added extra path restrictions beyond the builtin restriction that paths cannot contain duplicate relationships.</p>\r\n\r\n<p>So in total, we have relied on three different conditions for excluding a path:</p>\r\n\r\n<ol><li>duplicated nodes</li><li>duplicated edges</li><li>contains the prediction edge</li></ol>\r\n\r\n<p>It turns out that 1 implies 2 and 3 (1 ⇒ 2, 1 ⇒ 3). In other words, when we exclude paths with duplicate nodes, we also exclude paths with duplicate edges. And since we omit the length-one metapath that is simply the metaedge being modeled, all paths containing the prediction edge will contain either the source or target node at least twice.</p>\r\n\r\n<p>You may have noticed above that for hetio-dag <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1004259\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1004259\">1</a>]</span>, we masked the prediction edge if present. Masking an edge temporarily removes it from the network. Masking the prediction edge not only eliminates paths containing that edge, but also can affect path degrees, which go into the <em>DWPC</em> (degree-weighted path count).</p>\r\n\r\n<p>This discussion will focus on identifying the most sensible path restrictions and whether masking is warranted.</p>",
      "body_md": "Our algorithm relies on extracting paths of a certain type (metapath) between a source and target node. For our previous project where we predicted gene--disease associations, we placed two restrictions on paths [@10.1371/journal.pcbi.1004259]:\r\n\r\n> paths with duplicate nodes were excluded, and, if present, the association edge between the source gene and target disease was masked.\r\n\r\nHowever, since [migrating to neo4j](http://thinklab.com/discussion/using-the-neo4j-graph-database-for-hetnets/112), we have not added extra path restrictions beyond the builtin restriction that paths cannot contain duplicate relationships.\r\n\r\nSo in total, we have relied on three different conditions for excluding a path:\r\n\r\n1. duplicated nodes\r\n2. duplicated edges\r\n3. contains the prediction edge\r\n\r\nIt turns out that 1 implies 2 and 3 (1 ⇒ 2, 1 ⇒ 3). In other words, when we exclude paths with duplicate nodes, we also exclude paths with duplicate edges. And since we omit the length-one metapath that is simply the metaedge being modeled, all paths containing the prediction edge will contain either the source or target node at least twice.\r\n\r\nYou may have noticed above that for hetio-dag [@10.1371/journal.pcbi.1004259], we masked the prediction edge if present. Masking an edge temporarily removes it from the network. Masking the prediction edge not only eliminates paths containing that edge, but also can affect path degrees, which go into the *DWPC* (degree-weighted path count).\r\n\r\nThis discussion will focus on identifying the most sensible path restrictions and whether masking is warranted.",
      "profile": 17,
      "published": "2015-12-09T02:51:40.932445Z",
      "thread": 134,
      "url": "/discussion/path-exclusion-conditions/134"
    },
    {
      "body_html": "<h1>Cypher implementations of duplicate node exclusion</h1>\r\n\r\n<p>With help from Christophe Willemsen, <a href=\"/u/alizee\" class=\"username\">@alizee</a> and I <a href=\"https://twitter.com/dhimmel/status/674487386949087232\">identified</a> two methods for excluding paths with duplicate nodes in a Cypher query.</p>\r\n\r\n<p>We <a href=\"https://github.com/dhimmel/hetio/commit/f6deae3294c1d90ba9ba92153c91a40791d4ae8d\">implemented</a> both methods. Here, we'll describe them in the context of a length-four path matched using <code>MATCH paths = (n0)--(n1)--(n2)--(n3)--(n4)</code>.</p>\r\n\r\n<p>The <strong>nested</strong> method adds a <code>WHERE</code> clause specifying:</p>\r\n\r\n<pre><code class=\"no-highlight hljs\">ALL (x IN nodes(paths) WHERE size(filter(z IN nodes(paths) WHERE z = x)) = 1)</code></pre>\r\n\r\n<p>This method uses <a href=\"http://neo4j.com/docs/2.3.1/syntax-collections.html#_list_comprehension\">list comprehension</a> to iterate over each node in a path and ensure that it only occurs once. This clause can be applied to paths of any length and does not require assigning nodes to identifiers.</p>\r\n\r\n<p>The <strong>expanded</strong> method adds a <code>WHERE</code> clause specifying:</p>\r\n\r\n<pre><code class=\"no-highlight hljs\">NOT (n0=n1 OR n0=n2 OR n0=n3 OR n0=n4 OR n1=n2 OR n1=n3 OR n1=n4 OR n2=n3 OR n2=n4 OR n3=n4)</code></pre>\r\n\r\n<p>The method checks that no two nodes are equal by explicitly evaluating all combinations of two nodes. The method requires assigning node identifiers and is path-length dependent. However, it is intuitive and amenable to query plan optimization since filtering can be front-loaded to avoid expanding on illegitimate paths.</p>\r\n\r\n<h2>Alternative implementations</h2>\r\n\r\n<p>Another general solution would be to check whether the number of distinct nodes equals the length of the full path. We currently are unaware of a Cypher implementation for this <strong>distinct</strong> method, but <a href=\"https://twitter.com/A_Lizee/status/674655287899348992\">suspect</a> it could scale to longer paths better than the nested method.</p>\r\n\r\n<p>One final variant of the expanded method, called <strong>labeled</strong>, would avoid comparing nodes with different labels, as these nodes are implicitly different. This method requires the greatest <em>a priori</em> knowledge of path characteristics.</p>",
      "body_md": "# Cypher implementations of duplicate node exclusion\r\n\r\nWith help from Christophe Willemsen, @alizee and I [identified](https://twitter.com/dhimmel/status/674487386949087232) two methods for excluding paths with duplicate nodes in a Cypher query.\r\n\r\nWe [implemented](https://github.com/dhimmel/hetio/commit/f6deae3294c1d90ba9ba92153c91a40791d4ae8d) both methods. Here, we'll describe them in the context of a length-four path matched using `MATCH paths = (n0)--(n1)--(n2)--(n3)--(n4)`.\r\n\r\nThe **nested** method adds a `WHERE` clause specifying:\r\n\r\n```\r\nALL (x IN nodes(paths) WHERE size(filter(z IN nodes(paths) WHERE z = x)) = 1)\r\n```\r\n\r\nThis method uses [list comprehension](http://neo4j.com/docs/2.3.1/syntax-collections.html#_list_comprehension) to iterate over each node in a path and ensure that it only occurs once. This clause can be applied to paths of any length and does not require assigning nodes to identifiers.\r\n\r\nThe **expanded** method adds a `WHERE` clause specifying:\r\n\r\n```\r\nNOT (n0=n1 OR n0=n2 OR n0=n3 OR n0=n4 OR n1=n2 OR n1=n3 OR n1=n4 OR n2=n3 OR n2=n4 OR n3=n4)\r\n```\r\n\r\nThe method checks that no two nodes are equal by explicitly evaluating all combinations of two nodes. The method requires assigning node identifiers and is path-length dependent. However, it is intuitive and amenable to query plan optimization since filtering can be front-loaded to avoid expanding on illegitimate paths.\r\n\r\n## Alternative implementations\r\n\r\nAnother general solution would be to check whether the number of distinct nodes equals the length of the full path. We currently are unaware of a Cypher implementation for this **distinct** method, but [suspect](https://twitter.com/A_Lizee/status/674655287899348992) it could scale to longer paths better than the nested method.\r\n\r\nOne final variant of the expanded method, called **labeled**, would avoid comparing nodes with different labels, as these nodes are implicitly different. This method requires the greatest *a priori* knowledge of path characteristics.",
      "profile": 17,
      "published": "2015-12-13T23:30:33.944307Z",
      "thread": 134,
      "url": "/discussion/path-exclusion-conditions/134#2"
    },
    {
      "body_html": "<h1>Optimization dataset</h1>\r\n\r\n<p>To help optimize our cypher queries, we computed features for 347 positives and 317 negatives for the 1979 metapaths with length ≤ 4 (<a href=\"https://github.com/dhimmel/learn/blob/5d932818e58963b4f5ae5095294853f5bafbe729/unique-nodes/unique-nodes-extract.ipynb\">notebook</a>, <a href=\"https://github.com/dhimmel/learn/blob/5d932818e58963b4f5ae5095294853f5bafbe729/unique-nodes/dwpc.tsv.gz\">features</a>). Positives were randomly selected indications, while negatives were randomly selected non-indications. We computed the <em>PC</em> (path count) and <em>DWPC</em> for each compound–disease–metapath combination using each of three node uniqueness methods described <a href=\"#2\">above</a>. In addition, we computed features without excluding duplicate nodes.</p>\r\n\r\n<h1>Runtimes for duplicate node exclusions</h1>\r\n\r\n<p>We compared the average query runtime for each node uniqueness method (<a href=\"https://github.com/dhimmel/learn/blob/5d932818e58963b4f5ae5095294853f5bafbe729/unique-nodes/runtime-comparison.ipynb\">notebook</a>):</p>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th><code>unique_nodes</code> method</th><th>average query runtime</th><th>runtime_hit</th></tr></thead><tbody><tr><td><code>False</code></td><td>129.41 ms</td><td>0.0%</td></tr><tr><td><code>labeled</code></td><td>143.56 ms</td><td>10.9%</td></tr><tr><td><code>expanded</code></td><td>173.43 ms</td><td>34.0%</td></tr><tr><td><code>nested</code></td><td>179.76 ms</td><td>38.9%</td></tr></tbody></table>\r\n\r\n<p>We found that not performing any duplicate node exclusion was fastest. The most efficient method for exclusion was <code>labeled</code>, which explicitly checks that node pairs of the same label are not duplicates. This method only slowed down runtime by 11%, a very acceptable hit.</p>\r\n\r\n<p>The <code>nested</code> method was on average slightly slower than <code>expanded</code>. However, in the overwhelming majority of instances, <code>nested</code> was faster than <code>expected</code>, yet poor worst case runtime pushed <code>nested</code> to last place. This observation underscores the vast asymmetry in runtimes: most queries finish quickly, while a small percentage of queries form a long tail that contributes disproportionately to overall runtime.</p>\r\n\r\n<p>Remember that these findings are highly context-dependent. Here they are in the context of a subset of queries chosen to be representative of our specific HNEP (hetnet edge prediction) task.</p>\r\n\r\n<p>In conclusion, if excluding paths with duplicate nodes is desired, we will use the <code>labeled</code> method.</p>",
      "body_md": "# Optimization dataset\r\n\r\nTo help optimize our cypher queries, we computed features for 347 positives and 317 negatives for the 1979 metapaths with length ≤ 4 ([notebook](https://github.com/dhimmel/learn/blob/5d932818e58963b4f5ae5095294853f5bafbe729/unique-nodes/unique-nodes-extract.ipynb), [features](https://github.com/dhimmel/learn/blob/5d932818e58963b4f5ae5095294853f5bafbe729/unique-nodes/dwpc.tsv.gz)). Positives were randomly selected indications, while negatives were randomly selected non-indications. We computed the *PC* (path count) and *DWPC* for each compound--disease--metapath combination using each of three node uniqueness methods described [above](#2). In addition, we computed features without excluding duplicate nodes.\r\n\r\n# Runtimes for duplicate node exclusions\r\n\r\nWe compared the average query runtime for each node uniqueness method ([notebook](https://github.com/dhimmel/learn/blob/5d932818e58963b4f5ae5095294853f5bafbe729/unique-nodes/runtime-comparison.ipynb)):\r\n\r\n| `unique_nodes` method | average query runtime | runtime_hit |\r\n|---------------------|--------------------|---------------|\r\n| `False` | 129.41 ms | 0.0% |\r\n| `labeled` | 143.56 ms | 10.9% |\r\n| `expanded` | 173.43 ms | 34.0% |\r\n| `nested` | 179.76 ms | 38.9% |\r\n\r\nWe found that not performing any duplicate node exclusion was fastest. The most efficient method for exclusion was `labeled`, which explicitly checks that node pairs of the same label are not duplicates. This method only slowed down runtime by 11%, a very acceptable hit.\r\n\r\nThe `nested` method was on average slightly slower than `expanded`. However, in the overwhelming majority of instances, `nested` was faster than `expected`, yet poor worst case runtime pushed `nested` to last place. This observation underscores the vast asymmetry in runtimes: most queries finish quickly, while a small percentage of queries form a long tail that contributes disproportionately to overall runtime.\r\n\r\nRemember that these findings are highly context-dependent. Here they are in the context of a subset of queries chosen to be representative of our specific HNEP (hetnet edge prediction) task.\r\n\r\nIn conclusion, if excluding paths with duplicate nodes is desired, we will use the `labeled` method.",
      "profile": 17,
      "published": "2015-12-15T19:46:16.116618Z",
      "thread": 134,
      "url": "/discussion/path-exclusion-conditions/134#3"
    },
    {
      "body_html": "<h1>The effect of duplicate node exclusion on features</h1>\r\n\r\n<p><a href=\"#3\">Above</a>, we described computing features for 664 compound–disease pairs × 1,979 metapaths. In this comment, we'll investigate the effect of excluding paths with duplicate nodes on this dataset.</p>\r\n\r\n<p>Paths without the unique node constraint are still subject to neo4j's unique relationship constraint. In essence, the node uniqueness constraint determines whether paths containing a cycle are permitted.</p>\r\n\r\n<p>As <a href=\"#1\">previously discussed</a> the node uniqueness constraint also excludes paths containing the prediction edge. In our case, the prediction edge is an indication between the source compound and target disease.</p>\r\n\r\n<p>Therefore, feature performance could decline after applying the unique constraint because:</p>\r\n\r\n<ol><li>paths with cycles convey meaningful information that the unique node constraint overlooks</li><li>paths including the prediction edge cause overfitting by incorporating the outcome (indication status) into the predictor (<em>DWPC</em>)</li></ol>\r\n\r\n<p>For the 1,961 features that contained a duplicate metanode, we calculated the decline in AUROC of the <em>DWPC</em> resulting from duplicate node exclusion (<a href=\"http://nbviewer.ipython.org/github/dhimmel/learn/blob/2eb24875c5dc5d1017b0eaa6759562fc38c33e7c/unique-nodes/feature-comparison.ipynb\">notebook</a>, <a href=\"https://github.com/dhimmel/learn/blob/2eb24875c5dc5d1017b0eaa6759562fc38c33e7c/unique-nodes/metapaths.tsv\">table</a>). We investigated the occurrence of 1 and 2 by segregating metapaths based on whether they include an indication metaedge. If a metapath doesn't include an indication metaedge, then any change in performance must be due to 1. The distributions of AUROC declines identify 2 (overfitting) as a major factor, while showing 1 is at most a very minor factor.</p>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/dhimmel/learn/2eb24875c5dc5d1017b0eaa6759562fc38c33e7c/unique-nodes/AUROC-violins.png\" alt=\"\"></p>\r\n\r\n<h2>Conclusion</h2>\r\n\r\n<p>Therefore, we will adopt the unique node constraint because it omits paths that include the prediction edge, which leads to overfitting. Absent their potential for overfitting, paths with duplicate nodes did not contribute greatly to <em>DWPC</em> performance. Finally, paths that are excluded because they contain a cycle will still be incorporated into <em>DWPCs</em> for shorter metapaths that bypass the cyclical segment.</p>",
      "body_md": "# The effect of duplicate node exclusion on features \r\n\r\n[Above](#3), we described computing features for 664 compound--disease pairs × 1,979 metapaths. In this comment, we'll investigate the effect of excluding paths with duplicate nodes on this dataset.\r\n\r\nPaths without the unique node constraint are still subject to neo4j's unique relationship constraint. In essence, the node uniqueness constraint determines whether paths containing a cycle are permitted.\r\n\r\nAs [previously discussed](#1) the node uniqueness constraint also excludes paths containing the prediction edge. In our case, the prediction edge is an indication between the source compound and target disease.\r\n\r\nTherefore, feature performance could decline after applying the unique constraint because:\r\n\r\n1. paths with cycles convey meaningful information that the unique node constraint overlooks\r\n2. paths including the prediction edge cause overfitting by incorporating the outcome (indication status) into the predictor (*DWPC*)\r\n\r\nFor the 1,961 features that contained a duplicate metanode, we calculated the decline in AUROC of the *DWPC* resulting from duplicate node exclusion ([notebook](http://nbviewer.ipython.org/github/dhimmel/learn/blob/2eb24875c5dc5d1017b0eaa6759562fc38c33e7c/unique-nodes/feature-comparison.ipynb), [table](https://github.com/dhimmel/learn/blob/2eb24875c5dc5d1017b0eaa6759562fc38c33e7c/unique-nodes/metapaths.tsv)). We investigated the occurrence of 1 and 2 by segregating metapaths based on whether they include an indication metaedge. If a metapath doesn't include an indication metaedge, then any change in performance must be due to 1. The distributions of AUROC declines identify 2 (overfitting) as a major factor, while showing 1 is at most a very minor factor.\r\n\r\n![](https://raw.githubusercontent.com/dhimmel/learn/2eb24875c5dc5d1017b0eaa6759562fc38c33e7c/unique-nodes/AUROC-violins.png)\r\n\r\n## Conclusion\r\n\r\nTherefore, we will adopt the unique node constraint because it omits paths that include the prediction edge, which leads to overfitting. Absent their potential for overfitting, paths with duplicate nodes did not contribute greatly to *DWPC* performance. Finally, paths that are excluded because they contain a cycle will still be incorporated into *DWPCs* for shorter metapaths that bypass the cyclical segment.",
      "profile": 17,
      "published": "2015-12-21T19:20:05.708973Z",
      "thread": 134,
      "url": "/discussion/path-exclusion-conditions/134#4"
    },
    {
      "body_html": "<p>Network permutation randomizes edges in a graph to remove signal. Metrics computed on permuted networks provide a baseline to evaluate the extent of signal contained in the network. Different types of permutations destroy different aspects of the information encoded by a network.</p>\r\n\r\n<p>The method we've <a href=\"https://doi.org/10.1371/journal.pcbi.1004259#article1.body1.sec2.sec6.p1\">previously used</a> selects two random edges and swaps the endpoints (labeled <code>XSwap</code> in <span class=\"citation\">[<a href=\"/doi/10.1137/1.9781611972795.67\" class=\"citation\" data-key=\"10.1137/1.9781611972795.67\">1</a>]</span>). This method preserves node degree while destroying edge specificity.</p>\r\n\r\n<p>We <a href=\"https://github.com/dhimmel/hetio/blob/1aa1964f92881c54da652cda0c1162ee6e4664b9/hetio/permute.py\">adapted</a> the edge swapping technique to hetnets by permuting each metaedge separately—edges are only swapped with other edges of the same type. We found the permutation yielded valuable insights on which aspects of the network were informative and the quality of our predictions <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1004259\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1004259\">2</a>]</span>.</p>\r\n\r\n<p>We've subsequently <a href=\"http://thinklab.com/discussion/using-the-neo4j-graph-database-for-hetnets/112\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d112\">migrated</a> to neo4j, so are now looking to implement hetnet permutation in cypher. We <a href=\"https://twitter.com/dhimmel/status/677260913200644096\">tweeted</a> this problem, and Michael Hunger <a href=\"http://jexp.github.io/graphgist/?dropbox-14493611%2Fedge_swap_cypher.adoc\">started</a> us on the right track.</p>\r\n\r\n<p>We created a <a href=\"http://portal.graphgist.org/graph_gists/by_url?url=https://gist.github.com/dhimmel/f69730d8bdfb880c15ed/6663d64be53e5b8c438d0a2d55a5778676ccf0b1\">graphgist</a> with potential implementations and cypher questions.</p>",
      "body_md": "Network permutation randomizes edges in a graph to remove signal. Metrics computed on permuted networks provide a baseline to evaluate the extent of signal contained in the network. Different types of permutations destroy different aspects of the information encoded by a network.\r\n\r\nThe method we've [previously used](https://doi.org/10.1371/journal.pcbi.1004259#article1.body1.sec2.sec6.p1) selects two random edges and swaps the endpoints (labeled `XSwap` in [@10.1137/1.9781611972795.67]). This method preserves node degree while destroying edge specificity.\r\n\r\nWe [adapted](https://github.com/dhimmel/hetio/blob/1aa1964f92881c54da652cda0c1162ee6e4664b9/hetio/permute.py) the edge swapping technique to hetnets by permuting each metaedge separately---edges are only swapped with other edges of the same type. We found the permutation yielded valuable insights on which aspects of the network were informative and the quality of our predictions [@10.1371/journal.pcbi.1004259].\r\n\r\nWe've subsequently [migrated](http://thinklab.com/discussion/using-the-neo4j-graph-database-for-hetnets/112) to neo4j, so are now looking to implement hetnet permutation in cypher. We [tweeted](https://twitter.com/dhimmel/status/677260913200644096) this problem, and Michael Hunger [started](http://jexp.github.io/graphgist/?dropbox-14493611%2Fedge_swap_cypher.adoc) us on the right track.\r\n\r\nWe created a [graphgist](http://portal.graphgist.org/graph_gists/by_url?url=https://gist.github.com/dhimmel/f69730d8bdfb880c15ed/6663d64be53e5b8c438d0a2d55a5778676ccf0b1) with potential implementations and cypher questions.",
      "profile": 17,
      "published": "2015-12-22T01:25:50.982403Z",
      "thread": 136,
      "url": "/discussion/permuting-hetnets-and-implementing-randomized-edge-swaps-in-cypher/136"
    },
    {
      "body_html": "<h1>Official adoption of 'hetnet'</h1>\r\n\r\n<p>For the past four months, I have been widely using the term 'hetnet'. I now rarely use the term 'heterogeneous network'. If I think an audience will be unfamiliar with my usage, I'll clarify by saying \"a network with multiple types of nodes or edges\". Even when I don't explicitly define the term, unfamiliar recipients seem to instinctively understand the concept. And perhaps most importantly, I haven't encountered any objections. Hence, our adoption of hetnet is now official.</p>\r\n\r\n<p>In the spirit of this announcement, we'll be changing our project's title from \"Repurposing drugs on a heterogeneous network\" to \"Repurposing drugs on a hetnet\".</p>\r\n\r\n<p>Meanwhile, the community continues to use an assortment of terms to express the concept. For example, a recent study used the \"multiplex network\" <span class=\"citation\">[<a href=\"/doi/10.7717/peerj.1525\" class=\"citation\" data-key=\"10.7717/peerj.1525\">1</a>]</span>. The field appears ripe for standardized terminology to emerge.</p>",
      "body_md": "# Official adoption of 'hetnet'\r\n\r\nFor the past four months, I have been widely using the term 'hetnet'. I now rarely use the term 'heterogeneous network'. If I think an audience will be unfamiliar with my usage, I'll clarify by saying \"a network with multiple types of nodes or edges\". Even when I don't explicitly define the term, unfamiliar recipients seem to instinctively understand the concept. And perhaps most importantly, I haven't encountered any objections. Hence, our adoption of hetnet is now official.\r\n\r\nIn the spirit of this announcement, we'll be changing our project's title from \"Repurposing drugs on a heterogeneous network\" to \"Repurposing drugs on a hetnet\".\r\n\r\nMeanwhile, the community continues to use an assortment of terms to express the concept. For example, a recent study used the \"multiplex network\" [@10.7717/peerj.1525]. The field appears ripe for standardized terminology to emerge.",
      "profile": 17,
      "published": "2016-01-28T01:39:58.688429Z",
      "thread": 104,
      "url": "/discussion/renaming-heterogeneous-networks-to-a-more-concise-and-catchy-term/104#5"
    },
    {
      "body_html": "<h1>Partial Cypher solutions</h1>\r\n\r\n<p>We've made some headway implementing <code>XSwap</code> in Cypher. But first, here's the general algorithm we're aiming for:</p>\r\n\r\n<ol><li>Randomly select two relationships of the specified type</li><li>If valid, XSwap the two relationships</li><li>Repeat 1 and 2 until a certain number of swaps have succeeded</li></ol>\r\n\r\n<p>We <a href=\"https://gist.github.com/dhimmel/f69730d8bdfb880c15ed#gistcomment-1657527\">initially attempted</a> to implement the above steps in a single cypher query. However, as Michael Hunger <a href=\"https://gist.github.com/dhimmel/f69730d8bdfb880c15ed#gistcomment-1662589\">explained</a>, the eagerness behavior of cypher prevents looping 1 and 2. Instead of <code>1, 2, 1, 2, 1, 2</code>, cypher will do <code>1, 1, 1, 2, 2, 2</code>, which fails because our technique randomizes iteratively.</p>\r\n\r\n<p>Therefore, we switched to python for managing steps 1 and 3, while keeping step 2 in cypher. This external method is now <a href=\"https://github.com/dhimmel/hetio/blob/9facc4bd609d536e733c5297f76a75f8123dc042/hetio/neo4j.py#L257\">implemented in hetio</a>. The function starts by retrieving the ids for all relationships of the specified type. Next, iteration begins:</p>\r\n\r\n<ol><li>Two relationships ids are randomly selected and sent as parameters to a cypher query.</li><li>If the swap is invalid, the cypher query returns no rows. Otherwise, the query returns the ids for the two created relationships and the python id list is updated.</li></ol>\r\n\r\n<p>There are two outstanding issues with this method:</p>\r\n\r\n<p>First, retrieving ids for all relationships of a given type could become problematic for extremely abundant relationship types. Currently this is not a problem as we can retrieve over 1 million ids using py2neo without failure. If problematic, we could create an identity property to each relationship with an existence constraint for efficient lookup. Indexed property values are a practical must for this solution, yet are <a href=\"https://gist.github.com/dhimmel/f69730d8bdfb880c15ed#gistcomment-1655807\">currently</a> only available with Enterprise.</p>\r\n\r\n<p>Second, the rule planner in 2.3.1 does not do an indexed lookup of relationship by id—instead it scans all relationships. The cost planner works as desired with a <code>DirectedRelationshipByIdSeekPipe</code>. However the 2.3.1 cost planner doesn't support write operations (3.0 is <a href=\"https://github.com/neo4j/neo4j/wiki/Neo4j-3.0-changelog#300-m01\">slated to add</a> this functionality).</p>\r\n\r\n<p>So in conclusion, we are close to a workable solution for permuting a neo4j hetnet. Depending on developments, we'll choose whether to adopt a solution discussed here or permute outside of neo4j with the legacy <a href=\"https://github.com/dhimmel/hetio/blob/1aa1964f92881c54da652cda0c1162ee6e4664b9/hetio/permute.py\">hetio functionality</a>.</p>",
      "body_md": "# Partial Cypher solutions\r\n\r\nWe've made some headway implementing `XSwap` in Cypher. But first, here's the general algorithm we're aiming for:\r\n\r\n1. Randomly select two relationships of the specified type\r\n2. If valid, XSwap the two relationships\r\n3. Repeat 1 and 2 until a certain number of swaps have succeeded\r\n\r\nWe [initially attempted](https://gist.github.com/dhimmel/f69730d8bdfb880c15ed#gistcomment-1657527) to implement the above steps in a single cypher query. However, as Michael Hunger [explained](https://gist.github.com/dhimmel/f69730d8bdfb880c15ed#gistcomment-1662589), the eagerness behavior of cypher prevents looping 1 and 2. Instead of `1, 2, 1, 2, 1, 2`, cypher will do `1, 1, 1, 2, 2, 2`, which fails because our technique randomizes iteratively.\r\n\r\nTherefore, we switched to python for managing steps 1 and 3, while keeping step 2 in cypher. This external method is now [implemented in hetio](https://github.com/dhimmel/hetio/blob/9facc4bd609d536e733c5297f76a75f8123dc042/hetio/neo4j.py#L257). The function starts by retrieving the ids for all relationships of the specified type. Next, iteration begins:\r\n\r\n1. Two relationships ids are randomly selected and sent as parameters to a cypher query.\r\n2. If the swap is invalid, the cypher query returns no rows. Otherwise, the query returns the ids for the two created relationships and the python id list is updated.\r\n\r\nThere are two outstanding issues with this method:\r\n\r\nFirst, retrieving ids for all relationships of a given type could become problematic for extremely abundant relationship types. Currently this is not a problem as we can retrieve over 1 million ids using py2neo without failure. If problematic, we could create an identity property to each relationship with an existence constraint for efficient lookup. Indexed property values are a practical must for this solution, yet are [currently](https://gist.github.com/dhimmel/f69730d8bdfb880c15ed#gistcomment-1655807) only available with Enterprise.\r\n\r\nSecond, the rule planner in 2.3.1 does not do an indexed lookup of relationship by id---instead it scans all relationships. The cost planner works as desired with a `DirectedRelationshipByIdSeekPipe`. However the 2.3.1 cost planner doesn't support write operations (3.0 is [slated to add](https://github.com/neo4j/neo4j/wiki/Neo4j-3.0-changelog#300-m01) this functionality).\r\n\r\nSo in conclusion, we are close to a workable solution for permuting a neo4j hetnet. Depending on developments, we'll choose whether to adopt a solution discussed here or permute outside of neo4j with the legacy [hetio functionality](https://github.com/dhimmel/hetio/blob/1aa1964f92881c54da652cda0c1162ee6e4664b9/hetio/permute.py).",
      "profile": 17,
      "published": "2016-01-06T19:17:34.818794Z",
      "thread": 136,
      "url": "/discussion/permuting-hetnets-and-implementing-randomized-edge-swaps-in-cypher/136#2"
    },
    {
      "body_html": "<h1>Licensing and usage options</h1>\r\n\r\n<p><a href=\"http://neo4j.com/blog/contributor/trey-knowles/\">Trey Knowles</a>, the Startup Community Manager at Neo Technology, assisted us with our licensing questions.</p>\r\n\r\n<p>To clarify the situation, we specified four licensing options:</p>\r\n\r\n<p>A. Community edition with the default GPLv3 license<br>B. Community edition with a contractual education license<br>C. Enterprise edition with the default AGPLv3 license<br>D. Enterprise edition with a contractual education license</p>\r\n\r\n<p>And then we specified four desired uses of Neo4j for our project:</p>\r\n\r\n<ol><li>distribute the neo4j binaries with our network preloaded</li><li>run internal database queries</li><li>make a publicly-accessible neo4j server instance</li><li>release all of our code and data as CC0</li></ol>\r\n\r\n<p>Trey provided the following answer, reproduced here with permission, on which of our desired uses are allowed by each license:</p>\r\n\r\n<blockquote><p>A. Community edition with the default GPLv3 license</p><ul><li>2 (single server, no clustering / live backups) </li><li>3 (single server, no clustering / live backups) </li><li>4 </li></ul><p>B. Community edition with a contractual education license</p><ul><li>Neo Technology offers no contracts with respect to CE</li></ul><p>C. Enterprise edition with the default AGPLv3 license</p><ul><li>1 provided you license your work as AGPLv3</li><li>2 provided you license your work as AGPLv3</li><li>3 provided you license your work as AGPLv3</li><li>4 provided you license your work as AGPLv3</li></ul><p>D. Enterprise edition with a contractual education license</p><ul><li>1</li><li>2</li><li>3</li><li>4<em> </em>(pending legal review from Neo team)</li></ul></blockquote>\r\n\r\n<p>Since AGPLv3 would place undesirable restrictions on our work compared to CC0, C is not a good option for us. Therefore, we'll choose between A and D. We will default to A (GPLv3-licensed community edition) unless enterprise features are needed in which case we'll explore D (contractually-licensed enterprise edition). Sticking with the community edition whenever possible will lessen the burden on anyone wanting to replicate or reuse or work.</p>",
      "body_md": "# Licensing and usage options\r\n\r\n[Trey Knowles](http://neo4j.com/blog/contributor/trey-knowles/), the Startup Community Manager at Neo Technology, assisted us with our licensing questions.\r\n\r\nTo clarify the situation, we specified four licensing options:\r\n\r\nA. Community edition with the default GPLv3 license\r\nB. Community edition with a contractual education license\r\nC. Enterprise edition with the default AGPLv3 license\r\nD. Enterprise edition with a contractual education license\r\n\r\nAnd then we specified four desired uses of Neo4j for our project:\r\n \r\n1. distribute the neo4j binaries with our network preloaded\r\n2. run internal database queries\r\n3. make a publicly-accessible neo4j server instance\r\n4. release all of our code and data as CC0\r\n\r\nTrey provided the following answer, reproduced here with permission, on which of our desired uses are allowed by each license:\r\n\r\n> A. Community edition with the default GPLv3 license\r\n>\r\n+ 2 (single server, no clustering / live backups) \r\n+ 3 (single server, no clustering / live backups) \r\n+ 4 \r\n\r\n> B. Community edition with a contractual education license\r\n>\r\n+ Neo Technology offers no contracts with respect to CE\r\n\r\n> C. Enterprise edition with the default AGPLv3 license\r\n>\r\n+ 1 provided you license your work as AGPLv3\r\n+ 2 provided you license your work as AGPLv3\r\n+ 3 provided you license your work as AGPLv3\r\n+ 4 provided you license your work as AGPLv3\r\n\r\n> D. Enterprise edition with a contractual education license\r\n>\r\n+ 1\r\n+ 2\r\n+ 3\r\n+ 4* *(pending legal review from Neo team)\r\n\r\nSince AGPLv3 would place undesirable restrictions on our work compared to CC0, C is not a good option for us. Therefore, we'll choose between A and D. We will default to A (GPLv3-licensed community edition) unless enterprise features are needed in which case we'll explore D (contractually-licensed enterprise edition). Sticking with the community edition whenever possible will lessen the burden on anyone wanting to replicate or reuse or work.",
      "profile": 17,
      "published": "2016-01-11T19:57:18.731078Z",
      "thread": 130,
      "url": "/discussion/licensing-neo4j/130#2"
    },
    {
      "body_html": "<h1>Results from first two curators</h1>\r\n\r\n<p>The curations from <a href=\"https://github.com/dhimmel/indications/blob/2e4d9be7ef911e48fdd10d99628b22098010d935/curation/ajg/curation-AJG.tsv\" title=\"Ari Green's classifications\">AJG</a> and <a href=\"https://github.com/dhimmel/indications/blob/2e4d9be7ef911e48fdd10d99628b22098010d935/curation/csh/curation-CSH.csv\" title=\"Christine Hessler classifications\">CSH</a> (<a href=\"/u/chrissyhessler\" class=\"username\">@chrissyhessler</a>) are in. Both curators went through and classified each of the 1,388 compound–disease pairs. The breakdown of their classifications are as follows:</p>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th>class</th><th>AJG</th><th>CSH</th><th>AJG (as %)</th><th>CSH (as %)</th></tr></thead><tbody><tr><td>DM</td><td>599</td><td>593</td><td>43.2%</td><td>42.7%</td></tr><tr><td>SYM</td><td>514</td><td>517</td><td>37.0%</td><td>37.2%</td></tr><tr><td>NOT</td><td>275</td><td>278</td><td>19.8%</td><td>20.0%</td></tr></tbody></table>\r\n\r\n<p>Compared to the <a href=\"#3\">pilot</a>, the curators classified a higher percentage of pairs as non-indications, while classifying a lower percentage as disease-modifying. Similar to the pilot, the curators agreed 68.0% percent of the time. The Cohen's kappa coefficient <span class=\"citation\">[<a href=\"/doi/10.1177/001316446002000104\" class=\"citation\" data-key=\"10.1177/001316446002000104\">1</a>, <a href=\"/doi/10.11613/BM.2012.031\" class=\"citation\" data-key=\"10.11613/BM.2012.031\">2</a>]</span> between AJG and CSH was 49.9% (<a href=\"https://github.com/dhimmel/indications/blob/2e4d9be7ef911e48fdd10d99628b22098010d935/curation/tiebreaker-template.ipynb\">notebook</a>), indicating <a href=\"http://www.stfm.org/fmhub/fm2005/May/Anthony360.pdf#page=3\">moderate</a> agreement.</p>\r\n\r\n<p>Looking at only the 944 agreements, there were 447 disease-modifying, 351 symptomatic, and 146 non-indications. For the remaining <a href=\"https://github.com/dhimmel/indications/blob/2e4d9be7ef911e48fdd10d99628b22098010d935/curation/pk/template-pk.tsv\">444 disagreements</a>, we plan to have a third curator break the tie.</p>",
      "body_md": "# Results from first two curators\r\n\r\nThe curations from [AJG](https://github.com/dhimmel/indications/blob/2e4d9be7ef911e48fdd10d99628b22098010d935/curation/ajg/curation-AJG.tsv \"Ari Green's classifications\") and [CSH](https://github.com/dhimmel/indications/blob/2e4d9be7ef911e48fdd10d99628b22098010d935/curation/csh/curation-CSH.csv \"Christine Hessler classifications\") (@chrissyhessler) are in. Both curators went through and classified each of the 1,388 compound--disease pairs. The breakdown of their classifications are as follows:\r\n\r\n| class | AJG | CSH | AJG (as %) | CSH (as %) |\r\n|-------|-----|-----|------------|------------|\r\n| DM | 599 | 593 | 43.2% | 42.7% |\r\n| SYM | 514 | 517 | 37.0% | 37.2% |\r\n| NOT | 275 | 278 | 19.8% | 20.0% |\r\n\r\nCompared to the [pilot](#3), the curators classified a higher percentage of pairs as non-indications, while classifying a lower percentage as disease-modifying. Similar to the pilot, the curators agreed 68.0% percent of the time. The Cohen's kappa coefficient [@10.1177/001316446002000104 @10.11613/BM.2012.031] between AJG and CSH was 49.9% ([notebook](https://github.com/dhimmel/indications/blob/2e4d9be7ef911e48fdd10d99628b22098010d935/curation/tiebreaker-template.ipynb)), indicating [moderate](http://www.stfm.org/fmhub/fm2005/May/Anthony360.pdf#page=3) agreement.\r\n\r\nLooking at only the 944 agreements, there were 447 disease-modifying, 351 symptomatic, and 146 non-indications. For the remaining [444 disagreements](https://github.com/dhimmel/indications/blob/2e4d9be7ef911e48fdd10d99628b22098010d935/curation/pk/template-pk.tsv), we plan to have a third curator break the tie.",
      "profile": 17,
      "published": "2016-01-27T22:28:02.019578Z",
      "thread": 95,
      "url": "/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95#5"
    },
    {
      "body_html": "<h1>Recruitment of a third curator</h1>\r\n\r\n<p>We have recruited a third curator, Pouya Khankhanian, to break ties. Pouya is a resident physician in neurology at Penn. He received his in MD at UCSF.</p>\r\n\r\n<p>When learning of the task and before seeing this discussion and the three classifications, Pouya asked a few questions about what qualifies as an indication. While some of these may be cleared up by the <a href=\"#4\">definitions</a> we decided on, I thought it would be helpful to post his questions here along my opinions.</p>\r\n\r\n<blockquote><p>Suppose a drug was indicated for treatment of seizures 10 years ago, but now we have a new drug that is more efficacious and has fewer side effects, so the old drug is no longer \"indicated\" in clinical practice. However, I suppose for the purposes of your study you would still want to call this \"indicated\"?</p></blockquote>\r\n\r\n<p>I would not disqualify this indication because it's no longer optimal. It still treats the disease and will therefore be helpful in training and validating our model.</p>\r\n\r\n<blockquote><p>Or, suppose a drug is clinically used for a disease, but is not actually indicated. Classic example is that almost all of our MS drugs are \"not indicated\" for treatment of progressive MS (meaning no trial has ever shown efficacy), but as you know most of our progressive MS patients get treated.</p></blockquote>\r\n\r\n<p>Off-label usages are acceptable as long as there's <em>reasonable evidence</em> of efficacy from a clinical perspective.</p>\r\n\r\n<blockquote><p>Or suppose a drug is like fifth line, and is only indicated if someone is medically refractory (i.e. they have failed the first four lines of drugs). Would you consider this \"indicated\"?</p></blockquote>\r\n\r\n<p>I don't think being far down the line should be a disqualifying factor for the reasons above.</p>",
      "body_md": "# Recruitment of a third curator\r\n\r\nWe have recruited a third curator, Pouya Khankhanian, to break ties. Pouya is a resident physician in neurology at Penn. He received his in MD at UCSF.\r\n\r\nWhen learning of the task and before seeing this discussion and the three classifications, Pouya asked a few questions about what qualifies as an indication. While some of these may be cleared up by the [definitions](#4) we decided on, I thought it would be helpful to post his questions here along my opinions.\r\n\r\n> Suppose a drug was indicated for treatment of seizures 10 years ago, but now we have a new drug that is more efficacious and has fewer side effects, so the old drug is no longer \"indicated\" in clinical practice. However, I suppose for the purposes of your study you would still want to call this \"indicated\"?\r\n\r\nI would not disqualify this indication because it's no longer optimal. It still treats the disease and will therefore be helpful in training and validating our model.\r\n\r\n> Or, suppose a drug is clinically used for a disease, but is not actually indicated. Classic example is that almost all of our MS drugs are \"not indicated\" for treatment of progressive MS (meaning no trial has ever shown efficacy), but as you know most of our progressive MS patients get treated.\r\n\r\nOff-label usages are acceptable as long as there's _reasonable evidence_ of efficacy from a clinical perspective.\r\n\r\n> Or suppose a drug is like fifth line, and is only indicated if someone is medically refractory (i.e. they have failed the first four lines of drugs). Would you consider this \"indicated\"?\r\n\r\nI don't think being far down the line should be a disqualifying factor for the reasons above.",
      "profile": 17,
      "published": "2016-01-28T00:06:41.656343Z",
      "thread": 95,
      "url": "/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95#6"
    },
    {
      "body_html": "<p>Anaïs Baudot, coauthor of the <em>PeerJ</em> paper <span class=\"citation\">[<a href=\"/doi/10.7717/peerj.1525\" class=\"citation\" data-key=\"10.7717/peerj.1525\">1</a>]</span> mentioned in my <a href=\"#5\">previous post</a>, informed me of a fantastically thorough article <span class=\"citation\">[<a href=\"/doi/10.1093/comnet/cnu016\" class=\"citation\" data-key=\"10.1093/comnet/cnu016\">2</a>]</span> on the terminology of complex networks. Quoting from the article's introduction:</p>\r\n\r\n<blockquote><p>In the last couple of years, it has suddenly become very fashionable to study networks with multiple layers (or multiple types of edges) and networks of networks. Unfortunately, the sudden and immense explosion of papers on multilayer networks has produced an equally immense explosion of disparate terminology, and the lack of a consensus (or even generally accepted) set of terminology and mathematical framework for studying multilayer networks is extremely problematic. Additionally, research on generalizing monoplex-network concepts such as degree, transitivity, centrality and diffusion is only in its infancy. We also expect that it will be necessary to define many concepts that are intrinsic to multilayer networks.</p></blockquote>\r\n\r\n<p>This study adopts the term \"multilayer network\" as a general term for networks with multiple layers. From my understanding a layer is an edge type. However, the definition gets quite technical — the study is coming from a math/physics angle. So while the term multilayer definitely encompasses our concept of a hetnet, I think it fails in terms of simplicity. We like the term heterogeneous because it expresses the underlying and distinguishing feature of our networks: different types. Multilayer feels less intuitive: not everyone conceptualizes different types as layers.</p>\r\n\r\n<h2>A simple definition of hetnet</h2>\r\n\r\n<p>Whereas the authors of <em>Multilayer networks</em> have nailed the technical details, I think it's important to have easily accessible definitions to unite the field and help it grow. Therefore I propose the following simple and encompassing definition of hetnet:</p>\r\n\r\n<blockquote><p><strong>hetnet</strong> — a network with multiple node or edge types</p></blockquote>\r\n\r\n<p>Depending on your field, 'network' can be replaced with 'graph', 'node' with 'vertex' or 'entity', and 'edge' with 'link', 'arc', or 'relationship'.</p>\r\n\r\n<h2>The terminology nightmare</h2>\r\n\r\n<p>The authors of <em>Multilayer networks</em> <span class=\"citation\">[<a href=\"/doi/10.1093/comnet/cnu016\" class=\"citation\" data-key=\"10.1093/comnet/cnu016\">2</a>]</span> performed an extremely thorough review of existing terminology. I reproduced their Table 1 of network types that multilayer networks encompass below because it does a great job illustrating the terminology nightmare we face. Not only are there many names for the same concept, but the same name often refers to many concepts. Second, I wanted to make their extensive compilation of references extra accessible. One contributing factor to the lack of standards is poor communication between fields. I'm hoping this <em>Thinklab</em> discussion will help bridge the gaps. And what better way to start the ball rolling than by citing the studies that paved the way for the hetnet.</p>\r\n\r\n<p>The columns are defined as follows (see the study <span class=\"citation\">[<a href=\"/doi/10.1093/comnet/cnu016\" class=\"citation\" data-key=\"10.1093/comnet/cnu016\">2</a>]</span> for more information):</p>\r\n\r\n<blockquote><ul><li><strong>Aligned</strong>: Is the network node-aligned (all nodes are shared between all layers)?</li><li><strong>Disj.</strong>: Is the network layer-disjoint (each node is present only in a single layer)?</li><li><strong>Eq. Size</strong>: Do all of the layers have the same number of nodes?</li><li><strong>Diag.</strong>: Are the couplings diagonal?</li><li><strong>Lcoup.</strong>: Do the inter-layer couplings consist of layer couplings?</li><li><strong>Cat.</strong>: Are the inter-layer couplings categorical?</li><li><strong>|<em>L</em>|</strong> denotes the number of possible layers</li><li><strong><em>d</em></strong> denotes the number of ‘aspects’ (i.e. the ‘dimensionality’ of the layers)</li></ul></blockquote>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th>Name</th><th>Aligned</th><th>Disj.</th><th>Eq. Size</th><th>Diag.</th><th>Lcoup.</th><th>Cat.</th><th>|<em>L</em>|</th><th><em>d</em></th><th>Example refs.</th></tr></thead><tbody><tr><td>Multilayer network</td><td></td><td></td><td></td><td>✓</td><td>✓</td><td>✓</td><td>Any</td><td>1</td><td><span class=\"citation\">[<a href=\"/doi/10.1140/epjst/e2013-01712-8\" class=\"citation\" data-key=\"68\">3</a>]</span></td></tr><tr><td></td><td>✓</td><td></td><td>✓</td><td></td><td></td><td></td><td>Any</td><td>1</td><td><span class=\"citation\">[<a href=\"/doi/10.1103/physrevx.3.041022\" class=\"citation\" data-key=\"67\">4</a>]</span></td></tr><tr><td>Multiplex network</td><td>✓</td><td></td><td>✓</td><td>✓</td><td></td><td></td><td>Any</td><td>1</td><td><span class=\"citation\">[<a href=\"/doi/10.1073/pnas.1318469111\" class=\"citation\" data-key=\"69\">5</a>, <a href=\"/doi/10.1103/physrevx.3.041022\" class=\"citation\" data-key=\"67\">4</a>]</span></td></tr><tr><td></td><td>✓</td><td></td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>Any</td><td>1</td><td><span class=\"citation\">[<a href=\"/doi/10.1103/PhysRevE.86.036103\" class=\"citation\" data-key=\"70\">6</a>, <a href=\"/doi/10.1103/PhysRevE.85.045102\" class=\"citation\" data-key=\"71\">7</a>, <a href=\"/doi/10.1103/PhysRevLett.111.058701\" class=\"citation\" data-key=\"72\">8</a>, <a href=\"/doi/10.1103/PhysRevE.87.062806\" class=\"citation\" data-key=\"73\">9</a>, <a href=\"/doi/10.1103/PhysRevE.88.052811\" class=\"citation\" data-key=\"74\">10</a>, <a href=\"/doi/10.1103/PhysRevE.89.032804\" class=\"citation\" data-key=\"75\">11</a>, <a href=\"/doi/10.1109/asonam.2012.101\" class=\"citation\" data-key=\"76\">12</a>]</span></td></tr><tr><td></td><td>✓</td><td></td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>2</td><td>1</td><td><span class=\"citation\">[<a href=\"http://arxiv.org/abs/1307.2967\" class=\"citation\" data-key=\"77\">13</a>, <a href=\"/doi/10.1088/1367-2630/14/3/033027\" class=\"citation\" data-key=\"78\">14</a>, <a href=\"/doi/10.1103/PhysRevE.89.042811\" class=\"citation\" data-key=\"79\">15</a>]</span></td></tr><tr><td></td><td></td><td></td><td></td><td>✓</td><td>✓</td><td>✓</td><td>Any</td><td>1</td><td><span class=\"citation\">[<a href=\"/doi/10.1103/PhysRevE.86.036115\" class=\"citation\" data-key=\"80\">16</a>]</span></td></tr><tr><td></td><td>✓</td><td></td><td>✓</td><td>✓</td><td>✓</td><td></td><td>Any</td><td>1</td><td><span class=\"citation\">[<a href=\"/doi/10.1103/PhysRevE.88.032807\" class=\"citation\" data-key=\"81\">17</a>, <a href=\"/doi/10.1103/PhysRevE.88.050801\" class=\"citation\" data-key=\"82\">18</a>, <a href=\"/doi/10.1063/1.4818544\" class=\"citation\" data-key=\"83\">19</a>]</span></td></tr><tr><td>Multivariate network</td><td>✓</td><td></td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>Any</td><td>1</td><td><span class=\"citation\">[<a href=\"/doi/10.1348/000711099159053\" class=\"citation\" data-key=\"31\">20</a>]</span></td></tr><tr><td>Multinetwork</td><td>✓</td><td></td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>Any</td><td>1</td><td><span class=\"citation\">[<a href=\"/doi/10.1103/PhysRevE.81.046104\" class=\"citation\" data-key=\"84\">21</a>]</span></td></tr><tr><td></td><td>✓</td><td></td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>Any</td><td>2</td><td><span class=\"citation\">[<a href=\"/doi/10.1016/j.physa.2011.02.004\" class=\"citation\" data-key=\"85\">22</a>]</span></td></tr><tr><td>Multirelational network</td><td>✓</td><td></td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>Any</td><td>1</td><td><span class=\"citation\">[<a href=\"/doi/10.1017/cbo9780511815478\" class=\"citation\" data-key=\"2\">23</a>, <a href=\"/doi/10.1007/11564126_44\" class=\"citation\" data-key=\"50\">24</a>, <a href=\"/doi/10.1109/asonam.2012.100\" class=\"citation\" data-key=\"86\">25</a>, <a href=\"/doi/10.1109/cse.2009.69\" class=\"citation\" data-key=\"87\">26</a>]</span></td></tr><tr><td>Multirelational data</td><td>✓</td><td></td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>Any</td><td>1</td><td><span class=\"citation\">[<a href=\"/doi/10.1137/1.9781611972825.13\" class=\"citation\" data-key=\"88\">27</a>, <a href=\"/doi/10.1145/2020408.2020594\" class=\"citation\" data-key=\"89\">28</a>]</span></td></tr><tr><td>Multilayered network</td><td>✓</td><td></td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>Any</td><td>1</td><td><span class=\"citation\">[<a href=\"/doi/10.1063/1.4818544\" class=\"citation\" data-key=\"83\">19</a>, <a href=\"/doi/10.1007/978-3-642-16318-0_27\" class=\"citation\" data-key=\"90\">29</a>, <a href=\"/doi/10.1109/asonam.2011.67\" class=\"citation\" data-key=\"91\">30</a>, <a href=\"/doi/10.1080/18756891.2012.696922\" class=\"citation\" data-key=\"92\">31</a>]</span></td></tr><tr><td>Multidimensional network</td><td>✓</td><td></td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>Any</td><td>1</td><td><span class=\"citation\">[<a href=\"/doi/10.1016/j.jocs.2011.05.009\" class=\"citation\" data-key=\"93\">32</a>, <a href=\"/doi/10.1007/s10618-013-0331-0\" class=\"citation\" data-key=\"94\">33</a>, <a href=\"/doi/10.1007/s11280-012-0190-4\" class=\"citation\" data-key=\"95\">34</a>, <a href=\"/doi/10.1007/s10618-011-0231-0\" class=\"citation\" data-key=\"96\">35</a>, <a href=\"/doi/10.1098/rstb.2012.0113\" class=\"citation\" data-key=\"97\">36</a>, <a href=\"/doi/10.1109/TSMCA.2011.2132707\" class=\"citation\" data-key=\"98\">37</a>, <a href=\"/doi/10.1145/2492517.2492537\" class=\"citation\" data-key=\"99\">38</a>]</span></td></tr><tr><td></td><td>✓</td><td></td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>Any</td><td>3</td><td><span class=\"citation\">[<a href=\"/doi/10.1007/978-3-642-23935-9_37\" class=\"citation\" data-key=\"100\">39</a>]</span></td></tr><tr><td>Multislice network</td><td>✓</td><td></td><td>✓</td><td>✓</td><td></td><td></td><td>Any</td><td>1</td><td><span class=\"citation\">[<a href=\"/doi/10.1126/science.1184819\" class=\"citation\" data-key=\"66\">40</a>, <a href=\"/doi/10.1063/1.3518696\" class=\"citation\" data-key=\"101\">41</a>, <a href=\"/doi/10.1007/978-3-642-25501-4_19\" class=\"citation\" data-key=\"102\">42</a>, <a href=\"/doi/10.1063/1.4790830\" class=\"citation\" data-key=\"103\">43</a>]</span></td></tr><tr><td>Multiplex of interdependent networks</td><td>✓</td><td></td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>Any</td><td>1</td><td><span class=\"citation\">[<a href=\"/doi/10.1038/srep00620\" class=\"citation\" data-key=\"104\">44</a>]</span></td></tr><tr><td>Hypernetwork</td><td>✓</td><td></td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>Any</td><td>1</td><td><span class=\"citation\">[<a href=\"/doi/10.1103/PhysRevE.86.056102\" class=\"citation\" data-key=\"105\">45</a>, <a href=\"/doi/10.1088/1367-2630/14/3/033035\" class=\"citation\" data-key=\"106\">46</a>]</span></td></tr><tr><td>Overlay network</td><td>✓</td><td></td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>2</td><td>1</td><td><span class=\"citation\">[<a href=\"/doi/10.1103/PhysRevE.81.036118\" class=\"citation\" data-key=\"107\">47</a>, <a href=\"/doi/10.1103/PhysRevE.84.026105\" class=\"citation\" data-key=\"108\">48</a>]</span></td></tr><tr><td>Composite network</td><td>✓</td><td></td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>2</td><td>1</td><td><span class=\"citation\">[<a href=\"/doi/10.1145/2378956.2378958\" class=\"citation\" data-key=\"109\">49</a>]</span></td></tr><tr><td>Multilevel network</td><td></td><td>✓</td><td></td><td></td><td></td><td></td><td>Any</td><td>1</td><td><span class=\"citation\">[<a href=\"/doi/10.1016/j.socnet.2013.01.004\" class=\"citation\" data-key=\"110\">50</a>, <a href=\"/doi/10.1016/j.socnet.2008.02.001\" class=\"citation\" data-key=\"111\">51</a>]</span></td></tr><tr><td></td><td></td><td></td><td></td><td>✓</td><td>✓</td><td>✓</td><td>Any</td><td>1</td><td><span class=\"citation\">[<a href=\"/doi/10.1103/PhysRevE.86.036115\" class=\"citation\" data-key=\"80\">16</a>, <a href=\"/doi/10.1080/00207160.2011.577212\" class=\"citation\" data-key=\"112\">52</a>]</span></td></tr><tr><td>Multiweighted graph</td><td>✓</td><td></td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>Any</td><td>1</td><td><span class=\"citation\">[<a href=\"/doi/10.1080/15427951.2012.678191\" class=\"citation\" data-key=\"113\">53</a>]</span></td></tr><tr><td>Heterogeneous network</td><td></td><td>✓</td><td></td><td></td><td></td><td></td><td>2</td><td>1</td><td><span class=\"citation\">[<a href=\"/doi/10.1109/icdm.2007.57\" class=\"citation\" data-key=\"49\">54</a>, <a href=\"/doi/10.1007/11564126_44\" class=\"citation\" data-key=\"50\">24</a>]</span></td></tr><tr><td>Multitype network</td><td></td><td>✓</td><td></td><td></td><td></td><td></td><td>Any</td><td>1</td><td><span class=\"citation\">[<a href=\"/doi/10.1103/PhysRevE.88.012809\" class=\"citation\" data-key=\"114\">55</a>, <a href=\"/doi/10.1103/PhysRevE.79.036113\" class=\"citation\" data-key=\"115\">56</a>, <a href=\"/doi/10.1103/PhysRevE.74.066114\" class=\"citation\" data-key=\"65\">57</a>]</span></td></tr><tr><td>Interconnected networks</td><td></td><td>✓</td><td>✓</td><td></td><td></td><td></td><td>2</td><td>1</td><td><span class=\"citation\">[<a href=\"/doi/10.1103/PhysRevE.85.066109\" class=\"citation\" data-key=\"116\">58</a>, <a href=\"/doi/10.1038/srep03289\" class=\"citation\" data-key=\"117\">59</a>]</span></td></tr><tr><td></td><td></td><td>✓</td><td></td><td></td><td></td><td></td><td>2</td><td>1</td><td><span class=\"citation\">[<a href=\"/doi/10.1103/PhysRevE.86.026106\" class=\"citation\" data-key=\"118\">60</a>, <a href=\"/doi/10.1109/acc.2013.6580178\" class=\"citation\" data-key=\"119\">61</a>]</span></td></tr><tr><td>Interdependent networks</td><td></td><td>✓</td><td>✓</td><td></td><td></td><td></td><td>2</td><td>1</td><td><span class=\"citation\">[<a href=\"/doi/10.1038/nature08932\" class=\"citation\" data-key=\"57\">62</a>]</span></td></tr><tr><td></td><td></td><td>✓</td><td></td><td></td><td></td><td></td><td>2</td><td>1</td><td><span class=\"citation\">[<a href=\"/doi/10.1103/PhysRevLett.105.048701\" class=\"citation\" data-key=\"120\">63</a>]</span></td></tr><tr><td></td><td></td><td></td><td>✓</td><td></td><td></td><td></td><td>2</td><td>1</td><td><span class=\"citation\">[<a href=\"http://arxiv.org/abs/1304.4731\" class=\"citation\" data-key=\"121\">64</a>]</span></td></tr><tr><td></td><td></td><td>✓</td><td></td><td></td><td></td><td></td><td>2</td><td>1</td><td><span class=\"citation\">[<a href=\"/doi/10.1073/pnas.1110586109\" class=\"citation\" data-key=\"122\">65</a>, <a href=\"/doi/10.1038/nphys2727\" class=\"citation\" data-key=\"123\">66</a>]</span></td></tr><tr><td></td><td>✓</td><td></td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>Any</td><td>1</td><td><span class=\"citation\">[<a href=\"/doi/10.1103/PhysRevLett.109.248701\" class=\"citation\" data-key=\"124\">67</a>]</span></td></tr><tr><td>Partially interdependent networks</td><td></td><td>✓</td><td></td><td></td><td></td><td></td><td>2</td><td>1</td><td><span class=\"citation\">[<a href=\"/doi/10.1103/PhysRevE.87.052812\" class=\"citation\" data-key=\"125\">68</a>]</span></td></tr><tr><td>Network of networks</td><td></td><td></td><td>✓</td><td></td><td></td><td></td><td>Any</td><td>1</td><td><span class=\"citation\">[<a href=\"/doi/10.1103/PhysRevLett.107.195701\" class=\"citation\" data-key=\"126\">69</a>]</span></td></tr><tr><td>Coupled networks</td><td></td><td></td><td></td><td>✓</td><td>✓</td><td>✓</td><td>Any</td><td>1</td><td><span class=\"citation\">[<a href=\"/doi/10.1109/JSAC.2013.130606\" class=\"citation\" data-key=\"127\">70</a>]</span></td></tr><tr><td>Interconnecting networks</td><td></td><td></td><td></td><td>✓</td><td>✓</td><td>✓</td><td>2</td><td>1</td><td><span class=\"citation\">[<a href=\"/doi/10.1209/0295-5075/93/68002\" class=\"citation\" data-key=\"128\">71</a>]</span></td></tr><tr><td>Interacting networks</td><td></td><td>✓</td><td></td><td></td><td></td><td></td><td>Any</td><td>1</td><td><span class=\"citation\">[<a href=\"http://arxiv.org/abs/0907.0894\" class=\"citation\" data-key=\"56\">72</a>, <a href=\"/doi/10.1140/epjb/e2011-10795-8\" class=\"citation\" data-key=\"129\">73</a>]</span></td></tr><tr><td></td><td></td><td>✓</td><td></td><td></td><td></td><td></td><td>2</td><td>1</td><td><span class=\"citation\">[<a href=\"/doi/10.1073/pnas.1110586109\" class=\"citation\" data-key=\"122\">65</a>]</span></td></tr><tr><td>Heterogenous information network</td><td></td><td></td><td></td><td></td><td></td><td></td><td>Any</td><td>2</td><td><span class=\"citation\">[<a href=\"/doi/10.1145/2481244.2481248\" class=\"citation\" data-key=\"51\">74</a>, <a href=\"/doi/10.1109/asonam.2011.107\" class=\"citation\" data-key=\"130\">75</a>, <a href=\"http://www-dev.ccs.neu.edu/home/yzsun/papers/vldb11_topKSim.pdf\" class=\"citation\" data-key=\"131\">76</a>, <a href=\"http://hdl.handle.net/2142/42366\" class=\"citation\" data-key=\"132\">77</a>]</span></td></tr><tr><td></td><td></td><td>✓</td><td></td><td></td><td></td><td></td><td>Any</td><td>1</td><td><span class=\"citation\">[<a href=\"/doi/10.1145/1557019.1557107\" class=\"citation\" data-key=\"133\">78</a>]</span></td></tr><tr><td>Meta-matrix, meta-network</td><td></td><td></td><td></td><td></td><td></td><td></td><td>Any</td><td>2</td><td><span class=\"citation\">[<a href=\"/doi/10.1140/epjst/e2013-01712-8\" class=\"citation\" data-key=\"34\">79</a>, <a href=\"/doi/10.1016/j.dss.2006.04.003\" class=\"citation\" data-key=\"134\">80</a>, <a href=\"http://oai.dtic.mil/oai/oai?verb=getRecord&metadataPrefix=html&identifier=ADA459444\" class=\"citation\" data-key=\"135\">81</a>]</span></td></tr></tbody></table>\r\n\r\n",
      "body_md": "Anaïs Baudot, coauthor of the _PeerJ_ paper [@10.7717/peerj.1525] mentioned in my [previous post](#5), informed me of a fantastically thorough article [@10.1093/comnet/cnu016] on the terminology of complex networks. Quoting from the article's introduction:\r\n\r\n> In the last couple of years, it has suddenly become very fashionable to study networks with multiple layers (or multiple types of edges) and networks of networks. Unfortunately, the sudden and immense explosion of papers on multilayer networks has produced an equally immense explosion of disparate terminology, and the lack of a consensus (or even generally accepted) set of terminology and mathematical framework for studying multilayer networks is extremely problematic. Additionally, research on generalizing monoplex-network concepts such as degree, transitivity, centrality and diffusion is only in its infancy. We also expect that it will be necessary to define many concepts that are intrinsic to multilayer networks.\r\n\r\nThis study adopts the term \"multilayer network\" as a general term for networks with multiple layers. From my understanding a layer is an edge type. However, the definition gets quite technical -- the study is coming from a math/physics angle. So while the term multilayer definitely encompasses our concept of a hetnet, I think it fails in terms of simplicity. We like the term heterogeneous because it expresses the underlying and distinguishing feature of our networks: different types. Multilayer feels less intuitive: not everyone conceptualizes different types as layers.\r\n\r\n## A simple definition of hetnet\r\n\r\nWhereas the authors of _Multilayer networks_ have nailed the technical details, I think it's important to have easily accessible definitions to unite the field and help it grow. Therefore I propose the following simple and encompassing definition of hetnet:\r\n\r\n> **hetnet** -- a network with multiple node or edge types\r\n\r\nDepending on your field, 'network' can be replaced with 'graph', 'node' with 'vertex' or 'entity', and 'edge' with 'link', 'arc', or 'relationship'.\r\n\r\n## The terminology nightmare\r\n\r\nThe authors of _Multilayer networks_ [@10.1093/comnet/cnu016] performed an extremely thorough review of existing terminology. I reproduced their Table 1 of network types that multilayer networks encompass below because it does a great job illustrating the terminology nightmare we face. Not only are there many names for the same concept, but the same name often refers to many concepts. Second, I wanted to make their extensive compilation of references extra accessible. One contributing factor to the lack of standards is poor communication between fields. I'm hoping this _Thinklab_ discussion will help bridge the gaps. And what better way to start the ball rolling than by citing the studies that paved the way for the hetnet.\r\n\r\nThe columns are defined as follows (see the study [@10.1093/comnet/cnu016] for more information):\r\n\r\n>\r\n+ **Aligned**: Is the network node-aligned (all nodes are shared between all layers)?\r\n+ **Disj.**: Is the network layer-disjoint (each node is present only in a single layer)?\r\n+ **Eq. Size**: Do all of the layers have the same number of nodes?\r\n+ **Diag.**: Are the couplings diagonal?\r\n+ **Lcoup.**: Do the inter-layer couplings consist of layer couplings?\r\n+ **Cat.**: Are the inter-layer couplings categorical?\r\n+ **|_L_|** denotes the number of possible layers\r\n+ **_d_** denotes the number of ‘aspects’ (i.e. the ‘dimensionality’ of the layers)\r\n\r\n| Name | Aligned | Disj. | Eq. Size | Diag. | Lcoup. | Cat. | \\|_L_\\| | _d_ | Example refs. |\r\n|-----------|---------|-------|----------|-------|--------|------|-----|---|-------|\r\n| Multilayer network |  |  |  | ✓ | ✓ | ✓ | Any | 1 | [@68] |\r\n|  | ✓ |  | ✓ |  |  |  | Any | 1 | [@67] |\r\n| Multiplex network | ✓ |  | ✓ | ✓ |  |  | Any | 1 | [@69 @67] |\r\n|  | ✓ |  | ✓ | ✓ | ✓ | ✓ | Any | 1 | [@70 @71 @72 @73 @74 @75 @76] |\r\n|  | ✓ |  | ✓ | ✓ | ✓ | ✓ | 2 | 1 | [@77 @78 @79] |\r\n|  |  |  |  | ✓ | ✓ | ✓ | Any | 1 | [@80] |\r\n|  | ✓ |  | ✓ | ✓ | ✓ |  | Any | 1 | [@81 @82 @83] |\r\n| Multivariate network | ✓ |  | ✓ | ✓ | ✓ | ✓ | Any | 1 | [@31] |\r\n| Multinetwork | ✓ |  | ✓ | ✓ | ✓ | ✓ | Any | 1 | [@84] |\r\n|  | ✓ |  | ✓ | ✓ | ✓ | ✓ | Any | 2 | [@85] |\r\n| Multirelational network | ✓ |  | ✓ | ✓ | ✓ | ✓ | Any | 1 | [@2 @50 @86 @87] |\r\n| Multirelational data | ✓ |  | ✓ | ✓ | ✓ | ✓ | Any | 1 | [@88 @89] |\r\n| Multilayered network | ✓ |  | ✓ | ✓ | ✓ | ✓ | Any | 1 | [@83 @90 @91 @92] |\r\n| Multidimensional network | ✓ |  | ✓ | ✓ | ✓ | ✓ | Any | 1 | [@93 @94 @95 @96 @97 @98 @99] |\r\n|  | ✓ |  | ✓ | ✓ | ✓ | ✓ | Any | 3 | [@100] |\r\n| Multislice network | ✓ |  | ✓ | ✓ |  |  | Any | 1 | [@66 @101 @102 @103] |\r\n| Multiplex of interdependent networks | ✓ |  | ✓ | ✓ | ✓ | ✓ | Any | 1 | [@104] |\r\n| Hypernetwork | ✓ |  | ✓ | ✓ | ✓ | ✓ | Any | 1 | [@105 @106] |\r\n| Overlay network | ✓ |  | ✓ | ✓ | ✓ | ✓ | 2 | 1 | [@107 @108] |\r\n| Composite network | ✓ |  | ✓ | ✓ | ✓ | ✓ | 2 | 1 | [@109] |\r\n| Multilevel network |  | ✓ |  |  |  |  | Any | 1 | [@110 @111] |\r\n|  |  |  |  | ✓ | ✓ | ✓ | Any | 1 | [@80 @112] |\r\n| Multiweighted graph | ✓ |  | ✓ | ✓ | ✓ | ✓ | Any | 1 | [@113] |\r\n| Heterogeneous network |  | ✓ |  |  |  |  | 2 | 1 | [@49 @50] |\r\n| Multitype network |  | ✓ |  |  |  |  | Any | 1 | [@114 @115 @65] |\r\n| Interconnected networks |  | ✓ | ✓ |  |  |  | 2 | 1 | [@116 @117] |\r\n|  |  | ✓ |  |  |  |  | 2 | 1 | [@118 @119] |\r\n| Interdependent networks |  | ✓ | ✓ |  |  |  | 2 | 1 | [@57] |\r\n|  |  | ✓ |  |  |  |  | 2 | 1 | [@120] |\r\n|  |  |  | ✓ |  |  |  | 2 | 1 | [@121] |\r\n|  |  | ✓ |  |  |  |  | 2 | 1 | [@122 @123] |\r\n|  | ✓ |  | ✓ | ✓ | ✓ | ✓ | Any | 1 | [@124] |\r\n| Partially interdependent networks |  | ✓ |  |  |  |  | 2 | 1 | [@125] |\r\n| Network of networks |  |  | ✓ |  |  |  | Any | 1 | [@126] |\r\n| Coupled networks |  |  |  | ✓ | ✓ | ✓ | Any | 1 | [@127] |\r\n| Interconnecting networks |  |  |  | ✓ | ✓ | ✓ | 2 | 1 | [@128] |\r\n| Interacting networks |  | ✓ |  |  |  |  | Any | 1 | [@56 @129] |\r\n|  |  | ✓ |  |  |  |  | 2 | 1 | [@122] |\r\n| Heterogenous information network |  |  |  |  |  |  | Any | 2 | [@51 @130 @131 @132] |\r\n|  |  | ✓ |  |  |  |  | Any | 1 | [@133] |\r\n| Meta-matrix, meta-network |  |  |  |  |  |  | Any | 2 | [@34 @134 @135] |\r\n\r\n[@2]: 10.1017/cbo9780511815478\r\n[@31]: 10.1348/000711099159053\r\n[@34]: 10.1140/epjst/e2013-01712-8\r\n[@49]: 10.1109/icdm.2007.57\r\n[@50]: 10.1007/11564126_44\r\n[@51]: 10.1145/2481244.2481248\r\n[@56]: http://arxiv.org/abs/0907.0894 \"Leicht E. A., D'Souza R. M. Percolation on interacting networks. 2009. arXiv:0907.0894\"\r\n[@57]: 10.1038/nature08932\r\n[@65]: 10.1103/PhysRevE.74.066114\r\n[@66]: 10.1126/science.1184819\r\n[@67]: 10.1103/physrevx.3.041022\r\n[@68]: 10.1140/epjst/e2013-01712-8\r\n[@69]: 10.1073/pnas.1318469111\r\n[@70]: 10.1103/PhysRevE.86.036103\r\n[@71]: 10.1103/PhysRevE.85.045102\r\n[@72]: 10.1103/PhysRevLett.111.058701\r\n[@73]: 10.1103/PhysRevE.87.062806\r\n[@74]: 10.1103/PhysRevE.88.052811\r\n[@75]: 10.1103/PhysRevE.89.032804\r\n[@76]: 10.1109/asonam.2012.101\r\n[@77]: http://arxiv.org/abs/1307.2967 \"Min B., Goh K.-I. Layer-crossing overhead and information spreading in multiplex social networks. 2013. arXiv:1307.2967\"\r\n[@78]: 10.1088/1367-2630/14/3/033027\r\n[@79]: 10.1103/PhysRevE.89.042811\r\n[@80]: 10.1103/PhysRevE.86.036115\r\n[@81]: 10.1103/PhysRevE.88.032807\r\n[@82]: 10.1103/PhysRevE.88.050801\r\n[@83]: 10.1063/1.4818544\r\n[@84]: 10.1103/PhysRevE.81.046104\r\n[@85]: 10.1016/j.physa.2011.02.004\r\n[@86]: 10.1109/asonam.2012.100\r\n[@87]: 10.1109/cse.2009.69\r\n[@88]: 10.1137/1.9781611972825.13\r\n[@89]: 10.1145/2020408.2020594\r\n[@90]: 10.1007/978-3-642-16318-0_27\r\n[@91]: 10.1109/asonam.2011.67\r\n[@92]: 10.1080/18756891.2012.696922\r\n[@93]: 10.1016/j.jocs.2011.05.009\r\n[@94]: 10.1007/s10618-013-0331-0\r\n[@95]: 10.1007/s11280-012-0190-4\r\n[@96]: 10.1007/s10618-011-0231-0\r\n[@97]: 10.1098/rstb.2012.0113\r\n[@98]: 10.1109/TSMCA.2011.2132707\r\n[@99]: 10.1145/2492517.2492537\r\n[@100]: 10.1007/978-3-642-23935-9_37\r\n[@101]: 10.1063/1.3518696\r\n[@102]: 10.1007/978-3-642-25501-4_19\r\n[@103]: 10.1063/1.4790830\r\n[@104]: 10.1038/srep00620\r\n[@105]: 10.1103/PhysRevE.86.056102\r\n[@106]: 10.1088/1367-2630/14/3/033035\r\n[@107]: 10.1103/PhysRevE.81.036118\r\n[@108]: 10.1103/PhysRevE.84.026105\r\n[@109]: 10.1145/2378956.2378958\r\n[@110]: 10.1016/j.socnet.2013.01.004\r\n[@111]: 10.1016/j.socnet.2008.02.001\r\n[@112]: 10.1080/00207160.2011.577212\r\n[@113]: 10.1080/15427951.2012.678191\r\n[@114]: 10.1103/PhysRevE.88.012809\r\n[@115]: 10.1103/PhysRevE.79.036113\r\n[@116]: 10.1103/PhysRevE.85.066109\r\n[@117]: 10.1038/srep03289\r\n[@118]: 10.1103/PhysRevE.86.026106\r\n[@119]: 10.1109/acc.2013.6580178\r\n[@120]: 10.1103/PhysRevLett.105.048701\r\n[@121]: http://arxiv.org/abs/1304.4731 \"Martin-Hernandez J., Wang H., Van Mieghem P., D'Agostino G. On synchronization of interdependent networks. 2013. arXiv:1304.4731\"\r\n[@122]: 10.1073/pnas.1110586109\r\n[@123]: 10.1038/nphys2727\r\n[@124]: 10.1103/PhysRevLett.109.248701\r\n[@125]: 10.1103/PhysRevE.87.052812\r\n[@126]: 10.1103/PhysRevLett.107.195701\r\n[@127]: 10.1109/JSAC.2013.130606\r\n[@128]: 10.1209/0295-5075/93/68002\r\n[@129]: 10.1140/epjb/e2011-10795-8\r\n[@130]: 10.1109/asonam.2011.107\r\n[@131]: http://www-dev.ccs.neu.edu/home/yzsun/papers/vldb11_topKSim.pdf \"Sun Y., Han J., Yan X., Yu P. S., Wu T. PathSim: meta path-based top-k similarity search in heterogeneous information networks. Proceeding of the 2011 International Conference on Very Large Data Based (VLDB 2011) 2011. Seattle, WA.\"\r\n[@132]: http://hdl.handle.net/2142/42366 \"Sun Y. Mining heterogeneous information networks. Ph.D. Thesis 2012. University of Illinois at Urbana-Champaign.\"\r\n[@133]: 10.1145/1557019.1557107\r\n[@134]: 10.1016/j.dss.2006.04.003\r\n[@135]: http://oai.dtic.mil/oai/oai?verb=getRecord&metadataPrefix=html&identifier=ADA459444 \"Tsvetovat M., Reminga J., Carley K. M. DyNetML: interchange format for rich social network data. CASOS Technical Report 2004. Carnegie Mellon University, School of Computer Science, Institute for Software Research International, CMU-ISRI-04-105.\"",
      "profile": 17,
      "published": "2016-01-28T19:43:54.578535Z",
      "thread": 104,
      "url": "/discussion/renaming-heterogeneous-networks-to-a-more-concise-and-catchy-term/104#6"
    },
    {
      "body_html": "<h1>2016 GraphGist Challenge</h1>\r\n\r\n<p>Neo4j is hosting a GraphGist <a href=\"http://portal.graphgist.org/challenge/index.html\" title=\"GraphGist Challenge\">challenge</a>. GraphGists provide the <a href=\"http://portal.graphgist.org/about\" title=\"What is a GraphGist?\">following</a>:</p>\r\n\r\n<blockquote><p>With Neo4j GraphGists you can describe and model your domain in a simple text file (AsciiDoc) and render it as a rich, interactive, database-backed page in any browser. It is perfect to document a specific domain, use-case, question or graph problem.</p></blockquote>\r\n\r\n<p>This years competition is Star Wars themed — a theme we adhered to in <a href=\"http://portal.graphgist.org/graph_gists/drug-repurposing-by-hetnet-relationship-prediction-a-new-hope\" title=\"Drug repurposing by hetnet relationship prediction: a new hope\"><strong>our submission</strong></a>. To give you a taste, our prologue begins with:</p>\r\n\r\n<blockquote><p>A long time ago in a galaxy far, far away…​. It is a dark time for drug discovery. The Empire spends over a billion dollars in R&amp;D per new drug approval. The process takes decades, 9 out of 10 attempts fail, and the cost has been doubling every 9 years since 1970. But, a small band of Rebel scientists pursue an alternative. Using public data and open source software, the Rebels are predicting new uses for existing drugs.</p></blockquote>\r\n\r\n<p>Our goal in creating a submission was twofold. First, we're excited to interact with other members of the neo4j community who are doing complimentary work. Second, we designed the GraphGist to be a good introduction to our project and hetnet relationship prediction in general.</p>",
      "body_md": "# 2016 GraphGist Challenge\r\n\r\nNeo4j is hosting a GraphGist [challenge](http://portal.graphgist.org/challenge/index.html \"GraphGist Challenge\"). GraphGists provide the [following](http://portal.graphgist.org/about \"What is a GraphGist?\"):\r\n\r\n> With Neo4j GraphGists you can describe and model your domain in a simple text file (AsciiDoc) and render it as a rich, interactive, database-backed page in any browser. It is perfect to document a specific domain, use-case, question or graph problem.\r\n\r\nThis years competition is Star Wars themed -- a theme we adhered to in [**our submission**](http://portal.graphgist.org/graph_gists/drug-repurposing-by-hetnet-relationship-prediction-a-new-hope \"Drug repurposing by hetnet relationship prediction: a new hope\"). To give you a taste, our prologue begins with:\r\n\r\n> A long time ago in a galaxy far, far away…​. It is a dark time for drug discovery. The Empire spends over a billion dollars in R&D per new drug approval. The process takes decades, 9 out of 10 attempts fail, and the cost has been doubling every 9 years since 1970. But, a small band of Rebel scientists pursue an alternative. Using public data and open source software, the Rebels are predicting new uses for existing drugs.\r\n\r\nOur goal in creating a submission was twofold. First, we're excited to interact with other members of the neo4j community who are doing complimentary work. Second, we designed the GraphGist to be a good introduction to our project and hetnet relationship prediction in general.",
      "profile": 17,
      "published": "2016-01-29T18:38:18.936111Z",
      "thread": 112,
      "url": "/discussion/using-the-neo4j-graph-database-for-hetnets/112#8"
    },
    {
      "body_html": "<p>After an additional round of emails on February 4, 2016, Jill Mesirov got back to me. Dr. Mesirov was the principal investigator for the MSigDB project while at the Broad but has since moved to UCSD. She mentioned that they received permission to distribute certain parts of the database but that they did not receive permission to pass on those rights. She also added Helga Thorvaldsdottir — the MSigDB project manager at the Broad — to the conversation.</p>\r\n\r\n<p>I responded with the following message:</p>\r\n\r\n<blockquote><p>Dear Dr. Mesirov et al,</p><p>Thanks for the reply and involving Helga. Hopefully, we can now locate the appropriate parties to handle our request.</p><p>I had guessed that non-transferable distribution rights were part of the issue. I appreciate wanting to build the most comprehensive resource, even when that necessitates stricter licensing.</p><p>Do you know which resources forbid downstream distribution? Perhaps we could be given permission to redistribute the unencumbered portions of the database? And for encumbered portions, we could seek the needed additional permissions from the content owners.</p><p>We feel that distribution fulfills an important scientific need. We're integrating over 30 resources into a single network that we envision becoming a widely used community dataset. Much like MSigDB did with gene sets, our network will enable novel analyses that are only possible once the data has been unified into a single resource. Additionally, forbidding distribution has troubling consequences for reproducibility. See for example <a href=\"http://wpo.st/NUj91\" title=\"What happened when a group of researchers tried to repeat a headline-grabbing study\">this instance</a> where data copyright interfered with replication.</p><p>Given these considerations, we would appreciate help in finding a solution that allows us to distribute MSigDB data, even if only a subset of the database.</p><p>Best,<br>Daniel</p></blockquote>\r\n\r\n<p>In short, I asked if they could look into granting us permission to distribute the unencumbered portions of the database. Ms. Thorvaldsdottir responded that they will be meeting with the IP/Licensing team to discuss my request.</p>",
      "body_md": "After an additional round of emails on February 4, 2016, Jill Mesirov got back to me. Dr. Mesirov was the principal investigator for the MSigDB project while at the Broad but has since moved to UCSD. She mentioned that they received permission to distribute certain parts of the database but that they did not receive permission to pass on those rights. She also added Helga Thorvaldsdottir -- the MSigDB project manager at the Broad -- to the conversation.\r\n\r\nI responded with the following message:\r\n\r\n> Dear Dr. Mesirov et al,\r\n\r\n> Thanks for the reply and involving Helga. Hopefully, we can now locate the appropriate parties to handle our request.\r\n\r\n> I had guessed that non-transferable distribution rights were part of the issue. I appreciate wanting to build the most comprehensive resource, even when that necessitates stricter licensing.\r\n\r\n> Do you know which resources forbid downstream distribution? Perhaps we could be given permission to redistribute the unencumbered portions of the database? And for encumbered portions, we could seek the needed additional permissions from the content owners.\r\n\r\n> We feel that distribution fulfills an important scientific need. We're integrating over 30 resources into a single network that we envision becoming a widely used community dataset. Much like MSigDB did with gene sets, our network will enable novel analyses that are only possible once the data has been unified into a single resource. Additionally, forbidding distribution has troubling consequences for reproducibility. See for example [this instance](http://wpo.st/NUj91 \"What happened when a group of researchers tried to repeat a headline-grabbing study\") where data copyright interfered with replication.\r\n\r\n> Given these considerations, we would appreciate help in finding a solution that allows us to distribute MSigDB data, even if only a subset of the database.\r\n\r\n> Best,\r\n> Daniel\r\n\r\nIn short, I asked if they could look into granting us permission to distribute the unencumbered portions of the database. Ms. Thorvaldsdottir responded that they will be meeting with the IP/Licensing team to discuss my request.",
      "profile": 17,
      "published": "2016-02-19T19:49:47.864072Z",
      "thread": 108,
      "url": "/discussion/msigdb-licensing/108#2"
    },
    {
      "body_html": "<h1>Gene handling quality control</h1>\r\n\r\n<p>Currently, we have STARGEO case-control queries for 66 of our diseases. Of these 66 queries, 37 return differential expression results. The rest either have insufficient samples or fail <a href=\"https://github.com/idrdex/star_api/issues/13#issue-123599787\" title=\"idrdex/star_geo#13 Specific failing analyses\">due to errors</a>.</p>\r\n\r\n<p>In the past, I remember coming across a STARGEO output (gene rows × meta-analysis columns) where many rows contained duplicate gene symbols. <a href=\"/u/idrdex\" class=\"username\">@idrdex</a> had also mentioned to me that mapping the probe/gene names deposited in GEO can get complicated. Therefore, I wanted to do a few quality controls before proceeding.</p>\r\n\r\n<p>I checked into our current STARGEO analysis of differential expression for 37 diseases. I looked for three occurrences which could be due to problems with gene handling (<a href=\"https://github.com/dhimmel/stargeo/blob/6c5a348f2ea22edf475dc14a5eed93eb95290d7d/gene-fidelity.ipynb\">notebook</a>):</p>\r\n\r\n<ul><li>GeneID–Symbol mappings that don't exist in Entrez Gene (<a href=\"https://github.com/dhimmel/stargeo/blob/6c5a348f2ea22edf475dc14a5eed93eb95290d7d/diagnose/discord.tsv\">results</a>). There were 2,274 ID-Symbol pairs that didn't exist in my parsing of Entrez Gene. However, most of these were not protein-coding and appeared to stem from updates to the database over time.</li><li>Rows with duplicate GeneIDs (<a href=\"https://github.com/dhimmel/stargeo/blob/6c5a348f2ea22edf475dc14a5eed93eb95290d7d/diagnose/duplicate_ids.tsv\">results</a>). Only two rows were affected by this issue.</li><li>Rows with duplicate symbols (<a href=\"https://github.com/dhimmel/stargeo/blob/6c5a348f2ea22edf475dc14a5eed93eb95290d7d/diagnose/duplicate_symbols.tsv\">results</a>). 72 rows were affected by this issue. It did seem however that many gene symbols were not the approved symbols but rather synonyms. Since we <a href=\"http://thinklab.com/discussion/using-entrez-gene-as-our-gene-vocabulary/34\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d34\">use Entrez Gene</a> IDs for mapping, synonyms are not a major concern for us.</li></ul>\r\n\r\n<p>So in conclusion, I didn't detect any major issues with the gene handling in STARGEO. These quality controls do not assess the probe–gene mapping, but instead whether the gene information reported for the meta-analyses makes sense.</p>",
      "body_md": "# Gene handling quality control\r\n\r\nCurrently, we have STARGEO case-control queries for 66 of our diseases. Of these 66 queries, 37 return differential expression results. The rest either have insufficient samples or fail [due to errors](https://github.com/idrdex/star_api/issues/13#issue-123599787 \"idrdex/star_geo#13 Specific failing analyses\").\r\n\r\nIn the past, I remember coming across a STARGEO output (gene rows × meta-analysis columns) where many rows contained duplicate gene symbols. @idrdex had also mentioned to me that mapping the probe/gene names deposited in GEO can get complicated. Therefore, I wanted to do a few quality controls before proceeding.\r\n\r\nI checked into our current STARGEO analysis of differential expression for 37 diseases. I looked for three occurrences which could be due to problems with gene handling ([notebook](https://github.com/dhimmel/stargeo/blob/6c5a348f2ea22edf475dc14a5eed93eb95290d7d/gene-fidelity.ipynb)):\r\n\r\n+ GeneID--Symbol mappings that don't exist in Entrez Gene ([results](https://github.com/dhimmel/stargeo/blob/6c5a348f2ea22edf475dc14a5eed93eb95290d7d/diagnose/discord.tsv)). There were 2,274 ID-Symbol pairs that didn't exist in my parsing of Entrez Gene. However, most of these were not protein-coding and appeared to stem from updates to the database over time.\r\n+ Rows with duplicate GeneIDs ([results](https://github.com/dhimmel/stargeo/blob/6c5a348f2ea22edf475dc14a5eed93eb95290d7d/diagnose/duplicate_ids.tsv)). Only two rows were affected by this issue.\r\n+ Rows with duplicate symbols ([results](https://github.com/dhimmel/stargeo/blob/6c5a348f2ea22edf475dc14a5eed93eb95290d7d/diagnose/duplicate_symbols.tsv)). 72 rows were affected by this issue. It did seem however that many gene symbols were not the approved symbols but rather synonyms. Since we [use Entrez Gene](http://thinklab.com/discussion/using-entrez-gene-as-our-gene-vocabulary/34) IDs for mapping, synonyms are not a major concern for us.\r\n\r\nSo in conclusion, I didn't detect any major issues with the gene handling in STARGEO. These quality controls do not assess the probe--gene mapping, but instead whether the gene information reported for the meta-analyses makes sense.",
      "profile": 17,
      "published": "2016-02-10T01:59:27.854907Z",
      "thread": 96,
      "url": "/discussion/stargeo-expression-signatures-for-disease-using-crowdsourced-geo-annotation/96#9"
    },
    {
      "body_html": "<h1><em>Nature News</em> mentions our use of <em>Thinklab</em> to avoid publishing delays</h1>\r\n\r\n<p>A <em>Nature News</em> <a href=\"https://doi.org/10.1038/530148a\" title=\"The Waiting Game\">feature</a> published today <span class=\"citation\">[<a href=\"/doi/10.1038/530148a\" class=\"citation\" data-key=\"10.1038/530148a\">1</a>]</span> mentions our Thinklab project:</p>\r\n\r\n<blockquote><p>Some scientists are going a step further, and using platforms such as GitHub, Zenodo and figshare to publish each hypothesis, data collection or figure as they go along. Each file can be given a DOI, so that it is citable and trackable. Himmelstein, who already publishes his papers as preprints, has been using the Thinklab platform to progressively write up and publish the results of a new project since January 2015. “I push 'publish' and it gets a DOI with no delay,” he says. “Am I really gaining that much by publishing [in a conventional journal]? Or is it better to do what is fastest and most efficient to get your research out there?”</p></blockquote>\r\n\r\n<p>The feature also covers my <a href=\"http://blog.dhimmel.com/history-of-delays/\" title=\"The history of publishing delays\">blog post</a> on the history of publishing delays. Using data from PubMed, I found a median time from submission to acceptance of ~100 days and a median time from acceptance to online publication of ~25 days. Since we post most content on <em>Thinklab</em> several months before it will ever be submitted, we're getting our work out 200+ days sooner by using realtime open notebook publishing.</p>",
      "body_md": "# _Nature News_ mentions our use of _Thinklab_ to avoid publishing delays\r\n\r\nA *Nature News* [feature](https://doi.org/10.1038/530148a \"The Waiting Game\") published today [@10.1038/530148a] mentions our Thinklab project:\r\n\r\n> Some scientists are going a step further, and using platforms such as GitHub, Zenodo and figshare to publish each hypothesis, data collection or figure as they go along. Each file can be given a DOI, so that it is citable and trackable. Himmelstein, who already publishes his papers as preprints, has been using the Thinklab platform to progressively write up and publish the results of a new project since January 2015. “I push 'publish' and it gets a DOI with no delay,” he says. “Am I really gaining that much by publishing [in a conventional journal]? Or is it better to do what is fastest and most efficient to get your research out there?”\r\n\r\nThe feature also covers my [blog post](http://blog.dhimmel.com/history-of-delays/ \"The history of publishing delays\") on the history of publishing delays. Using data from PubMed, I found a median time from submission to acceptance of ~100 days and a median time from acceptance to online publication of ~25 days. Since we post most content on *Thinklab* several months before it will ever be submitted, we're getting our work out 200+ days sooner by using realtime open notebook publishing.",
      "profile": 17,
      "published": "2016-02-10T11:59:49.322011Z",
      "thread": 113,
      "url": "/discussion/tracking-project-reuse-citation-and-publicity/113#4"
    },
    {
      "body_html": "<p>We've created a <a href=\"http://thinklab.com/discussion/one-network-to-rule-them-all/102\" title=\"Preliminary hetnet release\">preliminary network</a> with 10 types of nodes (metanodes) and 27 types of edges (metaedges). Now an important detail is naming node and edge types appropriately.</p>\r\n\r\n<p>For each metanode and metaedge, we also need abbreviations. We use the abbreviations to make writing out complete paths less cumbersome. For example, in our previous project, we <a href=\"https://doi.org/10.1371/journal.pcbi.1004259.s010\" title=\"S1 Table. Features · Hetnet-Based Prioritization of Disease Associations\">abbreviated</a> the <code>Gene - interaction - Gene - expression - Tissue - localization - Disease</code> path to <code>GiGeTlD</code> <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1004259.s010\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1004259.s010\">1</a>]</span>.</p>\r\n\r\n<p>We have several conventions for naming and abbreviations, but they haven't been publicly explained or discussed. This discussion is now home to these topics.</p>",
      "body_md": "We've created a [preliminary network](http://thinklab.com/discussion/one-network-to-rule-them-all/102 \"Preliminary hetnet release\") with 10 types of nodes (metanodes) and 27 types of edges (metaedges). Now an important detail is naming node and edge types appropriately.\r\n\r\nFor each metanode and metaedge, we also need abbreviations. We use the abbreviations to make writing out complete paths less cumbersome. For example, in our previous project, we [abbreviated](https://doi.org/10.1371/journal.pcbi.1004259.s010 \"S1 Table. Features · Hetnet-Based Prioritization of Disease Associations\") the `Gene - interaction - Gene - expression - Tissue - localization - Disease` path to `GiGeTlD` [@10.1371/journal.pcbi.1004259.s010].\r\n\r\nWe have several conventions for naming and abbreviations, but they haven't been publicly explained or discussed. This discussion is now home to these topics.",
      "profile": 17,
      "published": "2016-02-17T19:43:44.550722Z",
      "thread": 162,
      "url": "/discussion/data-nomenclature-naming-and-abbreviating-our-network-types/162"
    },
    {
      "body_html": "<h1>Naming according to parts of speech</h1>\r\n\r\n<p>According to <a href=\"https://en.wikipedia.org/w/index.php?title=Entity%E2%80%93relationship_model&amp;oldid=704204795#Mapping_natural_language\" title=\"Mapping natural language · Entity–relationship model · Wikipedia\">Chen's rules of thumb</a>, we should use parts of speech as follows <span class=\"citation\">[<a href=\"/doi/10.1016/s0169-023x(97)00017-7\" class=\"citation\" data-key=\"10.1016/s0169-023x(97)00017-7\">1</a>]</span>:</p>\r\n\r\n<ul><li><em>common nouns</em> for node labels (types)</li><li><em>proper nouns</em> for node names</li><li><em>transitive verbs</em> for relationship (edge) types</li><li><em>intransitive verbs</em> for property (attribute) types</li><li><em>adjectives</em> for node properties</li><li><em>adverbs</em> for relationship properties</li></ul>\r\n\r\n<p>I'm not convinced about the last three, since our properties (data attributes for nodes and relationships) are often highly technical. However, I think we should adhere to the first three rules when possible.</p>\r\n\r\n<p>Our node labels are already common nouns. Our node names are already proper nouns. However, we were using common nouns for relationship types. Thus, I switched to transitive verbs for relationship types (<a href=\"https://github.com/dhimmel/integrate/commit/8ca7c9e971ce5a85c7729b3f1df7db54beb19d18\">commit</a>). The table below shows the noun (old) and verb (new) relationship type.</p>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th>Source</th><th>Target</th><th>Metaedge (noun)</th><th>Metaedge (verb)</th></tr></thead><tbody><tr><td>compound</td><td>gene</td><td>binding</td><td>binds</td></tr><tr><td>compound</td><td>side effect</td><td>causation</td><td>causes</td></tr><tr><td>compound</td><td>gene</td><td>downregulation</td><td>downregulates</td></tr><tr><td>compound</td><td>disease</td><td>indication</td><td>palliates</td></tr><tr><td>compound</td><td>compound</td><td>similarity</td><td>resembles</td></tr><tr><td>compound</td><td>disease</td><td>indication</td><td>treats</td></tr><tr><td>compound</td><td>gene</td><td>upregulation</td><td>upregulates</td></tr><tr><td>disease</td><td>gene</td><td>association</td><td>associates</td></tr><tr><td>disease</td><td>gene</td><td>downregulation</td><td>downregulates</td></tr><tr><td>disease</td><td>anatomy</td><td>localization</td><td>localizes</td></tr><tr><td>disease</td><td>symptom</td><td>presence</td><td>presents</td></tr><tr><td>disease</td><td>disease</td><td>similarity</td><td>resembles</td></tr><tr><td>disease</td><td>gene</td><td>upregulation</td><td>upregulates</td></tr><tr><td>gene</td><td>anatomy</td><td>downregulation</td><td>downregulates</td></tr><tr><td>gene</td><td>gene</td><td>evolution</td><td>evolves</td></tr><tr><td>gene</td><td>anatomy</td><td>expression</td><td>expresses</td></tr><tr><td>gene</td><td>gene</td><td>interaction</td><td>interacts</td></tr><tr><td>gene</td><td>biological process</td><td>participation</td><td>participates</td></tr><tr><td>gene</td><td>cellular component</td><td>participation</td><td>participates</td></tr><tr><td>gene</td><td>molecular function</td><td>participation</td><td>participates</td></tr><tr><td>gene</td><td>pathway</td><td>participation</td><td>participates</td></tr><tr><td>gene</td><td>perturbation</td><td>regulation</td><td>regulates</td></tr><tr><td>gene</td><td>anatomy</td><td>upregulation</td><td>upregulates</td></tr><tr><td>gene</td><td>gene</td><td>knockdown downregulation</td><td>knockdown downregulates</td></tr><tr><td>gene</td><td>gene</td><td>knockdown upregulation</td><td>knockdown upregulates</td></tr><tr><td>gene</td><td>gene</td><td>overexpression downregulation</td><td>overexpression downregulates</td></tr><tr><td>gene</td><td>gene</td><td>overexpression upregulation</td><td>overexpression upregulates</td></tr></tbody></table>\r\n\r\n<p>In several cases, switching from noun to verb cut out several characters — a welcome occurrence. Switching relationship types to verbs also makes sense as part of our <a href=\"http://thinklab.com/discussion/using-the-neo4j-graph-database-for-hetnets/112\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d112\">migration to neo4j</a>. The neo4j convention is to use verbs for relationship types. In fact, a neo4j company <a href=\"https://www.graphstory.com/elements-of-a-graph-database\">explains relationships</a> by saying:</p>\r\n\r\n<blockquote><p>Where nodes can be thought of as nouns, relationships can be thought of as verbs.</p></blockquote>",
      "body_md": "# Naming according to parts of speech\r\n\r\nAccording to [Chen's rules of thumb](https://en.wikipedia.org/w/index.php?title=Entity%E2%80%93relationship_model&oldid=704204795#Mapping_natural_language \"Mapping natural language · Entity–relationship model · Wikipedia\"), we should use parts of speech as follows [@10.1016/s0169-023x(97)00017-7]:\r\n\r\n+ _common nouns_ for node labels (types)\r\n+ _proper nouns_ for node names\r\n+ _transitive verbs_ for relationship (edge) types\r\n+ _intransitive verbs_ for property (attribute) types\r\n+ _adjectives_ for node properties\r\n+ _adverbs_ for relationship properties\r\n\r\nI'm not convinced about the last three, since our properties (data attributes for nodes and relationships) are often highly technical. However, I think we should adhere to the first three rules when possible.\r\n\r\nOur node labels are already common nouns. Our node names are already proper nouns. However, we were using common nouns for relationship types. Thus, I switched to transitive verbs for relationship types ([commit](https://github.com/dhimmel/integrate/commit/8ca7c9e971ce5a85c7729b3f1df7db54beb19d18)). The table below shows the noun (old) and verb (new) relationship type.\r\n\r\n| Source | Target | Metaedge (noun) | Metaedge (verb) |\r\n|----------|------------|--------|------------|\r\n| compound | gene | binding | binds |\r\n| compound | side effect | causation | causes |\r\n| compound | gene | downregulation | downregulates |\r\n| compound | disease | indication | palliates |\r\n| compound | compound | similarity | resembles |\r\n| compound | disease | indication | treats |\r\n| compound | gene | upregulation | upregulates |\r\n| disease | gene | association | associates |\r\n| disease | gene | downregulation | downregulates |\r\n| disease | anatomy | localization | localizes |\r\n| disease | symptom | presence | presents |\r\n| disease | disease | similarity | resembles |\r\n| disease | gene | upregulation | upregulates |\r\n| gene | anatomy | downregulation | downregulates |\r\n| gene | gene | evolution | evolves |\r\n| gene | anatomy | expression | expresses |\r\n| gene | gene | interaction | interacts |\r\n| gene | biological process | participation | participates |\r\n| gene | cellular component | participation | participates |\r\n| gene | molecular function | participation | participates |\r\n| gene | pathway | participation | participates |\r\n| gene | perturbation | regulation | regulates |\r\n| gene | anatomy | upregulation | upregulates |\r\n| gene | gene | knockdown downregulation | knockdown downregulates |\r\n| gene | gene | knockdown upregulation | knockdown upregulates |\r\n| gene | gene | overexpression downregulation | overexpression downregulates |\r\n| gene | gene | overexpression upregulation | overexpression upregulates |\r\n\r\nIn several cases, switching from noun to verb cut out several characters -- a welcome occurrence. Switching relationship types to verbs also makes sense as part of our [migration to neo4j](http://thinklab.com/discussion/using-the-neo4j-graph-database-for-hetnets/112). The neo4j convention is to use verbs for relationship types. In fact, a neo4j company [explains relationships](https://www.graphstory.com/elements-of-a-graph-database) by saying:\r\n\r\n> Where nodes can be thought of as nouns, relationships can be thought of as verbs.",
      "profile": 17,
      "published": "2016-02-17T22:39:04.375084Z",
      "thread": 162,
      "url": "/discussion/data-nomenclature-naming-and-abbreviating-our-network-types/162#2"
    },
    {
      "body_html": "<p>The compound-gene associations are not intuitive to me. I assume that when, for example, a compound downregulates a gene, it is supposed to mean that the compound inhibits the protein product encoded by the gene. However, if read at face value, it would mean that the compound binds to something else that through some signaling results in down-regulation of the gene (i.e. less transcription).</p>\r\n\r\n<p>The gene-gene association \"evolves\" is bit of a misnomer, I think. Unless you are looking at ancestral genes, one gene will not have evolved from another gene. Rather two genes will share ancestry. In that case, the term \"homology\" is would be much clearer. Also, you probably want to be able to distinguish between orthologs and paralogs in your network.</p>\r\n\r\n<p>Are the gene-anatomy relationships not backwards? I can understand what it means that means that the liver \"upregulates\" a gene (I assume it means that the gene is higher expressed in the liver than elsewhere). But I cannot comprehend what it would mean that a gene upregulates the liver.</p>\r\n\r\n<p>Same goes for gene-pertubation relationships. I can understand that a pertubation regulates a gene, but how can a gene regulate a pertubation? And why is this type of association not divided into up- and down-regulation like everything else?</p>\r\n\r\n<p>I am not entirely sure how useful the \"knockdown downregulates\" etc. types are. Usually \"knockdown downregulates\" would be interpreted to mean \"upregulates\" etc.</p>",
      "body_md": "The compound-gene associations are not intuitive to me. I assume that when, for example, a compound downregulates a gene, it is supposed to mean that the compound inhibits the protein product encoded by the gene. However, if read at face value, it would mean that the compound binds to something else that through some signaling results in down-regulation of the gene (i.e. less transcription).\r\n\r\nThe gene-gene association \"evolves\" is bit of a misnomer, I think. Unless you are looking at ancestral genes, one gene will not have evolved from another gene. Rather two genes will share ancestry. In that case, the term \"homology\" is would be much clearer. Also, you probably want to be able to distinguish between orthologs and paralogs in your network.\r\n\r\nAre the gene-anatomy relationships not backwards? I can understand what it means that means that the liver \"upregulates\" a gene (I assume it means that the gene is higher expressed in the liver than elsewhere). But I cannot comprehend what it would mean that a gene upregulates the liver.\r\n\r\nSame goes for gene-pertubation relationships. I can understand that a pertubation regulates a gene, but how can a gene regulate a pertubation? And why is this type of association not divided into up- and down-regulation like everything else?\r\n\r\nI am not entirely sure how useful the \"knockdown downregulates\" etc. types are. Usually \"knockdown downregulates\" would be interpreted to mean \"upregulates\" etc.",
      "profile": 125,
      "published": "2016-02-18T06:21:08.503360Z",
      "thread": 162,
      "url": "/discussion/data-nomenclature-naming-and-abbreviating-our-network-types/162#3"
    },
    {
      "body_html": "<blockquote><p>I assume that when, for example, a compound downregulates a gene, it is supposed to mean that the compound inhibits the protein product encoded by the gene. However, if read at face value, it would mean that the compound binds to something else that through some signaling results in down-regulation of the gene (i.e. less transcription).</p></blockquote>\r\n\r\n<p>Your face value interpretation is correct. Compound–downregulates–Gene means the compound decreases the transcriptional expression of the gene. We <a href=\"http://thinklab.com/discussion/computing-consensus-transcriptional-profiles-for-compounds-using-lincs/43#6\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d43\">extracted these relationships</a> from LINCS L1000.</p>\r\n\r\n<blockquote><p>The gene-gene association \"evolves\" is bit of a misnomer</p></blockquote>\r\n\r\n<p>I agree, \"evolves\" is not good. This edge signifies <a href=\"http://thinklab.com/discussion/selecting-informative-erc-evolutionary-rate-covariation-values-between-genes/57\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d57\">evolutionary rate covariation</a> <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pgen.1004967\" class=\"citation\" data-key=\"10.1371/journal.pgen.1004967\">1</a>]</span>. It's a mouthful, and I don't know the best way to shorten and verbify it. Perhaps \"covaries\" is an improvement?</p>\r\n\r\n<blockquote><p>Are the gene-anatomy relationships not backwards? … Same goes for gene-pertubation relationships.</p></blockquote>\r\n\r\n<p>Great point. We should present these edges in subject-verb-object order. I have switched the default orientation of the confusing metaedges (<a href=\"https://github.com/dhimmel/integrate/commit/3354a4cbb36d184f46e78831fa0f605ff92e7637)\" title=\"GitHub · dhimmel/integrate @ 3354a4cbb36d184f46e78831fa0f605ff92e7637\">commit</a>). In practice the object-verb-subject order may still arise, for example when representing paths.</p>\r\n\r\n<blockquote><p>I am not entirely sure how useful the \"knockdown downregulates\" etc. types are. Usually \"knockdown downregulates\" would be interpreted to mean \"upregulates\" etc.</p></blockquote>\r\n\r\n<p>I will look into collapsing:</p>\r\n\r\n<ul><li><em>knockdown downregulates</em> with <em>overexpression upregulates</em> to create an <em>upregulates</em> edge</li><li><em>knockdown upregulates</em> with <em>overexpression downregulates</em> to create a <em>downregulates</em> edge</li></ul>",
      "body_md": "> I assume that when, for example, a compound downregulates a gene, it is supposed to mean that the compound inhibits the protein product encoded by the gene. However, if read at face value, it would mean that the compound binds to something else that through some signaling results in down-regulation of the gene (i.e. less transcription).\r\n\r\nYour face value interpretation is correct. Compound--downregulates--Gene means the compound decreases the transcriptional expression of the gene. We [extracted these relationships](http://thinklab.com/discussion/computing-consensus-transcriptional-profiles-for-compounds-using-lincs/43#6) from LINCS L1000.\r\n\r\n> The gene-gene association \"evolves\" is bit of a misnomer\r\n\r\nI agree, \"evolves\" is not good. This edge signifies [evolutionary rate covariation](http://thinklab.com/discussion/selecting-informative-erc-evolutionary-rate-covariation-values-between-genes/57) [@10.1371/journal.pgen.1004967]. It's a mouthful, and I don't know the best way to shorten and verbify it. Perhaps \"covaries\" is an improvement?\r\n\r\n> Are the gene-anatomy relationships not backwards? … Same goes for gene-pertubation relationships.\r\n\r\nGreat point. We should present these edges in subject-verb-object order. I have switched the default orientation of the confusing metaedges ([commit](https://github.com/dhimmel/integrate/commit/3354a4cbb36d184f46e78831fa0f605ff92e7637) \"GitHub · dhimmel/integrate @ 3354a4cbb36d184f46e78831fa0f605ff92e7637\")). In practice the object-verb-subject order may still arise, for example when representing paths.\r\n\r\n> I am not entirely sure how useful the \"knockdown downregulates\" etc. types are. Usually \"knockdown downregulates\" would be interpreted to mean \"upregulates\" etc.\r\n\r\nI will look into collapsing:\r\n\r\n+ _knockdown downregulates_ with _overexpression upregulates_ to create an _upregulates_ edge\r\n+ _knockdown upregulates_ with _overexpression downregulates_ to create a _downregulates_ edge",
      "profile": 17,
      "published": "2016-02-18T19:59:49.447133Z",
      "thread": 162,
      "url": "/discussion/data-nomenclature-naming-and-abbreviating-our-network-types/162#4"
    },
    {
      "body_html": "<h1>Initial results from the third curator</h1>\r\n\r\n<p>I was initially recruited to break the 444 disagreements between the other curators. After an initial pilot review of the first 80 or so disagreements, I noted some ambiguity in definition of the three classes that appeared to be giving rise to some of the disagreements. I discussed these with Daniel and we reached a more precise amended set of definitions.</p>\r\n\r\n<h3>Definitions:</h3>\r\n\r\n<ul><li><strong>disease modifying (DM) </strong> — a drug that therapeutically changes the underlying or downstream biology of the disease</li><li><strong>symptomatic (SYM) </strong> — a drug that treats a significant symptom of the disease</li><li><strong>non-indication (NOT) </strong> — a drug that neither therapeutically changes the underlying or downstream biology nor treats a significant symptom of the disease</li></ul>\r\n\r\n<h3>Guidelines:</h3>\r\n\r\n<ul><li><strong>reasonable evidence</strong> of efficacy is required to be classified as disease modifying or symptomatic. This includes off-label use.</li><li>if no classification accurately describes an indication, the <strong>most appropriate</strong> (although imperfect) classification should be chosen</li></ul>\r\n\r\n<h3>Amendments: (created 1/27/16, not seen by AJG and CSH)</h3>\r\n\r\n<ul><li><strong>Amendment 1: </strong> if a drug was <strong>previously indicated, but is no longer used</strong> due to side effects, or because there are better drugs, it is still considered <strong>DM</strong></li><li><strong>Amendment 2: </strong> it <strong>doesn't matter whether it is first line or fifth line</strong>, it's still considered <strong>DM</strong></li></ul>\r\n\r\n<h3>Assumptions: (by PK)</h3>\r\n\r\n<ul><li><strong>Assumption 1: DM trumps SYM. </strong> If a drug is clearly both disease modifying and also treats symptoms, then I will call it disease modifying. This is because most disease modifying drugs also treat symptoms.</li><li><strong>Assumption 2: SYM trumps NOT.</strong> If a drug is clearly symptomatic treatment, but can actually exacerbate the downstream biology of disease, then I chose SYM. I made this choice because this was the choice I saw most often made by AJG and CSH</li></ul>\r\n\r\n<p>With the revised definitions above, I reviewed the 444 disagreements as well as the 944 agreements (and suggested a change on 124 of these). I was not blinded to the other curators' decisions. I was able to see both of their decisions and also any comments they had left regarding their reasoning. In general, my strategy was to look three sources for each drug (unless I clearly already knew that a drug was DM or SYM): uptodate.com, drugbank.ca (link provided by Daniel in the spreadsheet), and a basic google search (which also served as a proxy for a pubmed search). When I noted that one of the two curators was calling an indication which I was not aware of (either DM or SYM), I would do a much more detailed search including a more detailed google search and a direct pubmed search. </p>\r\n\r\n<p>Below is the breakdown of classifications</p>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th>class</th><th>AJG</th><th>CSH</th><th>PK</th><th></th><th>class</th><th>AJG</th><th>CSH</th><th>PK</th></tr></thead><tbody><tr><td>DM</td><td>599</td><td>593</td><td>755</td><td></td><td>DM</td><td>43.2%</td><td>42.7%</td><td>54.4%</td></tr><tr><td>SYM</td><td>514</td><td>517</td><td>390</td><td></td><td>SYM</td><td>37.0%</td><td>37.2%</td><td>28.1%</td></tr><tr><td>NOT</td><td>275</td><td>278</td><td>243</td><td></td><td>NOT</td><td>19.8%</td><td>20.0%</td><td>17.5%</td></tr><tr><td>total</td><td>1388</td><td>1388</td><td>1388</td><td></td><td>total</td><td>100.0%</td><td>100.0%</td><td>100.0%</td></tr></tbody></table>\r\n\r\n<p>The most notable difference was that I called DM more often than the other curators. There are at least two reasons for this. First, I was making use of amendment 1 and amendment 2 to make calls for DM, whereas the other curators were not using these amendments (in fact, when the other curators called NOT, they left comments such as \"no longer recommended due to side effects\", \"not used anymore\", or \"rarely used\"). Second, when I found a disagreement between two curators, I was more likely to agree with the curator who called DM. Specifically, of the 444 disagreements, there were 298 where one curator chose DM; of these 298 instances, I chose DM 204 times. I think this is because I did a more detailed search when I knew that one other curator thought that there was a DM indication. Of note, of the 146 times that the other curators were in disagreement between SYM and NOT, I chose SYM 76 times, I chose NOT 52 times, and I chose DM 18 times, likely for the same reasons described above.</p>\r\n\r\n<p>The <a href=\"https://github.com/dhimmel/indications/blob/d3d57b07abcf8d7af55f8e42ef7c4c99acd87ca8/curation/pk/template-pk%20final.xlsx\">excel spreadsheet</a> includes a detailed discussion of every decision that I made. I will not re-iterate here the instances where I used one of the amendments above to change a call or to resolve a disagreement. I will also not detail instances where I changed a call because I thought another curator made a human error (for example, not classifying two proton-pump inhibitors in the same way for the same disease). I will also not re-iterate cases where I felt that one of the two other curators knew about an indication (either DM or SYM) and I was able to confirm evidence of this indication.</p>\r\n\r\n<p>I would like to enter into the discussion the cases where there was a tough decision to be made, and would like to welcome an open discussion to come to a consensus. In general, when there was a tough decision, I did look at the other curators' calls to see what the consensus would be. Below is a summary of this discussion (with greater detail given in the spreadsheet), organized by disease and drug class.</p>\r\n\r\n<ul><li><p><strong>hypertension, general </strong>— Hypertension (the disease entity) is a heterogenous group of diseases. The most likely subtype of hypertension was likely Essential Hypertension (ET). The disease of Hypertension (including ET) progresses to have complications such as strokes and heart attacks. Hypertension (i.e. high blood pressure) is also a symptom (of many diseases, including diseases that are not called \"hypertension\"). Within the disease of ET, hypertension is not just a symptom but also a marker of disease progression, i.e. controlling blood pressure (treating this symptom) will slow the advancement of the disease hypertension and prevent downstream biology (proven by evidence, guideline 1). Therefore, by assumption 1, a total of 29 drugs were called <strong>DM</strong> rather than SYM.</p></li><li><p><strong>hypertension, diuretics </strong>—all called <strong>DM</strong> due to amendment 1 and amendment 2</p></li><li><p><strong>hypertension, drugs used to treat ocular hypertension or pulmonary hypertension </strong>— \"hypertension\" as defined in Daniel's link as \"chronic elevated blood pressure in the arteries\" It is therefore not the same as \"ocular hypertension\" or \"pulmonary hypertension\" (I think the spirit of the definition is systemic arteries, not pulmonary arteries. Also, pulmonary hypertension is quite a different disease with different pharmacology).Therefore, I chose to put <strong>NOT</strong> for all of these.</p></li><li><p><strong>type 2 diabetes, drugs that lower blood sugar </strong>—Diabetes is similar to hypertension. The disease is a tendency to have high blood sugar (hyperglycemia). Hyperglycemia is a symptom (of both diabetes and other diseases). Within the disease of diabetes, hyperglycemia is both a symptom and a marker of disease progression. Therefore anything that lowers hyperglycemia will be <strong>DM</strong>.</p></li><li><p><strong>type 1 and type 2 diabetes, ACE inhibitors and ARBs </strong>— The downstream biology of DM2 includes proteinuria and eventual renal failure. ACE inhibitors and ARBs prevent this downstream biology in DM2 patients. Therefore they are <strong>DM</strong>.</p></li><li><p><strong>epilepsy, anti-epileptic drugs </strong>—I think that for consistency, all anti-epileptics should be either DM or SYM, as there is only very limited evidence that any of these drugs are different from each other. My thoughts would be to label them all as <strong>DM</strong>. Here is why: epilepsy syndrome (disease) is defined as a propensity to have seizures (symptom). However, the natural downstream biology of the disease is that each seizure that you actually have makes you more likely to have worse epilepsy in the future (i.e. seizures beget more seizures). One mechanism is that when you have a lot of seizures, you develop mesial temporal sclerosis, and mesial temporal sclerosis is a risk factor for further seizures. Therefore, I would argue that any drug which treats the symptom of seizure is actually affecting downstream biology, and is therefore disease modifying. And by assumption 1, DM trumps SYM.</p></li><li><p><strong>osteoarthritis, NSAIDs and steroids </strong>— I put everything as <strong>SYM</strong>. From <a href=\"http://emedicine.medscape.com/article/330487-medication\">MedScape</a>: \"To date, no disease-modifying or structure-modifying intervention has been proved effective in osteoarthritis.\" CSH agreed with this interpretation, while it was clear that AJG was conflicted. To play devil's advocate, you could potentially say that the biology of osteoarthritis (OA) that it starts with inflammation, and the \"down-stream\" biology is the pain (the primary symptom as well), and therefore NSAIDs prevent \"down-stream\" biology. However, if we want to make that decision, I think we should change all the NSAIDs and steroids to DM.</p></li><li><p><strong>cancers, pain medications </strong>— I think pain is a symptom of cancer and therefore I put all of these as <strong>SYM</strong>. CSH agreed, while AJG was conflicted and sometimes called NOT. </p></li><li><p><strong>hematologic cancers, steroids </strong>—steroids actually \"treat\" hematologic cancers, even though these days there are much better meds and steroids are not considered \"treatment\", in the past they were the first line. By amendment 1, I put all of these as <strong>DM</strong>.</p></li><li><p><strong>non-hematologic cancers, steroids </strong>— steroids treat the nausea symptoms associated with cancers. While many cancers can potentially cause nausea, most nausea in cancer patients is due to side effect of chemo. However, I still put <strong>SYM</strong> for these because they can treat nausea and nausea is potentially a side effect of any cancer. CSH agreed with me on most of these, AJG was conflicted.</p></li><li><p><strong>cancers, hydroxyurea and other chemotherapies </strong>— AJG called this DM for all cancers. CSH called it DM only for the cancers for which it is indicated. The truth is, any chemotherapy has theoretical benefit against any cancer (any quickly-reproducing cell type). One possibility would be to label all chemotherapies as DM for all cancers. I thought it would be better to be selective and only label DM for chemo that is used (or has been used) in a particular cancer. That way, the results of the drug-repurposing search would yield different results for different cancers (rather than giving the exact same result for all cancers because the input was exactly the same for all cancers).Thus, I labeled some <strong>DM</strong> and some as <strong>NOT</strong>.</p></li><li><p><strong>cancers, bisphosphonates </strong>— I don't think of bone loss as a \"side effect\" of cancers (at least not any of the cancers listed). Some people are malnourished and/or have drug-induced bone loss, or may have bone metastases, but I don't think this captures the essence of cancer. I chose to put <strong>NOT</strong> for all of these (CSH agreed, AJG generally chose SYM).</p></li><li><p><strong>coronary artery disease, drug to treat hypertension or diabetes </strong>— I treated this as pure coronary artery disease (CAD) in the absence of other causes. I did not interpret this as \"CAD as a consequence of hypertension (HTN)\" or \"CAD as a consequence of diabetes (DM2)\". It is true that many of these medications would help prevent CAD if CAD is considered as the \"downstream biology\" of HTN or DM2. However, the medications to not treat any biology downstream of CAD in the absence of HTN or DM2. I therefore labeled these as <strong>NOT</strong>.</p></li><li><p><strong>coronary artery disease, diuretics and other drugs used for congestive heart failure (CHF) </strong>— I consider these to be <strong>DM</strong>. Consider CHF as a common dowstream biology of coronary artery disease (CAD), specifically let's consider HFrEF. The biology of HFrEF is that the heart has poor cardiac output, thus there is fluid retention, thus there is further strain on the heart, creating a vicious cycle. Thus, diuretics should help avoid the vicious cycle and slow the downstream progression of disease. While no trial may have ever showed mortality benefit, I think there is reasonable evidence that this would be true.</p></li><li><p><strong>migraine, general </strong>— While AJG called everything SYM, CSH called one drug (amitryptilene) DM and everything else SYM, citing that this medications \"may decrease frequency of migraines\". I agreed with CSH's interpretation and actually chose to include many other medications as DM based on the same reasoning. Migraine disease is a propensity to get migraine headaches. Therefore, anything that decreased migraine frequency was considered by me to be <strong>DM</strong> (decreases the downstream biology that leads to headache). Anything that treated the pain of the headache I considered <strong>SYM</strong>.</p></li><li><p><strong>autoimmune diseases, steroids and NSAIDs </strong>— I labeled the steroids as <strong>DM</strong> and the NSAIDs as <strong>SYM</strong>. It is true that steroids are rarely if ever actually used for chronic disease (though often used to treat the symptoms of flares), mostly because of their terrible side-effect profile long-term. However, they do actually affect disease biology and do not specifically treat any specific symptom (for example, steroids do not cure \"weakness\", but do change the biology of the multiple sclerosis flare to help the patient recover from \"weakness\"). It was a close call for me for NSAIDs (as they do have anti-inflammatory properties, especially useful in the auto-immune arthritidies), but I went with consensus and chose SYM. The other curators tended not to call steroids DM (probably because they were thinking about side-effect profile), and they were inconsistent on their calls on NSAIDs.</p></li><li><p><strong>asthma, steroids and beta-agonists and anticholinergics </strong>— I put <strong>DM</strong> for all of these. I felt steroids are DM (since they are given to prevent attacks), long-acting beta-agonists are also DM in my opinion since they prevent attacks (in conjunction with steroids, despite the small increased risk of asthma-related death in people who don't use steroids). I put short acting beta-agonists as DM because they too can prevent downstream biology (since they are sometimes used, for example before exercise, to prevent downstream biology from happening).</p></li><li><p><strong>allergic rhinitis, steroids and anti-histamines and decongestants </strong>— CSH admittedly had a problem with this, she even noted \"im having a hard time with allergic rhinitis. Maybe all of these meds are SYM.\".  AJG was conflicted as well.  I chose to mark all of the steroids and anti-histamines as <strong>DM</strong> because they alter the immune response (the allergy). I chose to mark the decongestants (i.e. pseudoephedrine) as <strong>SYM</strong> because they treat a symptom (congestion) but not the underyling biology (the immune reaction)</p></li><li><p><strong>chronic obstructive pulmonary disease (COPD), general </strong>— I put steroids and beta-agonists as <strong>DM</strong> for similar reasons to asthma, though admittedly there is less evidence for this. I put all the antibiotics as <strong>SYM</strong> (they don't eradicate infection, they don't delay progression, they treat attacks) and did not differentiate between the antibiotics</p></li><li><p><strong>glaucoma, general </strong>— I agreed with CSH that almost every drug is DM, AJG was conflicted. I think it's more <strong>DM</strong>, based on similar discussion as hypertension.</p></li><li><p><strong>alcohol dependence, general </strong>— First, I included symptoms of alcohol withdrawal along with alcohol dependence, presumably because withdrawal symptoms are probably felt at some point in any person with alcohol dependence. Thus, I chose to mark chlordiazepoxide and zofran (used to treat withdrawal) as <strong>SYM</strong> (CSH agreed with both, AJG agreed with one of these). Next, there was the question of drugs designed to curb drinking (Citalopram, Disulfiram, Naltrexone, Acamprosate); I chose to mark these as <strong>DM</strong> because they are different from the drugs above in that they treat the urge to drink (modifying the disease) rather than the symptoms of not drinking (AJG agreed with all 5, CSH agreed with 1)</p></li><li><p><strong>psychiatric diseases other than alcohol/drug dependence, general </strong>— I agreed with both CSH and AJG that in general, the medications used are all <strong>SYM</strong> rather than DM (other than alcohol and nicotine dependence as described).</p></li><li><p><strong>alzheimer's disease, general </strong>— donepezil was marked as <strong>DM</strong> (agreed with AJG) because it is supposed to slow disease, not treat any specific symptom. Other cholinesterase inhibitors were changed to DM to match donepezil. I chose to group all antipsychotics as <strong>SYM</strong> in order to be consistent (AJG and CSH agreed most of the time)</p></li><li><p><strong>anemia </strong>— there is no good way to do this. Anemia is not a single disease, it is a very heterogeneous set of diseases (with very little overlap between sub-types in terms of incidence or pathophysiology). Though the most common cause of anemia is likely iron deficiency anemia, iron deficiency anemia accounts for probably a minority of all anemias. Specific types of anemia will of course respond to specific drugs  (autoimmune anemia to steroids, folate deficiency responds to folate, etc...). I was faced with two choices: (1) choose DM for anything which could treat any type of anemia or (2) choose DM for anything which could treat most types of anemia. Choice (2) means nothing will link to anemia (no DM or SYM) and anemia will essentially be removed from analysis. Choice (1) means we are choosing a variety of drugs to treat a variety of illnesses. I chose choice (2) because I think it's best to ignore anemia, given that it is such a heterogeneous set of diseases. AJG and CSH were rather inconsistent in their answers, it is clear that they too had difficulty with anemia. In summary, everything was marked as <strong>NOT</strong>.</p></li></ul>",
      "body_md": "# Initial results from the third curator\r\nI was initially recruited to break the 444 disagreements between the other curators. After an initial pilot review of the first 80 or so disagreements, I noted some ambiguity in definition of the three classes that appeared to be giving rise to some of the disagreements. I discussed these with Daniel and we reached a more precise amended set of definitions.\r\n### Definitions:\r\n- **disease modifying (DM) ** — a drug that therapeutically changes the underlying or downstream biology of the disease\r\n- **symptomatic (SYM) ** — a drug that treats a significant symptom of the disease\r\n-  **non-indication (NOT) ** — a drug that neither therapeutically changes the underlying or downstream biology nor treats a significant symptom of the disease\r\n### Guidelines:\r\n- **reasonable evidence** of efficacy is required to be classified as disease modifying or symptomatic. This includes off-label use.\r\n- if no classification accurately describes an indication, the **most appropriate** (although imperfect) classification should be chosen\r\n### Amendments: (created 1/27/16, not seen by AJG and CSH)\r\n- **Amendment 1: ** if a drug was **previously indicated, but is no longer used** due to side effects, or because there are better drugs, it is still considered **DM**\r\n- **Amendment 2: ** it **doesn't matter whether it is first line or fifth line**, it's still considered **DM**\r\n### Assumptions: (by PK)\r\n- **Assumption 1: DM trumps SYM. ** If a drug is clearly both disease modifying and also treats symptoms, then I will call it disease modifying. This is because most disease modifying drugs also treat symptoms.\r\n- **Assumption 2: SYM trumps NOT.** If a drug is clearly symptomatic treatment, but can actually exacerbate the downstream biology of disease, then I chose SYM. I made this choice because this was the choice I saw most often made by AJG and CSH\r\n\r\nWith the revised definitions above, I reviewed the 444 disagreements as well as the 944 agreements (and suggested a change on 124 of these). I was not blinded to the other curators' decisions. I was able to see both of their decisions and also any comments they had left regarding their reasoning. In general, my strategy was to look three sources for each drug (unless I clearly already knew that a drug was DM or SYM): uptodate.com, drugbank.ca (link provided by Daniel in the spreadsheet), and a basic google search (which also served as a proxy for a pubmed search). When I noted that one of the two curators was calling an indication which I was not aware of (either DM or SYM), I would do a much more detailed search including a more detailed google search and a direct pubmed search. \r\n\r\nBelow is the breakdown of classifications\r\n\r\n|class | AJG | CSH | PK |  | class | AJG | CSH | PK\r\n|---|---|---|---|---|---|---|---|---|\r\n|DM | 599 | 593 | 755 |  | DM | 43.2% | 42.7% | 54.4%|\r\n|SYM | 514 | 517 | 390 |  | SYM | 37.0% | 37.2% | 28.1%|\r\n|NOT | 275 | 278 | 243 |  | NOT | 19.8% | 20.0% | 17.5%|\r\n|total | 1388 | 1388 | 1388 |  | total | 100.0% | 100.0% | 100.0%|\r\n\r\nThe most notable difference was that I called DM more often than the other curators. There are at least two reasons for this. First, I was making use of amendment 1 and amendment 2 to make calls for DM, whereas the other curators were not using these amendments (in fact, when the other curators called NOT, they left comments such as \"no longer recommended due to side effects\", \"not used anymore\", or \"rarely used\"). Second, when I found a disagreement between two curators, I was more likely to agree with the curator who called DM. Specifically, of the 444 disagreements, there were 298 where one curator chose DM; of these 298 instances, I chose DM 204 times. I think this is because I did a more detailed search when I knew that one other curator thought that there was a DM indication. Of note, of the 146 times that the other curators were in disagreement between SYM and NOT, I chose SYM 76 times, I chose NOT 52 times, and I chose DM 18 times, likely for the same reasons described above.\r\n\r\nThe [excel spreadsheet](https://github.com/dhimmel/indications/blob/d3d57b07abcf8d7af55f8e42ef7c4c99acd87ca8/curation/pk/template-pk%20final.xlsx) includes a detailed discussion of every decision that I made. I will not re-iterate here the instances where I used one of the amendments above to change a call or to resolve a disagreement. I will also not detail instances where I changed a call because I thought another curator made a human error (for example, not classifying two proton-pump inhibitors in the same way for the same disease). I will also not re-iterate cases where I felt that one of the two other curators knew about an indication (either DM or SYM) and I was able to confirm evidence of this indication.\r\n\r\nI would like to enter into the discussion the cases where there was a tough decision to be made, and would like to welcome an open discussion to come to a consensus. In general, when there was a tough decision, I did look at the other curators' calls to see what the consensus would be. Below is a summary of this discussion (with greater detail given in the spreadsheet), organized by disease and drug class.\r\n\r\n- **hypertension, general **— Hypertension (the disease entity) is a heterogenous group of diseases. The most likely subtype of hypertension was likely Essential Hypertension (ET). The disease of Hypertension (including ET) progresses to have complications such as strokes and heart attacks. Hypertension (i.e. high blood pressure) is also a symptom (of many diseases, including diseases that are not called \"hypertension\"). Within the disease of ET, hypertension is not just a symptom but also a marker of disease progression, i.e. controlling blood pressure (treating this symptom) will slow the advancement of the disease hypertension and prevent downstream biology (proven by evidence, guideline 1). Therefore, by assumption 1, a total of 29 drugs were called **DM** rather than SYM.\r\n\r\n- **hypertension, diuretics **—all called **DM** due to amendment 1 and amendment 2\r\n\r\n- **hypertension, drugs used to treat ocular hypertension or pulmonary hypertension **— \"hypertension\" as defined in Daniel's link as \"chronic elevated blood pressure in the arteries\" It is therefore not the same as \"ocular hypertension\" or \"pulmonary hypertension\" (I think the spirit of the definition is systemic arteries, not pulmonary arteries. Also, pulmonary hypertension is quite a different disease with different pharmacology).Therefore, I chose to put **NOT** for all of these.\r\n\r\n- **type 2 diabetes, drugs that lower blood sugar **—Diabetes is similar to hypertension. The disease is a tendency to have high blood sugar (hyperglycemia). Hyperglycemia is a symptom (of both diabetes and other diseases). Within the disease of diabetes, hyperglycemia is both a symptom and a marker of disease progression. Therefore anything that lowers hyperglycemia will be **DM**.\r\n\r\n- **type 1 and type 2 diabetes, ACE inhibitors and ARBs **— The downstream biology of DM2 includes proteinuria and eventual renal failure. ACE inhibitors and ARBs prevent this downstream biology in DM2 patients. Therefore they are **DM**.\r\n\r\n- **epilepsy, anti-epileptic drugs **—I think that for consistency, all anti-epileptics should be either DM or SYM, as there is only very limited evidence that any of these drugs are different from each other. My thoughts would be to label them all as **DM**. Here is why: epilepsy syndrome (disease) is defined as a propensity to have seizures (symptom). However, the natural downstream biology of the disease is that each seizure that you actually have makes you more likely to have worse epilepsy in the future (i.e. seizures beget more seizures). One mechanism is that when you have a lot of seizures, you develop mesial temporal sclerosis, and mesial temporal sclerosis is a risk factor for further seizures. Therefore, I would argue that any drug which treats the symptom of seizure is actually affecting downstream biology, and is therefore disease modifying. And by assumption 1, DM trumps SYM.\r\n\r\n- **osteoarthritis, NSAIDs and steroids **— I put everything as **SYM**. From [MedScape](http://emedicine.medscape.com/article/330487-medication): \"To date, no disease-modifying or structure-modifying intervention has been proved effective in osteoarthritis.\" CSH agreed with this interpretation, while it was clear that AJG was conflicted. To play devil's advocate, you could potentially say that the biology of osteoarthritis (OA) that it starts with inflammation, and the \"down-stream\" biology is the pain (the primary symptom as well), and therefore NSAIDs prevent \"down-stream\" biology. However, if we want to make that decision, I think we should change all the NSAIDs and steroids to DM.\r\n\r\n- **cancers, pain medications **— I think pain is a symptom of cancer and therefore I put all of these as **SYM**. CSH agreed, while AJG was conflicted and sometimes called NOT. \r\n\r\n- **hematologic cancers, steroids **—steroids actually \"treat\" hematologic cancers, even though these days there are much better meds and steroids are not considered \"treatment\", in the past they were the first line. By amendment 1, I put all of these as **DM**.\r\n\r\n- **non-hematologic cancers, steroids **— steroids treat the nausea symptoms associated with cancers. While many cancers can potentially cause nausea, most nausea in cancer patients is due to side effect of chemo. However, I still put **SYM** for these because they can treat nausea and nausea is potentially a side effect of any cancer. CSH agreed with me on most of these, AJG was conflicted.\r\n\r\n- **cancers, hydroxyurea and other chemotherapies **— AJG called this DM for all cancers. CSH called it DM only for the cancers for which it is indicated. The truth is, any chemotherapy has theoretical benefit against any cancer (any quickly-reproducing cell type). One possibility would be to label all chemotherapies as DM for all cancers. I thought it would be better to be selective and only label DM for chemo that is used (or has been used) in a particular cancer. That way, the results of the drug-repurposing search would yield different results for different cancers (rather than giving the exact same result for all cancers because the input was exactly the same for all cancers).Thus, I labeled some **DM** and some as **NOT**.\r\n\r\n- **cancers, bisphosphonates **— I don't think of bone loss as a \"side effect\" of cancers (at least not any of the cancers listed). Some people are malnourished and/or have drug-induced bone loss, or may have bone metastases, but I don't think this captures the essence of cancer. I chose to put **NOT** for all of these (CSH agreed, AJG generally chose SYM).\r\n\r\n- **coronary artery disease, drug to treat hypertension or diabetes **— I treated this as pure coronary artery disease (CAD) in the absence of other causes. I did not interpret this as \"CAD as a consequence of hypertension (HTN)\" or \"CAD as a consequence of diabetes (DM2)\". It is true that many of these medications would help prevent CAD if CAD is considered as the \"downstream biology\" of HTN or DM2. However, the medications to not treat any biology downstream of CAD in the absence of HTN or DM2. I therefore labeled these as **NOT**.\r\n\r\n- **coronary artery disease, diuretics and other drugs used for congestive heart failure (CHF) **— I consider these to be **DM**. Consider CHF as a common dowstream biology of coronary artery disease (CAD), specifically let's consider HFrEF. The biology of HFrEF is that the heart has poor cardiac output, thus there is fluid retention, thus there is further strain on the heart, creating a vicious cycle. Thus, diuretics should help avoid the vicious cycle and slow the downstream progression of disease. While no trial may have ever showed mortality benefit, I think there is reasonable evidence that this would be true.\r\n\r\n- **migraine, general **— While AJG called everything SYM, CSH called one drug (amitryptilene) DM and everything else SYM, citing that this medications \"may decrease frequency of migraines\". I agreed with CSH's interpretation and actually chose to include many other medications as DM based on the same reasoning. Migraine disease is a propensity to get migraine headaches. Therefore, anything that decreased migraine frequency was considered by me to be **DM** (decreases the downstream biology that leads to headache). Anything that treated the pain of the headache I considered **SYM**.\r\n\r\n- **autoimmune diseases, steroids and NSAIDs **— I labeled the steroids as **DM** and the NSAIDs as **SYM**. It is true that steroids are rarely if ever actually used for chronic disease (though often used to treat the symptoms of flares), mostly because of their terrible side-effect profile long-term. However, they do actually affect disease biology and do not specifically treat any specific symptom (for example, steroids do not cure \"weakness\", but do change the biology of the multiple sclerosis flare to help the patient recover from \"weakness\"). It was a close call for me for NSAIDs (as they do have anti-inflammatory properties, especially useful in the auto-immune arthritidies), but I went with consensus and chose SYM. The other curators tended not to call steroids DM (probably because they were thinking about side-effect profile), and they were inconsistent on their calls on NSAIDs.\r\n\r\n- **asthma, steroids and beta-agonists and anticholinergics **— I put **DM** for all of these. I felt steroids are DM (since they are given to prevent attacks), long-acting beta-agonists are also DM in my opinion since they prevent attacks (in conjunction with steroids, despite the small increased risk of asthma-related death in people who don't use steroids). I put short acting beta-agonists as DM because they too can prevent downstream biology (since they are sometimes used, for example before exercise, to prevent downstream biology from happening).\r\n\r\n- **allergic rhinitis, steroids and anti-histamines and decongestants **— CSH admittedly had a problem with this, she even noted \"im having a hard time with allergic rhinitis. Maybe all of these meds are SYM.\".  AJG was conflicted as well.  I chose to mark all of the steroids and anti-histamines as **DM** because they alter the immune response (the allergy). I chose to mark the decongestants (i.e. pseudoephedrine) as **SYM** because they treat a symptom (congestion) but not the underyling biology (the immune reaction)\r\n\r\n- **chronic obstructive pulmonary disease (COPD), general **— I put steroids and beta-agonists as **DM** for similar reasons to asthma, though admittedly there is less evidence for this. I put all the antibiotics as **SYM** (they don't eradicate infection, they don't delay progression, they treat attacks) and did not differentiate between the antibiotics\r\n\r\n- **glaucoma, general **— I agreed with CSH that almost every drug is DM, AJG was conflicted. I think it's more **DM**, based on similar discussion as hypertension.\r\n\r\n- **alcohol dependence, general **— First, I included symptoms of alcohol withdrawal along with alcohol dependence, presumably because withdrawal symptoms are probably felt at some point in any person with alcohol dependence. Thus, I chose to mark chlordiazepoxide and zofran (used to treat withdrawal) as **SYM** (CSH agreed with both, AJG agreed with one of these). Next, there was the question of drugs designed to curb drinking (Citalopram, Disulfiram, Naltrexone, Acamprosate); I chose to mark these as **DM** because they are different from the drugs above in that they treat the urge to drink (modifying the disease) rather than the symptoms of not drinking (AJG agreed with all 5, CSH agreed with 1)\r\n\r\n- **psychiatric diseases other than alcohol/drug dependence, general **— I agreed with both CSH and AJG that in general, the medications used are all **SYM** rather than DM (other than alcohol and nicotine dependence as described).\r\n\r\n- **alzheimer's disease, general **— donepezil was marked as **DM** (agreed with AJG) because it is supposed to slow disease, not treat any specific symptom. Other cholinesterase inhibitors were changed to DM to match donepezil. I chose to group all antipsychotics as **SYM** in order to be consistent (AJG and CSH agreed most of the time)\r\n\r\n- **anemia **— there is no good way to do this. Anemia is not a single disease, it is a very heterogeneous set of diseases (with very little overlap between sub-types in terms of incidence or pathophysiology). Though the most common cause of anemia is likely iron deficiency anemia, iron deficiency anemia accounts for probably a minority of all anemias. Specific types of anemia will of course respond to specific drugs  (autoimmune anemia to steroids, folate deficiency responds to folate, etc...). I was faced with two choices: (1) choose DM for anything which could treat any type of anemia or (2) choose DM for anything which could treat most types of anemia. Choice (2) means nothing will link to anemia (no DM or SYM) and anemia will essentially be removed from analysis. Choice (1) means we are choosing a variety of drugs to treat a variety of illnesses. I chose choice (2) because I think it's best to ignore anemia, given that it is such a heterogeneous set of diseases. AJG and CSH were rather inconsistent in their answers, it is clear that they too had difficulty with anemia. In summary, everything was marked as **NOT**.",
      "profile": 188,
      "published": "2016-02-20T03:13:15.931940Z",
      "thread": 95,
      "url": "/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95#7"
    },
    {
      "body_html": "<h1>Overly broad and thus uninformative diseases</h1>\r\n\r\n<p><a href=\"/u/pouyakhankhanian\" class=\"username\">@pouyakhankhanian</a>, fantastic curation!</p>\r\n\r\n<p>From your comments, it appears that some of our diseases are too general from a pharmacological perspective. For example, you mention anemia and hypertension as particularly troublesome. To recap how we arrived at our 137 diseases, I <a href=\"http://thinklab.com/discussion/unifying-disease-vocabularies/44#6\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d44\">selected the subset</a> of Disease Ontology terms that have been analyzed using GWAS or were a 'body system' cancer. When diseases were redundant, a single disease was chosen (for example, coronary artery disease was chosen over myocardial infarction). In retrospect, input from physicians would have been prudent during this stage.</p>\r\n\r\n<p>When we created our <a href=\"http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#21\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d21\">indication catalog</a> (which we're curating here), I propagated indications from specific to more general terms. For example, an indication for non-small cell lung carcinoma (<code>DOID:3908</code>) would be considered an indication for lung cancer (<code>DOID:1324</code>).</p>\r\n\r\n<p>In the cases of hypertension and anemia, <a href=\"/u/pouyakhankhanian\" class=\"username\">@pouyakhankhanian</a> found this practice problematic. Specifically, he considered pulmonary hypertension (<code>DOID:6432</code>) to be \"quite a different disease with different pharmacology\" than the definition of hypertension (<code>DOID:10763</code>). However the Disease Ontology defines pulmonary hypertension as a subtype of hypertension. Ocular hypertension (<code>DOID:9282</code>) is not a subtype — instead it's part of the glaucoma lineage. He also mentioned anemia as having heterogeneous subtypes.</p>\r\n\r\n<p>While my original guidance would have been to chose DM or SYM if the drug is DM or SYM for any subtype, I see the point that some diseases may be too broad and therefore uninformative for our prediction approach.</p>",
      "body_md": "# Overly broad and thus uninformative diseases\r\n\r\n@pouyakhankhanian, fantastic curation!\r\n\r\nFrom your comments, it appears that some of our diseases are too general from a pharmacological perspective. For example, you mention anemia and hypertension as particularly troublesome. To recap how we arrived at our 137 diseases, I [selected the subset](http://thinklab.com/discussion/unifying-disease-vocabularies/44#6) of Disease Ontology terms that have been analyzed using GWAS or were a 'body system' cancer. When diseases were redundant, a single disease was chosen (for example, coronary artery disease was chosen over myocardial infarction). In retrospect, input from physicians would have been prudent during this stage.\r\n\r\nWhen we created our [indication catalog](http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#21) (which we're curating here), I propagated indications from specific to more general terms. For example, an indication for non-small cell lung carcinoma (`DOID:3908`) would be considered an indication for lung cancer (`DOID:1324`).\r\n\r\nIn the cases of hypertension and anemia, @pouyakhankhanian found this practice problematic. Specifically, he considered pulmonary hypertension (`DOID:6432`) to be \"quite a different disease with different pharmacology\" than the definition of hypertension (`DOID:10763`). However the Disease Ontology defines pulmonary hypertension as a subtype of hypertension. Ocular hypertension (`DOID:9282`) is not a subtype -- instead it's part of the glaucoma lineage. He also mentioned anemia as having heterogeneous subtypes.\r\n\r\nWhile my original guidance would have been to chose DM or SYM if the drug is DM or SYM for any subtype, I see the point that some diseases may be too broad and therefore uninformative for our prediction approach.",
      "profile": 17,
      "published": "2016-02-23T00:20:34.010990Z",
      "thread": 95,
      "url": "/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95#8"
    },
    {
      "body_html": "<h1>Redundant terms removed from the slim DO</h1>\r\n\r\n<p>My <a href=\"#6\">above post</a> on creating the slim DO didn't specify which diseases were removed to \"resolve overlapping nodes\". The table below shows which diseases we removed and why (rules above). The exclusions counts by rule are: 7 for rule 1, 22 for rule 2, and 5 for rule 3.</p>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th>ID</th><th>Name</th><th>Source</th><th>Removed by</th></tr></thead><tbody><tr><td>DOID:201</td><td>Connective tissue cancer</td><td>DOcancerslim</td><td>rule 1</td></tr><tr><td>DOID:10155</td><td>Intestinal cancer</td><td>DOcancerslim</td><td>rule 1</td></tr><tr><td>DOID:5672</td><td>Large intestine cancer</td><td>DOcancerslim</td><td>rule 1</td></tr><tr><td>DOID:3119</td><td>Gastrointestinal system cancer</td><td>DOcancerslim</td><td>rule 1</td></tr><tr><td>DOID:8618</td><td>Oral cavity cancer</td><td>DOcancerslim</td><td>rule 1</td></tr><tr><td>DOID:170</td><td>Endocrine gland cancer</td><td>DOcancerslim</td><td>rule 1</td></tr><tr><td>DOID:3996</td><td>Urinary system cancer</td><td>DOcancerslim</td><td>rule 1</td></tr><tr><td>DOID:3459</td><td>breast carcinoma</td><td>hetio</td><td>rule 2</td></tr><tr><td>DOID:10286</td><td>prostate carcinoma</td><td>hetio</td><td>rule 2</td></tr><tr><td>DOID:1040</td><td>chronic lymphocytic leukemia</td><td>hetio</td><td>rule 2</td></tr><tr><td>DOID:3905</td><td>lung carcinoma</td><td>hetio</td><td>rule 2</td></tr><tr><td>DOID:1909</td><td>melanoma</td><td>hetio</td><td>rule 2</td></tr><tr><td>DOID:4001</td><td>ovarian carcinoma</td><td>hetio</td><td>rule 2</td></tr><tr><td>DOID:1107</td><td>esophageal carcinoma</td><td>hetio</td><td>rule 2</td></tr><tr><td>DOID:4007</td><td>bladder carcinoma</td><td>hetio</td><td>rule 2</td></tr><tr><td>DOID:289</td><td>endometriosis</td><td>hetio</td><td>rule 2</td></tr><tr><td>DOID:4450</td><td>renal cell carcinoma</td><td>hetio</td><td>rule 2</td></tr><tr><td>DOID:769</td><td>neuroblastoma</td><td>hetio</td><td>rule 2</td></tr><tr><td>DOID:8567</td><td>Hodgkin's lymphoma</td><td>hetio</td><td>rule 2</td></tr><tr><td>DOID:3963</td><td>thyroid carcinoma</td><td>hetio</td><td>rule 2</td></tr><tr><td>DOID:9538</td><td>multiple myeloma</td><td>hetio</td><td>rule 2</td></tr><tr><td>DOID:9952</td><td>acute lymphocytic leukemia</td><td>hetio</td><td>rule 2</td></tr><tr><td>DOID:5517</td><td>stomach carcinoma</td><td>hetio</td><td>rule 2</td></tr><tr><td>DOID:684</td><td>hepatocellular carcinoma</td><td>hetio</td><td>rule 2</td></tr><tr><td>DOID:1380</td><td>endometrial cancer</td><td>hetio</td><td>rule 2</td></tr><tr><td>DOID:4905</td><td>pancreatic carcinoma</td><td>hetio</td><td>rule 2</td></tr><tr><td>DOID:4960</td><td>bone marrow cancer</td><td>hetio</td><td>rule 2</td></tr><tr><td>DOID:706</td><td>mature B-cell neoplasm</td><td>hetio</td><td>rule 2</td></tr><tr><td>DOID:8552</td><td>chronic myeloid leukemia</td><td>hetio</td><td>rule 2</td></tr><tr><td>DOID:5844</td><td>myocardial infarction</td><td>hetio</td><td>rule 3</td></tr><tr><td>DOID:6713</td><td>cerebrovascular disease</td><td>hetio</td><td>rule 3</td></tr><tr><td>DOID:11829</td><td>degenerative myopia</td><td>hetio</td><td>rule 3</td></tr><tr><td>DOID:13641</td><td>exfoliation syndrome</td><td>hetio</td><td>rule 3</td></tr><tr><td>DOID:3324</td><td>mood disorder</td><td>hetio</td><td>rule 3</td></tr></tbody></table>\r\n\r\n<p>See the <a href=\"https://github.com/dhimmel/disease-ontology/blob/5cb93c38568536222b0a14fbcb7fb644a348931d/data/slim-terms.tsv\">remaining 137 diseases here</a>.</p>",
      "body_md": "# Redundant terms removed from the slim DO\r\n\r\nMy [above post](#6) on creating the slim DO didn't specify which diseases were removed to \"resolve overlapping nodes\". The table below shows which diseases we removed and why (rules above). The exclusions counts by rule are: 7 for rule 1, 22 for rule 2, and 5 for rule 3.\r\n\r\n| ID | Name | Source | Removed by |\r\n|------------|--------------------------------|--------------|------------|\r\n| DOID:201 | Connective tissue cancer | DOcancerslim | rule 1 |\r\n| DOID:10155 | Intestinal cancer | DOcancerslim | rule 1 |\r\n| DOID:5672 | Large intestine cancer | DOcancerslim | rule 1 |\r\n| DOID:3119 | Gastrointestinal system cancer | DOcancerslim | rule 1 |\r\n| DOID:8618 | Oral cavity cancer | DOcancerslim | rule 1 |\r\n| DOID:170 | Endocrine gland cancer | DOcancerslim | rule 1 |\r\n| DOID:3996 | Urinary system cancer | DOcancerslim | rule 1 |\r\n| DOID:3459 | breast carcinoma | hetio | rule 2 |\r\n| DOID:10286 | prostate carcinoma | hetio | rule 2 |\r\n| DOID:1040 | chronic lymphocytic leukemia | hetio | rule 2 |\r\n| DOID:3905 | lung carcinoma | hetio | rule 2 |\r\n| DOID:1909 | melanoma | hetio | rule 2 |\r\n| DOID:4001 | ovarian carcinoma | hetio | rule 2 |\r\n| DOID:1107 | esophageal carcinoma | hetio | rule 2 |\r\n| DOID:4007 | bladder carcinoma | hetio | rule 2 |\r\n| DOID:289 | endometriosis | hetio | rule 2 |\r\n| DOID:4450 | renal cell carcinoma | hetio | rule 2 |\r\n| DOID:769 | neuroblastoma | hetio | rule 2 |\r\n| DOID:8567 | Hodgkin's lymphoma | hetio | rule 2 |\r\n| DOID:3963 | thyroid carcinoma | hetio | rule 2 |\r\n| DOID:9538 | multiple myeloma | hetio | rule 2 |\r\n| DOID:9952 | acute lymphocytic leukemia | hetio | rule 2 |\r\n| DOID:5517 | stomach carcinoma | hetio | rule 2 |\r\n| DOID:684 | hepatocellular carcinoma | hetio | rule 2 |\r\n| DOID:1380 | endometrial cancer | hetio | rule 2 |\r\n| DOID:4905 | pancreatic carcinoma | hetio | rule 2 |\r\n| DOID:4960 | bone marrow cancer | hetio | rule 2 |\r\n| DOID:706 | mature B-cell neoplasm | hetio | rule 2 |\r\n| DOID:8552 | chronic myeloid leukemia | hetio | rule 2 |\r\n| DOID:5844 | myocardial infarction | hetio | rule 3 |\r\n| DOID:6713 | cerebrovascular disease | hetio | rule 3 |\r\n| DOID:11829 | degenerative myopia | hetio | rule 3 |\r\n| DOID:13641 | exfoliation syndrome | hetio | rule 3 |\r\n| DOID:3324 | mood disorder | hetio | rule 3 |\r\n\r\nSee the [remaining 137 diseases here](https://github.com/dhimmel/disease-ontology/blob/5cb93c38568536222b0a14fbcb7fb644a348931d/data/slim-terms.tsv).",
      "profile": 17,
      "published": "2016-02-20T23:36:08.170080Z",
      "thread": 44,
      "url": "/discussion/unifying-disease-vocabularies/44#7"
    },
    {
      "body_html": "<h1>Indication terminology</h1>\r\n\r\n<p>We've been referred to when a drug treats a disease as an \"<a href=\"https://en.wikipedia.org/w/index.php?title=Indication_(medicine)&amp;oldid=703054912\" title=\"Wikipedia · Indication (medicine)\">indication</a>\". While readers with a medical background understand the term, others find \"indication\" confusing. </p>\r\n\r\n<p>Now <a href=\"http://thinklab.com/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d95\">we've split our indications</a> into two categories: disease-modifying and symptomatic. Additionally, we've switched to <a href=\"#2\">using verbs</a> to describe relationships. </p>\r\n\r\n<p>Given these factors, I chose \"treats\" for disease-modifying indications and \"palliates\" for symptomatic indications. This terminology aligns with a recent repurposing study <span class=\"citation\">[<a href=\"/doi/10.1038/ncomms10331\" class=\"citation\" data-key=\"10.1038/ncomms10331\">1</a>]</span>, which refers to</p>\r\n\r\n<blockquote><p>distinguishing non-causative and palliative from causative and effective treatments</p></blockquote>\r\n\r\n<p>While readers may not be familiar with the term palliates, it has an applicable and precise <a href=\"http://www.oxforddictionaries.com/us/definition/american_english/palliate\">definition</a> (making lookup easier):</p>\r\n\r\n<blockquote><p>Make (a disease or its symptoms) less severe or unpleasant without removing the cause</p></blockquote>\r\n\r\n<p><a href=\"/u/pouyakhankhanian\" class=\"username\">@pouyakhankhanian</a>, do you think the treats/palliates terminology makes sense?</p>",
      "body_md": "# Indication terminology\r\n\r\nWe've been referred to when a drug treats a disease as an \"[indication](https://en.wikipedia.org/w/index.php?title=Indication_(medicine)&oldid=703054912 \"Wikipedia · Indication (medicine)\")\". While readers with a medical background understand the term, others find \"indication\" confusing. \r\n\r\nNow [we've split our indications](http://thinklab.com/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95) into two categories: disease-modifying and symptomatic. Additionally, we've switched to [using verbs](#2) to describe relationships. \r\n\r\nGiven these factors, I chose \"treats\" for disease-modifying indications and \"palliates\" for symptomatic indications. This terminology aligns with a recent repurposing study [@10.1038/ncomms10331], which refers to\r\n\r\n> distinguishing non-causative and palliative from causative and effective treatments\r\n\r\nWhile readers may not be familiar with the term palliates, it has an applicable and precise [definition](http://www.oxforddictionaries.com/us/definition/american_english/palliate) (making lookup easier):\r\n\r\n> Make (a disease or its symptoms) less severe or unpleasant without removing the cause\r\n\r\n@pouyakhankhanian, do you think the treats/palliates terminology makes sense?",
      "profile": 17,
      "published": "2016-02-22T23:47:56.149928Z",
      "thread": 162,
      "url": "/discussion/data-nomenclature-naming-and-abbreviating-our-network-types/162#5"
    },
    {
      "body_html": "<p><a href=\"/u/alessandrodidonna\" class=\"username\">@alessandrodidonna</a> suggested adding RNA interference data, so we <a href=\"http://thinklab.com/discussion/suggestions-for-additional-information-types/22#6\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d22\">incorporated genetic perturbation relationships</a> from LINCS L1000. The L1000 project measures how the expression of 978 genes (called landmark genes) changes in response to perturbation. Here we are focusing on gene knockdown (shRNA) and gene overexpression perturbations.</p>\r\n\r\n<p>We <a href=\"http://thinklab.com/discussion/computing-consensus-transcriptional-profiles-for-compounds-using-lincs/43#6\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d43\">computed consensus transcriptional profiles</a> for knockdown (<a href=\"http://support.lincscloud.org/hc/en-us/articles/202216073-Perturbation-Types\" title=\"lincscloud Perturbation Types\"><code>pert_type = trt_sh</code></a>) and overexpression (<code>pert_type = trt_oe</code>) perturbations. For each gene perturbation, we end up with a vector of 978 z-scores representing the change in expression of each landmark gene. Using a Bonferroni cutoff to correct for the 978 comparisons, we identify the significantly upregulated and downregulated genes for each perturbation. Using this approach, we generate four relationship types for our network:</p>\r\n\r\n<ol><li>Gene → knockdown downregulates → Gene</li><li>Gene → knockdown upregulates → Gene</li><li>Gene → overexpression downregulates → Gene</li><li>Gene → overexpression upregulates → Gene</li></ol>\r\n\r\n<p>In a <a href=\"http://thinklab.com/discussion/data-nomenclature-naming-and-abbreviating-our-network-types/162#3\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d162\">separate discussion</a>, <a href=\"/u/larsjuhljensen\" class=\"username\">@larsjuhljensen</a> commented:</p>\r\n\r\n<blockquote><p>I am not entirely sure how useful the \"knockdown downregulates\" etc. types are. Usually \"knockdown downregulates\" would be interpreted to mean \"upregulates\" etc.</p></blockquote>\r\n\r\n<p>In other words, shouldn't we combine relationship types 1 &amp; 4 above into \"Gene → upregulates → Gene\" and 2 &amp; 3 into \"Gene → downregulates → Gene\"? To investigate whether this makes sense, I looked into whether knockdown and overexpression profiles for the same gene were anticorrelated. Does knocking down a gene have the opposite trascriptional effect as overexpressing it?</p>\r\n\r\n<p>The results were surprising (<a href=\"https://github.com/dhimmel/lincs/blob/00c55f95ead78bec72b9c7255f38b512c4a3da30/binarize-consensi.ipynb\">notebook</a>). Knockdown and overexpression of the same gene resulted in positively correlated transcriptional profiles 65.0% of the time. And if we correlate the knockdown of a random gene with the overexpression of a different random gene, we see a positive correlation 65.3% of the time. In summary, the transcriptional profiles of knocking down and overexpressing genes are more often than not positively correlated. And profiles for the same gene show no more correlation or anticorrelation than profiles for two different genes. Hmm.</p>\r\n\r\n<p><img src=\"https://github.com/dhimmel/lincs/raw/00c55f95ead78bec72b9c7255f38b512c4a3da30/viz/knockdown-overexpression-corr.png\" alt=\"Violinplots of correlation distributions\"></p>\r\n\r\n<p>What could cause this counterintuitive finding?</p>\r\n\r\n<ul><li>We could have a mistake in our code. Does anyone know of a gold standard for genetic perturbations that we could compare to?</li><li>By looking only at the 978 landmark genes, we are overlooking the crucial genes and instead picking up on a general perturbation response.</li><li>Gene regulation is a non-linear process.</li><li>Our method of analysis or the LINCS L1000 data may be limitated.</li></ul>\r\n\r\n<p>Does anyone, specifically those with gene expression experience (<a href=\"/u/caseygreene\" class=\"username\">@caseygreene</a>, <a href=\"/u/larsjuhljensen\" class=\"username\">@larsjuhljensen</a>, <a href=\"/u/fbastian\" class=\"username\">@fbastian</a>), have any insight on what might be happening? I'll also reach out to the L1000 team.</p>",
      "body_md": "@alessandrodidonna suggested adding RNA interference data, so we [incorporated genetic perturbation relationships](http://thinklab.com/discussion/suggestions-for-additional-information-types/22#6) from LINCS L1000. The L1000 project measures how the expression of 978 genes (called landmark genes) changes in response to perturbation. Here we are focusing on gene knockdown (shRNA) and gene overexpression perturbations.\r\n\r\nWe [computed consensus transcriptional profiles](http://thinklab.com/discussion/computing-consensus-transcriptional-profiles-for-compounds-using-lincs/43#6) for knockdown ([`pert_type = trt_sh`](http://support.lincscloud.org/hc/en-us/articles/202216073-Perturbation-Types \"lincscloud Perturbation Types\")) and overexpression (`pert_type = trt_oe`) perturbations. For each gene perturbation, we end up with a vector of 978 z-scores representing the change in expression of each landmark gene. Using a Bonferroni cutoff to correct for the 978 comparisons, we identify the significantly upregulated and downregulated genes for each perturbation. Using this approach, we generate four relationship types for our network:\r\n\r\n1. Gene → knockdown downregulates → Gene\r\n2. Gene → knockdown upregulates → Gene\r\n3. Gene → overexpression downregulates → Gene\r\n4. Gene → overexpression upregulates → Gene\r\n\r\nIn a [separate discussion](http://thinklab.com/discussion/data-nomenclature-naming-and-abbreviating-our-network-types/162#3), @larsjuhljensen commented:\r\n\r\n> I am not entirely sure how useful the \"knockdown downregulates\" etc. types are. Usually \"knockdown downregulates\" would be interpreted to mean \"upregulates\" etc.\r\n\r\nIn other words, shouldn't we combine relationship types 1 & 4 above into \"Gene → upregulates → Gene\" and 2 & 3 into \"Gene → downregulates → Gene\"? To investigate whether this makes sense, I looked into whether knockdown and overexpression profiles for the same gene were anticorrelated. Does knocking down a gene have the opposite trascriptional effect as overexpressing it?\r\n\r\nThe results were surprising ([notebook](https://github.com/dhimmel/lincs/blob/00c55f95ead78bec72b9c7255f38b512c4a3da30/binarize-consensi.ipynb)). Knockdown and overexpression of the same gene resulted in positively correlated transcriptional profiles 65.0% of the time. And if we correlate the knockdown of a random gene with the overexpression of a different random gene, we see a positive correlation 65.3% of the time. In summary, the transcriptional profiles of knocking down and overexpressing genes are more often than not positively correlated. And profiles for the same gene show no more correlation or anticorrelation than profiles for two different genes. Hmm.\r\n\r\n![Violinplots of correlation distributions](https://github.com/dhimmel/lincs/raw/00c55f95ead78bec72b9c7255f38b512c4a3da30/viz/knockdown-overexpression-corr.png)\r\n\r\nWhat could cause this counterintuitive finding?\r\n\r\n+ We could have a mistake in our code. Does anyone know of a gold standard for genetic perturbations that we could compare to?\r\n+ By looking only at the 978 landmark genes, we are overlooking the crucial genes and instead picking up on a general perturbation response.\r\n+ Gene regulation is a non-linear process.\r\n+ Our method of analysis or the LINCS L1000 data may be limitated.\r\n\r\nDoes anyone, specifically those with gene expression experience (@caseygreene, @larsjuhljensen, @fbastian), have any insight on what might be happening? I'll also reach out to the L1000 team.",
      "profile": 17,
      "published": "2016-02-26T17:17:30.858537Z",
      "thread": 171,
      "url": "/discussion/positive-correlations-between-knockdown-and-overexpression-profiles-from-lincs-l1000/171"
    },
    {
      "body_html": "<p>Re: \"While my original guidance would have been to chose DM or SYM if the drug is DM or SYM for any subtype, I see the point that some diseases may be too broad and therefore uninformative for our prediction approach.\"</p>\r\n\r\n<p>I actually think either approach is reasonable. The key would be to maintain consistency throughout the curation process. After making a final decision on how to proceed, we can go back and ensure that we are being consistent.</p>\r\n\r\n<p>I also think we may want to consider the relative frequency of subtypes of disease. For example, \"lung cancer\" has probably three very common subtypes which each account for 25-30% of the total entity of \"lung cancer\". Similarly, 90-95% of \"Hypertension\" is accounted for by \"essential hypertension\", even if you include \"pulmonary hypertension\". in contrast, most of the subtypes of \"anemia\" included in this curation each account for probably less than 1-2% of all \"anemia\".</p>",
      "body_md": "Re: \"While my original guidance would have been to chose DM or SYM if the drug is DM or SYM for any subtype, I see the point that some diseases may be too broad and therefore uninformative for our prediction approach.\"\r\n\r\nI actually think either approach is reasonable. The key would be to maintain consistency throughout the curation process. After making a final decision on how to proceed, we can go back and ensure that we are being consistent.\r\n\r\nI also think we may want to consider the relative frequency of subtypes of disease. For example, \"lung cancer\" has probably three very common subtypes which each account for 25-30% of the total entity of \"lung cancer\". Similarly, 90-95% of \"Hypertension\" is accounted for by \"essential hypertension\", even if you include \"pulmonary hypertension\". in contrast, most of the subtypes of \"anemia\" included in this curation each account for probably less than 1-2% of all \"anemia\".",
      "profile": 188,
      "published": "2016-02-23T12:59:08.531425Z",
      "thread": 95,
      "url": "/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95#9"
    },
    {
      "body_html": "<p>I certainly agree with maintaining the terminology consistent with prior studies. I think the terms \"indication and \"palliates\" are well defined as you describe. My only concern is the use of the word \"treat\" to mean \"disease-modifying\" as opposed to symptom management, especially since it is very common to use the phrase \"treat symptoms\". </p>\r\n\r\n<p>If there are other prior studies that use alternate terminology, it might be best to align with those. Otherwise, I would think the two goals are (1) maintain previous terminology and (2) make sure to define our terminology very clearly.</p>",
      "body_md": "I certainly agree with maintaining the terminology consistent with prior studies. I think the terms \"indication and \"palliates\" are well defined as you describe. My only concern is the use of the word \"treat\" to mean \"disease-modifying\" as opposed to symptom management, especially since it is very common to use the phrase \"treat symptoms\". \r\n\r\nIf there are other prior studies that use alternate terminology, it might be best to align with those. Otherwise, I would think the two goals are (1) maintain previous terminology and (2) make sure to define our terminology very clearly.",
      "profile": 188,
      "published": "2016-02-23T14:43:26.748895Z",
      "thread": 162,
      "url": "/discussion/data-nomenclature-naming-and-abbreviating-our-network-types/162#6"
    },
    {
      "body_html": "<h1>Results from all three curators</h1>\r\n\r\n<p>To recap our curation effort thus far, we first had AJG and CSH (<a href=\"/u/chrissyhessler\" class=\"username\">@chrissyhessler</a>) independently classify the 1388 indications. Then a third curator, PK (<a href=\"/u/pouyakhankhanian\" class=\"username\">@pouyakhankhanian</a>), classified each indication with access to the picks and notes from the first two curators.</p>\r\n\r\n<p>PK provided <a href=\"#7\">detailed documentation</a> of his methodology, with a focus on instances of disagreement. I now report of the results from all three curators (<a href=\"https://github.com/dhimmel/indications/blob/49cf8e3858f124e552051fc1e8c1c229a43dadf6/curation/results-three-curators.ipynb\">notebook</a>, <a href=\"https://github.com/dhimmel/indications/blob/49cf8e3858f124e552051fc1e8c1c229a43dadf6/curation/results-three-curators.tsv\">dataset</a>).</p>\r\n\r\n<p>PK's kappa coefficient was 51.5% with AJG and 65.1% with CSH. PK classified many indications as disease-modifying that the other curators considered symptomatic. Overall, there were <a href=\"https://github.com/dhimmel/indications/blob/49cf8e3858f124e552051fc1e8c1c229a43dadf6/curation/results-threeway-disagreements.tsv\">34 threeway disagreements</a> and <a href=\"https://github.com/dhimmel/indications/blob/49cf8e3858f124e552051fc1e8c1c229a43dadf6/curation/results-PK-changes.tsv\">124 instances</a> where PK disagreed with the consensus of the first two curators.</p>\r\n\r\n<h1>Reaching a consensus</h1>\r\n\r\n<p>The next step is to agree upon a consensus classification for each indication. These indications will go into our network and will be used to train our model for predicting drug repurposing.</p>\r\n\r\n<p>We would like the first two curators to review <a href=\"#7\">PK's methodology</a> and voice their opinions. In particular, do AJG and CSH agree with PK's reasoning that led him to reverse <a href=\"https://github.com/dhimmel/indications/blob/49cf8e3858f124e552051fc1e8c1c229a43dadf6/curation/results-PK-changes.tsv\">124 instances where they agreed</a>? Given this feedback, we will determine how to proceed.</p>",
      "body_md": "# Results from all three curators\r\n\r\nTo recap our curation effort thus far, we first had AJG and CSH (@chrissyhessler) independently classify the 1388 indications. Then a third curator, PK (@pouyakhankhanian), classified each indication with access to the picks and notes from the first two curators.\r\n\r\nPK provided [detailed documentation](#7) of his methodology, with a focus on instances of disagreement. I now report of the results from all three curators ([notebook](https://github.com/dhimmel/indications/blob/49cf8e3858f124e552051fc1e8c1c229a43dadf6/curation/results-three-curators.ipynb), [dataset](https://github.com/dhimmel/indications/blob/49cf8e3858f124e552051fc1e8c1c229a43dadf6/curation/results-three-curators.tsv)).\r\n\r\nPK's kappa coefficient was 51.5% with AJG and 65.1% with CSH. PK classified many indications as disease-modifying that the other curators considered symptomatic. Overall, there were [34 threeway disagreements](https://github.com/dhimmel/indications/blob/49cf8e3858f124e552051fc1e8c1c229a43dadf6/curation/results-threeway-disagreements.tsv) and [124 instances](https://github.com/dhimmel/indications/blob/49cf8e3858f124e552051fc1e8c1c229a43dadf6/curation/results-PK-changes.tsv) where PK disagreed with the consensus of the first two curators.\r\n\r\n# Reaching a consensus\r\n\r\nThe next step is to agree upon a consensus classification for each indication. These indications will go into our network and will be used to train our model for predicting drug repurposing.\r\n\r\nWe would like the first two curators to review [PK's methodology](#7) and voice their opinions. In particular, do AJG and CSH agree with PK's reasoning that led him to reverse [124 instances where they agreed](https://github.com/dhimmel/indications/blob/49cf8e3858f124e552051fc1e8c1c229a43dadf6/curation/results-PK-changes.tsv)? Given this feedback, we will determine how to proceed.",
      "profile": 17,
      "published": "2016-02-24T00:15:33.603406Z",
      "thread": 95,
      "url": "/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95#10"
    },
    {
      "body_html": "<p>I'm not sure the phrase \"drug X treats symptom Y\" is that problematic, since symptom Y is the sentence's subject rather than a disease. I agree that we should maintain existing terminology, but I'm not finding much guidance in the literature.</p>\r\n\r\n<p>Potential alternatives to \"treats\" for representing disease-modifying indications are: modifies, medicates, indicates, remedies, ameliorates, betters, improves, corrects, affects, alleviates, repairs, and cures. <a href=\"/u/pouyakhankhanian\" class=\"username\">@pouyakhankhanian</a>, do you prefer any of these verbs to \"treats\"?</p>\r\n\r\n<p>And regardless of which term we pick, we'll make sure to define each relationship type.</p>",
      "body_md": "I'm not sure the phrase \"drug X treats symptom Y\" is that problematic, since symptom Y is the sentence's subject rather than a disease. I agree that we should maintain existing terminology, but I'm not finding much guidance in the literature.\r\n\r\nPotential alternatives to \"treats\" for representing disease-modifying indications are: modifies, medicates, indicates, remedies, ameliorates, betters, improves, corrects, affects, alleviates, repairs, and cures. @pouyakhankhanian, do you prefer any of these verbs to \"treats\"?\r\n\r\nAnd regardless of which term we pick, we'll make sure to define each relationship type.",
      "profile": 17,
      "published": "2016-02-24T01:31:28.304553Z",
      "thread": 162,
      "url": "/discussion/data-nomenclature-naming-and-abbreviating-our-network-types/162#7"
    },
    {
      "body_html": "<p>We've previously discussed <a href=\"http://thinklab.com/discussion/permuting-hetnets-and-implementing-randomized-edge-swaps-in-cypher/136#1\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d136\">what hetnet permutation is and why we do it</a>. To permute a hetnet, we go through each relationship type (metaedge) and repeatedly swap the target nodes of two random relationships (edges). This strategy is called <code>XSwap</code> <span class=\"citation\">[<a href=\"/doi/10.1137/1.9781611972795.67\" class=\"citation\" data-key=\"10.1137/1.9781611972795.67\">1</a>]</span>.</p>\r\n\r\n<p>We looked into performing the <a href=\"http://thinklab.com/discussion/permuting-hetnets-and-implementing-randomized-edge-swaps-in-cypher/136#2\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d136\">permutation in neo4j using cypher</a>, but decided to stick with our <a href=\"https://github.com/dhimmel/hetio/blob/a6c8a286ec0c7367673970c6ddda06cd47733034/hetio/permute.py#L7\" title=\"hetio.permute.permute_graph · dhimmel/hetio on GitHub\">python implementation</a> since cypher's cost planner currently lacks the needed abilities.</p>\r\n\r\n<h2>Implementation specifics</h2>\r\n\r\n<p>We closely followed the parameters from our previous study <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1004259\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1004259\">2</a>]</span> and did the following:</p>\r\n\r\n<ul><li>We created 5 permuted hetnets. The first permutated hetnet was created from the unpermuted hetnet; the second permutated hetnet was created from the first permutated hetnet; and so on until the fifth permutated hetnet was created from the fourth permutated hetnet. This iterative strategy is referred to as a Markov chain.</li><li>To create each permuted hetnet, we separately permuted each metaedge. For a given metaedge, we attempted <em>n</em> XSwaps where <em>n</em> equals four times the number of edges (<code>multipler = 4</code>).</li><li>Xswaps can be unsuccessful for several reasons. The same edge could have been randomly selected twice (referred to as <code>same_edge</code>). One or both of the potential new edges may already exist (<code>duplicate</code> or <code>undirected_duplicate</code> for select cases where a biderectional edge connects two nodes of the same type). One or both of the potential new edges may connect a node to itself (<code>self_loop</code>). In these instances, no swap is performed. In the future, we may switch to stopping a completing permutation after a certain number of successes rather than attempts.</li></ul>\r\n\r\n<h2>Assessing permutation effectiveness</h2>\r\n\r\n<p>For each permutation and each metaedge, we measure the progress of the randomization at 10 points (<a href=\"https://github.com/dhimmel/integrate/blob/ebd71cd2157d26e52646b5b483f5c70293a84f71/data/permuted/stats.tsv\">dataset</a>). The measure we're primarily interested in is the percent of edges that are unchanged after a permutation (<code>unchanged</code>).</p>\r\n\r\n<p>We find that the percent of unchanged edges varies by metaedge (<a href=\"http://nbviewer.jupyter.org/github/dhimmel/integrate/blob/ebd71cd2157d26e52646b5b483f5c70293a84f71/data/permuted/evaluate-permutations.ipynb#How-many-permutations-are-needed-to-randomize-an-edge\">notebook cell 4</a>). It appears that we could safely reduce our multiplier from 4 to 2.5 and still generate permuted networks that are maximally diversified from their predecessor.</p>\r\n\r\n<p>Of concern are metaedges where a high percentage of the edges do not change. This occurred when a high percentage of swaps resulted in already existing edges (<a href=\"https://github.com/dhimmel/integrate/blob/ebd71cd2157d26e52646b5b483f5c70293a84f71/data/permuted/evaluate-permutations.ipynb\">notebook cell 6</a>). Particularly troublesome was the <em>Anatomy–expresses–Gene</em> edge where almost all attempts yielded duplicated edges and only ~10% of edges changed from a permutation. I'm now inclined to revisit our <a href=\"http://thinklab.com/discussion/processing-bgee-for-tissue-specific-gene-presence-and-overunder-expression/124#2\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d124\">previous observation</a> that we're being too permissive regarding expression edge inclusion.</p>\r\n\r\n<p>Metaedges whose edges do not change from permutation are limited in informativeness. Such edges hold little information besides their degree contribution to the nodes they connect. In the context of our expression edge, the problem is visible in the node degree distribution: most anatomies express 0 genes while a minority of anatomies express an extremely high number of genes (<a href=\"http://nbviewer.jupyter.org/github/dhimmel/integrate/blob/ebd71cd2157d26e52646b5b483f5c70293a84f71/viz/degrees.pdf#page=10\">see the <code>anatomy - expresses - gene</code> panel on page 10</a>).</p>",
      "body_md": "We've previously discussed [what hetnet permutation is and why we do it](http://thinklab.com/discussion/permuting-hetnets-and-implementing-randomized-edge-swaps-in-cypher/136#1). To permute a hetnet, we go through each relationship type (metaedge) and repeatedly swap the target nodes of two random relationships (edges). This strategy is called `XSwap` [@10.1137/1.9781611972795.67].\r\n\r\nWe looked into performing the [permutation in neo4j using cypher](http://thinklab.com/discussion/permuting-hetnets-and-implementing-randomized-edge-swaps-in-cypher/136#2), but decided to stick with our [python implementation](https://github.com/dhimmel/hetio/blob/a6c8a286ec0c7367673970c6ddda06cd47733034/hetio/permute.py#L7 \"hetio.permute.permute_graph · dhimmel/hetio on GitHub\") since cypher's cost planner currently lacks the needed abilities.\r\n\r\n## Implementation specifics\r\n\r\nWe closely followed the parameters from our previous study [@10.1371/journal.pcbi.1004259] and did the following:\r\n\r\n+ We created 5 permuted hetnets. The first permutated hetnet was created from the unpermuted hetnet; the second permutated hetnet was created from the first permutated hetnet; and so on until the fifth permutated hetnet was created from the fourth permutated hetnet. This iterative strategy is referred to as a Markov chain.\r\n+ To create each permuted hetnet, we separately permuted each metaedge. For a given metaedge, we attempted _n_ XSwaps where _n_ equals four times the number of edges (`multipler = 4`).\r\n+ Xswaps can be unsuccessful for several reasons. The same edge could have been randomly selected twice (referred to as `same_edge`). One or both of the potential new edges may already exist (`duplicate` or `undirected_duplicate` for select cases where a biderectional edge connects two nodes of the same type). One or both of the potential new edges may connect a node to itself (`self_loop`). In these instances, no swap is performed. In the future, we may switch to stopping a completing permutation after a certain number of successes rather than attempts.\r\n\r\n## Assessing permutation effectiveness\r\n\r\nFor each permutation and each metaedge, we measure the progress of the randomization at 10 points ([dataset](https://github.com/dhimmel/integrate/blob/ebd71cd2157d26e52646b5b483f5c70293a84f71/data/permuted/stats.tsv)). The measure we're primarily interested in is the percent of edges that are unchanged after a permutation (`unchanged`).\r\n\r\nWe find that the percent of unchanged edges varies by metaedge ([notebook cell 4](http://nbviewer.jupyter.org/github/dhimmel/integrate/blob/ebd71cd2157d26e52646b5b483f5c70293a84f71/data/permuted/evaluate-permutations.ipynb#How-many-permutations-are-needed-to-randomize-an-edge)). It appears that we could safely reduce our multiplier from 4 to 2.5 and still generate permuted networks that are maximally diversified from their predecessor.\r\n\r\nOf concern are metaedges where a high percentage of the edges do not change. This occurred when a high percentage of swaps resulted in already existing edges ([notebook cell 6](https://github.com/dhimmel/integrate/blob/ebd71cd2157d26e52646b5b483f5c70293a84f71/data/permuted/evaluate-permutations.ipynb)). Particularly troublesome was the _Anatomy--expresses--Gene_ edge where almost all attempts yielded duplicated edges and only ~10% of edges changed from a permutation. I'm now inclined to revisit our [previous observation](http://thinklab.com/discussion/processing-bgee-for-tissue-specific-gene-presence-and-overunder-expression/124#2) that we're being too permissive regarding expression edge inclusion.\r\n\r\nMetaedges whose edges do not change from permutation are limited in informativeness. Such edges hold little information besides their degree contribution to the nodes they connect. In the context of our expression edge, the problem is visible in the node degree distribution: most anatomies express 0 genes while a minority of anatomies express an extremely high number of genes ([see the `anatomy - expresses - gene` panel on page 10](http://nbviewer.jupyter.org/github/dhimmel/integrate/blob/ebd71cd2157d26e52646b5b483f5c70293a84f71/viz/degrees.pdf#page=10)).",
      "profile": 17,
      "published": "2016-02-25T22:56:18.333496Z",
      "thread": 178,
      "url": "/discussion/assessing-the-effectiveness-of-our-hetnet-permutations/178"
    },
    {
      "body_html": "<p>Quick thoughts: Is there a specific set of genes driving the positive correlation? Maybe perturbations in general lead to some change in, for example, growth rate? What, specifically, is your high correlation measuring? Is it possible that highly expressed genes tend to remain highly expressed, or did you transform the data in some way to normalize gene expression across conditions per gene.</p>",
      "body_md": "Quick thoughts: Is there a specific set of genes driving the positive correlation? Maybe perturbations in general lead to some change in, for example, growth rate? What, specifically, is your high correlation measuring? Is it possible that highly expressed genes tend to remain highly expressed, or did you transform the data in some way to normalize gene expression across conditions per gene.",
      "profile": 22,
      "published": "2016-02-26T17:26:44.322718Z",
      "thread": 171,
      "url": "/discussion/positive-correlations-between-knockdown-and-overexpression-profiles-from-lincs-l1000/171#2"
    },
    {
      "body_html": "<p>Messing about with cells always tends to induce some degree of stress-induced global expression changes. This is the case pretty much no matter which perturbation you do to the cells, including overexpression of some gene, knockdown of some gene, cell-cycle synchronization, centrifugation, increasing temperature, decreasing temperature, etc.</p>\r\n\r\n<p>My guess is that the small positive correlation you see is caused by small changes in expression of a large number of genes, which you could summarize as \"not a happy cell\".</p>",
      "body_md": "Messing about with cells always tends to induce some degree of stress-induced global expression changes. This is the case pretty much no matter which perturbation you do to the cells, including overexpression of some gene, knockdown of some gene, cell-cycle synchronization, centrifugation, increasing temperature, decreasing temperature, etc.\r\n\r\nMy guess is that the small positive correlation you see is caused by small changes in expression of a large number of genes, which you could summarize as \"not a happy cell\".",
      "profile": 125,
      "published": "2016-02-26T17:28:07.784153Z",
      "thread": 171,
      "url": "/discussion/positive-correlations-between-knockdown-and-overexpression-profiles-from-lincs-l1000/171#3"
    },
    {
      "body_html": "<blockquote><p>Did you transform the data in some way to normalize gene expression across conditions per gene.</p></blockquote>\r\n\r\n<p><a href=\"/u/caseygreene\" class=\"username\">@caseygreene</a>, our profiles contain <em>z</em>-scores measuring the differential expression for 978 genes. The profiles (called consensus signatures in <a href=\"http://support.lincscloud.org/hc/en-us/articles/202099616-Signature-Generation-and-Analysis-L1000-\">L1000 terminology</a>) are at the CONSENSUS stage in the following pipeline:</p>\r\n\r\n<p><img src=\"http://support.lincscloud.org/hc/en-us/article_attachments/200733106/data_flow.png\" alt=\"Processing of Broad LINCS data\"></p>\r\n\r\n<p>The <em>z</em>-scores compare a gene's expression level in cells given the perturbation to cells without the perturbation (controls). I believe the controls account for the non-specific disturbances caused by delivering the molecular payload, but will confirm.</p>\r\n\r\n<p>Now I will look into the following questions:</p>\r\n\r\n<ul><li><a href=\"/u/larsjuhljensen\" class=\"username\">@larsjuhljensen</a>: My guess is that the small positive correlation you see is caused by small changes in expression of a large number of genes.</li><li><a href=\"/u/caseygreene\" class=\"username\">@caseygreene</a>: Is there a specific set of genes driving the positive correlation?</li></ul>",
      "body_md": "> Did you transform the data in some way to normalize gene expression across conditions per gene.\r\n\r\n@caseygreene, our profiles contain _z_-scores measuring the differential expression for 978 genes. The profiles (called consensus signatures in [L1000 terminology](http://support.lincscloud.org/hc/en-us/articles/202099616-Signature-Generation-and-Analysis-L1000-)) are at the CONSENSUS stage in the following pipeline:\r\n\r\n![Processing of Broad LINCS data](http://support.lincscloud.org/hc/en-us/article_attachments/200733106/data_flow.png)\r\n\r\nThe _z_-scores compare a gene's expression level in cells given the perturbation to cells without the perturbation (controls). I believe the controls account for the non-specific disturbances caused by delivering the molecular payload, but will confirm.\r\n\r\nNow I will look into the following questions:\r\n\r\n+ @larsjuhljensen: My guess is that the small positive correlation you see is caused by small changes in expression of a large number of genes.\r\n+ @caseygreene: Is there a specific set of genes driving the positive correlation?",
      "profile": 17,
      "published": "2016-02-26T19:53:18.806257Z",
      "thread": 171,
      "url": "/discussion/positive-correlations-between-knockdown-and-overexpression-profiles-from-lincs-l1000/171#4"
    },
    {
      "body_html": "<h1>Methods for reducing the number of Bgee expression relationships</h1>\r\n\r\n<p><a href=\"/u/fbastian\" class=\"username\">@fbastian</a>, we're looking to reduce the number of expression relationships extracted from Bgee. Our motivation is twofold:</p>\r\n\r\n<ol><li>Our hetnet <a href=\"https://github.com/dhimmel/integrate/blob/dff453c020bbea953adc6cc3225235e445ba94f9/data/summary/metaedges.tsv#L3\" title=\"hetio-ind metaedge summaries\">currently contains</a> over 1 million <em>Anatomy–expresses–Gene</em> relationships. This high number of relationships is causing computational bottlenecks.</li><li>Our network permutations <a href=\"http://thinklab.com/discussion/assessing-the-effectiveness-of-our-hetnet-permutations/178#1\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d178\">do little to randomize the expression relationships</a>. In other words, too many expression relationships connect super anatomies — anatomies which express most genes — limiting the information content of the edge.</li></ol>\r\n\r\n<p>From our previous conversations, it appears that there are three ways to proceed:</p>\r\n\r\n<ol><li>Exclude relationships for general anatomies, as <a href=\"#3\">suggested above</a>.</li><li>Use only RNA-Seq experiments. <a href=\"/u/fbastian\" class=\"username\">@fbastian</a> <a href=\"http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#5\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d81\">mentions</a> that using only RNA-Seq data avoids the ambiguity states. So what happens when a gene is present in one experiment but not the other for the same data type and conditions? With RNA-seq we should be able to adjust the RPKM inclusion threshold.</li><li>Use unpropagated relationships, so for example genes expressed in for brain gray matter would not automatically be transmitted to brain.</li></ol>\r\n\r\n<p>These options are not mutually exclusive. We can choose any combination of the three.</p>\r\n\r\n<p>I am leaning towards option 3, because I think studies will often be performed on clinically relevant anatomies. In other words, we may accomplish the goal of 1 (removing overly broad anatomies) by including only relationships to their directly annotated anatomies. Additionally, since our hetnet contains <a href=\"http://thinklab.com/discussion/tissue-node/41#16\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d41\">many nested levels Uberon terms</a>,  we would not be throwing out too many experiments entirely. In the future, we can even write queries that perform the propagation in realtime.</p>\r\n\r\n<p><strong>Data complications:</strong> In <code>Homo_sapiens_expr-complete.tsv</code> version 13.1, all the values for <code>In situ call quality</code> and <code>In situ call quality</code> equal <code>no data</code>. Additionally, all values for <code>Including in situ observed data</code> are <code>no</code>. <a href=\"/u/fbastian\" class=\"username\">@fbastian</a>, I assume this is a bug? Is there a workaround? Advice on the specific filters to apply on which files to achieve options 2 or 3 would be appreciated.</p>",
      "body_md": "# Methods for reducing the number of Bgee expression relationships\r\n\r\n@fbastian, we're looking to reduce the number of expression relationships extracted from Bgee. Our motivation is twofold:\r\n\r\n1. Our hetnet [currently contains](https://github.com/dhimmel/integrate/blob/dff453c020bbea953adc6cc3225235e445ba94f9/data/summary/metaedges.tsv#L3 \"hetio-ind metaedge summaries\") over 1 million _Anatomy--expresses--Gene_ relationships. This high number of relationships is causing computational bottlenecks.\r\n2. Our network permutations [do little to randomize the expression relationships](http://thinklab.com/discussion/assessing-the-effectiveness-of-our-hetnet-permutations/178#1). In other words, too many expression relationships connect super anatomies -- anatomies which express most genes -- limiting the information content of the edge.\r\n\r\nFrom our previous conversations, it appears that there are three ways to proceed:\r\n\r\n1. Exclude relationships for general anatomies, as [suggested above](#3).\r\n2. Use only RNA-Seq experiments. @fbastian [mentions](http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#5) that using only RNA-Seq data avoids the ambiguity states. So what happens when a gene is present in one experiment but not the other for the same data type and conditions? With RNA-seq we should be able to adjust the RPKM inclusion threshold.\r\n3. Use unpropagated relationships, so for example genes expressed in for brain gray matter would not automatically be transmitted to brain.\r\n\r\nThese options are not mutually exclusive. We can choose any combination of the three.\r\n\r\nI am leaning towards option 3, because I think studies will often be performed on clinically relevant anatomies. In other words, we may accomplish the goal of 1 (removing overly broad anatomies) by including only relationships to their directly annotated anatomies. Additionally, since our hetnet contains [many nested levels Uberon terms](http://thinklab.com/discussion/tissue-node/41#16),  we would not be throwing out too many experiments entirely. In the future, we can even write queries that perform the propagation in realtime.\r\n\r\n**Data complications:** In `Homo_sapiens_expr-complete.tsv` version 13.1, all the values for `In situ call quality` and `In situ call quality` equal `no data`. Additionally, all values for `Including in situ observed data` are `no`. @fbastian, I assume this is a bug? Is there a workaround? Advice on the specific filters to apply on which files to achieve options 2 or 3 would be appreciated.",
      "profile": 17,
      "published": "2016-03-01T19:30:53.637385Z",
      "thread": 124,
      "url": "/discussion/processing-bgee-for-tissue-specific-gene-presence-and-overunder-expression/124#4"
    },
    {
      "body_html": "<p>Very thorough analysis by Pouya. I agree with the slight change in the definitions of categories. One clarification: is a drug still considered disease modifying if there are significant and dangerous side effects? amendment 1 seems to indicate that the answer is yes.</p>\r\n\r\n<p>For the most part, I agree with how you classified the following. I have made some comments for the particularly challenging disease categories:</p>\r\n\r\n<ul><li><strong>hypertension, general</strong></li><li><strong>hypertension, diuretics</strong></li><li><strong>hypertension, ocular and pulmonary htn</strong></li><li><strong>diabetes, drugs that lower blood sugar</strong></li><li><strong>diabetes, ACEi and ARBs</strong></li><li><strong>epilepsy</strong>: I think I agree with categorizing all AEDs as DM instead of SYM. I thought the pathophysiology behind MTS was not clearcut and treating someone with AEDs does not necessarily prevent MTS, although I think it may prevent morbidity and mortality.</li><li><strong>OA, NSAIDs and steroids</strong></li><li><strong>cancers, pain meds</strong></li><li><strong>hematologic cancers, steroids</strong></li><li><strong>non-heme cancers, steroids</strong></li><li><strong>cancers, hydroxyura and other chemotx</strong></li><li><strong>cancers, bisphos</strong></li><li><strong>CAD, drugs for htn or dm</strong></li><li><strong>CAD, diuretics</strong></li><li><strong>migraine</strong>: I think prophylactic medications should be DM; abortives may be better classified as SYM</li><li><strong>asthma, steroids, beta gonists, antichol</strong></li><li><strong>allergic rhinitis</strong>: this categorization makes sense to me.</li><li><strong>COPD, general</strong></li><li><strong>etoh dependence</strong></li><li><strong>psych disease</strong></li><li><strong>AD</strong></li><li><strong>anemia</strong>: I agree. It is like trying to categorize \"leukocytosis\" or some other lab abnormality, without getting at the etiology.</li></ul>\r\n\r\n<p>I am still stuck on one disease entity:</p>\r\n\r\n<ul><li><strong>Autoimmune diseases, steroids</strong>: do steroids actually change the long-term disease of autoimmune diseases? Do they reduce morbidity and mortality?</li></ul>",
      "body_md": "Very thorough analysis by Pouya. I agree with the slight change in the definitions of categories. One clarification: is a drug still considered disease modifying if there are significant and dangerous side effects? amendment 1 seems to indicate that the answer is yes.\r\n \r\nFor the most part, I agree with how you classified the following. I have made some comments for the particularly challenging disease categories:\r\n\r\n+ **hypertension, general**\r\n+ **hypertension, diuretics**\r\n+ **hypertension, ocular and pulmonary htn**\r\n+ **diabetes, drugs that lower blood sugar**\r\n+ **diabetes, ACEi and ARBs**\r\n+ **epilepsy**: I think I agree with categorizing all AEDs as DM instead of SYM. I thought the pathophysiology behind MTS was not clearcut and treating someone with AEDs does not necessarily prevent MTS, although I think it may prevent morbidity and mortality.\r\n+ **OA, NSAIDs and steroids**\r\n+ **cancers, pain meds**\r\n+ **hematologic cancers, steroids**\r\n+ **non-heme cancers, steroids**\r\n+ **cancers, hydroxyura and other chemotx**\r\n+ **cancers, bisphos**\r\n+ **CAD, drugs for htn or dm**\r\n+ **CAD, diuretics**\r\n+ **migraine**: I think prophylactic medications should be DM; abortives may be better classified as SYM\r\n+ **asthma, steroids, beta gonists, antichol**\r\n+ **allergic rhinitis**: this categorization makes sense to me.\r\n+ **COPD, general**\r\n+ **etoh dependence**\r\n+ **psych disease**\r\n+ **AD**\r\n+ **anemia**: I agree. It is like trying to categorize \"leukocytosis\" or some other lab abnormality, without getting at the etiology.\r\n\r\nI am still stuck on one disease entity:\r\n\r\n+ **Autoimmune diseases, steroids**: do steroids actually change the long-term disease of autoimmune diseases? Do they reduce morbidity and mortality?",
      "profile": 172,
      "published": "2016-03-02T16:49:17.802833Z",
      "thread": 95,
      "url": "/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95#11"
    },
    {
      "body_html": "<h1>Initial release of STARGEO analyses</h1>\r\n\r\n<p>We've released the first complete version of our STARGEO analysis (<a href=\"https://github.com/dhimmel/stargeo\">repository</a> <span class=\"citation\">[<a href=\"/doi/10.5281/zenodo.46866\" class=\"citation\" data-key=\"10.5281/zenodo.46866\">1</a>]</span>). Thanks <a href=\"/u/idrdex\" class=\"username\">@idrdex</a> for helping us get all of the queries <a href=\"https://github.com/idrdex/star_api/issues/13#issuecomment-191414422\">running smoothly</a>.</p>\r\n\r\n<p>In summary, we defined case-control queries for <a href=\"https://github.com/dhimmel/stargeo/blob/1a11633b5e0095454453335be82012a9f0f482e4/data/queries.tsv\">66 diseases</a>. Of these diseases, 49 contained sufficient data (multiple studies with at least 3 samples per class). We used STARGEO's random effects meta-analysis and applied an FDR <em>p</em>-value threshold of 0.05 to identify deferentially expressed genes for each disease. </p>\r\n\r\n<p>48,688 <em>Disease–downregulates–Gene</em> and 50,287 <em>Disease–upregulates–Gene</em> relationships were identified (<a href=\"https://github.com/dhimmel/stargeo/blob/1a11633b5e0095454453335be82012a9f0f482e4/data/diffex.tsv\">dataset</a>). The number of dysregulated genes <a href=\"https://github.com/dhimmel/stargeo/blob/1a11633b5e0095454453335be82012a9f0f482e4/data/summary.tsv\">varied widely</a> by disease. No deferentially expressed genes were identified for endogenous depression, which had a combined sample size of <a href=\"https://github.com/dhimmel/stargeo/blob/1a11633b5e0095454453335be82012a9f0f482e4/data/doslim/DOID_1595/samples.tsv\">533 cases and controls</a>.</p>\r\n\r\n<p>Now we will integrate these relationships into our hetnet. We may choose to limit each disease to the 500 most significantly up and 500 most significantly down-regulated genes.</p>",
      "body_md": "# Initial release of STARGEO analyses\r\n\r\nWe've released the first complete version of our STARGEO analysis ([repository](https://github.com/dhimmel/stargeo) [@10.5281/zenodo.46866]). Thanks @idrdex for helping us get all of the queries [running smoothly](https://github.com/idrdex/star_api/issues/13#issuecomment-191414422).\r\n\r\nIn summary, we defined case-control queries for [66 diseases](https://github.com/dhimmel/stargeo/blob/1a11633b5e0095454453335be82012a9f0f482e4/data/queries.tsv). Of these diseases, 49 contained sufficient data (multiple studies with at least 3 samples per class). We used STARGEO's random effects meta-analysis and applied an FDR _p_-value threshold of 0.05 to identify deferentially expressed genes for each disease. \r\n\r\n48,688 _Disease--downregulates--Gene_ and 50,287 _Disease--upregulates--Gene_ relationships were identified ([dataset](https://github.com/dhimmel/stargeo/blob/1a11633b5e0095454453335be82012a9f0f482e4/data/diffex.tsv)). The number of dysregulated genes [varied widely](https://github.com/dhimmel/stargeo/blob/1a11633b5e0095454453335be82012a9f0f482e4/data/summary.tsv) by disease. No deferentially expressed genes were identified for endogenous depression, which had a combined sample size of [533 cases and controls](https://github.com/dhimmel/stargeo/blob/1a11633b5e0095454453335be82012a9f0f482e4/data/doslim/DOID_1595/samples.tsv).\r\n\r\nNow we will integrate these relationships into our hetnet. We may choose to limit each disease to the 500 most significantly up and 500 most significantly down-regulated genes.",
      "profile": 17,
      "published": "2016-03-03T00:50:58.434610Z",
      "thread": 96,
      "url": "/discussion/stargeo-expression-signatures-for-disease-using-crowdsourced-geo-annotation/96#10"
    },
    {
      "body_html": "<ul><li><p><strong>Re: Autoimmune diseases and steroids</strong>. I do believe that steroids are DM in a variety of auto-immune diseases. The easiest examples are Lupus and RA where the occasional difficult-to-control patient is given low-dose maintenance steroids to prevent disease flares (i.e. reduce morbidity). Tougher examples include MS but there is some evidence that it may help decrease disease activity and relapse rate <span class=\"citation\">[<a href=\"/doi/10.1212/wnl.57.7.1239\" class=\"citation\" data-key=\"10.1212/wnl.57.7.1239\">1</a>]</span>, which puts it is a similar category to say Copaxone which we would probably mark as DM (even though copaxone also does not delay progression of disease). Given that, I felt it was reasonable to assume the same for other auto-immune diseases, although I must admit I'm probably under-qualified to comment on the subtleties of this (perhaps we can curb-side a rheumatologist). I fully agree that there is a high side-effect burden and chronic steroids are probably not clinically indicated for treatment, but perhaps they should still DM for the purposes of this study. I think of it like this, if Daniel's analysis could suggest a drug because it acted on some of the same molecular targets as do steroids, but that drug had zero side effects, then would that drug be of interest in treating auto-immune diseases? If yes, then I think we should classify steroids as DM.</p></li><li><p><strong>Re: epilepsy</strong>. Good point that AEDs prevent morbidity and mortality, it's clearly better than my hand-waving and highly disputed MTS argument. Additionally, I think we should consider the same argument as above, if Daniel's network were to find a drug that acts on the same molecular targets as our AEDs, then would we consider that drug when treating epilepsy? I would think yes.</p></li><li><p><strong>Re: migraine</strong>. I wholly agree with what you wrote. I think I just didn't state it as clearly as you did.</p></li></ul>",
      "body_md": "- **Re: Autoimmune diseases and steroids**. I do believe that steroids are DM in a variety of auto-immune diseases. The easiest examples are Lupus and RA where the occasional difficult-to-control patient is given low-dose maintenance steroids to prevent disease flares (i.e. reduce morbidity). Tougher examples include MS but there is some evidence that it may help decrease disease activity and relapse rate [@10.1212/wnl.57.7.1239], which puts it is a similar category to say Copaxone which we would probably mark as DM (even though copaxone also does not delay progression of disease). Given that, I felt it was reasonable to assume the same for other auto-immune diseases, although I must admit I'm probably under-qualified to comment on the subtleties of this (perhaps we can curb-side a rheumatologist). I fully agree that there is a high side-effect burden and chronic steroids are probably not clinically indicated for treatment, but perhaps they should still DM for the purposes of this study. I think of it like this, if Daniel's analysis could suggest a drug because it acted on some of the same molecular targets as do steroids, but that drug had zero side effects, then would that drug be of interest in treating auto-immune diseases? If yes, then I think we should classify steroids as DM.\r\n\r\n- **Re: epilepsy**. Good point that AEDs prevent morbidity and mortality, it's clearly better than my hand-waving and highly disputed MTS argument. Additionally, I think we should consider the same argument as above, if Daniel's network were to find a drug that acts on the same molecular targets as our AEDs, then would we consider that drug when treating epilepsy? I would think yes.\r\n\r\n- **Re: migraine**. I wholly agree with what you wrote. I think I just didn't state it as clearly as you did.",
      "profile": 188,
      "published": "2016-03-03T01:18:31.088155Z",
      "thread": 95,
      "url": "/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95#12"
    },
    {
      "body_html": "<p>For me, the two working solutions would either be:  </p>\r\n\r\n<ul><li>\"3. Use unpropagated relationships\": actually, you would still use the propagated data, but you would consider only the anatomical structures described in the experiments. This is what our \"simple\" expression file contains.</li><li>or manually select a list of 50-100 anatomical structures you are interested in, and use the propagated data in them. Maybe you can also include all cell types. This is very similar to the previous solution, except that you'd have more freedom about the choice of the organs. Although you should already have lots of organs experimentally described once we include the GTEx data (see bottom of this message), so the previous solution might be good enough.</li></ul>\r\n\r\n<p>\"2. Use only RNA-Seq experiments\" is incorrect: you'll still get the ambiguity states if two libraries provide contradicting information. And, it would be sad not to use other data types – lots of good information come from Affymetrix data.<br>And adjusting the RPKM threshold will not solve your problem: your problem comes from the propagation of data to upper level terms.</p>\r\n\r\n<p>To implement solution \"3\", you can either use our \"simple\" expression file, or use the \"complete\" file, but filter thanks to the column \"Observed data\": <a href=\"http://bgee.org/?page=doc&amp;action=call_files#single_expr_complete_col9\">http://bgee.org/?page=doc&amp;action=call_files#single_expr_complete_col9</a></p>\r\n\r\n<p>Otherwise, we can think of alternative solutions: </p>\r\n\r\n<ul><li>Using only over-/under-expression data. We should have a lot of them thanks to the GTEx data in the near future</li><li>Using only the best-ranked anatomical structures for each gene, rather than all data, and do the propagation based on that (e.g., in TISSUES the anatomical structures are ranked – we are going to release a gene page next week also providing ranked anatomical structures). </li><li>Using a completely different approach, based on gene lists rather than individual genes: see our new GO-like expression enrichment test: <a href=\"http://bgee.org/?page=top_anat\">http://bgee.org/?page=top_anat</a>. I don't know if it can be applied to your network, though, but maybe you can link groups of genes to anatomical structures, rather than individual genes.</li></ul>\r\n\r\n<p><strong>About in situ data:</strong> we don't have in situ data for human. I don't know of any resource providing systematic gene expression analyses in human using in situ hybridization. This data type is not well suited for human (e.g., I think you're not supposed to use frozen tissues). </p>\r\n\r\n<p><strong>Additional information:</strong> we have started the pipeline for Bgee 14, that will include the GTEx data. The full pipeline run should take a few months, but you'll get the data ultimately. Also, I'll let you know about the status of our work of re-annotation in the other thread, as we have completed it.</p>",
      "body_md": "For me, the two working solutions would either be:  \r\n\r\n* \"3. Use unpropagated relationships\": actually, you would still use the propagated data, but you would consider only the anatomical structures described in the experiments. This is what our \"simple\" expression file contains.\r\n* or manually select a list of 50-100 anatomical structures you are interested in, and use the propagated data in them. Maybe you can also include all cell types. This is very similar to the previous solution, except that you'd have more freedom about the choice of the organs. Although you should already have lots of organs experimentally described once we include the GTEx data (see bottom of this message), so the previous solution might be good enough.\r\n\r\n\"2. Use only RNA-Seq experiments\" is incorrect: you'll still get the ambiguity states if two libraries provide contradicting information. And, it would be sad not to use other data types – lots of good information come from Affymetrix data.\r\nAnd adjusting the RPKM threshold will not solve your problem: your problem comes from the propagation of data to upper level terms.\r\n\r\nTo implement solution \"3\", you can either use our \"simple\" expression file, or use the \"complete\" file, but filter thanks to the column \"Observed data\": http://bgee.org/?page=doc&action=call_files#single_expr_complete_col9\r\n\r\nOtherwise, we can think of alternative solutions: \r\n\r\n* Using only over-/under-expression data. We should have a lot of them thanks to the GTEx data in the near future\r\n* Using only the best-ranked anatomical structures for each gene, rather than all data, and do the propagation based on that (e.g., in TISSUES the anatomical structures are ranked – we are going to release a gene page next week also providing ranked anatomical structures). \r\n* Using a completely different approach, based on gene lists rather than individual genes: see our new GO-like expression enrichment test: http://bgee.org/?page=top_anat. I don't know if it can be applied to your network, though, but maybe you can link groups of genes to anatomical structures, rather than individual genes.\r\n\r\n\r\n**About in situ data:** we don't have in situ data for human. I don't know of any resource providing systematic gene expression analyses in human using in situ hybridization. This data type is not well suited for human (e.g., I think you're not supposed to use frozen tissues). \r\n\r\n**Additional information:** we have started the pipeline for Bgee 14, that will include the GTEx data. The full pipeline run should take a few months, but you'll get the data ultimately. Also, I'll let you know about the status of our work of re-annotation in the other thread, as we have completed it.",
      "profile": 111,
      "published": "2016-03-03T17:50:32.818528Z",
      "thread": 124,
      "url": "/discussion/processing-bgee-for-tissue-specific-gene-presence-and-overunder-expression/124#5"
    },
    {
      "body_html": "<h2>Clarification</h2>\r\n\r\n<p>Let's see if I understand: It is not possible to tell whether gene presence for a specific condition (anatomy–developmental stage) was from an experiment on that exact condition or was propagated. However, the simple file (or complete file filtered for <code>Observed data</code>) contains only conditions with directly annotated experiments, but any given gene presence call may be from propagation.</p>\r\n\r\n<h2>Proposed gene presence method</h2>\r\n\r\n<p>I think the ideal setup for our network would be <em>propagation by developmental stage but not by anatomical structure</em>. Using just the simple or complete datasets, this doesn't currently seem to be possible. However, I've what about the following workaround: using all adult stages on the simple dataset.</p>\r\n\r\n<p>Using the simple dataset, I found all gene–anatomy pairs where <code>Expression</code> is <code>present</code> and <code>Call quality</code> is <code>high quality</code> for any adult developmental stage. To identify adult developmental stages, I filtered for <code>HsapDv:0000087</code> and its descendants (<a href=\"https://github.com/dhimmel/bgee/blob/5cfed945305d7ec118fe79a1486c28d6a97ee8ee/developmental-stages.ipynb\">notebook</a>, <a href=\"https://github.com/dhimmel/bgee/blob/5cfed945305d7ec118fe79a1486c28d6a97ee8ee/data/stages.tsv#L76\">dataset</a>).</p>\r\n\r\n<p>The resulting presence/absence matrix of gene expression is 16,257 genes × 188 anatomies  (<a href=\"https://github.com/dhimmel/bgee/blob/5cfed945305d7ec118fe79a1486c28d6a97ee8ee/bgee.ipynb\">notebook</a>, <a href=\"https://github.com/dhimmel/bgee/blob/5cfed945305d7ec118fe79a1486c28d6a97ee8ee/data/present-in-adult.tsv.gz\">dataset</a>) compared to the <a href=\"http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#8\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d81\">previous dimensions</a> of 16,278 genes × 666 anatomies. We hope that this pruning of anatomies will help address the network problems we <a href=\"#4\">were facing</a>.</p>\r\n\r\n<h2>Miscellaneous</h2>\r\n\r\n<p>Thanks <a href=\"/u/fbastian\" class=\"username\">@fbastian</a> for suggesting other possible approaches. For future networks, we will revisit these options. For now we're looking for the most straightforward and immediately actionable option.</p>\r\n\r\n<blockquote><p>We don't have in situ data for human. I don't know of any resource providing systematic gene expression analyses in human using in situ hybridization.</p></blockquote>\r\n\r\n<p>I had misinterpreted <code>Including in situ observed data</code> to mean <code>Observed data</code></p>",
      "body_md": "## Clarification\r\n\r\nLet's see if I understand: It is not possible to tell whether gene presence for a specific condition (anatomy--developmental stage) was from an experiment on that exact condition or was propagated. However, the simple file (or complete file filtered for `Observed data`) contains only conditions with directly annotated experiments, but any given gene presence call may be from propagation.\r\n\r\n## Proposed gene presence method\r\n\r\nI think the ideal setup for our network would be *propagation by developmental stage but not by anatomical structure*. Using just the simple or complete datasets, this doesn't currently seem to be possible. However, I've what about the following workaround: using all adult stages on the simple dataset.\r\n\r\nUsing the simple dataset, I found all gene--anatomy pairs where `Expression` is `present` and `Call quality` is `high quality` for any adult developmental stage. To identify adult developmental stages, I filtered for `HsapDv:0000087` and its descendants ([notebook](https://github.com/dhimmel/bgee/blob/5cfed945305d7ec118fe79a1486c28d6a97ee8ee/developmental-stages.ipynb), [dataset](https://github.com/dhimmel/bgee/blob/5cfed945305d7ec118fe79a1486c28d6a97ee8ee/data/stages.tsv#L76)).\r\n\r\nThe resulting presence/absence matrix of gene expression is 16,257 genes × 188 anatomies  ([notebook](https://github.com/dhimmel/bgee/blob/5cfed945305d7ec118fe79a1486c28d6a97ee8ee/bgee.ipynb), [dataset](https://github.com/dhimmel/bgee/blob/5cfed945305d7ec118fe79a1486c28d6a97ee8ee/data/present-in-adult.tsv.gz)) compared to the [previous dimensions](http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#8) of 16,278 genes × 666 anatomies. We hope that this pruning of anatomies will help address the network problems we [were facing](#4).\r\n\r\n## Miscellaneous\r\n\r\nThanks @fbastian for suggesting other possible approaches. For future networks, we will revisit these options. For now we're looking for the most straightforward and immediately actionable option.\r\n\r\n> We don't have in situ data for human. I don't know of any resource providing systematic gene expression analyses in human using in situ hybridization.\r\n\r\nI had misinterpreted `Including in situ observed data` to mean `Observed data`",
      "profile": 17,
      "published": "2016-03-04T01:41:27.690045Z",
      "thread": 124,
      "url": "/discussion/processing-bgee-for-tissue-specific-gene-presence-and-overunder-expression/124#6"
    },
    {
      "body_html": "<blockquote><p> the simple file (or complete file filtered for Observed data) contains only conditions with directly annotated experiments, but any given gene presence call may be from propagation.</p></blockquote>\r\n\r\n<p>No, you have the guarantee that there exists an unpropagated call for that very gene. What you can't know for sure from these files, is where the quality level comes from. For instance: </p>\r\n\r\n<pre><code>gene A expressed in brain with low quality from experiment A\r\ngene A expressed in brain substructure with high quality from experiment B\r\n=&gt; gene A expressed in brain with high quality in Bgee files</code></pre>\r\n\r\n<p>Or </p>\r\n\r\n<pre><code>gene A NOT expressed in brain from RNA-Seq experiment A\r\ngene A expressed in brain substructure from Affymetrix experiment B\r\n=&gt; gene A with lowly conflicting status in brain substructure, in Bgee files </code></pre>\r\n\r\n<p>Data in adult for human must represent 90% of the data, so you shouldn't add much by considering all developmental stages. </p>\r\n\r\n<p>If you want, I can provide you with a completely unpropagated dataset. But I think it's good to benefit from propagation (e.g., to determine that 2 genes are both expressed in brain, which you might miss if they were expressed in different brain substructures). </p>\r\n\r\n<blockquote><p>I had misinterpreted <code>Including in situ observed data</code> to mean <code>Observed data</code></p></blockquote>\r\n\r\n<p>I see, we should rename these fields with <code>in situ hybridization</code> then.</p>",
      "body_md": ">  the simple file (or complete file filtered for Observed data) contains only conditions with directly annotated experiments, but any given gene presence call may be from propagation.\r\n\r\nNo, you have the guarantee that there exists an unpropagated call for that very gene. What you can't know for sure from these files, is where the quality level comes from. For instance: \r\n\r\n    gene A expressed in brain with low quality from experiment A\r\n    gene A expressed in brain substructure with high quality from experiment B\r\n    => gene A expressed in brain with high quality in Bgee files\r\n\r\nOr \r\n\r\n    gene A NOT expressed in brain from RNA-Seq experiment A\r\n    gene A expressed in brain substructure from Affymetrix experiment B\r\n    => gene A with lowly conflicting status in brain substructure, in Bgee files \r\n\r\n\r\nData in adult for human must represent 90% of the data, so you shouldn't add much by considering all developmental stages. \r\n\r\nIf you want, I can provide you with a completely unpropagated dataset. But I think it's good to benefit from propagation (e.g., to determine that 2 genes are both expressed in brain, which you might miss if they were expressed in different brain substructures). \r\n\r\n> I had misinterpreted `Including in situ observed data` to mean `Observed data`\r\n\r\nI see, we should rename these fields with `in situ hybridization` then.",
      "profile": 111,
      "published": "2016-03-04T13:08:49.847183Z",
      "thread": 124,
      "url": "/discussion/processing-bgee-for-tissue-specific-gene-presence-and-overunder-expression/124#7"
    },
    {
      "body_html": "<h1>Concensus signatures version 2.0</h1>\r\n\r\n<p>We've released <code>v2.0</code> of our analysis of LINCS L1000. This release brings major updates including:</p>\r\n\r\n<ul><li><strong>Inferred genes</strong>. <a href=\"#6\">Previously</a>, dysregulation scores were only reported for the 978 landmark (directly measured) genes. Now we've expanded our analysis to include 6,489 imputed genes from the best inferred gene set (<code>is_bing</code>), which covers genes imputed with high accuracy. However, we've maintained backwards compatibility by only using landmark probes for signature weighting.</li><li><strong>Significantly dysregulated genes</strong>. We now report significantly down or upregulated perturbagen–gene pairs.</li><li><strong>Improved knockdown and overexpression pipeline</strong>. We now convert gene symbols to entrez genes at the earliest stage. Now two genetic perturbations with different symbols that map to the same entrez gene will benefit from <a href=\"#5\"><em>z</em>-score meta-analysis</a>.</li></ul>\r\n\r\n<h2>Consensus signature datasets</h2>\r\n\r\n<p>Our consensus signatures are available on <a href=\"https://github.com/dhimmel/lincs/tree/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi\">GitHub</a> <span class=\"citation\">[<a href=\"/doi/10.5281/zenodo.47223\" class=\"citation\" data-key=\"10.5281/zenodo.47223\">1</a>]</span> and <a href=\"https://doi.org/10.6084/m9.figshare.3085426\" title=\"Consensus signatures for LINCS L1000 perturbations · figshare data deposition\">figshare</a> <span class=\"citation\">[<a href=\"/doi/10.6084/m9.figshare.3085426\" class=\"citation\" data-key=\"10.6084/m9.figshare.3085426\">2</a>]</span>. Each consensus signature measures the transcriptional response to a perturbation at <a href=\"https://github.com/dhimmel/lincs/blob/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/genes.tsv\">7,467 genes</a>. Genes are identified by their Entrez GeneID. Consensi are produced for:</p>\r\n\r\n<ul><li><strong>DrugBank compounds</strong> — 1,170 small molecule compounds identified by their DrugBank ID. L1000 compounds were mapped to DrugBank <a href=\"http://thinklab.com/discussion/unichem-mapping-to-lincs-small-molecules/51#8\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d51\">using atomic connectivity</a>.</li><li><strong>Gene knockdowns</strong> — 4,326 knocked-down genes identified by their Entrez GeneID.</li><li><strong>Gene overexpressions</strong> — 2,413 overexpressed genes identified by their Entrez GeneID</li><li><strong>All L1000 pertubations</strong> — 38,327 perturbagens identified by their L1000 <code>pert_id</code>.</li></ul>\r\n\r\n<h2>Methodology</h2>\r\n\r\n<p>To recap our methodology, our objective is to compute a consensus signature from multiple input signatures. Each input signature measures the dysregulation caused by a specific perturbation and condition. The purpose of computing consensi is to combine signatures for the same perturbation under different conditions (for examples different dosages, cell types, or time points). </p>\r\n\r\n<p>Our method, developed in consultation with the L1000 team, arrives at a consensus signature from a set of input signatures by:</p>\r\n\r\n<p>A) Starting with a probe × signature matrix of dysregulation <em>z</em>-scores with the following filters:</p>\r\n\r\n<ul><li><strong>Initial probe filter</strong>: include all landmark or <code>is_bing</code> probes with the following exclusions: a) inferred probes for genes with a landmark probe. b) probes with non-existent Entrez GeneIDs.</li><li><strong>Initial signature filter</strong>: use only <a href=\"#3\">gold signatures</a> to remove non-replicating or indistinct signatures</li></ul>\r\n\r\n<p>B) Then a gene-level consensus signature is computed by:</p>\r\n\r\n<ol><li>Calculating an input signature weight. Each input signature gets a weight equal to its average Spearman's correlation with other input signatures. We set a minimum correlation value of 0.05 to ensure all signatures make at least a small contribution and to prevent negative weights. Weights are computed using only landmark probes.</li><li><a href=\"#5\">Meta-analyzing <em>z</em>-scores</a> with Stouffer's method to compute a probe-level consensus signature.</li><li>Condensing to a gene-level consensus by averaging probe <em>z</em>-scores for the same entrez gene.</li></ol>\r\n\r\n<p>C) Finally, significant <em>Perturbagen–regulates–Gene</em> relationships are extracted for a given perturbation by:</p>\r\n\r\n<ol><li>Converting gene <em>z</em>-scores to <em>p</em>-values by via a normal distribution</li><li>Correcting p-values for multiple testing using a Bonferroni adjustment. The correction is applied separately to measured genes and inferred genes. Hence, inferred genes are more heavily penalized for multiple testing.</li><li>Filter genes for corrected <em>p</em>-value ≤ 0.05.</li><li>Allowing at most 1000 significant inferred genes. In cases with more than 1000 significant inferred genes, filter to the 1000 smallest <em>p</em>-values.</li></ol>\r\n\r\n<p>The methods described above are executed in the <a href=\"https://github.com/dhimmel/lincs/blob/abcb12f942f93e3ee839e5e3593f930df2c56845/consensi.ipynb\"><code>consensi.ipynb</code></a> and <a href=\"https://github.com/dhimmel/lincs/blob/abcb12f942f93e3ee839e5e3593f930df2c56845/significance.ipynb\"><code>significance.ipynb</code></a> notebooks.</p>",
      "body_md": "# Concensus signatures version 2.0\r\n\r\nWe've released `v2.0` of our analysis of LINCS L1000. This release brings major updates including:\r\n\r\n+ **Inferred genes**. [Previously](#6), dysregulation scores were only reported for the 978 landmark (directly measured) genes. Now we've expanded our analysis to include 6,489 imputed genes from the best inferred gene set (`is_bing`), which covers genes imputed with high accuracy. However, we've maintained backwards compatibility by only using landmark probes for signature weighting.\r\n+ **Significantly dysregulated genes**. We now report significantly down or upregulated perturbagen--gene pairs.\r\n+ **Improved knockdown and overexpression pipeline**. We now convert gene symbols to entrez genes at the earliest stage. Now two genetic perturbations with different symbols that map to the same entrez gene will benefit from [_z_-score meta-analysis](#5).\r\n\r\n## Consensus signature datasets\r\n\r\nOur consensus signatures are available on [GitHub](https://github.com/dhimmel/lincs/tree/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi) [@10.5281/zenodo.47223] and [figshare](https://doi.org/10.6084/m9.figshare.3085426 \"Consensus signatures for LINCS L1000 perturbations · figshare data deposition\") [@10.6084/m9.figshare.3085426]. Each consensus signature measures the transcriptional response to a perturbation at [7,467 genes](https://github.com/dhimmel/lincs/blob/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/genes.tsv). Genes are identified by their Entrez GeneID. Consensi are produced for:\r\n\r\n+ **DrugBank compounds** -- 1,170 small molecule compounds identified by their DrugBank ID. L1000 compounds were mapped to DrugBank [using atomic connectivity](http://thinklab.com/discussion/unichem-mapping-to-lincs-small-molecules/51#8).\r\n+ **Gene knockdowns** -- 4,326 knocked-down genes identified by their Entrez GeneID.\r\n+ **Gene overexpressions** -- 2,413 overexpressed genes identified by their Entrez GeneID\r\n+ **All L1000 pertubations** -- 38,327 perturbagens identified by their L1000 `pert_id`.\r\n\r\n## Methodology\r\n\r\nTo recap our methodology, our objective is to compute a consensus signature from multiple input signatures. Each input signature measures the dysregulation caused by a specific perturbation and condition. The purpose of computing consensi is to combine signatures for the same perturbation under different conditions (for examples different dosages, cell types, or time points). \r\n\r\nOur method, developed in consultation with the L1000 team, arrives at a consensus signature from a set of input signatures by:\r\n\r\nA) Starting with a probe × signature matrix of dysregulation _z_-scores with the following filters:\r\n\r\n+ **Initial probe filter**: include all landmark or `is_bing` probes with the following exclusions: a) inferred probes for genes with a landmark probe. b) probes with non-existent Entrez GeneIDs.\r\n+ **Initial signature filter**: use only [gold signatures](#3) to remove non-replicating or indistinct signatures\r\n\r\nB) Then a gene-level consensus signature is computed by:\r\n\r\n1. Calculating an input signature weight. Each input signature gets a weight equal to its average Spearman's correlation with other input signatures. We set a minimum correlation value of 0.05 to ensure all signatures make at least a small contribution and to prevent negative weights. Weights are computed using only landmark probes.\r\n2. [Meta-analyzing _z_-scores](#5) with Stouffer's method to compute a probe-level consensus signature.\r\n3. Condensing to a gene-level consensus by averaging probe _z_-scores for the same entrez gene.\r\n\r\nC) Finally, significant _Perturbagen--regulates--Gene_ relationships are extracted for a given perturbation by:\r\n\r\n1. Converting gene _z_-scores to _p_-values by via a normal distribution\r\n2. Correcting p-values for multiple testing using a Bonferroni adjustment. The correction is applied separately to measured genes and inferred genes. Hence, inferred genes are more heavily penalized for multiple testing.\r\n3. Filter genes for corrected _p_-value ≤ 0.05.\r\n4. Allowing at most 1000 significant inferred genes. In cases with more than 1000 significant inferred genes, filter to the 1000 smallest _p_-values.\r\n\r\nThe methods described above are executed in the [`consensi.ipynb`](https://github.com/dhimmel/lincs/blob/abcb12f942f93e3ee839e5e3593f930df2c56845/consensi.ipynb) and [`significance.ipynb`](https://github.com/dhimmel/lincs/blob/abcb12f942f93e3ee839e5e3593f930df2c56845/significance.ipynb) notebooks.",
      "profile": 17,
      "published": "2016-03-07T23:55:51.378082Z",
      "thread": 43,
      "url": "/discussion/computing-consensus-transcriptional-profiles-for-lincs-l1000-perturbations/43#7"
    },
    {
      "body_html": "<h1>Method for mapping L1000 compounds to external vocabularies</h1>\r\n\r\n<p>We chose to map LINCS L1000 compounds to external vocabularies by querying UniChem with the InChIKey of each L1000 compound (strategy 2 <a href=\"#2\">above</a>). This approach enabled us to map L1000 compounds not only to DrugBank, but also to the other vocabularies covered by UniChem.</p>\r\n\r\n<p>The InChIKey for each L1000 compound was retrieved from the <a href=\"http://api.lincscloud.org/a2/docs/pertinfo\" title=\"L1000 pertinfo API\">L1000 API</a> (<a href=\"https://github.com/dhimmel/lincs/blob/093dc3a0497028ad2b9549a3f9fa12f8ce38461f/api.ipynb\" title=\"api.ipynb in the dhimmel/lincs GitHub repository\">notebook</a>). Only perturbations with <code>pert_type == 'trt_cp'</code> and a non-null <code>inchi_key</code> were mapped. We queried the <a href=\"https://www.ebi.ac.uk/unichem/info/widesearchInfo\" title=\"Connectivity Search Documentation\">UniChem API</a> for each L1000 InChIKey to retrieve matches (<a href=\"https://github.com/dhimmel/lincs/blob/093dc3a0497028ad2b9549a3f9fa12f8ce38461f/unichem.ipynb\" title=\"unichem.ipynb in the dhimmel/lincs GitHub repository\">notebook</a>).</p>\r\n\r\n<p>We used the same UniChem Connectivity Search parameters that we <a href=\"http://thinklab.com/discussion/unifying-drug-vocabularies/40#5\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d40\">used for mapping DrugBank</a>. Our search permissively matches compounds by atomic structure, ignoring small molecular details <span class=\"citation\">[<a href=\"/doi/10.1186/s13321-014-0043-5\" class=\"citation\" data-key=\"10.1186/s13321-014-0043-5\">1</a>]</span>. We store all the UniChem output in our SQLite database (<code>l1000.db</code> <span class=\"citation\">[<a href=\"/doi/10.6084/m9.figshare.3085837.v1\" class=\"citation\" data-key=\"10.6084/m9.figshare.3085837.v1\">2</a>]</span>, <code>unichem</code> table), so users could later choose more restrictive parameters without having to requery UniChem.</p>",
      "body_md": "# Method for mapping L1000 compounds to external vocabularies\r\n\r\nWe chose to map LINCS L1000 compounds to external vocabularies by querying UniChem with the InChIKey of each L1000 compound (strategy 2 [above](#2)). This approach enabled us to map L1000 compounds not only to DrugBank, but also to the other vocabularies covered by UniChem.\r\n\r\nThe InChIKey for each L1000 compound was retrieved from the [L1000 API](http://api.lincscloud.org/a2/docs/pertinfo \"L1000 pertinfo API\") ([notebook](https://github.com/dhimmel/lincs/blob/093dc3a0497028ad2b9549a3f9fa12f8ce38461f/api.ipynb \"api.ipynb in the dhimmel/lincs GitHub repository\")). Only perturbations with `pert_type == 'trt_cp'` and a non-null `inchi_key` were mapped. We queried the [UniChem API](https://www.ebi.ac.uk/unichem/info/widesearchInfo \"Connectivity Search Documentation\") for each L1000 InChIKey to retrieve matches ([notebook](https://github.com/dhimmel/lincs/blob/093dc3a0497028ad2b9549a3f9fa12f8ce38461f/unichem.ipynb \"unichem.ipynb in the dhimmel/lincs GitHub repository\")).\r\n\r\nWe used the same UniChem Connectivity Search parameters that we [used for mapping DrugBank](http://thinklab.com/discussion/unifying-drug-vocabularies/40#5). Our search permissively matches compounds by atomic structure, ignoring small molecular details [@10.1186/s13321-014-0043-5]. We store all the UniChem output in our SQLite database (`l1000.db` [@10.6084/m9.figshare.3085837.v1], `unichem` table), so users could later choose more restrictive parameters without having to requery UniChem.",
      "profile": 17,
      "published": "2016-03-07T23:26:38.528312Z",
      "thread": 51,
      "url": "/discussion/unichem-mapping-to-lincs-small-molecules/51#8"
    },
    {
      "body_html": "<h1>Releasing <code>dhimmel/bgee v1.0</code></h1>\r\n\r\n<p>We've released <a href=\"https://github.com/dhimmel/bgee/tree/08ba54e83ee8e28dec22b4351d29e23f1d034d30\" title=\"GitHub repository\">version 1.0</a> of our Bgee processing <span class=\"citation\">[<a href=\"/doi/10.5281/zenodo.47157\" class=\"citation\" data-key=\"10.5281/zenodo.47157\">1</a>]</span>.</p>\r\n\r\n<h2>Datasets</h2>\r\n\r\n<p>Genes are identified with Entrez GeneIDs and anatomical structures (anatomies) are identified by Cell Ontology or Uberon terms.</p>\r\n\r\n<ul><li><a href=\"https://github.com/dhimmel/bgee/blob/08ba54e83ee8e28dec22b4351d29e23f1d034d30/data/present-in-adult.tsv.gz\"><code>present-in-adult.tsv.gz</code></a> indicates whether a gene is present (<code>1</code>) or absent (<code>0</code>) for a given anatomy in human adults. The dataset is a matrix of 16,257 genes × 188 anatomies.</li><li><a href=\"https://github.com/dhimmel/bgee/blob/08ba54e83ee8e28dec22b4351d29e23f1d034d30/data/diffex.tsv.gz\"><code>diffex.tsv.gz</code></a> indicates whether a gene is over-expressed (<code>1</code>), under-expressed (<code>-1</code>), or non-deferentially expressed (<code>0</code>) for a given anatomy. The dataset is a matrix of 18,620 genes × 98 anatomies.</li></ul>\r\n\r\n<h2>Changelog</h2>\r\n\r\n<p>Compared to <a href=\"http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#8\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d81\">version 0</a> — the Bgee data in the <a href=\"http://thinklab.com/discussion/one-network-to-rule-them-all/102\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d102\">initial version</a> of our hetnet — the following change was made:</p>\r\n\r\n<ul><li>We adopted the modification <a href=\"#6\">proposed above</a>: gene presence was extracted from the simple Bgee dataset, which <a href=\"#6\">guarantees</a> that each <em>Anatomy–expresses–Gene</em> relationship contains direct (unpropagated) experimental data.</li></ul>",
      "body_md": "# Releasing `dhimmel/bgee v1.0`\r\n\r\nWe've released [version 1.0](https://github.com/dhimmel/bgee/tree/08ba54e83ee8e28dec22b4351d29e23f1d034d30 \"GitHub repository\") of our Bgee processing [@10.5281/zenodo.47157].\r\n\r\n## Datasets\r\n\r\nGenes are identified with Entrez GeneIDs and anatomical structures (anatomies) are identified by Cell Ontology or Uberon terms.\r\n\r\n+ [`present-in-adult.tsv.gz`](https://github.com/dhimmel/bgee/blob/08ba54e83ee8e28dec22b4351d29e23f1d034d30/data/present-in-adult.tsv.gz) indicates whether a gene is present (`1`) or absent (`0`) for a given anatomy in human adults. The dataset is a matrix of 16,257 genes × 188 anatomies.\r\n+ [`diffex.tsv.gz`](https://github.com/dhimmel/bgee/blob/08ba54e83ee8e28dec22b4351d29e23f1d034d30/data/diffex.tsv.gz) indicates whether a gene is over-expressed (`1`), under-expressed (`-1`), or non-deferentially expressed (`0`) for a given anatomy. The dataset is a matrix of 18,620 genes × 98 anatomies.\r\n\r\n## Changelog\r\n\r\nCompared to [version 0](http://thinklab.com/discussion/tissue-specific-gene-expression-resources/81#8) -- the Bgee data in the [initial version](http://thinklab.com/discussion/one-network-to-rule-them-all/102) of our hetnet -- the following change was made:\r\n\r\n+ We adopted the modification [proposed above](#6): gene presence was extracted from the simple Bgee dataset, which [guarantees](#6) that each _Anatomy--expresses--Gene_ relationship contains direct (unpropagated) experimental data.",
      "profile": 17,
      "published": "2016-03-08T02:21:02.188057Z",
      "thread": 124,
      "url": "/discussion/processing-bgee-for-tissue-specific-gene-presence-and-overunder-expression/124#8"
    },
    {
      "body_html": "<p>Today, I'm teaching a workshop for the <a href=\"http://coursecatalog.ucsf.edu/course/1266\" title=\"Pharmacogenomics 245B · UCSF Registrar\">Systems Pharmacology</a> course at UCSF. The course primarily consists of first year students in the <a href=\"http://pspg.ucsf.edu/\" title=\"PSPG PhD Program at UCSF\">Pharmaceutical Sciences and Pharmacogenomics</a> graduate program.</p>\r\n\r\n<p>The topic of my workshop is \"Big data\". Therefore, I thought a perfect activity would be to analyze the transcriptional perturbation data from <a href=\"http://www.lincscloud.org/l1000/\" title=\"Gene Expression Data · L1000 · Broad Institute\">LINCS L1000</a>. And stars have aligned: first, we've <a href=\"http://thinklab.com/discussion/computing-consensus-transcriptional-profiles-for-lincs-l1000-perturbations/43#7\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d43\">just released</a> version 2 of our consensus signatures; second, we recently noticed some <a href=\"http://thinklab.com/discussion/positive-correlations-between-knockdown-and-overexpression-profiles-from-lincs-l1000/171\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d171\">counterintuitive occurrences</a> in the genetic perturbation data. </p>\r\n\r\n<p>Hence, I've designed a set of questions. Each pupil will be assigned a question. The pupils will then use R to attempt to answer the question. At the end of the three hour workshop, we will encourage pupils to post their findings as a comment on this discussion.</p>\r\n\r\n<p>I'm hoping to teach my <a href=\"http://thinklab.com/discussion/r-best-practices/83\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d83\">R best practices</a> as well as introduce several packages for modern data science. We will strive for the following workflow in R (not every step is needed for each question):</p>\r\n\r\n<ol><li>Read the appropriate file into a dataframe using <a href=\"https://github.com/hadley/readr\"><code>readr</code></a>. The <code>readr::read_tsv()</code> function should come in handy. Datasets are <a href=\"https://github.com/dhimmel/lincs/tree/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi\">available on GitHub</a> (<code>readr</code> should be able to read from the raw dataset URL).</li><li>Tidy the dataframe using <a href=\"https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html\"><code>tidyr</code></a>. The <code>tidyr::spread()</code> function will help convert the wide (matrix) format to a long format.</li><li>Manipulate the dataframe using <a href=\"https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html\"><code>dplyr</code></a>. Common operations here will be <code>dplyr::filter</code> and <code>dplyr::mutate</code>.</li><li>Join dataframes using <code>dplyr::inner_join()</code> or <code>dplyr::left_join()</code>. </li><li>Answer the question, either by using <code>dplyr::group_by()</code> followed by <code>dplyr::summarize()</code> or by using <a href=\"http://docs.ggplot2.org/\"><code>ggplot2</code></a> to visualize the results.</li></ol>\r\n\r\n<p>Questions will follow!</p>",
      "body_md": "Today, I'm teaching a workshop for the [Systems Pharmacology](http://coursecatalog.ucsf.edu/course/1266 \"Pharmacogenomics 245B · UCSF Registrar\") course at UCSF. The course primarily consists of first year students in the [Pharmaceutical Sciences and Pharmacogenomics](http://pspg.ucsf.edu/ \"PSPG PhD Program at UCSF\") graduate program.\r\n\r\nThe topic of my workshop is \"Big data\". Therefore, I thought a perfect activity would be to analyze the transcriptional perturbation data from [LINCS L1000](http://www.lincscloud.org/l1000/ \"Gene Expression Data · L1000 · Broad Institute\"). And stars have aligned: first, we've [just released](http://thinklab.com/discussion/computing-consensus-transcriptional-profiles-for-lincs-l1000-perturbations/43#7) version 2 of our consensus signatures; second, we recently noticed some [counterintuitive occurrences](http://thinklab.com/discussion/positive-correlations-between-knockdown-and-overexpression-profiles-from-lincs-l1000/171) in the genetic perturbation data. \r\n\r\nHence, I've designed a set of questions. Each pupil will be assigned a question. The pupils will then use R to attempt to answer the question. At the end of the three hour workshop, we will encourage pupils to post their findings as a comment on this discussion.\r\n\r\nI'm hoping to teach my [R best practices](http://thinklab.com/discussion/r-best-practices/83) as well as introduce several packages for modern data science. We will strive for the following workflow in R (not every step is needed for each question):\r\n\r\n1. Read the appropriate file into a dataframe using [`readr`](https://github.com/hadley/readr). The `readr::read_tsv()` function should come in handy. Datasets are [available on GitHub](https://github.com/dhimmel/lincs/tree/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi) (`readr` should be able to read from the raw dataset URL).\r\n2. Tidy the dataframe using [`tidyr`](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html). The `tidyr::spread()` function will help convert the wide (matrix) format to a long format.\r\n3. Manipulate the dataframe using [`dplyr`](https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html). Common operations here will be `dplyr::filter` and `dplyr::mutate`.\r\n4. Join dataframes using `dplyr::inner_join()` or `dplyr::left_join()`. \r\n5. Answer the question, either by using `dplyr::group_by()` followed by `dplyr::summarize()` or by using [`ggplot2`](http://docs.ggplot2.org/) to visualize the results.\r\n\r\nQuestions will follow!",
      "profile": 17,
      "published": "2016-03-08T20:48:53.558018Z",
      "thread": 181,
      "url": "/discussion/workshop-to-analyze-lincs-data-for-the-systems-pharmacology-course-at-ucsf/181"
    },
    {
      "body_html": "<h1>Datasets</h1>\r\n\r\n<p>The <a href=\"#2\">above questions</a> can all be answered using the following three datasets.</p>\r\n\r\n<h3>Gene information <a href=\"https://github.com/dhimmel/lincs/blob/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/genes.tsv\"><code>genes.tsv</code></a></h3>\r\n\r\n<p>This dataset contains which genes have regulation scores. It also contains whether the gene's expression was directly measured or imputed. The raw dataset is available at:</p>\r\n\r\n<pre><code class=\"no-highlight hljs\">https://github.com/dhimmel/lincs/raw/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/genes.tsv</code></pre>\r\n\r\n<p>Below is a preview:</p>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th>entrez_gene_id</th><th>status</th><th>symbol</th><th>type_of_gene</th><th>description</th></tr></thead><tbody><tr><td>100</td><td>imputed</td><td>ADA</td><td>protein-coding</td><td>adenosine deaminase</td></tr><tr><td>1000</td><td>imputed</td><td>CDH2</td><td>protein-coding</td><td>cadherin 2, type 1, N-cadherin (neuronal)</td></tr><tr><td>10000</td><td>imputed</td><td>AKT3</td><td>protein-coding</td><td>v-akt murine thymoma viral oncogene homolog 3</td></tr></tbody></table>\r\n\r\n<h3>Genes dysregulated by knockdowns <a href=\"https://github.com/dhimmel/lincs/blob/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/signif/dysreg-knockdown.tsv\"><code>dysreg-knockdown.tsv</code></a></h3>\r\n\r\n<p>This dataset contains significantly dysregulated genes due to knockdown perturbations. The raw dataset is available at:</p>\r\n\r\n<pre><code class=\"no-highlight hljs\">https://github.com/dhimmel/lincs/raw/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/signif/dysreg-knockdown.tsv</code></pre>\r\n\r\n<p>Below is a preview:</p>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th>perturbagen</th><th>entrez_gene_id</th><th>z_score</th><th>symbol</th><th>status</th><th>direction</th><th>nlog10_bonferroni_pval</th></tr></thead><tbody><tr><td>2</td><td>133</td><td>-5.495</td><td>ADM</td><td>imputed</td><td>down</td><td>3.596</td></tr><tr><td>2</td><td>501</td><td>-4.317</td><td>ALDH7A1</td><td>measured</td><td>down</td><td>1.811</td></tr><tr><td>2</td><td>9915</td><td>-5.579</td><td>ARNT2</td><td>measured</td><td>down</td><td>4.626</td></tr></tbody></table>\r\n\r\n<h3>Genes dysregulated by overexpressions <a href=\"https://github.com/dhimmel/lincs/blob/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/signif/dysreg-overexpression.tsv\"><code>dysreg-overexpression.tsv</code></a></h3>\r\n\r\n<p>This dataset contains significantly dysregulated genes due to overexpression perturbations. The raw dataset is available at:</p>\r\n\r\n<pre><code class=\"no-highlight hljs\">https://github.com/dhimmel/lincs/raw/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/signif/dysreg-overexpression.tsv</code></pre>\r\n\r\n<p>Below is a preview:</p>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th>perturbagen</th><th>entrez_gene_id</th><th>z_score</th><th>symbol</th><th>status</th><th>direction</th><th>nlog10_bonferroni_pval</th></tr></thead><tbody><tr><td>2</td><td>991</td><td>-4.687</td><td>CDC20</td><td>measured</td><td>down</td><td>2.567</td></tr><tr><td>2</td><td>54438</td><td>4.551</td><td>GFOD1</td><td>measured</td><td>up</td><td>2.282</td></tr><tr><td>2</td><td>5950</td><td>4.590</td><td>RBP4</td><td>imputed</td><td>up</td><td>1.541</td></tr></tbody></table>",
      "body_md": "# Datasets\r\n\r\nThe [above questions](#2) can all be answered using the following three datasets.\r\n\r\n### Gene information [`genes.tsv`](https://github.com/dhimmel/lincs/blob/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/genes.tsv)\r\n\r\nThis dataset contains which genes have regulation scores. It also contains whether the gene's expression was directly measured or imputed. The raw dataset is available at:\r\n\r\n```\r\nhttps://github.com/dhimmel/lincs/raw/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/genes.tsv\r\n```\r\n\r\nBelow is a preview:\r\n\r\n| entrez_gene_id | status | symbol | type_of_gene | description |\r\n|----------------|---------|--------|----------------|----------------------|\r\n| 100 | imputed | ADA | protein-coding | adenosine deaminase |\r\n| 1000 | imputed | CDH2 | protein-coding | cadherin 2, type 1, N-cadherin (neuronal) |\r\n| 10000 | imputed | AKT3 | protein-coding | v-akt murine thymoma viral oncogene homolog 3 |\r\n\r\n### Genes dysregulated by knockdowns [`dysreg-knockdown.tsv`](https://github.com/dhimmel/lincs/blob/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/signif/dysreg-knockdown.tsv)\r\n\r\nThis dataset contains significantly dysregulated genes due to knockdown perturbations. The raw dataset is available at:\r\n\r\n```\r\nhttps://github.com/dhimmel/lincs/raw/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/signif/dysreg-knockdown.tsv\r\n```\r\n\r\nBelow is a preview:\r\n\r\n| perturbagen | entrez_gene_id | z_score | symbol | status | direction | nlog10_bonferroni_pval |\r\n|-------------|----------------|---------|---------|----------|-----------|------------------------|\r\n| 2 | 133 | -5.495 | ADM | imputed | down | 3.596 |\r\n| 2 | 501 | -4.317 | ALDH7A1 | measured | down | 1.811 |\r\n| 2 | 9915 | -5.579 | ARNT2 | measured | down | 4.626 |\r\n\r\n### Genes dysregulated by overexpressions [`dysreg-overexpression.tsv`](https://github.com/dhimmel/lincs/blob/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/signif/dysreg-overexpression.tsv)\r\n\r\nThis dataset contains significantly dysregulated genes due to overexpression perturbations. The raw dataset is available at:\r\n\r\n```\r\nhttps://github.com/dhimmel/lincs/raw/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/signif/dysreg-overexpression.tsv\r\n```\r\n\r\nBelow is a preview:\r\n\r\n| perturbagen | entrez_gene_id | z_score | symbol | status | direction | nlog10_bonferroni_pval |\r\n|-------------|----------------|---------|--------|----------|-----------|------------------------|\r\n| 2 | 991 | -4.687 | CDC20 | measured | down | 2.567 |\r\n| 2 | 54438 | 4.551 | GFOD1 | measured | up | 2.282 |\r\n| 2 | 5950 | 4.590 | RBP4 | imputed | up | 1.541 |",
      "profile": 17,
      "published": "2016-03-08T21:50:14.240676Z",
      "thread": 181,
      "url": "/discussion/workshop-to-analyze-lincs-data-for-the-systems-pharmacology-course-at-ucsf/181#3"
    },
    {
      "body_html": "<h1>Questions</h1>\r\n\r\n<p>Here are the 13 questions for the workshop. They all focus on understanding the transcriptional response to genetic perturbation.</p>\r\n\r\n<ol><li><p>How many knockdowns significantly downregulated their target gene (expected)? How many knockdowns significantly upregulated their target gene (unexpected)?</p></li><li><p>How many overexpressions significantly upregulated their target gene (expected)? How many overexpressions significantly downregulated their target gene (unexpected)?</p></li><li><p>How many genes were never significantly dysregulated by any knockdown perturbation? Report this number for both the measured and inferred gene sets.</p></li><li><p>How many genes were never significantly dysregulated by any knockdown perturbation? Report this number for both the measured and inferred gene sets. (<strong>same as 3 by accident</strong>)</p></li><li><p>Which ten genes were most frequently significantly downregulated by gene knockdowns? How many knockdowns significantly downregulated these genes? How many knockdowns significantly upregulated these genes?</p></li><li><p>Which ten genes were most frequently significantly upregulated by gene knockdowns? How many knockdowns significantly upregulated these genes? How many knockdowns significantly downregulated these genes?</p></li><li><p>Which ten genes were most frequently significantly downregulated by gene overexpression? How many overexpressions significantly downregulated these genes? How many overexpressions significantly upregulated these genes?</p></li><li><p>Which ten genes were most frequently significantly upregulated by gene overexpression? How many overexpressions significantly upregulated these genes? How many overexpressions significantly downregulated these genes?</p></li><li><p>For knockdown perturbations, what is the correlation between number of significantly down and upregulated measured genes.</p></li></ol>\r\n\r\n<p>Dataset documentation will follow.</p>",
      "body_md": "# Questions\r\n\r\nHere are the 13 questions for the workshop. They all focus on understanding the transcriptional response to genetic perturbation.\r\n\r\n1. How many knockdowns significantly downregulated their target gene (expected)? How many knockdowns significantly upregulated their target gene (unexpected)?\r\n\r\n2. How many overexpressions significantly upregulated their target gene (expected)? How many overexpressions significantly downregulated their target gene (unexpected)?\r\n\r\n3. How many genes were never significantly dysregulated by any knockdown perturbation? Report this number for both the measured and inferred gene sets.\r\n\r\n4. How many genes were never significantly dysregulated by any knockdown perturbation? Report this number for both the measured and inferred gene sets. (**same as 3 by accident**)\r\n\r\n5. Which ten genes were most frequently significantly downregulated by gene knockdowns? How many knockdowns significantly downregulated these genes? How many knockdowns significantly upregulated these genes?\r\n\r\n6. Which ten genes were most frequently significantly upregulated by gene knockdowns? How many knockdowns significantly upregulated these genes? How many knockdowns significantly downregulated these genes?\r\n\r\n7. Which ten genes were most frequently significantly downregulated by gene overexpression? How many overexpressions significantly downregulated these genes? How many overexpressions significantly upregulated these genes?\r\n\r\n8. Which ten genes were most frequently significantly upregulated by gene overexpression? How many overexpressions significantly upregulated these genes? How many overexpressions significantly downregulated these genes?\r\n\r\n9. For knockdown perturbations, what is the correlation between number of significantly down and upregulated measured genes.\r\n\r\nDataset documentation will follow.",
      "profile": 17,
      "published": "2016-03-08T21:30:42.249146Z",
      "thread": 181,
      "url": "/discussion/workshop-to-analyze-lincs-data-for-the-systems-pharmacology-course-at-ucsf/181#2"
    },
    {
      "body_html": "<h1>Question 2</h1>\r\n\r\n<p>I answered:</p>\r\n\r\n<blockquote><p>How many overexpressions significantly upregulated their target gene (expected)? How many overexpressions significantly downregulated their target gene (unexpected)?</p></blockquote>\r\n\r\n<p>R code: <br></p>\r\n\r\n<pre><code class=\"no-highlight hljs\"># workshop with dan himmelstein\r\n\r\n# which genes have regulation scores. It also contains whether the gene's expression was directly measured or imputed\r\n\r\npath &lt;- 'https://github.com/dhimmel/lincs/raw/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/genes.tsv'\r\ngene_df &lt;- readr::read_tsv(path)\r\n\r\n# significantly dysregulated genes due to knockdown perturbations\r\n\r\npath2 &lt;- 'https://github.com/dhimmel/lincs/raw/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/signif/dysreg-knockdown.tsv'\r\nkd_df &lt;- readr::read_tsv(path2)\r\n\r\n# significantly dysregulated genes due to overexpression perturbations\r\n\r\npath3 &lt;- 'https://github.com/dhimmel/lincs/raw/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/signif/dysreg-overexpression.tsv'\r\noe_df &lt;- readr::read_tsv(path3)\r\n\r\ndat &lt;- filter(oe_df, perturbagen == entrez_gene_id)\r\n\r\ndat %&gt;%\r\n  dplyr::group_by(direction) %&gt;%\r\n  dplyr::summarize(\r\n    count = n()\r\n  )</code></pre>\r\n\r\n<p>R output: <br></p>\r\n\r\n<pre><code class=\"no-highlight hljs\">Source: local data frame [2 x 2]\r\n\r\n  direction count\r\n      (chr) (int)\r\n1      down     4\r\n2        up   124\r\n/</code></pre>\r\n\r\n<p>So, 4 genes were significantly downregulated after being overexpressed (unexpected) and 124 genes were significantly upregulated after being overexpressed (expected). </p>\r\n\r\n<p>Thanks Dan!</p>",
      "body_md": "# Question 2\r\n\r\nI answered:\r\n\r\n> How many overexpressions significantly upregulated their target gene (expected)? How many overexpressions significantly downregulated their target gene (unexpected)?\r\n\r\nR code: \r\n```R\r\n# workshop with dan himmelstein\r\n\r\n# which genes have regulation scores. It also contains whether the gene's expression was directly measured or imputed\r\n\r\npath <- 'https://github.com/dhimmel/lincs/raw/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/genes.tsv'\r\ngene_df <- readr::read_tsv(path)\r\n\r\n# significantly dysregulated genes due to knockdown perturbations\r\n\r\npath2 <- 'https://github.com/dhimmel/lincs/raw/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/signif/dysreg-knockdown.tsv'\r\nkd_df <- readr::read_tsv(path2)\r\n\r\n# significantly dysregulated genes due to overexpression perturbations\r\n\r\npath3 <- 'https://github.com/dhimmel/lincs/raw/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/signif/dysreg-overexpression.tsv'\r\noe_df <- readr::read_tsv(path3)\r\n\r\ndat <- filter(oe_df, perturbagen == entrez_gene_id)\r\n\r\ndat %>%\r\n  dplyr::group_by(direction) %>%\r\n  dplyr::summarize(\r\n    count = n()\r\n  )\r\n```\r\n\r\nR output: \r\n```R\r\nSource: local data frame [2 x 2]\r\n\r\n  direction count\r\n      (chr) (int)\r\n1      down     4\r\n2        up   124\r\n/\r\n```\r\n\r\nSo, 4 genes were significantly downregulated after being overexpressed (unexpected) and 124 genes were significantly upregulated after being overexpressed (expected). \r\n\r\nThanks Dan!",
      "profile": 203,
      "published": "2016-03-08T22:59:21.544674Z",
      "thread": 181,
      "url": "/discussion/workshop-to-analyze-lincs-data-for-the-systems-pharmacology-course-at-ucsf/181#4"
    },
    {
      "body_html": "<p>For question 3,</p>\r\n\r\n<blockquote><p>How many genes were never significantly dysregulated by any knockdown perturbation? Report this number for both the measured and inferred gene sets.</p></blockquote>\r\n\r\n<p>The elegant dplyr solution (thanks Daniel) looks like:</p>\r\n\r\n<pre><code class=\"r\">#how many times are genes disregulated in all?\r\ncount_df = knockdown_df %&gt;%\r\n  dplyr::group_by(entrez_gene_id) %&gt;%\r\n  dplyr::summarise(count=n())\r\n\r\n#join the table of counts with the full table of genes. the genes that were not present\r\n#are automatically converted to missing data\r\nfull=gene_df %&gt;% \r\n  dplyr::left_join(count_df)\r\n\r\n#divide the missing data by imputed vs. measured\r\nresult = full %&gt;% dplyr::filter(is.na(count)) %&gt;% \r\n  dplyr::group_by(status) %&gt;% \r\n  dplyr::summarise(count=n())</code></pre>\r\n\r\n<p>The solution: of all the genes, very few avoid disregulation!</p>\r\n\r\n<pre><code class=\"no-highlight hljs\">    status count\r\n     (chr) (int)\r\n1  imputed    55\r\n2 measured     1</code></pre>",
      "body_md": "For question 3,\r\n\r\n> How many genes were never significantly dysregulated by any knockdown perturbation? Report this number for both the measured and inferred gene sets.\r\n\r\nThe elegant dplyr solution (thanks Daniel) looks like:\r\n\r\n```r\r\n#how many times are genes disregulated in all?\r\ncount_df = knockdown_df %>%\r\n  dplyr::group_by(entrez_gene_id) %>%\r\n  dplyr::summarise(count=n())\r\n\r\n#join the table of counts with the full table of genes. the genes that were not present\r\n#are automatically converted to missing data\r\nfull=gene_df %>% \r\n  dplyr::left_join(count_df)\r\n\r\n#divide the missing data by imputed vs. measured\r\nresult = full %>% dplyr::filter(is.na(count)) %>% \r\n  dplyr::group_by(status) %>% \r\n  dplyr::summarise(count=n())\r\n```\r\n\r\nThe solution: of all the genes, very few avoid disregulation!\r\n\r\n```\r\n    status count\r\n     (chr) (int)\r\n1  imputed    55\r\n2 measured     1\r\n```",
      "profile": 205,
      "published": "2016-03-08T23:45:28.542693Z",
      "thread": 181,
      "url": "/discussion/workshop-to-analyze-lincs-data-for-the-systems-pharmacology-course-at-ucsf/181#5"
    },
    {
      "body_html": "<h1>Question 1</h1>\r\n\r\n<blockquote><p>How many knockdowns significantly downregulated their target gene (expected)? How many knockdowns significantly upregulated their target gene (unexpected)?</p></blockquote>\r\n\r\n<p>Here is the Code :)</p>\r\n\r\n<pre><code class=\"r\">path = 'https://github.com/dhimmel/lincs/raw/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/signif/dysreg-knockdown.tsv'\r\ngene_kd = readr::read_tsv(path)\r\n\r\ngene_kd %&gt;%\r\n  dplyr::filter(perturbagen == entrez_gene_id) %&gt;%\r\n  dplyr::group_by(direction) %&gt;%\r\n  dplyr::summarize(\r\n    count = n()\r\n  )\r\n\r\n</code></pre>\r\n\r\n<p>Output:</p>\r\n\r\n<pre><code class=\"no-highlight hljs\">  direction count\r\n      (chr) (int)\r\n1      down   806\r\n2        up     9</code></pre>\r\n\r\n<p><strong>Conclusion:</strong> Of the knockdown genes, 806 significantly downregulated their gene (expected) while 9 upregulated their gene (unexpected)</p>",
      "body_md": "# Question 1\r\n\r\n>How many knockdowns significantly downregulated their target gene (expected)? How many knockdowns significantly upregulated their target gene (unexpected)?\r\n\r\nHere is the Code :)\r\n\r\n```r\r\npath = 'https://github.com/dhimmel/lincs/raw/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/signif/dysreg-knockdown.tsv'\r\ngene_kd = readr::read_tsv(path)\r\n\r\ngene_kd %>%\r\n  dplyr::filter(perturbagen == entrez_gene_id) %>%\r\n  dplyr::group_by(direction) %>%\r\n  dplyr::summarize(\r\n    count = n()\r\n  )\r\n\r\n```\r\n\r\nOutput:\r\n\r\n```\r\n  direction count\r\n      (chr) (int)\r\n1      down   806\r\n2        up     9\r\n```\r\n\r\n**Conclusion:** Of the knockdown genes, 806 significantly downregulated their gene (expected) while 9 upregulated their gene (unexpected)",
      "profile": 206,
      "published": "2016-03-08T23:56:35.270382Z",
      "thread": 181,
      "url": "/discussion/workshop-to-analyze-lincs-data-for-the-systems-pharmacology-course-at-ucsf/181#6"
    },
    {
      "body_html": "<h2>Question 6</h2>\r\n\r\n<pre><code>knock_down_path =    \"https://github.com/dhimmel/lincs/raw/abcb12f942f93e3ee839e5e3593f930df2c56845/data/con    sensi/signif/dysreg-knockdown.tsv\"\r\nkd_genes = readr::read_tsv(knock_down_path)\r\nkd_genes$direction = as.factor(kd_genes$direction)\r\n\r\nkd_genes %&gt;%\r\n  group_by(symbol, direction) %&gt;%\r\n  dplyr::summarise(count=n()) %&gt;%\r\n  tidyr::spread(key = direction, value = count, fill = 0) %&gt;%\r\n  arrange(desc(up)) %&gt;% top_n(n = 10, wt = desc(up))</code></pre>\r\n\r\n<h2>Resulting Table</h2>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th>symbol</th><th>down</th><th>up</th></tr></thead><tbody><tr><td>MCOLN1</td><td>2</td><td>1128</td></tr><tr><td>MAL</td><td>0</td><td>985</td></tr><tr><td>WIF1</td><td>0</td><td>884</td></tr><tr><td>SERPINA3</td><td>0</td><td>873</td></tr><tr><td>SATB1</td><td>0</td><td>862</td></tr><tr><td>CES1</td><td>0</td><td>849</td></tr><tr><td>XIST</td><td>30</td><td>764</td></tr><tr><td>CRIP1</td><td>0</td><td>713</td></tr><tr><td>KLHL21</td><td>2</td><td>602</td></tr><tr><td>COL11A1</td><td>0</td><td>562</td></tr><tr><td>TF</td><td>0</td><td>527</td></tr><tr><td>ERAP2</td><td>0</td><td>512</td></tr><tr><td>ABCC5</td><td>3</td><td>501</td></tr><tr><td>AGR2</td><td>2</td><td>478</td></tr><tr><td>CPVL</td><td>1</td><td>476</td></tr></tbody></table>\r\n\r\n<h2>Notes</h2>\r\n\r\n<p>These are the top 10 most unregulated genes. These up-regulated genes do not appear to be down-regulated with any significant frequency</p>",
      "body_md": "## Question 6\r\n    knock_down_path =    \"https://github.com/dhimmel/lincs/raw/abcb12f942f93e3ee839e5e3593f930df2c56845/data/con    sensi/signif/dysreg-knockdown.tsv\"\r\n    kd_genes = readr::read_tsv(knock_down_path)\r\n    kd_genes$direction = as.factor(kd_genes$direction)\r\n\r\n    kd_genes %>%\r\n      group_by(symbol, direction) %>%\r\n      dplyr::summarise(count=n()) %>%\r\n      tidyr::spread(key = direction, value = count, fill = 0) %>%\r\n      arrange(desc(up)) %>% top_n(n = 10, wt = desc(up))\r\n\r\n## Resulting Table\r\n| symbol   | down | up   |\r\n|----------|------|------|\r\n| MCOLN1   | 2    | 1128 |\r\n| MAL      | 0    | 985  |\r\n| WIF1     | 0    | 884  |\r\n| SERPINA3 | 0    | 873  |\r\n| SATB1    | 0    | 862  |\r\n| CES1     | 0    | 849  |\r\n| XIST     | 30   | 764  |\r\n| CRIP1    | 0    | 713  |\r\n| KLHL21   | 2    | 602  |\r\n| COL11A1  | 0    | 562  |\r\n| TF       | 0    | 527  |\r\n| ERAP2    | 0    | 512  |\r\n| ABCC5    | 3    | 501  |\r\n| AGR2     | 2    | 478  |\r\n| CPVL     | 1    | 476  |\r\n\r\n## Notes\r\nThese are the top 10 most unregulated genes. These up-regulated genes do not appear to be down-regulated with any significant frequency",
      "profile": 204,
      "published": "2016-03-09T00:01:36.493127Z",
      "thread": 181,
      "url": "/discussion/workshop-to-analyze-lincs-data-for-the-systems-pharmacology-course-at-ucsf/181#7"
    },
    {
      "body_html": "<h1>Question 5</h1>\r\n\r\n<blockquote><p>Which ten genes were most frequently significantly downregulated by gene knockdowns? How many knockdowns significantly downregulated these genes? How many knockdowns significantly upregulated these genes?</p></blockquote>\r\n\r\n<p>Here's my code:</p>\r\n\r\n<pre><code class=\"r\">path=\"https://raw.githubusercontent.com/dhimmel/lincs/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/genes.tsv\"\r\npath2=\"https://raw.githubusercontent.com/dhimmel/lincs/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/signif/dysreg-knockdown.tsv\"\r\npath3=\"https://raw.githubusercontent.com/dhimmel/lincs/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/signif/dysreg-overexpression.tsv\"\r\ngene_df = readr::read_tsv(path)\r\nkd_gene = readr::read_tsv(path2)\r\noexp_gene= readr::read_tsv(path3)\r\nhead(gene_df)\r\nhead(kd_gene)\r\nView(kd_gene)\r\nlibrary(dplyr)\r\nlibrary(tidyr)\r\n\r\ngene_df %&gt;%\r\n  dplyr::group_by(status) %&gt;%\r\n  dplyr::summarize(\r\n    count=n()\r\n  )\r\n\r\ngene_df %&gt;% \r\n  dplyr::mutate(kind='gene')\r\n\r\n#which 10 genes were most frequently dowregulated by KDs\r\n\r\n#first, find number of distinct genes downregulated by KDs (7411)\r\nkd_gene$entrez_gene_id %&gt;% \r\n  n_distinct()\r\n#next, find number of pertubagens (4312)\r\nkd_gene$perturbagen %&gt;% \r\n  n_distinct()\r\n\r\n#from the top 10 genes, how many times were they downregulated? \r\n#genes most frequently DOWNREGULATED by the KNOCKDOWNS\r\n\r\n\r\n#filter to only downregulated KDs\r\ndownregulated_kds &lt;- kd_gene %&gt;% \r\n  filter(direction==\"down\")\r\n#sort by count to downregulated KDs\r\ndownreg_kd_sorted &lt;- downregulated_kds %&gt;%\r\n  dplyr::group_by(symbol) %&gt;%\r\n  dplyr::summarise(\r\n    count=n()\r\n  ) %&gt;%\r\n  dplyr::arrange(desc(count))\r\nhead(downreg_kd_sorted, 10)\r\nView(downreg_kd_sorted)\r\n\r\n#from the top 10 genes, how many times were they UPREGULATED? \r\n#genes most frequently UPREGULATED by the KNOCKDOWNS\r\n\r\n#filter to only upregulated KDs\r\nupregulated_kds &lt;- kd_gene %&gt;% \r\n  filter(direction==\"up\")\r\n#sort by count to upregulated KDs\r\nupreg_kd_sorted &lt;- upregulated_kds %&gt;%\r\n  dplyr::group_by(symbol) %&gt;%\r\n  dplyr::summarise(\r\n    count=n()\r\n  ) %&gt;%\r\n  dplyr::arrange(desc(count))\r\nhead(upreg_kd_sorted, 10)\r\nView(upreg_kd_sorted)\r\n\r\n\r\n#How many knockdowns downregulated these genes? 195,786\r\n#How many knockdowns upregulated these genes? 132,282\r\nnrow(kd_gene)\r\nkd_gene %&gt;%\r\n  dplyr::group_by(direction) %&gt;%\r\n  dplyr::summarize(\r\n    count=n()\r\n  )\r\n\r\nupreg_kd_sorted&lt;- upreg_kd_sorted %&gt;% \r\n  dplyr::rename(up_count=count)\r\ndim(upreg_kd_sorted)\r\nView(upreg_kd_sorted)\r\n\r\ndownreg_kd_sorted &lt;-downreg_kd_sorted %&gt;%\r\n  dplyr::rename(down_count=count)\r\ndim(downreg_kd_sorted)\r\n\r\nMYANSWER&lt;- dplyr::full_join(downreg_kd_sorted, upreg_kd_sorted)\r\n\r\nMYANSWER[is.na(MYANSWER)] = 0\r\nMYANSWER</code></pre>\r\n\r\n<p>Here's my answer:</p>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th>symbol</th><th>down_count</th><th>up_count</th></tr></thead><tbody><tr><td>RPS4Y1</td><td>1637</td><td>0</td></tr><tr><td>CDC20</td><td>1456</td><td>1</td></tr><tr><td>PCNA</td><td>1360</td><td>0</td></tr><tr><td>NME1</td><td>1182</td><td>0</td></tr><tr><td>MIF</td><td>1052</td><td>0</td></tr><tr><td>CSRP1</td><td>1031</td><td>1</td></tr><tr><td>STUB1</td><td>996</td><td>10</td></tr><tr><td>TIMM9</td><td>989</td><td>4</td></tr><tr><td>TYMS</td><td>881</td><td>0</td></tr><tr><td>GDF15</td><td>866</td><td>0</td></tr></tbody></table>",
      "body_md": "# Question 5\r\n\r\n> Which ten genes were most frequently significantly downregulated by gene knockdowns? How many knockdowns significantly downregulated these genes? How many knockdowns significantly upregulated these genes?\r\n\r\nHere's my code:\r\n\r\n```r\r\npath=\"https://raw.githubusercontent.com/dhimmel/lincs/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/genes.tsv\"\r\npath2=\"https://raw.githubusercontent.com/dhimmel/lincs/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/signif/dysreg-knockdown.tsv\"\r\npath3=\"https://raw.githubusercontent.com/dhimmel/lincs/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/signif/dysreg-overexpression.tsv\"\r\ngene_df = readr::read_tsv(path)\r\nkd_gene = readr::read_tsv(path2)\r\noexp_gene= readr::read_tsv(path3)\r\nhead(gene_df)\r\nhead(kd_gene)\r\nView(kd_gene)\r\nlibrary(dplyr)\r\nlibrary(tidyr)\r\n\r\ngene_df %>%\r\n  dplyr::group_by(status) %>%\r\n  dplyr::summarize(\r\n    count=n()\r\n  )\r\n\r\ngene_df %>% \r\n  dplyr::mutate(kind='gene')\r\n\r\n#which 10 genes were most frequently dowregulated by KDs\r\n\r\n#first, find number of distinct genes downregulated by KDs (7411)\r\nkd_gene$entrez_gene_id %>% \r\n  n_distinct()\r\n#next, find number of pertubagens (4312)\r\nkd_gene$perturbagen %>% \r\n  n_distinct()\r\n\r\n#from the top 10 genes, how many times were they downregulated? \r\n#genes most frequently DOWNREGULATED by the KNOCKDOWNS\r\n\r\n\r\n#filter to only downregulated KDs\r\ndownregulated_kds <- kd_gene %>% \r\n  filter(direction==\"down\")\r\n#sort by count to downregulated KDs\r\ndownreg_kd_sorted <- downregulated_kds %>%\r\n  dplyr::group_by(symbol) %>%\r\n  dplyr::summarise(\r\n    count=n()\r\n  ) %>%\r\n  dplyr::arrange(desc(count))\r\nhead(downreg_kd_sorted, 10)\r\nView(downreg_kd_sorted)\r\n\r\n#from the top 10 genes, how many times were they UPREGULATED? \r\n#genes most frequently UPREGULATED by the KNOCKDOWNS\r\n\r\n#filter to only upregulated KDs\r\nupregulated_kds <- kd_gene %>% \r\n  filter(direction==\"up\")\r\n#sort by count to upregulated KDs\r\nupreg_kd_sorted <- upregulated_kds %>%\r\n  dplyr::group_by(symbol) %>%\r\n  dplyr::summarise(\r\n    count=n()\r\n  ) %>%\r\n  dplyr::arrange(desc(count))\r\nhead(upreg_kd_sorted, 10)\r\nView(upreg_kd_sorted)\r\n\r\n\r\n#How many knockdowns downregulated these genes? 195,786\r\n#How many knockdowns upregulated these genes? 132,282\r\nnrow(kd_gene)\r\nkd_gene %>%\r\n  dplyr::group_by(direction) %>%\r\n  dplyr::summarize(\r\n    count=n()\r\n  )\r\n\r\nupreg_kd_sorted<- upreg_kd_sorted %>% \r\n  dplyr::rename(up_count=count)\r\ndim(upreg_kd_sorted)\r\nView(upreg_kd_sorted)\r\n\r\ndownreg_kd_sorted <-downreg_kd_sorted %>%\r\n  dplyr::rename(down_count=count)\r\ndim(downreg_kd_sorted)\r\n\r\nMYANSWER<- dplyr::full_join(downreg_kd_sorted, upreg_kd_sorted)\r\n\r\nMYANSWER[is.na(MYANSWER)] = 0\r\nMYANSWER\r\n```\r\n\r\nHere's my answer:\r\n\r\n| symbol | down_count | up_count |\r\n|--------|------------|----------|\r\n| RPS4Y1 | 1637       | 0        |\r\n| CDC20  | 1456       | 1        |\r\n| PCNA   | 1360       | 0        |\r\n| NME1   | 1182       | 0        |\r\n| MIF    | 1052       | 0        |\r\n| CSRP1  | 1031       | 1        |\r\n| STUB1  | 996        | 10       |\r\n| TIMM9  | 989        | 4        |\r\n| TYMS   | 881        | 0        |\r\n| GDF15  | 866        | 0        |",
      "profile": 208,
      "published": "2016-03-09T00:09:28.230131Z",
      "thread": 181,
      "url": "/discussion/workshop-to-analyze-lincs-data-for-the-systems-pharmacology-course-at-ucsf/181#8"
    },
    {
      "body_html": "<h1>Question 7, 8, and 9</h1>\r\n\r\n<pre><code class=\"r\">#read in data \r\npath_genes = 'https://github.com/dhimmel/lincs/raw/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/genes.tsv'\r\ngene_df = readr::read_tsv(path_genes)\r\n\r\npath_down = 'https://github.com/dhimmel/lincs/raw/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/signif/dysreg-knockdown.tsv'\r\ndown_df = readr::read_tsv(path_down)\r\n\r\npath_over = 'https://github.com/dhimmel/lincs/raw/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/signif/dysreg-overexpression.tsv'\r\nover_df = readr::read_tsv(path_over)</code></pre>\r\n\r\n<h2>Question 7- Emmalyn Chen</h2>\r\n\r\n<pre><code class=\"r\">q.7 = over_df %&gt;% subset(z_score &lt; 0) %&gt;% group_by(entrez_gene_id) %&gt;% summarize(count=n()) %&gt;% arrange(-count)\r\nq.7 = q.7[1:10,]</code></pre>\r\n\r\n<p>a. Which ten genes were most frequently significantly downregulated by gene overexpression? <br></p>\r\n\r\n<pre><code class=\"no-highlight hljs\">entrez_gene_id count\r\n            (int) (int)\r\n1            6192   166\r\n2             991   165\r\n3            5111   165\r\n4            5018   122\r\n5             994   105\r\n6           26520   102\r\n7            1738    91\r\n8            9133    86\r\n9            7298    84\r\n10          22827    83\r\n</code></pre>\r\n\r\n<p>b. How many overexpressions significantly downregulated these genes?</p>\r\n\r\n<pre><code class=\"r\">q.7.2 = over_df %&gt;% subset(z_score &lt; 0) %&gt;% filter(entrez_gene_id %in% q.7$entrez_gene_id) %&gt;% \r\n  group_by(perturbagen) %&gt;% summarize(count = n())</code></pre>\r\n\r\n<p>612 overexpressed genes </p>\r\n\r\n<p>c. How many overexpressions significantly upregulated these genes?</p>\r\n\r\n<pre><code class=\"r\">q.7.3 = over_df %&gt;% subset(z_score &gt; 0) %&gt;% filter(entrez_gene_id %in% q.7$entrez_gene_id) %&gt;% \r\n  group_by(perturbagen) %&gt;% summarize(count = n())</code></pre>\r\n\r\n<p>4 overexpressed genes</p>\r\n\r\n<h2>Question 8 - Liz Levy</h2>\r\n\r\n<pre><code class=\"r\">q.8 = over_df %&gt;% subset(z_score &gt; 0) %&gt;% group_by(entrez_gene_id) %&gt;% summarize(count=n()) %&gt;% arrange(-count)\r\nq.8 = q.8[1:10,]</code></pre>\r\n\r\n<p>a. Which ten genes were most frequently significantly upregulated by gene overexpression? <br></p>\r\n\r\n<pre><code class=\"no-highlight hljs\">entrez_gene_id count\r\n            (int) (int)\r\n1           57192   180\r\n2            5331   152\r\n3           25966   140\r\n4           23378   113\r\n5            4118   104\r\n6            9903    99\r\n7           55008    98\r\n8            1066    96\r\n9            5971    94\r\n10           7503    94</code></pre>\r\n\r\n<p>b. How many overexpressions significantly upregulated these genes?<br></p>\r\n\r\n<pre><code class=\"r\">q.8.2 = over_df %&gt;% subset(z_score &gt; 0) %&gt;% filter(entrez_gene_id %in% q.8$entrez_gene_id) %&gt;% \r\n  group_by(perturbagen) %&gt;% summarize(count = n())</code></pre>\r\n\r\n<p>792 overexpressed genes  </p>\r\n\r\n<p>c. How many overexpressions significantly downregulated these genes?<br></p>\r\n\r\n<pre><code class=\"r\">q.8.3 = over_df %&gt;% subset(z_score &lt; 0) %&gt;% filter(entrez_gene_id %in% q.8$entrez_gene_id) %&gt;% \r\n  group_by(perturbagen) %&gt;% summarize(count = n())</code></pre>\r\n\r\n<p>14 overexpressed genes </p>\r\n\r\n<h2>Question 9 - Marjorie Imperial</h2>\r\n\r\n<p>For knockdown perturbations, what is the correlation between number of significantly down and upregulated measured genes.<br></p>\r\n\r\n<pre><code class=\"r\">q.9.down.reg = down_df %&gt;% subset(z_score &lt; 0) %&gt;% group_by(perturbagen) %&gt;% summarize(count.down.reg = n())\r\nq.9.up.reg = down_df %&gt;% subset(z_score &gt; 0) %&gt;% group_by(perturbagen) %&gt;% summarize(count.up.reg = n())\r\n\r\njoined_df = dplyr::full_join(q.9.down.reg, q.9.up.reg)\r\njoined_df[is.na(joined_df)] = 0</code></pre>\r\n\r\n<p>Pearson correlation, R =0.9371317<br></p>\r\n\r\n<pre><code class=\"r\">cor(joined_df$count.down.reg, joined_df$count.up.reg)</code></pre>\r\n\r\n<p>Kendall correlation R = 0.732456 <br></p>\r\n\r\n<pre><code class=\"no-highlight hljs\">cor(joined_df$count.down.reg, joined_df$count.up.reg, method = 'kendall')</code></pre>",
      "body_md": "# Question 7, 8, and 9\r\n\r\n```r\r\n#read in data \r\npath_genes = 'https://github.com/dhimmel/lincs/raw/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/genes.tsv'\r\ngene_df = readr::read_tsv(path_genes)\r\n\r\npath_down = 'https://github.com/dhimmel/lincs/raw/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/signif/dysreg-knockdown.tsv'\r\ndown_df = readr::read_tsv(path_down)\r\n\r\npath_over = 'https://github.com/dhimmel/lincs/raw/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/signif/dysreg-overexpression.tsv'\r\nover_df = readr::read_tsv(path_over)\r\n```\r\n\r\n## Question 7- Emmalyn Chen \r\n\r\n```r\r\nq.7 = over_df %>% subset(z_score < 0) %>% group_by(entrez_gene_id) %>% summarize(count=n()) %>% arrange(-count)\r\nq.7 = q.7[1:10,]\r\n```\r\na. Which ten genes were most frequently significantly downregulated by gene overexpression? \r\n```\r\nentrez_gene_id count\r\n            (int) (int)\r\n1            6192   166\r\n2             991   165\r\n3            5111   165\r\n4            5018   122\r\n5             994   105\r\n6           26520   102\r\n7            1738    91\r\n8            9133    86\r\n9            7298    84\r\n10          22827    83\r\n\r\n```\r\nb. How many overexpressions significantly downregulated these genes?\r\n\r\n```r\r\nq.7.2 = over_df %>% subset(z_score < 0) %>% filter(entrez_gene_id %in% q.7$entrez_gene_id) %>% \r\n  group_by(perturbagen) %>% summarize(count = n())\r\n```\r\n612 overexpressed genes \r\n\r\nc. How many overexpressions significantly upregulated these genes?\r\n\r\n```r\r\nq.7.3 = over_df %>% subset(z_score > 0) %>% filter(entrez_gene_id %in% q.7$entrez_gene_id) %>% \r\n  group_by(perturbagen) %>% summarize(count = n())\r\n```\r\n4 overexpressed genes\r\n\r\n## Question 8 - Liz Levy \r\n\r\n```r\r\nq.8 = over_df %>% subset(z_score > 0) %>% group_by(entrez_gene_id) %>% summarize(count=n()) %>% arrange(-count)\r\nq.8 = q.8[1:10,]\r\n``` \r\na. Which ten genes were most frequently significantly upregulated by gene overexpression? \r\n``` \r\nentrez_gene_id count\r\n            (int) (int)\r\n1           57192   180\r\n2            5331   152\r\n3           25966   140\r\n4           23378   113\r\n5            4118   104\r\n6            9903    99\r\n7           55008    98\r\n8            1066    96\r\n9            5971    94\r\n10           7503    94\r\n```\r\nb. How many overexpressions significantly upregulated these genes?\r\n```r\r\nq.8.2 = over_df %>% subset(z_score > 0) %>% filter(entrez_gene_id %in% q.8$entrez_gene_id) %>% \r\n  group_by(perturbagen) %>% summarize(count = n())\r\n```\r\n792 overexpressed genes  \r\n\r\nc. How many overexpressions significantly downregulated these genes?\r\n```r\r\nq.8.3 = over_df %>% subset(z_score < 0) %>% filter(entrez_gene_id %in% q.8$entrez_gene_id) %>% \r\n  group_by(perturbagen) %>% summarize(count = n())\r\n```\r\n14 overexpressed genes \r\n\r\n## Question 9 - Marjorie Imperial \r\n\r\nFor knockdown perturbations, what is the correlation between number of significantly down and upregulated measured genes.\r\n```r\r\nq.9.down.reg = down_df %>% subset(z_score < 0) %>% group_by(perturbagen) %>% summarize(count.down.reg = n())\r\nq.9.up.reg = down_df %>% subset(z_score > 0) %>% group_by(perturbagen) %>% summarize(count.up.reg = n())\r\n\r\njoined_df = dplyr::full_join(q.9.down.reg, q.9.up.reg)\r\njoined_df[is.na(joined_df)] = 0\r\n```\r\nPearson correlation, R =0.9371317\r\n```r\r\ncor(joined_df$count.down.reg, joined_df$count.up.reg)\r\n```\r\nKendall correlation R = 0.732456 \r\n``` r\r\ncor(joined_df$count.down.reg, joined_df$count.up.reg, method = 'kendall')\r\n```",
      "profile": 209,
      "published": "2016-03-09T00:35:44.397866Z",
      "thread": 181,
      "url": "/discussion/workshop-to-analyze-lincs-data-for-the-systems-pharmacology-course-at-ucsf/181#9"
    },
    {
      "body_html": "<p>See Question 7 above. </p>",
      "body_md": "See Question 7 above.",
      "profile": 210,
      "published": "2016-03-09T00:36:44.588057Z",
      "thread": 181,
      "url": "/discussion/workshop-to-analyze-lincs-data-for-the-systems-pharmacology-course-at-ucsf/181#10"
    },
    {
      "body_html": "<h1>Question 4</h1>\r\n\r\n<blockquote><p>How many genes were never significantly dysregulated by any knockdown perturbation? </p></blockquote>\r\n\r\n<pre><code class=\"r\">library(dplyr)\r\ninstall.packages(\"tidyr\")\r\nlibrary(readr)\r\nlibrary(ggplot2)\r\n\r\npath = 'https://github.com/dhimmel/lincs/raw/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/genes.tsv'\r\npath_ko = 'https://github.com/dhimmel/lincs/raw/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/signif/dysreg-knockdown.tsv'\r\ngene_df = readr::read_tsv(path)\r\ngene_ko_df = readr::read_tsv(path_ko)\r\n\r\nghost_df = gene_df[! (gene_df$entrez_gene_id %in% gene_ko_df$entrez_gene_id), ]\r\nnrow(ghost_df)\r\n\r\n#for a list of these genes\r\nghost_df$symbol</code></pre>\r\n\r\n<p>The number of genes that were not sig dysregulated by knockdown perturbation (on main list of genes, but not on knockdown list of genes) = 56!</p>",
      "body_md": "# Question 4\r\n\r\n> How many genes were never significantly dysregulated by any knockdown perturbation? \r\n\r\n```r\r\nlibrary(dplyr)\r\ninstall.packages(\"tidyr\")\r\nlibrary(readr)\r\nlibrary(ggplot2)\r\n\r\npath = 'https://github.com/dhimmel/lincs/raw/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/genes.tsv'\r\npath_ko = 'https://github.com/dhimmel/lincs/raw/abcb12f942f93e3ee839e5e3593f930df2c56845/data/consensi/signif/dysreg-knockdown.tsv'\r\ngene_df = readr::read_tsv(path)\r\ngene_ko_df = readr::read_tsv(path_ko)\r\n\r\nghost_df = gene_df[! (gene_df$entrez_gene_id %in% gene_ko_df$entrez_gene_id), ]\r\nnrow(ghost_df)\r\n\r\n#for a list of these genes\r\nghost_df$symbol\r\n```\r\n\r\nThe number of genes that were not sig dysregulated by knockdown perturbation (on main list of genes, but not on knockdown list of genes) = 56!",
      "profile": 211,
      "published": "2016-03-09T00:38:17.837026Z",
      "thread": 181,
      "url": "/discussion/workshop-to-analyze-lincs-data-for-the-systems-pharmacology-course-at-ucsf/181#11"
    },
    {
      "body_html": "<p>See question 8 above.</p>",
      "body_md": "See question 8 above.",
      "profile": 213,
      "published": "2016-03-09T01:40:45.660945Z",
      "thread": 181,
      "url": "/discussion/workshop-to-analyze-lincs-data-for-the-systems-pharmacology-course-at-ucsf/181#12"
    },
    {
      "body_html": "<h1>Closing remarks</h1>\r\n\r\n<p>Impressive work!</p>\r\n\r\n<p>Each of the nine pupils in attendance answered their question. Most finished within two hours — despite several having little R experience — after an initial 30 minute tutorial. The workshop succeeded at introducing a broad range of topics: R, the hadleyverse, transcriptomics, LINCS L1000, markdown, <em>Thinklab</em>, and open science.</p>\r\n\r\n<p>I enjoyed helping the pupils learn while they performed original and noteworthy analyses. And meanwhile, through the power of realtime open science on <em>Thinklab</em>, we're now coauthors on a citeable work <span class=\"citation\">[<a href=\"/discussion/workshop-to-analyze-lincs-data-for-the-systems-pharmacology-course-at-ucsf/181\" class=\"citation\" data-key=\"10.15363/thinklab.d181\">1</a>]</span>.</p>\r\n\r\n<p>The workshop built off of many developments in scientific education: specifically, solving problems <span class=\"citation\">[<a href=\"/doi/10.1038/523272a\" class=\"citation\" data-key=\"10.1038/523272a\">2</a>]</span> in contemporary research <span class=\"citation\">[<a href=\"/doi/10.1126/science.1216570\" class=\"citation\" data-key=\"10.1126/science.1216570\">3</a>]</span> while contributing to the scientific record <span class=\"citation\">[<a href=\"/doi/10.1038/521263f\" class=\"citation\" data-key=\"10.1038/521263f\">4</a>]</span>.</p>\r\n\r\n<p>Next, I'll review the answers to see what we have learned.</p>",
      "body_md": "# Closing remarks\r\n\r\nImpressive work!\r\n\r\nEach of the nine pupils in attendance answered their question. Most finished within two hours -- despite several having little R experience -- after an initial 30 minute tutorial. The workshop succeeded at introducing a broad range of topics: R, the hadleyverse, transcriptomics, LINCS L1000, markdown, _Thinklab_, and open science.\r\n\r\nI enjoyed helping the pupils learn while they performed original and noteworthy analyses. And meanwhile, through the power of realtime open science on _Thinklab_, we're now coauthors on a citeable work [@10.15363/thinklab.d181].\r\n\r\nThe workshop built off of many developments in scientific education: specifically, solving problems [@10.1038/523272a] in contemporary research [@10.1126/science.1216570] while contributing to the scientific record [@10.1038/521263f].\r\n\r\nNext, I'll review the answers to see what we have learned.",
      "profile": 17,
      "published": "2016-03-09T03:23:31.969515Z",
      "thread": 181,
      "url": "/discussion/workshop-to-analyze-lincs-data-for-the-systems-pharmacology-course-at-ucsf/181#13"
    },
    {
      "body_html": "<h1>Workshop conclusions</h1>\r\n\r\n<p>Here's my analysis of the answers from today's workshop. Thanks again to the pupils for their hard work.</p>\r\n\r\n<h3>Do target genes of genetic perturbation respond in the expected direction?</h3>\r\n\r\n<p>Yes, we established this important control. Knockdown overwhelming downregulated (806 instances) rather than upregulated (9 instances) its target gene (<a href=\"#6\">Q1</a> by <a href=\"/u/jeffreykim\" class=\"username\">@jeffreykim</a>). Overexpression overwhelming upregulated (124 instances) rather than downregulated (4 instances) its target gene (<a href=\"#4\">Q2</a> by <a href=\"/u/kathleenk\" class=\"username\">@kathleenk</a>).</p>\r\n\r\n<h3>Are the many genes that never respond to genetic perturbation?</h3>\r\n\r\n<p>No, we saw that almost all genes were dysregulated by at least one genetic perturbation. Only 0.7% of genes (56 out of 7,467) were never dysregulated by a knockdown (<a href=\"#11\">Q4</a> by <a href=\"/u/jasleensodhi\" class=\"username\">@jasleensodhi</a>). Only 1 of these genes was measured, while the remaining 55 were imputed (<a href=\"#5\">Q3</a> by <a href=\"/u/mishavysotskiy\" class=\"username\">@mishavysotskiy</a>). This imbalance makes sense since imputed genes were <a href=\"http://thinklab.com/discussion/computing-consensus-transcriptional-profiles-for-lincs-l1000-perturbations/43#70\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d43\">subject to</a> a more stringent significance threshold. The low number of never-dysregulated genes is a welcome result from a network perspective, where pervasive connectivity is important.</p>\r\n\r\n<h3>Which genes are most frequently dysregulated?</h3>\r\n\r\n<p>Next, we identified which genes were most frequently dysregulated due to a knockdown. <em>RPS4Y1</em> was downregulated by 37.8% (1,637 out of 4,326) of knockdowns (<a href=\"#8\">Q5</a> by <a href=\"/u/juliacluceru\" class=\"username\">@juliacluceru</a>). <em>MCOLN1</em> was upregulated by 26.1% (1,128 out of 4,326) of knockdowns (<a href=\"#7\">Q6</a> by <a href=\"/u/beaunorgeot\" class=\"username\">@beaunorgeot</a>). The top-ten-most-frequently-downregulated-by-knockdown genes were rarely upregulated by knockdown. The same consistency in direction of dysregulation applied to the top-ten-most-frequently-upregulated genes as well.</p>\r\n\r\n<p>Next, we identified which genes were most frequently dysregulated due to overexpression. <em>RPS4Y1</em> was downregulated by 6.9% (166 out of 2,413) of overepressions (<a href=\"#12\">Q7</a> by <a href=\"/u/emmalynchen\" class=\"username\">@emmalynchen</a>). <em>MCOLN1</em> was upregulated by 7.5% (180 out of 2,413) of overepressions (<a href=\"#12\">Q8</a> by <a href=\"/u/elizabethlevy1\" class=\"username\">@elizabethlevy1</a>). Interestingly, <em>RPS4Y1</em> was the most downregulated gene by both knockdown and overexpression. Conversely, <em>MCOLN1</em> was the most upregulated gene for both perturbation types.</p>\r\n\r\n<p>The findings from Q5–8 fit with <a href=\"/u/larsjuhljensen\" class=\"username\">@larsjuhljensen</a>'s <a href=\"http://thinklab.com/discussion/positive-correlations-between-knockdown-and-overexpression-profiles-from-lincs-l1000/171#3\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d171\">hypothesis</a> that a general stress response may cause many genes to respond to any genetic perturbation in a consistent direction. Q5–8 also help address <a href=\"/u/caseygreene\" class=\"username\">@caseygreene</a>'s question on <a href=\"http://thinklab.com/discussion/positive-correlations-between-knockdown-and-overexpression-profiles-from-lincs-l1000/171#2\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d171\">which genes</a> are driving the signals.</p>\r\n\r\n<h3>Does broad downregulation occur in tandem with broad upregulation?</h3>\r\n\r\n<p>Finally, there was a strong correlation between the number of downregulated and upregulated genes per knockdown (<a href=\"#12\">Q9</a> by <a href=\"/u/marjorieimperial\" class=\"username\">@marjorieimperial</a>). In other words, a perturbation which downregulates many genes will also likely upregulate many genes.</p>",
      "body_md": "# Workshop conclusions\r\n\r\nHere's my analysis of the answers from today's workshop. Thanks again to the pupils for their hard work.\r\n\r\n### Do target genes of genetic perturbation respond in the expected direction?\r\n\r\nYes, we established this important control. Knockdown overwhelming downregulated (806 instances) rather than upregulated (9 instances) its target gene ([Q1](#6) by @jeffreykim). Overexpression overwhelming upregulated (124 instances) rather than downregulated (4 instances) its target gene ([Q2](#4) by @kathleenk).\r\n\r\n### Are the many genes that never respond to genetic perturbation?\r\n\r\nNo, we saw that almost all genes were dysregulated by at least one genetic perturbation. Only 0.7% of genes (56 out of 7,467) were never dysregulated by a knockdown ([Q4](#11) by @jasleensodhi). Only 1 of these genes was measured, while the remaining 55 were imputed ([Q3](#5) by @mishavysotskiy). This imbalance makes sense since imputed genes were [subject to](http://thinklab.com/discussion/computing-consensus-transcriptional-profiles-for-lincs-l1000-perturbations/43#70) a more stringent significance threshold. The low number of never-dysregulated genes is a welcome result from a network perspective, where pervasive connectivity is important.\r\n\r\n### Which genes are most frequently dysregulated?\r\n\r\nNext, we identified which genes were most frequently dysregulated due to a knockdown. _RPS4Y1_ was downregulated by 37.8% (1,637 out of 4,326) of knockdowns ([Q5](#8) by @juliacluceru). _MCOLN1_ was upregulated by 26.1% (1,128 out of 4,326) of knockdowns ([Q6](#7) by @beaunorgeot). The top-ten-most-frequently-downregulated-by-knockdown genes were rarely upregulated by knockdown. The same consistency in direction of dysregulation applied to the top-ten-most-frequently-upregulated genes as well.\r\n\r\nNext, we identified which genes were most frequently dysregulated due to overexpression. _RPS4Y1_ was downregulated by 6.9% (166 out of 2,413) of overepressions ([Q7](#12) by @emmalynchen). _MCOLN1_ was upregulated by 7.5% (180 out of 2,413) of overepressions ([Q8](#12) by @elizabethlevy1). Interestingly, _RPS4Y1_ was the most downregulated gene by both knockdown and overexpression. Conversely, _MCOLN1_ was the most upregulated gene for both perturbation types.\r\n\r\nThe findings from Q5--8 fit with @larsjuhljensen's [hypothesis](http://thinklab.com/discussion/positive-correlations-between-knockdown-and-overexpression-profiles-from-lincs-l1000/171#3) that a general stress response may cause many genes to respond to any genetic perturbation in a consistent direction. Q5--8 also help address @caseygreene's question on [which genes](http://thinklab.com/discussion/positive-correlations-between-knockdown-and-overexpression-profiles-from-lincs-l1000/171#2) are driving the signals.\r\n\r\n### Does broad downregulation occur in tandem with broad upregulation?\r\n\r\nFinally, there was a strong correlation between the number of downregulated and upregulated genes per knockdown ([Q9](#12) by @marjorieimperial). In other words, a perturbation which downregulates many genes will also likely upregulate many genes.",
      "profile": 17,
      "published": "2016-03-09T04:48:50.298369Z",
      "thread": 181,
      "url": "/discussion/workshop-to-analyze-lincs-data-for-the-systems-pharmacology-course-at-ucsf/181#14"
    },
    {
      "body_html": "<h1>Update with workshop findings</h1>\r\n\r\n<p>I recently led a Systems Pharmacology workshop for first-year graduate students. <a href=\"http://thinklab.com/discussion/workshop-to-analyze-lincs-data-for-the-systems-pharmacology-course-at-ucsf/181\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d181\">We analyzed</a> the L1000 genetic perturbation data with the goal of shedding light on the issues in this discussion. The workshop was based on significant dysregulation due to knockdown or overexpression from <a href=\"http://thinklab.com/discussion/computing-consensus-transcriptional-profiles-for-lincs-l1000-perturbations/43#7\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d43\"><code>dhimmel/lincs v2.0</code></a> <span class=\"citation\">[<a href=\"/doi/10.5281/zenodo.47223\" class=\"citation\" data-key=\"10.5281/zenodo.47223\">1</a>, <a href=\"/doi/10.6084/m9.figshare.3085426\" class=\"citation\" data-key=\"10.6084/m9.figshare.3085426\">2</a>]</span>. Compared to <code>v1.0</code> (what the <a href=\"#1\">leadoff post</a> was based on), <code>v2.0</code> adds dysregulation scores for imputed genes.</p>\r\n\r\n<p>See the <a href=\"http://thinklab.com/discussion/workshop-to-analyze-lincs-data-for-the-systems-pharmacology-course-at-ucsf/181#14\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d181\">summary of our findings</a>. In short, certain genes responded in the same direction to a large number of perturbations. For example, <em>RPS4Y1</em> was frequently downregulated and <em>MCOLN1</em> was frequently upregulated, regardless of which gene was perturbed in which direction.</p>\r\n\r\n<p><a href=\"/u/larsjuhljensen\" class=\"username\">@larsjuhljensen</a> noted:</p>\r\n\r\n<blockquote><p>Reassuring to see that things behave the way I would expect. This should make it fairly easy to derive a scoring scheme that extracts only associations that are specifically associated with a small number of perturbations, as opposed to associated with any perturbation.</p></blockquote>\r\n\r\n<p>I think Lars makes a great suggestion, worthy of investigation. However, due to time constraints, we will have to postpone this analysis for a future undertaking.</p>\r\n\r\n<h1>Proposed quick fix</h1>\r\n\r\n<p>Currently, I'm leaning towards collapsing all four types of regulation into a single relationship type (<em>Gene → regulates → Gene</em>), which means perturbation of the source gene significantly dysregulated the target gene. In other words, we'll take the union of the four <a href=\"#1\">aforementioned</a> regulation relationships.</p>\r\n\r\n<p>Our <em>DWPC</em> method for quantifying the connectivity between two nodes downweights paths through high degree nodes <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1004259\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1004259\">3</a>]</span>. Thus the pervasively dysregulated genes should not be too problematic.</p>",
      "body_md": "# Update with workshop findings\r\n\r\nI recently led a Systems Pharmacology workshop for first-year graduate students. [We analyzed](http://thinklab.com/discussion/workshop-to-analyze-lincs-data-for-the-systems-pharmacology-course-at-ucsf/181) the L1000 genetic perturbation data with the goal of shedding light on the issues in this discussion. The workshop was based on significant dysregulation due to knockdown or overexpression from [`dhimmel/lincs v2.0`](http://thinklab.com/discussion/computing-consensus-transcriptional-profiles-for-lincs-l1000-perturbations/43#7) [@10.5281/zenodo.47223 @10.6084/m9.figshare.3085426]. Compared to `v1.0` (what the [leadoff post](#1) was based on), `v2.0` adds dysregulation scores for imputed genes.\r\n\r\nSee the [summary of our findings](http://thinklab.com/discussion/workshop-to-analyze-lincs-data-for-the-systems-pharmacology-course-at-ucsf/181#14). In short, certain genes responded in the same direction to a large number of perturbations. For example, _RPS4Y1_ was frequently downregulated and _MCOLN1_ was frequently upregulated, regardless of which gene was perturbed in which direction.\r\n\r\n@larsjuhljensen noted:\r\n\r\n> Reassuring to see that things behave the way I would expect. This should make it fairly easy to derive a scoring scheme that extracts only associations that are specifically associated with a small number of perturbations, as opposed to associated with any perturbation.\r\n\r\nI think Lars makes a great suggestion, worthy of investigation. However, due to time constraints, we will have to postpone this analysis for a future undertaking.\r\n\r\n# Proposed quick fix \r\n\r\nCurrently, I'm leaning towards collapsing all four types of regulation into a single relationship type (_Gene → regulates → Gene_), which means perturbation of the source gene significantly dysregulated the target gene. In other words, we'll take the union of the four [aforementioned](#1) regulation relationships.\r\n\r\nOur _DWPC_ method for quantifying the connectivity between two nodes downweights paths through high degree nodes [@10.1371/journal.pcbi.1004259]. Thus the pervasively dysregulated genes should not be too problematic.",
      "profile": 17,
      "published": "2016-03-09T18:23:08.023532Z",
      "thread": 171,
      "url": "/discussion/positive-correlations-between-knockdown-and-overexpression-profiles-from-lincs-l1000/171#5"
    },
    {
      "body_html": "<p>Time is short to finalize our indication catalog by consensus.</p>\r\n\r\n<p><a href=\"/u/pouyakhankhanian\" class=\"username\">@pouyakhankhanian</a>, are any changes needed to your original classifications to reach a consensus with <a href=\"/u/chrissyhessler\" class=\"username\">@chrissyhessler</a> regarding epilepsy and migraine indications?</p>\r\n\r\n<p><a href=\"/u/pouyakhankhanian\" class=\"username\">@pouyakhankhanian</a>, how many indications are subject to what we decide for autoimmune diseases and steroids? I agree this is a tough call. In practice, steroids are <a href=\"http://www.nationalmssociety.org/For-Professionals/Clinical-Care/Managing-MS/Disease-Modification\" title=\"Disease Modification · National MS Society\">not referred to</a> as disease modifying for multiple sclerosis. However, I'm not convinced there is a logic to this omission that could be consistently applied to other diseases. Thoughts?</p>\r\n\r\n<p>We do want our catalog to be broadly applicable to projects beyond our specific study. In other words, we want the catalog to be generally useful to train and test computational approaches without too many disputable calls. </p>\r\n\r\n<blockquote><p>One clarification: is a drug still considered disease modifying if there are significant and dangerous side effects?</p></blockquote>\r\n\r\n<p>My take here is yes as long as the drug has been indicated in a disease-modifying capacity in some context. The context may be the time before the dangerous side effects were fully appreciated or the time before better tolerated therapies came to market.</p>",
      "body_md": "Time is short to finalize our indication catalog by consensus.\r\n\r\n@pouyakhankhanian, are any changes needed to your original classifications to reach a consensus with @chrissyhessler regarding epilepsy and migraine indications?\r\n\r\n@pouyakhankhanian, how many indications are subject to what we decide for autoimmune diseases and steroids? I agree this is a tough call. In practice, steroids are [not referred to](http://www.nationalmssociety.org/For-Professionals/Clinical-Care/Managing-MS/Disease-Modification \"Disease Modification · National MS Society\") as disease modifying for multiple sclerosis. However, I'm not convinced there is a logic to this omission that could be consistently applied to other diseases. Thoughts?\r\n\r\nWe do want our catalog to be broadly applicable to projects beyond our specific study. In other words, we want the catalog to be generally useful to train and test computational approaches without too many disputable calls. \r\n\r\n> One clarification: is a drug still considered disease modifying if there are significant and dangerous side effects?\r\n\r\nMy take here is yes as long as the drug has been indicated in a disease-modifying capacity in some context. The context may be the time before the dangerous side effects were fully appreciated or the time before better tolerated therapies came to market.",
      "profile": 17,
      "published": "2016-03-09T20:45:38.038438Z",
      "thread": 95,
      "url": "/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95#13"
    },
    {
      "body_html": "<ul><li><p><strong>Re: migraines and epilepsy.</strong>  No changes need to be made to the spreadsheet, the proposed classification presented by CSH matches what I chose.</p></li><li><p><strong>Re: steroids and auto-immune diseases.</strong> There are about 70 steroid-autoimmune connections that would need to be subject to this decision. I think there are about 20 which are Rheumatoid Arthritis and Lupus, which I think are safely DM. That leaves another 50 which would have to be re-evaluated based on the decision. For the case of multiple sclerosis, I still tend to favor calling steroids DM for a few reasons. First, let's consider an easy (but  rare) example. Suppose a patient comes in with painful trigeminal neuralgia due to an active demyelinating lesion. One would give this patient steroids. The steroids would decrease active inflammation and demyelination, thereby decreasing the duration of time of the symptom of pain. The steroid does not actually treat the pain directly (like a \"pain-killer\"), but it treats the biology behind the pain. While it is true that the steroid does not slow the progression from RRMS to SPMS, nor does it prevent future attacks (hence it is not called \"DM\" in the clinic), it does affect the biology of the current attack. Next, let's consdier a more complex (but more common) example. Suppose a patient comes in with leg weakness making her unable to walk, due to an active demyelinating lesion. One would give this patient steroids. The steroids would decrease active inflammation and demyelination, thereby decreasing the duration of time of the symptom of weakness. The steroid does not directly treat weakness (like a drug like Ampyra might do). Again, while it is true that the steroid does not slow the progression from RRMS to SPMS, nor does it prevent future attacks (hence it is not called \"DM\" in the clinic), it does affect the biology of the current attack. Finally, let's consider <a href=\"http://journals.lww.com/neurotodayonline/Fulltext/2009/07020/Monthly_Steroid_Pulses_Cut_MS_Relapses_38_Percent.12.aspx\" title=\"Neurology Green journal\">this article</a>. In that article, they give monthly steroids and in order to prevent future attacks. They find that the number of future attacks is decreased (though it is likely not a large enough effect to justify the use of chronic steroids long-term given all the side effects that go along with chronic steroid use). Decreasing the number of attacks is exactly what defines nearly all of the drugs typically which are <a href=\"http://www.nationalmssociety.org/For-Professionals/Clinical-Care/Managing-MS/Disease-Modification\" title=\"National MS Society definition of DM\">\"in practice... referred to as disease modifying\"</a> (most of these drugs to not prevent progression from RRMS to SPMS). For the three reasons above, I would still tend to favor calling steroids DM in MS. If we choose otherwise, then I think we should use MS as an example to set up a precise definition of what qualifies as disease modifying in auto-immune diseases, and then re-evaluate the other 50 steroid-autoimmune indications based on that definition.</p></li></ul>",
      "body_md": "- **Re: migraines and epilepsy.**  No changes need to be made to the spreadsheet, the proposed classification presented by CSH matches what I chose.\r\n\r\n- **Re: steroids and auto-immune diseases.** There are about 70 steroid-autoimmune connections that would need to be subject to this decision. I think there are about 20 which are Rheumatoid Arthritis and Lupus, which I think are safely DM. That leaves another 50 which would have to be re-evaluated based on the decision. For the case of multiple sclerosis, I still tend to favor calling steroids DM for a few reasons. First, let's consider an easy (but  rare) example. Suppose a patient comes in with painful trigeminal neuralgia due to an active demyelinating lesion. One would give this patient steroids. The steroids would decrease active inflammation and demyelination, thereby decreasing the duration of time of the symptom of pain. The steroid does not actually treat the pain directly (like a \"pain-killer\"), but it treats the biology behind the pain. While it is true that the steroid does not slow the progression from RRMS to SPMS, nor does it prevent future attacks (hence it is not called \"DM\" in the clinic), it does affect the biology of the current attack. Next, let's consdier a more complex (but more common) example. Suppose a patient comes in with leg weakness making her unable to walk, due to an active demyelinating lesion. One would give this patient steroids. The steroids would decrease active inflammation and demyelination, thereby decreasing the duration of time of the symptom of weakness. The steroid does not directly treat weakness (like a drug like Ampyra might do). Again, while it is true that the steroid does not slow the progression from RRMS to SPMS, nor does it prevent future attacks (hence it is not called \"DM\" in the clinic), it does affect the biology of the current attack. Finally, let's consider [this article] (http://journals.lww.com/neurotodayonline/Fulltext/2009/07020/Monthly_Steroid_Pulses_Cut_MS_Relapses_38_Percent.12.aspx \"Neurology Green journal\"). In that article, they give monthly steroids and in order to prevent future attacks. They find that the number of future attacks is decreased (though it is likely not a large enough effect to justify the use of chronic steroids long-term given all the side effects that go along with chronic steroid use). Decreasing the number of attacks is exactly what defines nearly all of the drugs typically which are [\"in practice... referred to as disease modifying\"] (http://www.nationalmssociety.org/For-Professionals/Clinical-Care/Managing-MS/Disease-Modification  \"National MS Society definition of DM\") (most of these drugs to not prevent progression from RRMS to SPMS). For the three reasons above, I would still tend to favor calling steroids DM in MS. If we choose otherwise, then I think we should use MS as an example to set up a precise definition of what qualifies as disease modifying in auto-immune diseases, and then re-evaluate the other 50 steroid-autoimmune indications based on that definition.",
      "profile": 188,
      "published": "2016-03-10T08:15:17.165414Z",
      "thread": 95,
      "url": "/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95#14"
    },
    {
      "body_html": "<h2>Introducing PharmacotherapyDB</h2>\r\n\r\n<p>I'm excited to announce the initial release of our catalog of drug therapies for disease. The catalog  contains physician curated medical indications. It's available on <a href=\"https://doi.org/10.6084/m9.figshare.3103054\">figshare</a> <span class=\"citation\">[<a href=\"/doi/10.6084/m9.figshare.3103054\" class=\"citation\" data-key=\"10.6084/m9.figshare.3103054\">1</a>]</span> and <a href=\"https://github.com/dhimmel/indications/tree/11d535ba0884ee56c3cd5756fdfb4985f313bd80\" title=\"dhimmel/indications at v1.0\">GitHub</a> <span class=\"citation\">[<a href=\"/doi/10.5281/zenodo.47664\" class=\"citation\" data-key=\"10.5281/zenodo.47664\">2</a>]</span> and licensed to be maximally reusable.</p>\r\n\r\n<p>This initial release contains 97 diseases and 601 drugs. Between these drug–disease pairs, there are 755 disease-modifying therapies, 390 symptomatic therapies, and 243 non-indications. To enable integrative analyses, drugs and diseases are coded using <a href=\"http://www.drugbank.ca/\">DrugBank</a> and <a href=\"http://disease-ontology.org/\">Disease Ontology</a> identifiers.</p>\r\n\r\n<p>The catalog adheres to pathophysiological principals first. Therefore, the catalog includes indications with a poor risk–benefit ratio that are rarely used in the modern clinic. Contributions are welcome as we hope to expand and refine the catalog over time.</p>\r\n\r\n<h2>History &amp; Methods</h2>\r\n\r\n<p>One of our priorities from the beginning of this project was to construct a catalog of efficacious pharmacotherapies. Since our approach learns how to repurpose drugs based on the indications we feed it, a high quality indication catalog was a crucial. </p>\r\n\r\n<h3>Compilation and data integration</h3>\r\n\r\n<p>We began by looking for existing indication resources. In a <a href=\"http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d21\">discussion</a> which generated 23 comments — the most of any Thinklab discussion to date — we received helpful suggestions from the community. Based on these suggestions and our research, we proceeded by integrating four resources:</p>\r\n\r\n<ul><li><strong>MEDI-HPS</strong> <span class=\"citation\">[<a href=\"/doi/10.1136/amiajnl-2012-001431\" class=\"citation\" data-key=\"10.1136/amiajnl-2012-001431\">3</a>]</span> — indications from RxNorm, SIDER 2, MedlinePlus, and Wikipedia (<a href=\"http://thinklab.com/discussion/medi-indications-data-discrepancy-in-resource-specific-counts/31\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d31\">discussed</a>).</li><li><strong>LabeledIn</strong> — indications extracted from drug labels by experts <span class=\"citation\">[<a href=\"/doi/10.1016/j.jbi.2014.08.004\" class=\"citation\" data-key=\"10.1016/j.jbi.2014.08.004\">4</a>]</span> and crowdsourced non-experts <span class=\"citation\">[<a href=\"/doi/10.1093/database/bav016\" class=\"citation\" data-key=\"10.1093/database/bav016\">5</a>]</span> (<a href=\"http://thinklab.com/discussion/processing-labeledin-to-extract-indications/46\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d46\">discussed</a>).</li><li><strong>ehrlink</strong> <span class=\"citation\">[<a href=\"/doi/10.1136/amiajnl-2012-000852\" class=\"citation\" data-key=\"10.1136/amiajnl-2012-000852\">6</a>]</span> — indications from electronic health records where physicians linked medications to problems (<a href=\"http://thinklab.com/discussion/extracting-indications-from-the-ehrlink-resource/62\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d62\">discussed</a>).</li><li><strong>PREDICT</strong> <span class=\"citation\">[<a href=\"/doi/10.1038/msb.2011.26\" class=\"citation\" data-key=\"10.1038/msb.2011.26\">7</a>]</span> — indications from UMLS relationships, drugs.com, and drug labels (<a href=\"http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#17\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d21\">discussed</a>).</li></ul>\r\n\r\n<p>We mapped these resources onto our slim sets of <a href=\"http://thinklab.com/discussion/unifying-disease-vocabularies/44#6\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d44\">137 diseases</a> and <a href=\"http://thinklab.com/discussion/unifying-drug-vocabularies/40#5\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d40\">1,552 small molecule compounds</a>. Taking the union of the four resources, <a href=\"http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#21\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d21\">we extracted</a> 1,388 high-confidence indications.</p>\r\n\r\n<h3>Curation and categorization</h3>\r\n\r\n<p>Next, <a href=\"http://thinklab.com/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d95\">we decided</a> physician curation was needed to separate disease-modifying from symptomatic indications. We recruited two physician curators (<a href=\"/u/chrissyhessler\" class=\"username\">@chrissyhessler</a> &amp; Ari J. Green) to perform a pilot on 50 random indications. Then together, we defined disease modifying as \"a drug that therapeutically changes the underlying or downstream biology of the disease\" and symptomatic as \"a drug that treats a significant symptom of the disease.\"</p>\r\n\r\n<p>The two curators then each reviewed all 1,388 indications and classified them as disease modifying (<code>DM</code>), symptomatic (<code>SYM</code>), or a non-indication (<code>NOT</code>). The initial two curators <a href=\"http://thinklab.com/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95#5\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d95\">disagreed</a> 444 times. We recruited a third curator (<a href=\"/u/pouyakhankhanian\" class=\"username\">@pouyakhankhanian</a>) who had access to the prior curations. The third curator developed a <a href=\"http://thinklab.com/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95#7\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d95\">detailed methodology</a> that helped us <a href=\"http://thinklab.com/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95#15\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d95\">reach consensus</a> for the time being.</p>\r\n\r\n<h3>Future plans</h3>\r\n\r\n<p>We're receptive to feedback on how to improve PharmacotherapyDB. For future releases, we hope to curate the unpropagated indications, include additional sources, and expand our disease and drug vocabularies.</p>",
      "body_md": "## Introducing PharmacotherapyDB\r\n\r\nI'm excited to announce the initial release of our catalog of drug therapies for disease. The catalog  contains physician curated medical indications. It's available on [figshare](https://doi.org/10.6084/m9.figshare.3103054) [@10.6084/m9.figshare.3103054] and [GitHub](https://github.com/dhimmel/indications/tree/11d535ba0884ee56c3cd5756fdfb4985f313bd80 \"dhimmel/indications at v1.0\") [@10.5281/zenodo.47664] and licensed to be maximally reusable.\r\n\r\nThis initial release contains 97 diseases and 601 drugs. Between these drug–disease pairs, there are 755 disease-modifying therapies, 390 symptomatic therapies, and 243 non-indications. To enable integrative analyses, drugs and diseases are coded using [DrugBank](http://www.drugbank.ca/) and [Disease Ontology](http://disease-ontology.org/) identifiers.\r\n\r\nThe catalog adheres to pathophysiological principals first. Therefore, the catalog includes indications with a poor risk–benefit ratio that are rarely used in the modern clinic. Contributions are welcome as we hope to expand and refine the catalog over time.\r\n\r\n## History & Methods\r\n\r\nOne of our priorities from the beginning of this project was to construct a catalog of efficacious pharmacotherapies. Since our approach learns how to repurpose drugs based on the indications we feed it, a high quality indication catalog was a crucial. \r\n\r\n### Compilation and data integration\r\n\r\nWe began by looking for existing indication resources. In a [discussion](http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21) which generated 23 comments -- the most of any Thinklab discussion to date -- we received helpful suggestions from the community. Based on these suggestions and our research, we proceeded by integrating four resources:\r\n\r\n+ **MEDI-HPS** [@10.1136/amiajnl-2012-001431] -- indications from RxNorm, SIDER 2, MedlinePlus, and Wikipedia ([discussed](http://thinklab.com/discussion/medi-indications-data-discrepancy-in-resource-specific-counts/31)).\r\n+ **LabeledIn** -- indications extracted from drug labels by experts [@10.1016/j.jbi.2014.08.004] and crowdsourced non-experts [@10.1093/database/bav016] ([discussed](http://thinklab.com/discussion/processing-labeledin-to-extract-indications/46)).\r\n+ **ehrlink** [@10.1136/amiajnl-2012-000852] -- indications from electronic health records where physicians linked medications to problems ([discussed](http://thinklab.com/discussion/extracting-indications-from-the-ehrlink-resource/62)).\r\n+ **PREDICT** [@10.1038/msb.2011.26] -- indications from UMLS relationships, drugs.com, and drug labels ([discussed](http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#17)).\r\n\r\nWe mapped these resources onto our slim sets of [137 diseases](http://thinklab.com/discussion/unifying-disease-vocabularies/44#6) and [1,552 small molecule compounds](http://thinklab.com/discussion/unifying-drug-vocabularies/40#5). Taking the union of the four resources, [we extracted](http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#21) 1,388 high-confidence indications.\r\n\r\n### Curation and categorization\r\n\r\nNext, [we decided](http://thinklab.com/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95) physician curation was needed to separate disease-modifying from symptomatic indications. We recruited two physician curators (@chrissyhessler & Ari J. Green) to perform a pilot on 50 random indications. Then together, we defined disease modifying as \"a drug that therapeutically changes the underlying or downstream biology of the disease\" and symptomatic as \"a drug that treats a significant symptom of the disease.\"\r\n\r\nThe two curators then each reviewed all 1,388 indications and classified them as disease modifying (`DM`), symptomatic (`SYM`), or a non-indication (`NOT`). The initial two curators [disagreed](http://thinklab.com/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95#5) 444 times. We recruited a third curator (@pouyakhankhanian) who had access to the prior curations. The third curator developed a [detailed methodology](http://thinklab.com/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95#7) that helped us [reach consensus](http://thinklab.com/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95#15) for the time being.\r\n\r\n### Future plans\r\n\r\nWe're receptive to feedback on how to improve PharmacotherapyDB. For future releases, we hope to curate the unpropagated indications, include additional sources, and expand our disease and drug vocabularies.",
      "profile": 17,
      "published": "2016-03-15T05:24:35.132602Z",
      "thread": 182,
      "url": "/discussion/announcing-pharmacotherapydb-the-open-catalog-of-drug-therapies-for-disease/182"
    },
    {
      "body_html": "<h1>Announcing the consensus curation</h1>\r\n\r\n<p>In the interest of time, we are finalizing the consensus curation now. We have chosen the <a href=\"#7\">PK curation</a> as the consensus. Discussion is still welcome and will be helpful for future incarnations of our catalog.</p>\r\n\r\n<h2>Resolving steroids for autoimmune disease</h2>\r\n\r\n<p>Both original curators were given a change to respond to the PK curation and methodology. In CSH's <a href=\"#11\">response</a> and offline discussion with AJG, questions were raised regarding calling steroids DM for autoimmune diseases. Further discussion with PK, both above and offline, helped clarify the issue and convinced <a href=\"/u/sergiobaranzini\" class=\"username\">@sergiobaranzini</a> and I that the DM classification was appropriate.</p>\r\n\r\n<p>According to PK, steroids are not considered DM in the clinic because their poor risk–benefit ratios generally preclude longterm use. Clinicians interpret \"disease modifying\" to mean a therapy for changing the longterm disease course and therefore do not consider steroids, which are usually given for only a short period of time, disease modifying. However, our definition of DM does not require longterm modification. Nevertheless, PK points to some evidence <span class=\"citation\">[<a href=\"/doi/10.1097/01.NT.0000357562.58878.0a\" class=\"citation\" data-key=\"10.1097/01.NT.0000357562.58878.0a\">1</a>, <a href=\"/doi/10.1212/wnl.57.7.1239\" class=\"citation\" data-key=\"10.1212/wnl.57.7.1239\">2</a>]</span> that steroids do modify the longterm disease course when administered over a prolonged period.</p>\r\n\r\n<p>While a clinician's decision to prescribe a steroid for an autoimmune disease is motivated by reducing symptoms, PK believes the steroid reduces symptoms by modifying the underlying disease biology. In his opinion, steroids lead to a short-term suppression of the underlying biology — in the case of autoimmune disease, the overactive immune response — leading to a short-term improvement in symptoms. One litmus test is that while a steroid may be prescribed to treat a specific symptom of a multiple sclerosis relapse, the steroid would not treat the symptom in the absence of MS.</p>\r\n\r\n<p>In conclusion, we are conformable with the decision that steroids modify autoimmune disease rather than treat their symptoms according to our <a href=\"#7\">definition</a>. However, it's important to clarify that our indication catalog is designed primarily from a perspective of pathophysiology rather than clinical best practice.</p>",
      "body_md": "# Announcing the consensus curation\r\n\r\nIn the interest of time, we are finalizing the consensus curation now. We have chosen the [PK curation](#7) as the consensus. Discussion is still welcome and will be helpful for future incarnations of our catalog.\r\n\r\n## Resolving steroids for autoimmune disease\r\n\r\nBoth original curators were given a change to respond to the PK curation and methodology. In CSH's [response](#11) and offline discussion with AJG, questions were raised regarding calling steroids DM for autoimmune diseases. Further discussion with PK, both above and offline, helped clarify the issue and convinced @sergiobaranzini and I that the DM classification was appropriate.\r\n\r\nAccording to PK, steroids are not considered DM in the clinic because their poor risk–benefit ratios generally preclude longterm use. Clinicians interpret \"disease modifying\" to mean a therapy for changing the longterm disease course and therefore do not consider steroids, which are usually given for only a short period of time, disease modifying. However, our definition of DM does not require longterm modification. Nevertheless, PK points to some evidence [@10.1097/01.NT.0000357562.58878.0a @10.1212/wnl.57.7.1239] that steroids do modify the longterm disease course when administered over a prolonged period.\r\n\r\nWhile a clinician's decision to prescribe a steroid for an autoimmune disease is motivated by reducing symptoms, PK believes the steroid reduces symptoms by modifying the underlying disease biology. In his opinion, steroids lead to a short-term suppression of the underlying biology -- in the case of autoimmune disease, the overactive immune response -- leading to a short-term improvement in symptoms. One litmus test is that while a steroid may be prescribed to treat a specific symptom of a multiple sclerosis relapse, the steroid would not treat the symptom in the absence of MS.\r\n\r\nIn conclusion, we are conformable with the decision that steroids modify autoimmune disease rather than treat their symptoms according to our [definition](#7). However, it's important to clarify that our indication catalog is designed primarily from a perspective of pathophysiology rather than clinical best practice.",
      "profile": 17,
      "published": "2016-03-12T20:24:37.834807Z",
      "thread": 95,
      "url": "/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95#15"
    },
    {
      "body_html": "<h1>Improved randomization of expression edges</h1>\r\n\r\n<p>We <a href=\"http://thinklab.com/discussion/processing-bgee-for-tissue-specific-gene-presence-and-overunder-expression/124#6\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d124\">updated our method</a> for extracting <em>Anatomy–expresses–Gene</em> relationships from Bgee. This update <a href=\"https://github.com/dhimmel/integrate/commit/d68b823bf2167e7ab7f0e784d1280200c33fb3bf#diff-c849eed0ccfe917ca2fceb4f57045444R3\">reduced the number</a> of expression edges in our hetnet from 1,006,278 to 526,407. The number of genes with an expression edge went from 18,147 to 18,094. The number of anatomies with an expression edge went from 256 to 241.</p>\r\n\r\n<p>The permutation of expression edges increased in effectiveness. Now ~25% of expression edges (as opposed to ~10% <a href=\"#1\">previously</a>) <a href=\"http://nbviewer.jupyter.org/github/dhimmel/integrate/blob/d68b823bf2167e7ab7f0e784d1280200c33fb3bf/data/permuted/evaluate-permutations.ipynb#How-many-permutations-are-needed-to-randomize-an-edge\">change in</a> a permuted hetnet. And this is in spite of fewer attempted swaps per permutation: I decreased the multiplier from 4 to 3 to reduce runtime.</p>",
      "body_md": "# Improved randomization of expression edges\r\n\r\nWe [updated our method](http://thinklab.com/discussion/processing-bgee-for-tissue-specific-gene-presence-and-overunder-expression/124#6) for extracting _Anatomy–expresses–Gene_ relationships from Bgee. This update [reduced the number](https://github.com/dhimmel/integrate/commit/d68b823bf2167e7ab7f0e784d1280200c33fb3bf#diff-c849eed0ccfe917ca2fceb4f57045444R3) of expression edges in our hetnet from 1,006,278 to 526,407. The number of genes with an expression edge went from 18,147 to 18,094. The number of anatomies with an expression edge went from 256 to 241.\r\n\r\nThe permutation of expression edges increased in effectiveness. Now ~25% of expression edges (as opposed to ~10% [previously](#1)) [change in](http://nbviewer.jupyter.org/github/dhimmel/integrate/blob/d68b823bf2167e7ab7f0e784d1280200c33fb3bf/data/permuted/evaluate-permutations.ipynb#How-many-permutations-are-needed-to-randomize-an-edge) a permuted hetnet. And this is in spite of fewer attempted swaps per permutation: I decreased the multiplier from 4 to 3 to reduce runtime.",
      "profile": 17,
      "published": "2016-03-11T17:13:45.157056Z",
      "thread": 178,
      "url": "/discussion/assessing-the-effectiveness-of-our-hetnet-permutations/178#2"
    },
    {
      "body_html": "<p>We <a href=\"http://thinklab.com/discussion/computing-consensus-transcriptional-profiles-for-lincs-l1000-perturbations/43#7\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d43\">recently released</a> version 2.0 of our LINCS L1000 analysis <span class=\"citation\">[<a href=\"/doi/10.6084/m9.figshare.3085426\" class=\"citation\" data-key=\"10.6084/m9.figshare.3085426\">1</a>, <a href=\"/doi/10.5281/zenodo.47223\" class=\"citation\" data-key=\"10.5281/zenodo.47223\">2</a>]</span>. This release added dysregulation <em>z</em>-scores for 6,489 imputed genes, in addition to the 978 directly measured genes on the L1000 <a href=\"http://support.lincscloud.org/hc/en-us/articles/202264116-What-are-L1000-probe-pools-\" title=\"What Are L1000 Probe Pools? · LINCS Cloud Support\">epsilon</a> platform. We only added imputed genes that were part of the best inferred gene set (BING, genes <a href=\"http://thinklab.com/discussion/computing-consensus-transcriptional-profiles-for-lincs-l1000-perturbations/43#3\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d43\">supposedly</a> imputed with high quality).</p>\r\n\r\n<p>We've also been <a href=\"http://thinklab.com/discussion/positive-correlations-between-knockdown-and-overexpression-profiles-from-lincs-l1000/171\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d171\">looking into</a> the genetic perturbation data in L1000. Here, we will assess the quality of the Broad's gene imputation using genetic perturbation consensus signatures. Specifically, we'll use whether a genetic perturbation dysregulates its target gene in the correct direction as a quality metric.</p>\r\n\r\n<p>Below we show the distribution of dysregulation <em>z</em>-scores by imputation status and perturbation type (<a href=\"https://github.com/dhimmel/lincs/blob/7f0f937cc3f3346f88a5d18eae8b07c6822ce653/imputation-assess.ipynb\" title=\"imputation-assess.ipynb · dhimmel/lincs · GitHub\">notebook</a>):</p>\r\n\r\n<p><img src=\"https://github.com/dhimmel/lincs/raw/7f0f937cc3f3346f88a5d18eae8b07c6822ce653/viz/self-targeting-perts.png\" alt=\"Violin plots of perturbagen-self z-scores\" title=\"Violin plots of perturbagen-self z-scores\"></p>\r\n\r\n<p>In general, the measured genes responded in the expected direction. For genetic perturbations whose targets were measured, 97% of knockdowns downregulated their targets (negative <em>z</em>-score), and 64% of overexpressions upregulated their targets (positive <em>z</em>-score). Instances where a measured gene responded in the reverse direction could be due to problems with perturbation delivery or <a href=\"http://arxiv.org/abs/1602.06316\" title=\"Model-based clustering with data correction for removing artifacts in gene expression data · Young et al. · arXiv\">expression quantification</a>.</p>\r\n\r\n<p>For genetic perturbations whose targets were imputed, 54% of knockdowns downregulated their targets, and 51% of overexpressions upregulated their targets. Using the success rates of measured genes as a baseline, we're led to conclude that the imputation quality of BING genes is poor.</p>\r\n\r\n<p>If we instead judge the imputation based only significantly dysregulated genes, the results improve. For significant, imputed perturbagen–target pairs, 67% of knockdowns (18 of 24) downregulated their target, while 80% of overexpressions (4 of 5) upregulated their target. Since these sample sizes are small, I'm hesitant to declare that filtering for significant genes is sufficient to overcome the imputation problems.</p>\r\n\r\n<p>For reader reference, recent research <span class=\"citation\">[<a href=\"/doi/10.1093/bioinformatics/btw074\" class=\"citation\" data-key=\"10.1093/bioinformatics/btw074\">3</a>]</span> looked at improved imputation techniques that presumably could be applied to reimpute LINCS L1000 gene expression.</p>",
      "body_md": "We [recently released](http://thinklab.com/discussion/computing-consensus-transcriptional-profiles-for-lincs-l1000-perturbations/43#7) version 2.0 of our LINCS L1000 analysis [@10.6084/m9.figshare.3085426 @10.5281/zenodo.47223]. This release added dysregulation _z_-scores for 6,489 imputed genes, in addition to the 978 directly measured genes on the L1000 [epsilon](http://support.lincscloud.org/hc/en-us/articles/202264116-What-are-L1000-probe-pools- \"What Are L1000 Probe Pools? · LINCS Cloud Support\") platform. We only added imputed genes that were part of the best inferred gene set (BING, genes [supposedly](http://thinklab.com/discussion/computing-consensus-transcriptional-profiles-for-lincs-l1000-perturbations/43#3) imputed with high quality).\r\n\r\nWe've also been [looking into](http://thinklab.com/discussion/positive-correlations-between-knockdown-and-overexpression-profiles-from-lincs-l1000/171) the genetic perturbation data in L1000. Here, we will assess the quality of the Broad's gene imputation using genetic perturbation consensus signatures. Specifically, we'll use whether a genetic perturbation dysregulates its target gene in the correct direction as a quality metric.\r\n\r\nBelow we show the distribution of dysregulation _z_-scores by imputation status and perturbation type ([notebook](https://github.com/dhimmel/lincs/blob/7f0f937cc3f3346f88a5d18eae8b07c6822ce653/imputation-assess.ipynb \"imputation-assess.ipynb · dhimmel/lincs · GitHub\")):\r\n\r\n![Violin plots of perturbagen-self z-scores](https://github.com/dhimmel/lincs/raw/7f0f937cc3f3346f88a5d18eae8b07c6822ce653/viz/self-targeting-perts.png \"Violin plots of perturbagen-self z-scores\")\r\n\r\nIn general, the measured genes responded in the expected direction. For genetic perturbations whose targets were measured, 97% of knockdowns downregulated their targets (negative _z_-score), and 64% of overexpressions upregulated their targets (positive _z_-score). Instances where a measured gene responded in the reverse direction could be due to problems with perturbation delivery or [expression quantification](http://arxiv.org/abs/1602.06316 \"Model-based clustering with data correction for removing artifacts in gene expression data · Young et al. · arXiv\").\r\n\r\nFor genetic perturbations whose targets were imputed, 54% of knockdowns downregulated their targets, and 51% of overexpressions upregulated their targets. Using the success rates of measured genes as a baseline, we're led to conclude that the imputation quality of BING genes is poor.\r\n\r\nIf we instead judge the imputation based only significantly dysregulated genes, the results improve. For significant, imputed perturbagen--target pairs, 67% of knockdowns (18 of 24) downregulated their target, while 80% of overexpressions (4 of 5) upregulated their target. Since these sample sizes are small, I'm hesitant to declare that filtering for significant genes is sufficient to overcome the imputation problems.\r\n\r\nFor reader reference, recent research [@10.1093/bioinformatics/btw074] looked at improved imputation techniques that presumably could be applied to reimpute LINCS L1000 gene expression.",
      "profile": 17,
      "published": "2016-03-11T22:41:36.326470Z",
      "thread": 185,
      "url": "/discussion/assessing-the-imputation-quality-of-gene-expression-in-lincs-l1000/185"
    },
    {
      "body_html": "<h1>Mention in <em>Storybench</em> piece on BLAZE</h1>\r\n\r\n<p>Margaux Phares wrote an article <a href=\"http://www.storybench.org/science-search-engine-visualizing-discovery-process/\" title=\"How a Science Search Engine Is Visualizing the Discovery Process\">published today on <em>Storybench</em></a> exploring BLAZE. <a href=\"http://openknowledgemaps.org/search/\">BLAZE</a> is a new type of scholarly search engine that returns search results as bubble maps rather than lists. The article quotes me:</p>\r\n\r\n<blockquote><p>“I’ll often work on problems for years before encountering seminal works that would have been helpful from day one,” Himmelstein said. “The [scientific literature search problem] is worst at the cross-section of fields as each field has its own specialized terminology.” He thinks Blaze may help solve this problem.</p></blockquote>\r\n\r\n<p>This quote was motivated by this project: specifically, the difficultly we faced when finding literature on hetnets. As I wrote to Margaux:</p>\r\n\r\n<blockquote><p>I work on networks with multiple types of relationships. I call these networks \"hetnets\". It wasn't until the fifth year of my PhD, that I <a href=\"http://thinklab.com/discussion/renaming-heterogeneous-networks-to-a-more-concise-and-catchy-term/104#6\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d104\">learned of a plethora</a> of other terms referring to the same concept. And had I not been highly proactive at reaching out to diverse players, my ignorance would persist to this day.</p></blockquote>",
      "body_md": "# Mention in _Storybench_ piece on BLAZE\r\n\r\nMargaux Phares wrote an article [published today on _Storybench_](http://www.storybench.org/science-search-engine-visualizing-discovery-process/ \"How a Science Search Engine Is Visualizing the Discovery Process\") exploring BLAZE. [BLAZE](http://openknowledgemaps.org/search/) is a new type of scholarly search engine that returns search results as bubble maps rather than lists. The article quotes me:\r\n\r\n> “I’ll often work on problems for years before encountering seminal works that would have been helpful from day one,” Himmelstein said. “The [scientific literature search problem] is worst at the cross-section of fields as each field has its own specialized terminology.” He thinks Blaze may help solve this problem.\r\n\r\nThis quote was motivated by this project: specifically, the difficultly we faced when finding literature on hetnets. As I wrote to Margaux:\r\n\r\n> I work on networks with multiple types of relationships. I call these networks \"hetnets\". It wasn't until the fifth year of my PhD, that I [learned of a plethora](http://thinklab.com/discussion/renaming-heterogeneous-networks-to-a-more-concise-and-catchy-term/104#6) of other terms referring to the same concept. And had I not been highly proactive at reaching out to diverse players, my ignorance would persist to this day.",
      "profile": 17,
      "published": "2016-03-14T18:17:15.121105Z",
      "thread": 113,
      "url": "/discussion/tracking-project-reuse-citation-and-publicity/113#5"
    },
    {
      "body_html": "<ul><li><strong>I think this is a valuable data source to maintain.</strong> I understand the need to freeze it at the moment for your analysis. Going forward, I'd love to see a couple additional data sources. One would be uptodate, which would add to the total number of indications (including those where side effects outweight potential benefit) and would give you precise disease-indications for drugs (no need for curation by experts). When thinking about a second data source which may help add to your total number of indications, I might suggest a more clinically relevant source (like medscape). I think your current data-sources are heavy on government approval and, per our discussion earlier, for often politico-economic reasons, drugs may be very commonly used but not had any pharma funding for official approval, and these drugs may not all be caught when surveying pharmacy or doctor's \"indication notes\" as those may lack sensitivity (due to under-reporting of key \"disease\" designations).</li><li><strong>I would keep an eye out for the steroids in auto-immune diseases</strong> Steroid represent a relatively large fraction of the drugs, and auto-immune diseases a reasonable fraction of the total diseases (approximately 5%). I would interpret any results that your algorithm suggests in light of this. For example, I expect this will drive your algorithm into picking things that \"look\" like steroids (in terms of molecular structure, and known targets of possible action). As you know, steroids are molecularly quite similar to each other, and are often associated with the same limited number of key molecular targets. The other immunosuppressive agents (i.e. all the other drugs on your typical clinical list of choices) represent a variety of shapes (molecular structure) and known targets, and may provider richer (but more subtle, and probably lower powered to get a trustworthy result) information, and hopefully provide a more nuanced drug suggestion rather than picking things that \"look\" like steroids (e.g. suggesting a drug that nobody would ever have considered) .</li><li><strong>Consider assigning mechanisms to drugs</strong>. If you note that something richer is to be gained by \"decreasing the gain\" in large drug classes (i.e. the class \"steroids\" includes about 10 drugs in the list), consider using drug classes as an attribute. This will also aid any person who will have to curate the disease-drug connections.</li><li><strong>The few remaining discrepancies, if using my calls as final calls</strong>. After the discussion of the major discrepancies (where multiple discrepant drug-disease connections hinged on a single discussion), there are still minor discrepancies. There appear to be 55 other discrepancies to eventually be evaluated, totaling less than 5 percent of the total number of connections. Given the small number of total calls (less than 5% of total calls) and the large amount of discussion that would be required to solve each one, it makes sense to go forward with a data freeze for your downstream analysis. But I think it would be great to have you at least aware of these, and we can decide what to do on future versions. <br>Of the 55 cases of discrepant calls, there are include 31 cases where my curation changed a previous agreement between the prior 2 curators, and those 24 cases where my curation resulted in a three way tie. <br>— 7 are explained directly by ammendment 1<br>— 14 where I made a call which was explained by a prior call regarding the same disease being connected to another drug within the same drug class<br>— 11 can be encapsulated in a discussion regarding hormone therapy in breast cancer. This is a more complex discussion than that of steroids, because the hormone therapies include \"partial agonists\". <br>— 3 which treated symptom of chemo rather than symptom of disease (therefore changed to NOT)<br>— 3 regarding SSRIs in parkinson's<br>— 17 discrepancies would each require a long discussion (similar to our discussions of diseases earlier). These are briefly denoted below.</li></ul>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th>drug</th><th>disease</th><th>CSH</th><th>AJG</th><th>PK</th><th>PK_notes</th></tr></thead><tbody><tr><td>Memantine</td><td>Alzheimer's disease</td><td>SYM</td><td>SYM</td><td>DM</td><td>\"neuroprotective\", doesn't treat any symptom</td></tr><tr><td>Colchicine</td><td>primary biliary cirrhosis</td><td>NOT</td><td>NOT</td><td>DM</td><td>admittedly less evidence, but see <a href=\"http://www.uptodate.com/contents/overview-of-the-treatment-of-primary-biliary-cholangitis-primary-biliary-cirrhosis\">\"uptodate 'Overview of the treatment of primary biliary cholangitis (primary biliary cirrhosis)' \"</a></td></tr><tr><td>Pentoxifylline</td><td>systemic scleroderma</td><td>SYM</td><td>SYM</td><td>DM</td><td>affects the biology of the disease, thereby easing symptoms</td></tr><tr><td>Tretinoin</td><td>peripheral nervous system neoplasm</td><td>NOT</td><td>NOT</td><td>DM</td><td>can treat sarcomas <span class=\"citation\">[<a href=\"/doi/10.1002/14651858.CD003256.pub2\" class=\"citation\" data-key=\"10.1002/14651858.CD003256.pub2\">1</a>]</span> and per amendment 1</td></tr><tr><td>Ursodeoxycholic acid</td><td>primary biliary cirrhosis</td><td>SYM</td><td>SYM</td><td>DM</td><td>doesn't treat any symptom, only modifies disease, see <a href=\"http://www.uptodate.com/contents/overview-of-the-treatment-of-primary-biliary-cholangitis-primary-biliary-cirrhosis\">\"uptodate 'Overview of the treatment of primary biliary cholangitis (primary biliary cirrhosis)' \"</a></td></tr><tr><td>Colchicine</td><td>systemic scleroderma</td><td>NOT</td><td>DM</td><td>SYM</td><td>for arthralgia, not aware of disease modification</td></tr><tr><td>Chenodeoxycholic acid</td><td>primary biliary cirrhosis</td><td>SYM</td><td>NOT</td><td>DM</td><td>in trials, see <a href=\"http://www.uptodate.com/contents/overview-of-the-treatment-of-primary-biliary-cholangitis-primary-biliary-cirrhosis\">\"uptodate 'Overview of the treatment of primary biliary cholangitis (primary biliary cirrhosis)' \"</a></td></tr><tr><td>Dimenhydrinate</td><td>allergic rhinitis</td><td>SYM</td><td>NOT</td><td>DM</td><td>it blocks histamine-H1 just like benadryl. Granted, one would never actually use this in clinic (you would just use benadryl given better efficacy and less side effects), but given the mechanism of action there is reasonable evidence of efficacy</td></tr><tr><td>Dimenhydrinate</td><td>atopic dermatitis</td><td>SYM</td><td>NOT</td><td>DM</td><td>it blocks histamine-H1 just like benadryl. Granted, one would never actually use this in clinic (you would just use benadryl given better efficacy and less side effects), but given the mechanism of action there is reasonable evidence of efficacy</td></tr><tr><td>Sildenafil</td><td>type 1 diabetes mellitus</td><td>NOT</td><td>DM</td><td>SYM</td><td>may be DM for DM2 but not for DM1 <span class=\"citation\">[<a href=\"/doi/10.2337/diacare.26.2.279\" class=\"citation\" data-key=\"10.2337/diacare.26.2.279\">2</a>]</span></td></tr><tr><td>Temozolomide</td><td>skin cancer</td><td>NOT</td><td>NOT</td><td>DM</td><td>melanoma</td></tr><tr><td>Acetylcysteine</td><td>chronic obstructive pulmonary disease</td><td>DM</td><td>DM</td><td>SYM</td><td>mucolytic, does not affect disease biology</td></tr><tr><td>Epoprostenol</td><td>systemic scleroderma</td><td>NOT</td><td>DM</td><td>SYM</td><td>not aware of disease modification, agree with CSH comment re: symptom</td></tr><tr><td>Timolol</td><td>coronary artery disease</td><td>NOT</td><td>SYM</td><td>DM</td><td>per <a href=\"http://www.uptodate.com/contents/timolol-systemic-drug-information?source=search_result&amp;search=timolol&amp;selectedTitle=1~36\">\"uptodate - Timolol Drug info\"</a></td></tr><tr><td></td></tr><tr><td>Finasteride</td><td>prostate cancer</td><td>SYM</td><td>DM</td><td>NOT</td><td>prevents but doesn't treat it. Not sure what the symptomatic thing CSH refers to, does she mean symptoms of BPH? (and if so isn't that a different disease?)</td></tr><tr><td>Digoxin</td><td>dilated cardiomyopathy</td><td>DM</td><td>NOT</td><td>SYM</td><td>see <a href=\"http://www.uptodate.com/contents/overview-of-the-therapy-of-heart-failure-with-reduced-ejection-fraction\">\"uptodate: Overview of the therapy of heart failure with reduced ejection fraction\"</a></td></tr><tr><td>Ergocalciferol</td><td>metabolic syndrome X</td><td>SYM</td><td>DM</td><td>NOT</td><td>would you give this med to anyone who has metabolic syndrome X but does not also have vitamin D deficiency?</td></tr></tbody></table>",
      "body_md": "- **I think this is a valuable data source to maintain.** I understand the need to freeze it at the moment for your analysis. Going forward, I'd love to see a couple additional data sources. One would be uptodate, which would add to the total number of indications (including those where side effects outweight potential benefit) and would give you precise disease-indications for drugs (no need for curation by experts). When thinking about a second data source which may help add to your total number of indications, I might suggest a more clinically relevant source (like medscape). I think your current data-sources are heavy on government approval and, per our discussion earlier, for often politico-economic reasons, drugs may be very commonly used but not had any pharma funding for official approval, and these drugs may not all be caught when surveying pharmacy or doctor's \"indication notes\" as those may lack sensitivity (due to under-reporting of key \"disease\" designations).\r\n- **I would keep an eye out for the steroids in auto-immune diseases** Steroid represent a relatively large fraction of the drugs, and auto-immune diseases a reasonable fraction of the total diseases (approximately 5%). I would interpret any results that your algorithm suggests in light of this. For example, I expect this will drive your algorithm into picking things that \"look\" like steroids (in terms of molecular structure, and known targets of possible action). As you know, steroids are molecularly quite similar to each other, and are often associated with the same limited number of key molecular targets. The other immunosuppressive agents (i.e. all the other drugs on your typical clinical list of choices) represent a variety of shapes (molecular structure) and known targets, and may provider richer (but more subtle, and probably lower powered to get a trustworthy result) information, and hopefully provide a more nuanced drug suggestion rather than picking things that \"look\" like steroids (e.g. suggesting a drug that nobody would ever have considered) .\r\n- **Consider assigning mechanisms to drugs**. If you note that something richer is to be gained by \"decreasing the gain\" in large drug classes (i.e. the class \"steroids\" includes about 10 drugs in the list), consider using drug classes as an attribute. This will also aid any person who will have to curate the disease-drug connections.\r\n- **The few remaining discrepancies, if using my calls as final calls**. After the discussion of the major discrepancies (where multiple discrepant drug-disease connections hinged on a single discussion), there are still minor discrepancies. There appear to be 55 other discrepancies to eventually be evaluated, totaling less than 5 percent of the total number of connections. Given the small number of total calls (less than 5% of total calls) and the large amount of discussion that would be required to solve each one, it makes sense to go forward with a data freeze for your downstream analysis. But I think it would be great to have you at least aware of these, and we can decide what to do on future versions. \r\nOf the 55 cases of discrepant calls, there are include 31 cases where my curation changed a previous agreement between the prior 2 curators, and those 24 cases where my curation resulted in a three way tie. \r\n--- 7 are explained directly by ammendment 1\r\n--- 14 where I made a call which was explained by a prior call regarding the same disease being connected to another drug within the same drug class\r\n--- 11 can be encapsulated in a discussion regarding hormone therapy in breast cancer. This is a more complex discussion than that of steroids, because the hormone therapies include \"partial agonists\". \r\n--- 3 which treated symptom of chemo rather than symptom of disease (therefore changed to NOT)\r\n--- 3 regarding SSRIs in parkinson's\r\n--- 17 discrepancies would each require a long discussion (similar to our discussions of diseases earlier). These are briefly denoted below.\r\n\r\n|drug|disease|CSH|AJG|PK|PK_notes|\r\n|----|-------|---|---|--|--------|\r\n|Memantine|Alzheimer's disease|SYM|SYM|DM|\"neuroprotective\", doesn't treat any symptom|\r\n|Colchicine|primary biliary cirrhosis|NOT|NOT|DM|admittedly less evidence, but see [\"uptodate 'Overview of the treatment of primary biliary cholangitis (primary biliary cirrhosis)' \"](http://www.uptodate.com/contents/overview-of-the-treatment-of-primary-biliary-cholangitis-primary-biliary-cirrhosis)|\r\n|Pentoxifylline|systemic scleroderma|SYM|SYM|DM|affects the biology of the disease, thereby easing symptoms|\r\n|Tretinoin|peripheral nervous system neoplasm|NOT|NOT|DM|can treat sarcomas [@10.1002/14651858.CD003256.pub2] and per amendment 1|\r\n|Ursodeoxycholic acid|primary biliary cirrhosis|SYM|SYM|DM|doesn't treat any symptom, only modifies disease, see [\"uptodate 'Overview of the treatment of primary biliary cholangitis (primary biliary cirrhosis)' \"](http://www.uptodate.com/contents/overview-of-the-treatment-of-primary-biliary-cholangitis-primary-biliary-cirrhosis)|\r\n|Colchicine|systemic scleroderma|NOT|DM|SYM|for arthralgia, not aware of disease modification|\r\n|Chenodeoxycholic acid|primary biliary cirrhosis|SYM|NOT|DM|in trials, see [\"uptodate 'Overview of the treatment of primary biliary cholangitis (primary biliary cirrhosis)' \"](http://www.uptodate.com/contents/overview-of-the-treatment-of-primary-biliary-cholangitis-primary-biliary-cirrhosis)|\r\n|Dimenhydrinate|allergic rhinitis|SYM|NOT|DM|it blocks histamine-H1 just like benadryl. Granted, one would never actually use this in clinic (you would just use benadryl given better efficacy and less side effects), but given the mechanism of action there is reasonable evidence of efficacy|\r\n|Dimenhydrinate|atopic dermatitis|SYM|NOT|DM|it blocks histamine-H1 just like benadryl. Granted, one would never actually use this in clinic (you would just use benadryl given better efficacy and less side effects), but given the mechanism of action there is reasonable evidence of efficacy|\r\n|Sildenafil|type 1 diabetes mellitus|NOT|DM|SYM|may be DM for DM2 but not for DM1 [@10.2337/diacare.26.2.279]|\r\n|Temozolomide|skin cancer|NOT|NOT|DM|melanoma|\r\n|Acetylcysteine|chronic obstructive pulmonary disease|DM|DM|SYM|mucolytic, does not affect disease biology|\r\n|Epoprostenol|systemic scleroderma|NOT|DM|SYM|not aware of disease modification, agree with CSH comment re: symptom|\r\n|Timolol|coronary artery disease|NOT|SYM|DM|per [\"uptodate - Timolol Drug info\"](http://www.uptodate.com/contents/timolol-systemic-drug-information?source=search_result&search=timolol&selectedTitle=1~36)\r\n|\r\n|Finasteride|prostate cancer|SYM|DM|NOT|prevents but doesn't treat it. Not sure what the symptomatic thing CSH refers to, does she mean symptoms of BPH? (and if so isn't that a different disease?)|\r\n|Digoxin|dilated cardiomyopathy|DM|NOT|SYM|see [\"uptodate: Overview of the therapy of heart failure with reduced ejection fraction\"](http://www.uptodate.com/contents/overview-of-the-therapy-of-heart-failure-with-reduced-ejection-fraction)|\r\n|Ergocalciferol|metabolic syndrome X|SYM|DM|NOT|would you give this med to anyone who has metabolic syndrome X but does not also have vitamin D deficiency?|",
      "profile": 188,
      "published": "2016-03-14T18:26:05.738184Z",
      "thread": 95,
      "url": "/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95#16"
    },
    {
      "body_html": "<h1>Data licensing and compliance report</h1>\r\n\r\n<p>As a refresher, we released an initial version of our network built from publicly-availabe resources. I had assumed that as long as a resource was public, we could use it for our research. In addition, we're committed to open science — releasing our network and intermediate data, both for reproducibility and to allow others to build off of our research. However, as <a href=\"/u/larsjuhljensen\" class=\"username\">@larsjuhljensen</a> <a href=\"http://thinklab.com/discussion/one-network-to-rule-them-all/102#2\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d102\">pointed out</a>, legal issues arise when using public data that isn't specifically licensed to permit reuse.</p>\r\n\r\n<p>It has now been 212 days since Lar's alert and 199 days since I started this discussion seeking expert advice. Here I'll report on the strategy we chose. Our goals were: to bring us into compliance with copyright law and license agreements; to respect the intent of resource creators; to preserve our sunk time investment; and to retain the scientific value of our network. Unfortunately, no one solution satisfied every objective. We were left to choose between several imperfect ways forward.</p>\r\n\r\n<h2>Compliance efforts</h2>\r\n\r\n<p>First, I compiled the <a href=\"https://github.com/dhimmel/integrate/blob/145810796f0729d1ed5255bcb83c658490a198f9/licenses/README.md\">licenses for all of the resources</a> we included in our network. Of the 28 resources we integrated, only 12 had licenses that <a href=\"http://opendefinition.org/licenses/0\">met</a> the <a href=\"http://opendefinition.org/od/2.1/en/\" title=\"Open Definition 2.1\">criteria for open knowledge</a>. As a result, our project would not be a possibility under a paradigm of absolute compliance.</p>\r\n\r\n<p>Resources fell into four categories regarding their licensing:</p>\r\n\r\n<ol><li>Resources that are in the <strong>public domain</strong>.</li><li>Resources with a license that <strong>allows</strong> use, redistribution, and modification.</li><li>Resources with a license that <strong>forbids</strong> use, redistribution, or modification.</li><li>Resources that <strong>do not have</strong> a license.</li></ol>\r\n\r\n<p>While I retrospectively assigned these categories while writing this post, the approach we pursued for a given resource aligned with its category. We approached category 1 &amp; 2 resources by specifying their license wherever we use, redistribute, or modify them. We approached category 3 &amp; 4 resources by requesting permission from their creators or owners. I attempted attribution for all resources, regardless of category, to maintain data provenance.</p>\r\n\r\n<h3>Category 1 &amp; 2 resources</h3>\r\n\r\n<p>There were 4 <strong>category 1</strong> resources — Entrez Gene, MEDLINE, LabeledIn, and MeSH — all due to US federal Government creations <a href=\"https://en.wikipedia.org/w/index.php?title=Copyright_status_of_work_by_the_U.S._government&amp;oldid=708981516\">not being entitled</a> to copyright protection. These resources were easy to integrate: I could proceed without restriction and released derivative works under CC0.</p>\r\n\r\n<p>There were 14 <strong>category 2</strong> resources. If the resource uses a standard license, such as a license by <a href=\"https://creativecommons.org/licenses/\">Creative Commons</a> or <a href=\"http://opendatacommons.org/licenses/\">Open Data Commons</a>, I used the same license including version for redistribution and derivative works. Examples include Disease Ontology, DISEASES, Gene Ontology, TISSUES, Uberon, WikiPathways, BindingDB, DisGeNET. If the resource used a custom license, then I applied a Creative Commons license that abided by the custom stipulations. For example, <a href=\"https://creativecommons.org/licenses/by/4.0/\">CC BY 4.0</a> for custom licenses that require attribution — GWAS Catalog &amp; LINCS L1000 — and <a href=\"https://creativecommons.org/licenses/by-nc/4.0/\">CC BY-NC 4.0</a> for custom licenses that forbid commercial use or specify academic use only — DrugBank.</p>\r\n\r\n<p>I embedded licensing into the network as node/relationship properties. Therefore, users can filter to retain only specific licenses when querying or parsing our network. Prior to the network stage when data for each resource still resides in separate repositories, I specified licensing via a <code>LICENSE.md</code> file or a section in the <code>README.md</code> file.</p>\r\n\r\n<h3>Category 3 &amp; 4 resources</h3>\r\n\r\n<p>Originally I identified 3 <strong>category 3</strong> resources — MSigDB, Incomplete Interactome, LINCS L1000. I chronicled these permission requests on <em>Thinklab</em>. Through our permission requests, we learned that the <a href=\"https://doi.org/10.15363/thinklab.d111\">Incomplete Interactome was</a> actually category 4 and <a href=\"https://doi.org/10.15363/thinklab.d110\">LINCS L1000 was</a> actually category 2. Our permission request to MSigDB <a href=\"http://doi.org/10.15363/thinklab.d108\">is ongoing</a>.</p>\r\n\r\n<p>There were 9 <strong>category 4</strong> resources — ADEPTUS, Bgee, DOAF, ehrlink, ERC, hetio-dag, Incomplete Interactome, Human Interactome Database, STARGEO. Since I am the creator of hetio-dag and our STARGEO analysis, these resources did not require any action. For the remaining resources, I sent permission requests.</p>\r\n\r\n<p>For category 3 &amp; 4 resources, I opted to continue including the resource in our network regardless of whether we affirmatively received permission. I deemed these resources too critical from a scientific perspective to justify their removal. Several factors shaped my decision: many scientists who post their data assume it will automatically be reusable; the resources were publicly funded with the intent to be used for science; copyright may not apply if our network is fair use or the underlying data is factual; and reuse of scientific data despite all rights reserved is prevalent throughout academia.</p>\r\n\r\n<p>There are several unpleasant consequences to my decision to include category 3 &amp; 4 works. First, I risk the legal consequences of infringement. Second, we could have to purge content from our network if a data creator/owner requests that we discontinue use of their resource. Third, anyone who wants to use or build off of our network will have to revisit the same issues we're facing here.</p>\r\n\r\n<h4>Permission requests by outcome</h4>\r\n\r\n<p>For category 3 &amp; 4 resources, I requested permission to use the resource for our project. I've organized my requests into four outcomes:</p>\r\n\r\n<ul><li>EXST — We received a response referring us to an existing license. In the four instances, we had overlooked the license because it was difficult to find or unclear whether it applied.</li><li>PERM — We received a response granting us permission to use the resource. In both cases, the authors granted their permission but acknowledged that they may not be the rights holder.</li><li>INC — We received an inconclusive response. In all three cases, the authors indicated they would take licensing actions which have yet to happen.</li><li>NORESP — No response.</li></ul>\r\n\r\n<p>Each resource for which we requested permissions is below. Days indicates the time till first response. When present, public documentation of our request is linked to in Contact Method. The table is sorted by outcome and then by days.</p>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th>Resource</th><th>Outcome</th><th>Days</th><th>Contact Method</th></tr></thead><tbody><tr><td><a href=\"http://uberon.org\">Uberon</a></td><td>EXST</td><td>0</td><td><a href=\"https://github.com/obophenotype/uberon/issues/1139\">GitHub Issue</a></td></tr><tr><td><a href=\"http://www.ncbi.nlm.nih.gov/gene\">Entrez Gene</a></td><td>EXST</td><td>2</td><td>helpdesk</td></tr><tr><td><a href=\"http://www.lincscloud.org/l1000/\">LINCS L1000</a></td><td>EXST</td><td>16</td><td><a href=\"https://doi.org/10.15363/thinklab.d110\">email</a></td></tr><tr><td><a href=\"https://www.ebi.ac.uk/gwas/\">GWAS Catalog</a></td><td>EXST</td><td>19</td><td>email</td></tr><tr><td><a href=\"http://science.sciencemag.org/content/suppl/2015/02/18/347.6224.1257601.DC1\">Incomplete Interactome</a></td><td>PERM</td><td>0</td><td><a href=\"https://doi.org/10.15363/thinklab.d111\">email</a></td></tr><tr><td><a href=\"http://csb.pitt.edu/erc_analysis/Methods.php\">Evolutionary Rate Covariation</a></td><td>PERM</td><td>16</td><td>email</td></tr><tr><td><a href=\"http://doa.nubic.northwestern.edu/pages/search.php\">DOAF</a></td><td>INC</td><td>2</td><td>email</td></tr><tr><td><a href=\"http://bgee.org\">Bgee</a></td><td>INC</td><td>9</td><td>email/<a href=\"https://doi.org/10.15363/thinklab.d82#15\">note</a></td></tr><tr><td><a href=\"http://software.broadinstitute.org/gsea/msigdb\">MSigDB</a></td><td>INC</td><td>129</td><td><a href=\"https://doi.org/10.15363/thinklab.d108\">email</a></td></tr><tr><td><a href=\"http://interactome.dfci.harvard.edu/H_sapiens/index.php?page=download\">Human Interactome Database</a></td><td>NORESP</td><td>189+</td><td>email</td></tr><tr><td><a href=\"http://acgt.cs.tau.ac.il/adeptus/\">ADEPTUS</a></td><td>NORESP</td><td>198+</td><td>email</td></tr></tbody></table>\r\n\r\n<h2>Conclusion</h2>\r\n\r\n<p>We've gone to great lengths and invested substantial time in complying with data copyright and licensing. However, under a strict interpretation, our project may infringe upon the rights of publicly-funded scholarly resources.</p>",
      "body_md": "# Data licensing and compliance report\r\n\r\nAs a refresher, we released an initial version of our network built from publicly-availabe resources. I had assumed that as long as a resource was public, we could use it for our research. In addition, we're committed to open science — releasing our network and intermediate data, both for reproducibility and to allow others to build off of our research. However, as @larsjuhljensen [pointed out](http://thinklab.com/discussion/one-network-to-rule-them-all/102#2), legal issues arise when using public data that isn't specifically licensed to permit reuse.\r\n\r\nIt has now been 212 days since Lar's alert and 199 days since I started this discussion seeking expert advice. Here I'll report on the strategy we chose. Our goals were: to bring us into compliance with copyright law and license agreements; to respect the intent of resource creators; to preserve our sunk time investment; and to retain the scientific value of our network. Unfortunately, no one solution satisfied every objective. We were left to choose between several imperfect ways forward.\r\n\r\n## Compliance efforts\r\n\r\nFirst, I compiled the [licenses for all of the resources](https://github.com/dhimmel/integrate/blob/145810796f0729d1ed5255bcb83c658490a198f9/licenses/README.md) we included in our network. Of the 28 resources we integrated, only 12 had licenses that [met](http://opendefinition.org/licenses/0) the [criteria for open knowledge](http://opendefinition.org/od/2.1/en/ \"Open Definition 2.1\"). As a result, our project would not be a possibility under a paradigm of absolute compliance.\r\n\r\nResources fell into four categories regarding their licensing:\r\n\r\n1. Resources that are in the **public domain**.\r\n2. Resources with a license that **allows** use, redistribution, and modification.\r\n3. Resources with a license that **forbids** use, redistribution, or modification.\r\n4. Resources that **do not have** a license.\r\n\r\nWhile I retrospectively assigned these categories while writing this post, the approach we pursued for a given resource aligned with its category. We approached category 1 & 2 resources by specifying their license wherever we use, redistribute, or modify them. We approached category 3 & 4 resources by requesting permission from their creators or owners. I attempted attribution for all resources, regardless of category, to maintain data provenance.\r\n\r\n### Category 1 & 2 resources\r\n\r\nThere were 4 **category 1** resources — Entrez Gene, MEDLINE, LabeledIn, and MeSH — all due to US federal Government creations [not being entitled](https://en.wikipedia.org/w/index.php?title=Copyright_status_of_work_by_the_U.S._government&oldid=708981516) to copyright protection. These resources were easy to integrate: I could proceed without restriction and released derivative works under CC0.\r\n\r\nThere were 14 **category 2** resources. If the resource uses a standard license, such as a license by [Creative Commons](https://creativecommons.org/licenses/) or [Open Data Commons](http://opendatacommons.org/licenses/), I used the same license including version for redistribution and derivative works. Examples include Disease Ontology, DISEASES, Gene Ontology, TISSUES, Uberon, WikiPathways, BindingDB, DisGeNET. If the resource used a custom license, then I applied a Creative Commons license that abided by the custom stipulations. For example, [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/) for custom licenses that require attribution — GWAS Catalog & LINCS L1000 — and [CC BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/) for custom licenses that forbid commercial use or specify academic use only — DrugBank.\r\n\r\nI embedded licensing into the network as node/relationship properties. Therefore, users can filter to retain only specific licenses when querying or parsing our network. Prior to the network stage when data for each resource still resides in separate repositories, I specified licensing via a `LICENSE.md` file or a section in the `README.md` file.\r\n\r\n### Category 3 & 4 resources\r\n\r\nOriginally I identified 3 **category 3** resources — MSigDB, Incomplete Interactome, LINCS L1000. I chronicled these permission requests on _Thinklab_. Through our permission requests, we learned that the [Incomplete Interactome was](https://doi.org/10.15363/thinklab.d111) actually category 4 and [LINCS L1000 was](https://doi.org/10.15363/thinklab.d110) actually category 2. Our permission request to MSigDB [is ongoing](http://doi.org/10.15363/thinklab.d108).\r\n\r\nThere were 9 **category 4** resources — ADEPTUS, Bgee, DOAF, ehrlink, ERC, hetio-dag, Incomplete Interactome, Human Interactome Database, STARGEO. Since I am the creator of hetio-dag and our STARGEO analysis, these resources did not require any action. For the remaining resources, I sent permission requests.\r\n\r\nFor category 3 & 4 resources, I opted to continue including the resource in our network regardless of whether we affirmatively received permission. I deemed these resources too critical from a scientific perspective to justify their removal. Several factors shaped my decision: many scientists who post their data assume it will automatically be reusable; the resources were publicly funded with the intent to be used for science; copyright may not apply if our network is fair use or the underlying data is factual; and reuse of scientific data despite all rights reserved is prevalent throughout academia.\r\n\r\nThere are several unpleasant consequences to my decision to include category 3 & 4 works. First, I risk the legal consequences of infringement. Second, we could have to purge content from our network if a data creator/owner requests that we discontinue use of their resource. Third, anyone who wants to use or build off of our network will have to revisit the same issues we're facing here.\r\n\r\n#### Permission requests by outcome\r\n\r\nFor category 3 & 4 resources, I requested permission to use the resource for our project. I've organized my requests into four outcomes:\r\n\r\n+ EXST — We received a response referring us to an existing license. In the four instances, we had overlooked the license because it was difficult to find or unclear whether it applied.\r\n+ PERM — We received a response granting us permission to use the resource. In both cases, the authors granted their permission but acknowledged that they may not be the rights holder.\r\n+ INC — We received an inconclusive response. In all three cases, the authors indicated they would take licensing actions which have yet to happen.\r\n+ NORESP — No response.\r\n\r\nEach resource for which we requested permissions is below. Days indicates the time till first response. When present, public documentation of our request is linked to in Contact Method. The table is sorted by outcome and then by days.\r\n\r\n| Resource | Outcome | Days | Contact Method |\r\n|----------|----------|------|----------------|\r\n| [Uberon](http://uberon.org) | EXST | 0 | [GitHub Issue](https://github.com/obophenotype/uberon/issues/1139) |\r\n| [Entrez Gene](http://www.ncbi.nlm.nih.gov/gene) | EXST | 2 | helpdesk |\r\n| [LINCS L1000](http://www.lincscloud.org/l1000/) | EXST | 16 | [email](https://doi.org/10.15363/thinklab.d110) |\r\n| [GWAS Catalog](https://www.ebi.ac.uk/gwas/) | EXST | 19 | email |\r\n| [Incomplete Interactome](http://science.sciencemag.org/content/suppl/2015/02/18/347.6224.1257601.DC1) | PERM | 0 | [email](https://doi.org/10.15363/thinklab.d111) |\r\n| [Evolutionary Rate Covariation](http://csb.pitt.edu/erc_analysis/Methods.php) | PERM | 16 | email |\r\n| [DOAF](http://doa.nubic.northwestern.edu/pages/search.php) | INC | 2 | email |\r\n| [Bgee](http://bgee.org) | INC | 9 | email/[note](https://doi.org/10.15363/thinklab.d82#15) |\r\n| [MSigDB](http://software.broadinstitute.org/gsea/msigdb) | INC | 129 | [email](https://doi.org/10.15363/thinklab.d108) |\r\n| [Human Interactome Database](http://interactome.dfci.harvard.edu/H_sapiens/index.php?page=download) | NORESP | 189+ | email |\r\n| [ADEPTUS](http://acgt.cs.tau.ac.il/adeptus/) | NORESP | 198+ | email |\r\n\r\n## Conclusion\r\n\r\nWe've gone to great lengths and invested substantial time in complying with data copyright and licensing. However, under a strict interpretation, our project may infringe upon the rights of publicly-funded scholarly resources.",
      "profile": 17,
      "published": "2016-03-15T03:01:43.148728Z",
      "thread": 107,
      "url": "/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#14"
    },
    {
      "body_html": "<h2>Category breakdown by resource</h2>\r\n\r\n<p>Using the consensus curation, we have gone back and calculated the composition of indication category by resource (<a href=\"https://github.com/dhimmel/indications/blob/11d535ba0884ee56c3cd5756fdfb4985f313bd80/curation/catalog.ipynb\">notebook</a>).</p>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th>Resource</th><th>DM</th><th>SYM</th><th>NOT</th><th>Total</th></tr></thead><tbody><tr><td>MEDI-HPS</td><td>532 (67.1%)</td><td>168 (21.2%)</td><td>93 (11.7%)</td><td>793 (100%)</td></tr><tr><td>PREDICT</td><td>346 (59.7%)</td><td>158 (27.2%)</td><td>76 (13.1%)</td><td>580 (100%)</td></tr><tr><td>EHRLink</td><td>205 (44.3%)</td><td>163 (35.2%)</td><td>95 (20.5%)</td><td>463 (100%)</td></tr><tr><td>LabeledIn</td><td>183 (66.1%)</td><td>72 (26.0%)</td><td>22 (7.9%)</td><td>277 (100%)</td></tr></tbody></table>\r\n\r\n<p>The table indicates that of the 793 indications we extracted from MEDI-HPS, 532 (67.1%) were disease modifying. In short, we found that MEDI-HPS and LabeledIn contained the highest percentage of disease-modifying indications. EHRLink, which is based on electronic health records, contained the highest percentage of symptomatic (35.2%) and non (20.5%) indications.</p>\r\n\r\n<h2>Category breakdown by number of resources</h2>\r\n\r\n<p>Next, we looked at the category composition based on the number of resources reporting each indication.</p>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th># of Resources</th><th>DM</th><th>SYM</th><th>NOT</th><th>Total</th></tr></thead><tbody><tr><td>1</td><td>433 (47.4%)</td><td>271 (29.6%)</td><td>210 (23.0%)</td><td>914 (100%)</td></tr><tr><td>2</td><td>190 (66.2%)</td><td>74 (25.8%)</td><td>23 (8.0%)</td><td>287 (100%)</td></tr><tr><td>3</td><td>75 (61.0%)</td><td>38 (30.9%)</td><td>10 (8.1%)</td><td>123 (100%)</td></tr><tr><td>4</td><td>57 (89.1%)</td><td>7 (10.9%)</td><td>0 (0.0%)</td><td>64 (100%)</td></tr></tbody></table>\r\n\r\n<p>The more resources that reported an indication the more likely it was to be disease modifying: indications in only a single resource were disease modifying 47.4% of the time whereas indications in all four resources were disease modifying 89.1% of the time.</p>",
      "body_md": "## Category breakdown by resource\r\n\r\nUsing the consensus curation, we have gone back and calculated the composition of indication category by resource ([notebook](https://github.com/dhimmel/indications/blob/11d535ba0884ee56c3cd5756fdfb4985f313bd80/curation/catalog.ipynb)).\r\n\r\n| Resource | DM | SYM | NOT | Total |\r\n|-----------|-------------|-------------|------------|------------|\r\n| MEDI-HPS | 532 (67.1%) | 168 (21.2%) | 93 (11.7%) | 793 (100%) |\r\n| PREDICT | 346 (59.7%) | 158 (27.2%) | 76 (13.1%) | 580 (100%) |\r\n| EHRLink | 205 (44.3%) | 163 (35.2%) | 95 (20.5%) | 463 (100%) |\r\n| LabeledIn | 183 (66.1%) | 72 (26.0%) | 22 (7.9%) | 277 (100%) |\r\n\r\nThe table indicates that of the 793 indications we extracted from MEDI-HPS, 532 (67.1%) were disease modifying. In short, we found that MEDI-HPS and LabeledIn contained the highest percentage of disease-modifying indications. EHRLink, which is based on electronic health records, contained the highest percentage of symptomatic (35.2%) and non (20.5%) indications.\r\n\r\n## Category breakdown by number of resources\r\n\r\nNext, we looked at the category composition based on the number of resources reporting each indication.\r\n\r\n| # of Resources | DM | SYM | NOT | Total |\r\n|----------------|-------------|-------------|-------------|------------|\r\n| 1 | 433 (47.4%) | 271 (29.6%) | 210 (23.0%) | 914 (100%) |\r\n| 2 | 190 (66.2%) | 74 (25.8%) | 23 (8.0%) | 287 (100%) |\r\n| 3 | 75 (61.0%) | 38 (30.9%) | 10 (8.1%) | 123 (100%) |\r\n| 4 | 57 (89.1%) | 7 (10.9%) | 0 (0.0%) | 64 (100%) |\r\n\r\nThe more resources that reported an indication the more likely it was to be disease modifying: indications in only a single resource were disease modifying 47.4% of the time whereas indications in all four resources were disease modifying 89.1% of the time.",
      "profile": 17,
      "published": "2016-03-15T05:25:20.118976Z",
      "thread": 182,
      "url": "/discussion/announcing-pharmacotherapydb-the-open-catalog-of-drug-therapies-for-disease/182#2"
    },
    {
      "body_html": "<h1>2016 Neo4j GraphGist Challenge</h1>\r\n\r\n<p>We <a href=\"http://thinklab.com/discussion/using-the-neo4j-graph-database-for-hetnets/112#8\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d112\">created</a> a Star Wars themed entry to the 2016 Neo4j GraphGist challenge. The competition aimed to showcase exciting uses of Neo4j — a graph database designed for hetnets. The winners were <a href=\"http://neo4j.com/blog/graphgist-challenge-winners/\" title=\"The Graph Is Strong with This One: GraphGist Challenge Winners!\">announced today</a> and <a href=\"http://neo4j.com/graphgist/c4eab62c-7f5e-4e17-8f75-811d65d83127\" title=\"Drug repurposing by hetnet relationship prediction: a hew hope\">our GraphGist</a> won the \"Open/Government Data and Politics\" category.</p>",
      "body_md": "# 2016 Neo4j GraphGist Challenge\r\n\r\nWe [created](http://thinklab.com/discussion/using-the-neo4j-graph-database-for-hetnets/112#8) a Star Wars themed entry to the 2016 Neo4j GraphGist challenge. The competition aimed to showcase exciting uses of Neo4j -- a graph database designed for hetnets. The winners were [announced today](http://neo4j.com/blog/graphgist-challenge-winners/ \"The Graph Is Strong with This One: GraphGist Challenge Winners!\") and [our GraphGist](http://neo4j.com/graphgist/c4eab62c-7f5e-4e17-8f75-811d65d83127 \"Drug repurposing by hetnet relationship prediction: a hew hope\") won the \"Open/Government Data and Politics\" category.",
      "profile": 17,
      "published": "2016-03-15T23:58:06.449545Z",
      "thread": 113,
      "url": "/discussion/tracking-project-reuse-citation-and-publicity/113#6"
    },
    {
      "body_html": "<h1>PharmacotherapyDB Version 1.0</h1>\r\n\r\n<p>We <a href=\"http://thinklab.com/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95#15\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d95\">completed physician curation</a> for the time being and <a href=\"http://thinklab.com/discussion/announcing-pharmacotherapydb-the-open-catalog-of-drug-therapies-for-disease/182\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d182\">released the first version</a> of our indications catalog called PharmacotherapyDB.</p>\r\n\r\n<p>Thanks <a href=\"/u/b_good\" class=\"username\">@b_good</a>, <a href=\"/u/TIOprea\" class=\"username\">@TIOprea</a>, <a href=\"/u/allisonmccoy\" class=\"username\">@allisonmccoy</a>, <a href=\"/u/ritukhare\" class=\"username\">@ritukhare</a>, and <a href=\"/u/alizee\" class=\"username\">@alizee</a> — your suggestions and feedback were immensely helpful!</p>\r\n\r\n<p>We'll keep this discussion alive for any suggestions of new resources or methods to improve future versions of PharmacotherapyDB.</p>",
      "body_md": "# PharmacotherapyDB Version 1.0\r\n\r\nWe [completed physician curation](http://thinklab.com/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95#15) for the time being and [released the first version](http://thinklab.com/discussion/announcing-pharmacotherapydb-the-open-catalog-of-drug-therapies-for-disease/182) of our indications catalog called PharmacotherapyDB.\r\n\r\nThanks @b_good, @TIOprea, @allisonmccoy, @ritukhare, and @alizee -- your suggestions and feedback were immensely helpful!\r\n\r\nWe'll keep this discussion alive for any suggestions of new resources or methods to improve future versions of PharmacotherapyDB.",
      "profile": 17,
      "published": "2016-03-16T21:56:38.375668Z",
      "thread": 21,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#24"
    },
    {
      "body_html": "<h1>Therapeutic Target Database</h1>\r\n\r\n<p>The Therapeutic Target Database (<a href=\"http://bidd.nus.edu.sg/group/cjttd/\" title=\"Therapeutic Target Database Homepage\">TTD</a>) is a target focused resource with pharmacological relationships. <a href=\"/u/janispi\" class=\"username\">@janispi</a> <a href=\"https://twitter.com/Janis3_14159/status/709844572600475648\" title=\"Twitter\">suggested</a> we check out TTD as a source of drug–disease therapies.</p>\r\n\r\n<p>Specifically, TTD has a dataset of indications, which range from approved to investigational, available online (<a href=\"http://database.idrb.cqu.edu.cn/TTD/download/drug-disease_TTD2016.txt\"><code>drug-disease_TTD2016.txt</code></a>). I couldn't find how these indications were constructed from their publications <span class=\"citation\">[<a href=\"/doi/10.1093/nar/30.1.412\" class=\"citation\" data-key=\"10.1093/nar/30.1.412\">1</a>, <a href=\"/doi/10.1093/nar/gkp1014\" class=\"citation\" data-key=\"10.1093/nar/gkp1014\">2</a>, <a href=\"/doi/10.1093/nar/gkr797\" class=\"citation\" data-key=\"10.1093/nar/gkr797\">3</a>, <a href=\"/doi/10.1093/nar/gkt1129\" class=\"citation\" data-key=\"10.1093/nar/gkt1129\">4</a>, <a href=\"/doi/10.1093/nar/gkv1230\" class=\"citation\" data-key=\"10.1093/nar/gkv1230\">5</a>]</span>, although I may have missed it. I emailed Professor Yu Zong (<code>csccyz@nus.edu.sg</code>), who indicated their drug-disease relationships were human curated.</p>\r\n\r\n<p>Just wanted to note this information, so we remember to keep TTD in mind.</p>",
      "body_md": "# Therapeutic Target Database\r\n\r\nThe Therapeutic Target Database ([TTD](http://bidd.nus.edu.sg/group/cjttd/ \"Therapeutic Target Database Homepage\")) is a target focused resource with pharmacological relationships. @janispi [suggested](https://twitter.com/Janis3_14159/status/709844572600475648 \"Twitter\") we check out TTD as a source of drug--disease therapies.\r\n\r\nSpecifically, TTD has a dataset of indications, which range from approved to investigational, available online ([`drug-disease_TTD2016.txt`](http://database.idrb.cqu.edu.cn/TTD/download/drug-disease_TTD2016.txt)). I couldn't find how these indications were constructed from their publications [@10.1093/nar/30.1.412 @10.1093/nar/gkp1014 @10.1093/nar/gkr797 @10.1093/nar/gkt1129 @10.1093/nar/gkv1230], although I may have missed it. I emailed Professor Yu Zong (`csccyz@nus.edu.sg`), who indicated their drug-disease relationships were human curated.\r\n\r\nJust wanted to note this information, so we remember to keep TTD in mind.",
      "profile": 17,
      "published": "2016-03-16T22:14:15.042713Z",
      "thread": 21,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#25"
    },
    {
      "body_html": "<p>I spoke with <a href=\"/u/TIOprea\" class=\"username\">@TIOprea</a> and Oleg Ursu from the University of New Mexico. They are constructing a highly curated yet highly integrative database of pharmacology named <a href=\"http://datascience.unm.edu/drugdb/\">DrugCentral</a>. They have not yet published a journal article detailing their database. However, they have posted an alpha <a href=\"http://pasilla.health.unm.edu/tomcat/drugcentral/drugcentral\" title=\"DrugCentral Browser\">webapp</a> and <a href=\"https://github.com/olegursu/drugtarget\" title=\"olegursu/drugtarget on GitHub\">data repository</a>, which provide access to select components of the database.</p>\r\n\r\n<p>My impression was that the database is similar in concept to <a href=\"http://www.drugbank.ca/\">DrugBank</a> but has key advantages in certain areas. First, it has integrated types of data which are not currently part of DrugBank. Second, it takes a more clinical approach to curation compared to DrugBank. For example, drug–target relationships in DrugCentral adhere more to the \"three pillars <span class=\"citation\">[<a href=\"/doi/10.1016/j.drudis.2011.12.020\" class=\"citation\" data-key=\"10.1016/j.drudis.2011.12.020\">1</a>]</span>\" of pharmacological activity.</p>\r\n\r\n<p>I created a repository (<a href=\"https://github.com/dhimmel/drugcentral\" title=\"dhimmel/drugcentral on GitHub\"><code>dhimmel/drugcentral</code></a>) to process parts of DrugCentral for inclusion in our network. Details of the integration will follow.</p>",
      "body_md": "I spoke with @TIOprea and Oleg Ursu from the University of New Mexico. They are constructing a highly curated yet highly integrative database of pharmacology named [DrugCentral](http://datascience.unm.edu/drugdb/). They have not yet published a journal article detailing their database. However, they have posted an alpha [webapp](http://pasilla.health.unm.edu/tomcat/drugcentral/drugcentral \"DrugCentral Browser\") and [data repository](https://github.com/olegursu/drugtarget \"olegursu/drugtarget on GitHub\"), which provide access to select components of the database.\r\n\r\nMy impression was that the database is similar in concept to [DrugBank](http://www.drugbank.ca/) but has key advantages in certain areas. First, it has integrated types of data which are not currently part of DrugBank. Second, it takes a more clinical approach to curation compared to DrugBank. For example, drug--target relationships in DrugCentral adhere more to the \"three pillars [@10.1016/j.drudis.2011.12.020]\" of pharmacological activity.\r\n\r\nI created a repository ([`dhimmel/drugcentral`](https://github.com/dhimmel/drugcentral \"dhimmel/drugcentral on GitHub\")) to process parts of DrugCentral for inclusion in our network. Details of the integration will follow.",
      "profile": 17,
      "published": "2016-03-20T16:41:10.716611Z",
      "thread": 186,
      "url": "/discussion/incorporating-drugcentral-data-in-our-network/186"
    },
    {
      "body_html": "<h1>Contributions to our hetnet</h1>\r\n\r\n<p>I processed DrugCentral data and converted it into the identifier systems used by our network (<a href=\"https://github.com/dhimmel/drugcentral/blob/e80a0c966a53ce48650d98069b126801c2793517/drugcentral-to-rephetio.ipynb\">notebook</a>). I have initially added two relationship types from DrugCentral into the hetnet (<a href=\"https://github.com/dhimmel/integrate/commit/0f2ef740197dd2767cb0de80f57d9f47e2e91c7a\">commit</a>). </p>\r\n\r\n<h2>Drug targets</h2>\r\n\r\n<p>I extracted drug–target relationships from DrugCentral and converted them into the DrugBank and Entrez Gene identifiers in our network (<a href=\"https://github.com/dhimmel/drugcentral/blob/e80a0c966a53ce48650d98069b126801c2793517/rephetio/targets.tsv\">dataset</a>). The table below shows the sources from which DrugCentral compiled drug targets and how many relationships each source contributed. </p>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th>Resource</th><th>Count</th></tr></thead><tbody><tr><td>DrugCentral (ChEMBL)</td><td>2,922</td></tr><tr><td>DrugCentral (literature)</td><td>182</td></tr><tr><td>DrugCentral (label)</td><td>89</td></tr><tr><td>DrugCentral (IUPHAR)</td><td>56</td></tr><tr><td>DrugCentral (KEGG DRUG)</td><td>25</td></tr></tbody></table>\r\n\r\n<p>Prior to including DrugCentral, our network contained 10,747 <em>Compound–binds–Gene</em> relationships from <a href=\"http://thinklab.com/discussion/protein-target-carrier-transporter-and-enzyme-interactions-in-drugbank/65#1\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d65\">DrugBank</a> and <a href=\"http://thinklab.com/discussion/integrating-drug-target-information-from-bindingdb/53#6\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d53\">BindingDB</a>. Drug targets from DrugCentral added 824 additional binding relationships.</p>\r\n\r\n<h2>Pharmacologic classes</h2>\r\n\r\n<p>DrugCentral has compiled the membership of compounds in pharmacologic classes from several <a href=\"https://github.com/olegursu/drugtarget/blob/9a6d84bed8650c6c507a2d3d786814c774568610/README.md#pharmacologic-class-table\">sources</a>, which contain the following types of classes:</p>\r\n\r\n<ul><li>FDA — Mechanism of Action</li><li>FDA — Physiologic Effect</li><li>FDA — Chemical/Ingredient</li><li>FDA — Established Pharmacologic Class</li><li>MeSH — Pharmacological Action</li><li>CHEBI — Application</li></ul>\r\n\r\n<p>I decided to assign all of these classes to a single node type (<em>Pharmacologic Class</em>). I added a new relationship type for <em>Pharmacologic Class–includes–Compound</em>. DrugCentral contributed 10,959 relationships for 1,262 <a href=\"https://github.com/dhimmel/drugcentral/blob/e80a0c966a53ce48650d98069b126801c2793517/rephetio/classes.tsv\" title=\"Table of Pharmacologic Classes extracted from DrugCentral\">pharmacologic classes</a>.</p>",
      "body_md": "# Contributions to our hetnet\r\n\r\nI processed DrugCentral data and converted it into the identifier systems used by our network ([notebook](https://github.com/dhimmel/drugcentral/blob/e80a0c966a53ce48650d98069b126801c2793517/drugcentral-to-rephetio.ipynb)). I have initially added two relationship types from DrugCentral into the hetnet ([commit](https://github.com/dhimmel/integrate/commit/0f2ef740197dd2767cb0de80f57d9f47e2e91c7a)). \r\n\r\n\r\n## Drug targets\r\n\r\nI extracted drug--target relationships from DrugCentral and converted them into the DrugBank and Entrez Gene identifiers in our network ([dataset](https://github.com/dhimmel/drugcentral/blob/e80a0c966a53ce48650d98069b126801c2793517/rephetio/targets.tsv)). The table below shows the sources from which DrugCentral compiled drug targets and how many relationships each source contributed. \r\n\r\n| Resource | Count |\r\n| ------------- | -------- |\r\n| DrugCentral (ChEMBL) | 2,922 |\r\n| DrugCentral (literature) | 182 |\r\n| DrugCentral (label) | 89|\r\n| DrugCentral (IUPHAR) | 56 |\r\n| DrugCentral (KEGG DRUG) | 25 |\r\n\r\nPrior to including DrugCentral, our network contained 10,747 _Compound--binds--Gene_ relationships from [DrugBank](http://thinklab.com/discussion/protein-target-carrier-transporter-and-enzyme-interactions-in-drugbank/65#1) and [BindingDB](http://thinklab.com/discussion/integrating-drug-target-information-from-bindingdb/53#6). Drug targets from DrugCentral added 824 additional binding relationships.\r\n\r\n## Pharmacologic classes\r\n\r\nDrugCentral has compiled the membership of compounds in pharmacologic classes from several [sources](https://github.com/olegursu/drugtarget/blob/9a6d84bed8650c6c507a2d3d786814c774568610/README.md#pharmacologic-class-table), which contain the following types of classes:\r\n\r\n+ FDA -- Mechanism of Action\r\n+ FDA -- Physiologic Effect\r\n+ FDA -- Chemical/Ingredient\r\n+ FDA -- Established Pharmacologic Class\r\n+ MeSH -- Pharmacological Action\r\n+ CHEBI -- Application\r\n\r\nI decided to assign all of these classes to a single node type (_Pharmacologic Class_). I added a new relationship type for _Pharmacologic Class--includes--Compound_. DrugCentral contributed 10,959 relationships for 1,262 [pharmacologic classes](https://github.com/dhimmel/drugcentral/blob/e80a0c966a53ce48650d98069b126801c2793517/rephetio/classes.tsv \"Table of Pharmacologic Classes extracted from DrugCentral\").",
      "profile": 17,
      "published": "2016-03-20T17:21:28.930112Z",
      "thread": 186,
      "url": "/discussion/incorporating-drugcentral-data-in-our-network/186#2"
    },
    {
      "body_html": "<h1>Medical indications</h1>\r\n\r\n<p>In my conversation with DrugCentral team members, we first discussed PharmacotherapyDB, our <a href=\"http://thinklab.com/discussion/announcing-pharmacotherapydb-the-open-catalog-of-drug-therapies-for-disease/182\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d182\">recently-released</a> physician-curated catalog of indications. One major takeaway was that we needed to more clearly explain that our definition of disease modifying differs from the clinical definition. Also, we need to more clearly state that <code>NOT</code> refers to non-indications.</p>\r\n\r\n<p>As part of DrugCentral, they've constructed their own indications catalog. Their seeded their catalog from <a href=\"http://omop.org/\" title=\"Observational Medical Outcomes Partnership\">OMOP</a> in 2012 and have since then manually added additional indications. OMOP has now become <a href=\"http://www.ohdsi.org/\" title=\"Observational Health Data Sciences and Informatics\">OHDSI</a> and hosts their vocabular on GitHub at <a href=\"https://github.com/OHDSI/Vocabulary-v5.0\"><code>OHDSI/Vocabulary-v5.0</code></a>. As a side note, we were not aware of OMOP <span class=\"citation\">[<a href=\"/doi/10.7326/0003-4819-153-9-201011020-00010\" class=\"citation\" data-key=\"10.7326/0003-4819-153-9-201011020-00010\">1</a>]</span> or OHDSI <span class=\"citation\">[<a href=\"/doi/10.3233/978-1-61499-564-7-574\" class=\"citation\" data-key=\"10.3233/978-1-61499-564-7-574\">2</a>]</span> when we <a href=\"http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#21\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d21\">assembled</a> our indications for version 1.0 of PharmacotherapyDB.</p>\r\n\r\n<h2>Aligning indications with PharmacotherapyDB</h2>\r\n\r\n<p>I converted the DrugCentral indications to the slim sets of DrugBank drugs and Disease Ontology diseases in PharmacotherapyDB 1.0 (<a href=\"https://github.com/dhimmel/drugcentral/blob/e80a0c966a53ce48650d98069b126801c2793517/drugcentral-to-rephetio.ipynb\">notebook</a>, <a href=\"https://github.com/dhimmel/drugcentral/blob/e80a0c966a53ce48650d98069b126801c2793517/rephetio/indications.tsv\">dataset</a>). For each disease, I aggregated direct indications as well as indications for subtypes (referred to as propagation). </p>\r\n\r\n<p>In the <a href=\"https://github.com/dhimmel/drugcentral/blob/e80a0c966a53ce48650d98069b126801c2793517/rephetio/indications.tsv\">converted dataset</a>, I included a <code>category</code> column giving the indication's PharmacotherapyDB 1.0 status. Of a total of 671 indications extracted from DrugCentral, 210 were not in PharmacotherapyDB 1.0. Of the 461 indications in PharmacotherapyDB, 359 were classified as disease modifying (78%), 77 were classified as symptomatic (17%), and 25 were classified as non-indications (5%). </p>\r\n\r\n<p>6 of the non-indications were <a href=\"https://github.com/dhimmel/drugcentral/blob/e80a0c966a53ce48650d98069b126801c2793517/rephetio/indications.tsv#L38\">for anemia</a> and 8 were <a href=\"https://github.com/dhimmel/drugcentral/blob/e80a0c966a53ce48650d98069b126801c2793517/rephetio/indications.tsv#L296\">for hypertension</a>, two diseases for which we have a <a href=\"http://thinklab.com/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95#8\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d95\">known problem</a> with their generality. <a href=\"http://thinklab.com/discussion/announcing-pharmacotherapydb-the-open-catalog-of-drug-therapies-for-disease/182#2\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d182\">Compared to the four sources</a> of PharmacotherapyDB indications, DrugCentral appears to have a higher percentage of disease modifying indications. However, we're basing this assessment on indications that appeared in DrugCentral and at least one other resource, so it's potentially biased.</p>\r\n\r\n<p><a href=\"/u/pouyakhankhanian\" class=\"username\">@pouyakhankhanian</a>, if you are up for curating the 210 new indications as <code>DM</code>, <code>SYM</code>, or <code>NOT</code>, we could potentially:</p>\r\n\r\n<ol><li>add these indications to a future release of PharmacotherapyDB</li><li>use these indications to test our predictions</li></ol>",
      "body_md": "# Medical indications\r\n\r\nIn my conversation with DrugCentral team members, we first discussed PharmacotherapyDB, our [recently-released](http://thinklab.com/discussion/announcing-pharmacotherapydb-the-open-catalog-of-drug-therapies-for-disease/182) physician-curated catalog of indications. One major takeaway was that we needed to more clearly explain that our definition of disease modifying differs from the clinical definition. Also, we need to more clearly state that `NOT` refers to non-indications.\r\n\r\nAs part of DrugCentral, they've constructed their own indications catalog. Their seeded their catalog from [OMOP](http://omop.org/ \"Observational Medical Outcomes Partnership\") in 2012 and have since then manually added additional indications. OMOP has now become [OHDSI](http://www.ohdsi.org/ \"Observational Health Data Sciences and Informatics\") and hosts their vocabular on GitHub at [`OHDSI/Vocabulary-v5.0`](https://github.com/OHDSI/Vocabulary-v5.0). As a side note, we were not aware of OMOP [@10.7326/0003-4819-153-9-201011020-00010] or OHDSI [@10.3233/978-1-61499-564-7-574] when we [assembled](http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#21) our indications for version 1.0 of PharmacotherapyDB.\r\n\r\n## Aligning indications with PharmacotherapyDB\r\n\r\nI converted the DrugCentral indications to the slim sets of DrugBank drugs and Disease Ontology diseases in PharmacotherapyDB 1.0 ([notebook](https://github.com/dhimmel/drugcentral/blob/e80a0c966a53ce48650d98069b126801c2793517/drugcentral-to-rephetio.ipynb), [dataset](https://github.com/dhimmel/drugcentral/blob/e80a0c966a53ce48650d98069b126801c2793517/rephetio/indications.tsv)). For each disease, I aggregated direct indications as well as indications for subtypes (referred to as propagation). \r\n\r\nIn the [converted dataset](https://github.com/dhimmel/drugcentral/blob/e80a0c966a53ce48650d98069b126801c2793517/rephetio/indications.tsv), I included a `category` column giving the indication's PharmacotherapyDB 1.0 status. Of a total of 671 indications extracted from DrugCentral, 210 were not in PharmacotherapyDB 1.0. Of the 461 indications in PharmacotherapyDB, 359 were classified as disease modifying (78%), 77 were classified as symptomatic (17%), and 25 were classified as non-indications (5%). \r\n\r\n6 of the non-indications were [for anemia](https://github.com/dhimmel/drugcentral/blob/e80a0c966a53ce48650d98069b126801c2793517/rephetio/indications.tsv#L38) and 8 were [for hypertension](https://github.com/dhimmel/drugcentral/blob/e80a0c966a53ce48650d98069b126801c2793517/rephetio/indications.tsv#L296), two diseases for which we have a [known problem](http://thinklab.com/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95#8) with their generality. [Compared to the four sources](http://thinklab.com/discussion/announcing-pharmacotherapydb-the-open-catalog-of-drug-therapies-for-disease/182#2) of PharmacotherapyDB indications, DrugCentral appears to have a higher percentage of disease modifying indications. However, we're basing this assessment on indications that appeared in DrugCentral and at least one other resource, so it's potentially biased.\r\n\r\n@pouyakhankhanian, if you are up for curating the 210 new indications as `DM`, `SYM`, or `NOT`, we could potentially:\r\n\r\n1. add these indications to a future release of PharmacotherapyDB\r\n2. use these indications to test our predictions",
      "profile": 17,
      "published": "2016-03-20T19:18:45.927561Z",
      "thread": 186,
      "url": "/discussion/incorporating-drugcentral-data-in-our-network/186#3"
    },
    {
      "body_html": "<h1>Pharmacologic Classes that are indications</h1>\r\n\r\n<p>We've noticed that many of the <a href=\"https://github.com/dhimmel/drugcentral/blob/e80a0c966a53ce48650d98069b126801c2793517/rephetio/classes.tsv\">pharmacologic classes</a> are essentially indications. This could be problematic since it could confound our classification approach. Specifically, it could lead to the appearance that our method predicts indications when in reality it just regurgitates indications which were encoded by a pharmacologic class.</p>\r\n\r\n<p>Some examples of classes that resemble indications are:</p>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th>class_id</th><th>class_name</th><th>class_source</th><th>class_type</th></tr></thead><tbody><tr><td><a href=\"http://identifiers.org/chebi/CHEBI%3A35469\">CHEBI:35469</a></td><td>antidepressant</td><td>CHEBI</td><td>Application</td></tr><tr><td><a href=\"http://purl.bioontology.org/ontology/NDFRT/N0000175482\">N0000175482</a></td><td>Antimalarial</td><td>FDA</td><td>FDA Established Pharmacologic Class</td></tr><tr><td><a href=\"http://identifiers.org/mesh/D018501\">D018501</a></td><td>Antirheumatic Agents</td><td>MeSH</td><td>Pharmacological Action</td></tr></tbody></table>\r\n\r\n<p><a href=\"/u/sergiobaranzini\" class=\"username\">@sergiobaranzini</a> and I looked through the 6 sources and found that 3 were <strong>less problematic</strong>:</p>\r\n\r\n<ul><li>FDA — Chemical/Ingredient</li><li>FDA — Mechanism of Action</li><li>FDA — Physiologic Effect</li></ul>\r\n\r\n<p>The other 3 were <strong>more problematic</strong>:</p>\r\n\r\n<ul><li>FDA — Established Pharmacologic Class</li><li>MeSH — Pharmacological Action</li><li>CHEBI — Application</li></ul>\r\n\r\n<p>Therefore, I excluded classes from the 3 more problematic sources. This reduced the number of classes from 1,262 to 345, the number of edges from 10,959 to 1,029, and the number of compounds in a class from 1,423 to 724 (<a href=\"https://github.com/dhimmel/integrate/commit/1229536c6d2146c4cae97f045cf8cbdd272420f6\">commit</a>).</p>\r\n\r\n<p>One step would be to salvage many of the filtered classes by manual curation. The majority of the removed classes did not overlap with <a href=\"http://thinklab.com/discussion/unifying-disease-vocabularies/44#6\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d44\">DO Slim</a> diseases and thus shouldn't confound our analysis. If we decide to curate, we'll have to decide whether to exclude all indications or just indications in DO Slim.</p>",
      "body_md": "# Pharmacologic Classes that are indications\r\n\r\nWe've noticed that many of the [pharmacologic classes](https://github.com/dhimmel/drugcentral/blob/e80a0c966a53ce48650d98069b126801c2793517/rephetio/classes.tsv) are essentially indications. This could be problematic since it could confound our classification approach. Specifically, it could lead to the appearance that our method predicts indications when in reality it just regurgitates indications which were encoded by a pharmacologic class.\r\n\r\nSome examples of classes that resemble indications are:\r\n\r\n| class_id | class_name | class_source | class_type |\r\n|------------------|------------|--------------|---------------------|\r\n| [CHEBI:35469](http://identifiers.org/chebi/CHEBI%3A35469) | antidepressant | CHEBI | Application |\r\n| [N0000175482](http://purl.bioontology.org/ontology/NDFRT/N0000175482) | Antimalarial | FDA | FDA Established Pharmacologic Class |\r\n| [D018501](http://identifiers.org/mesh/D018501) | Antirheumatic Agents | MeSH | Pharmacological Action |\r\n\r\n@sergiobaranzini and I looked through the 6 sources and found that 3 were **less problematic**:\r\n\r\n+ FDA — Chemical/Ingredient\r\n+ FDA — Mechanism of Action\r\n+ FDA — Physiologic Effect\r\n\r\nThe other 3 were **more problematic**:\r\n\r\n+ FDA — Established Pharmacologic Class\r\n+ MeSH — Pharmacological Action\r\n+ CHEBI — Application\r\n\r\nTherefore, I excluded classes from the 3 more problematic sources. This reduced the number of classes from 1,262 to 345, the number of edges from 10,959 to 1,029, and the number of compounds in a class from 1,423 to 724 ([commit](https://github.com/dhimmel/integrate/commit/1229536c6d2146c4cae97f045cf8cbdd272420f6)).\r\n\r\nOne step would be to salvage many of the filtered classes by manual curation. The majority of the removed classes did not overlap with [DO Slim](http://thinklab.com/discussion/unifying-disease-vocabularies/44#6) diseases and thus shouldn't confound our analysis. If we decide to curate, we'll have to decide whether to exclude all indications or just indications in DO Slim.",
      "profile": 17,
      "published": "2016-03-24T00:57:39.774095Z",
      "thread": 186,
      "url": "/discussion/incorporating-drugcentral-data-in-our-network/186#4"
    },
    {
      "body_html": "<p>Our approach quantifies the hetnet topology between compound–disease pairs by calculating the prevalence of different path types (metapaths) <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1004259\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1004259\">1</a>]</span>. We would like to be able to estimate the complexity of a metapath, given the graph. We'll define complexity as the number of Neo4j database hits (<code>dbhits</code>) needed to execute a query. Specifically, we're interested in  assessing the runtime of queries for a given metapath, without having to run the queries. </p>\r\n\r\n<p>If we can accurately predict runtime using estimated complexity, we can selectively avoid computing features for overly complex metapaths. Since the number of potential metapaths (as well as paths per metapath) scales combinatorially with increasing path length, our method will be always run into computational limits. We're hoping to use complexity estimates to help choose a tractable set of metpaths.</p>\r\n\r\n<h2>Metapath runtime on a trial feature extraction</h2>\r\n\r\n<p>In the past, we've used a length cutoff for metapaths. In a trial run of our feature extraction, I computed features for all metapaths with lengths 2–4. However, I terminated the process midway because progress was too slow. What we saw was that a few metapaths took a disproportionate amount of time to query.</p>\r\n\r\n<p>See <a href=\"http://nbviewer.jupyter.org/github/dhimmel/learn/blob/5c94a0b53d71f2ac0a2f48133a8e1d4e4e0ee26c/all-features-pyviz.ipynb#Time-per-query\">cells 11–12 in this notebook</a>, noting that ~half of metapaths with lengths 2–4 are missing and that paths with duplicate nodes were not excluded as they <a href=\"http://thinklab.com/discussion/path-exclusion-conditions/134#4\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d134\">should have been</a>. Nonetheless, the results are clear: the metapaths with long runtimes were only mildly predictive (<code>auroc</code>), did not decline in predictiveness due to <a href=\"http://thinklab.com/discussion/assessing-the-effectiveness-of-our-hetnet-permutations/178\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d178\">network permutation</a> (<code>delta_auroc</code>).</p>\r\n\r\n<p>The worst metapath was <em>CdGeAeGuD</em>, taking on average 37 seconds per query (compound–disease pair).  This metapath combines four gene-expression-based edges, which can be terribly high degree on their non-gene terminus.</p>\r\n\r\n<p>Hence, we will look into estimating metapath complexity to exclude such runtime outliers.</p>",
      "body_md": "Our approach quantifies the hetnet topology between compound--disease pairs by calculating the prevalence of different path types (metapaths) [@10.1371/journal.pcbi.1004259]. We would like to be able to estimate the complexity of a metapath, given the graph. We'll define complexity as the number of Neo4j database hits (`dbhits`) needed to execute a query. Specifically, we're interested in  assessing the runtime of queries for a given metapath, without having to run the queries. \r\n\r\nIf we can accurately predict runtime using estimated complexity, we can selectively avoid computing features for overly complex metapaths. Since the number of potential metapaths (as well as paths per metapath) scales combinatorially with increasing path length, our method will be always run into computational limits. We're hoping to use complexity estimates to help choose a tractable set of metpaths.\r\n\r\n## Metapath runtime on a trial feature extraction\r\n\r\nIn the past, we've used a length cutoff for metapaths. In a trial run of our feature extraction, I computed features for all metapaths with lengths 2--4. However, I terminated the process midway because progress was too slow. What we saw was that a few metapaths took a disproportionate amount of time to query.\r\n\r\nSee [cells 11--12 in this notebook](http://nbviewer.jupyter.org/github/dhimmel/learn/blob/5c94a0b53d71f2ac0a2f48133a8e1d4e4e0ee26c/all-features-pyviz.ipynb#Time-per-query), noting that ~half of metapaths with lengths 2--4 are missing and that paths with duplicate nodes were not excluded as they [should have been](http://thinklab.com/discussion/path-exclusion-conditions/134#4). Nonetheless, the results are clear: the metapaths with long runtimes were only mildly predictive (`auroc`), did not decline in predictiveness due to [network permutation](http://thinklab.com/discussion/assessing-the-effectiveness-of-our-hetnet-permutations/178) (`delta_auroc`).\r\n\r\nThe worst metapath was _CdGeAeGuD_, taking on average 37 seconds per query (compound--disease pair).  This metapath combines four gene-expression-based edges, which can be terribly high degree on their non-gene terminus.\r\n\r\nHence, we will look into estimating metapath complexity to exclude such runtime outliers.",
      "profile": 17,
      "published": "2016-03-22T17:03:53.425951Z",
      "thread": 187,
      "url": "/discussion/estimating-the-complexity-of-hetnet-traversal/187"
    },
    {
      "body_html": "<h1>Estimating the complexity of a sequential traversal</h1>\r\n\r\n<p>In sequential traversal, the query begins on a single node and expands to create a tree of paths conforming to the specified metapath. Once the tree has been fully expanded, only paths whose leaves match the desired ending node are retained.</p>\r\n\r\n<p>To estimate the sequential complexity of a metapath on a given graph, I first calculated the average degree of each metaedge based on our specific hetnet. Then, I took the log10 of the product of the average degrees along the path. Each metaedge contributes one degree: the average degree of whichever metanode is traversed first by the path. Since you can start a sequential traversal from either end of a path, we'll refer to forward versus backward sequential traversal.</p>\r\n\r\n<p><a href=\"/u/alizee\" class=\"username\">@alizee</a> proved to me in whiteboard discussion, that there is always a constant difference between the complexity of a forward and reverse sequential traversal, which is based only on the number of nodes of the source and target metanodes. The estimated complexity of a sequential traversal is less when starting on the node whose type is more abundant. I'll let <a href=\"/u/alizee\" class=\"username\">@alizee</a> explain the details.</p>\r\n\r\n<p>On the trial feature extraction, forward sequential complexity was a good estimator of runtime (<a href=\"http://nbviewer.jupyter.org/github/dhimmel/learn/blob/5c94a0b53d71f2ac0a2f48133a8e1d4e4e0ee26c/time.ipynb#sequential_complexity\" title=\"Disregard cells 14--15, which use a flawed method for computing join complexity\">notebook cell 13</a>). However, it's important to note that our Neo4j queries were not performed sequentially. Instead, traversal began from both termini and met in the middle where the results were joined. Even so, estimated sequential complexity may be a good metric to use for metapath selection.</p>",
      "body_md": "# Estimating the complexity of a sequential traversal\r\n\r\nIn sequential traversal, the query begins on a single node and expands to create a tree of paths conforming to the specified metapath. Once the tree has been fully expanded, only paths whose leaves match the desired ending node are retained.\r\n\r\nTo estimate the sequential complexity of a metapath on a given graph, I first calculated the average degree of each metaedge based on our specific hetnet. Then, I took the log10 of the product of the average degrees along the path. Each metaedge contributes one degree: the average degree of whichever metanode is traversed first by the path. Since you can start a sequential traversal from either end of a path, we'll refer to forward versus backward sequential traversal.\r\n\r\n@alizee proved to me in whiteboard discussion, that there is always a constant difference between the complexity of a forward and reverse sequential traversal, which is based only on the number of nodes of the source and target metanodes. The estimated complexity of a sequential traversal is less when starting on the node whose type is more abundant. I'll let @alizee explain the details.\r\n\r\nOn the trial feature extraction, forward sequential complexity was a good estimator of runtime ([notebook cell 13](http://nbviewer.jupyter.org/github/dhimmel/learn/blob/5c94a0b53d71f2ac0a2f48133a8e1d4e4e0ee26c/time.ipynb#sequential_complexity \"Disregard cells 14--15, which use a flawed method for computing join complexity\")). However, it's important to note that our Neo4j queries were not performed sequentially. Instead, traversal began from both termini and met in the middle where the results were joined. Even so, estimated sequential complexity may be a good metric to use for metapath selection.",
      "profile": 17,
      "published": "2016-03-22T17:31:44.934453Z",
      "thread": 187,
      "url": "/discussion/estimating-the-complexity-of-hetnet-traversal/187#2"
    },
    {
      "body_html": "<h1>A note on traversal complexity</h1>\r\n\r\n<h2>Intro</h2>\r\n\r\n<p>The question here is to estimate the complexity of graph traversal when following a 'metapath': a specification for which kind of nodes we need to traverse, in which order. This complexity will be useful to get a proxy for computation time when create the features we currently use in <em>rephetio</em> for prediction problems, the DWPC. We mainly look at sequential complexity, which is estimating complexity of traversal from one point to another, in only one direction, without joining two partial traversals in the middle. (even though the latter is almost always a better option)</p>\r\n\r\n<p>We can reasonably make the hypothesis that sequential complexity is equal to the number of possible paths that must be traversed. You propose above, and I would agree, that we use something along the <a href=\"https://en.wikipedia.org/wiki/Mean_field_theory\">mean-field approximation</a> and consider that the average number of paths to be traversed when following a Metapath is equal to the multiplication of the average degrees (outward connections) of all node types along the metapath. Doing so, we intrinsically base our analysis on the number of edges in the networks, which gives us some simple, neat properties.</p>\r\n\r\n<h2>Formal definition</h2>\r\n\r\n<p>We consider as an example 4 types of Nodes in our Network, <span class=\"math\">$$A, B, C, D$$</span>, linked with symmetrical edges, and the Metapath joining them <span class=\"math\">$$A-B-C-D$$</span>. We consider these nodes in order, since the difference between 'forward' (starting from <span class=\"math\">$$A$$</span>) and 'backward' (starting from <span class=\"math\">$$D$$</span>) traversal matters - as defined by <a href=\"/u/dhimmel\" class=\"username\">@dhimmel</a> above. Each node type has respectively <span class=\"math\">$$N_A, N_B, N_C, N_D$$</span> nodes, noted  <span class=\"math\">$$A_i, B_j,$$</span>... Each of these single nodes has a forward degree, noted <span class=\"math\">$$a_i$$</span> for instance, and a backward degree, noted with a prime, eg <span class=\"math\">$$b_i'$$</span>. These degrees represent the number of edges that connect the node at stake (<span class=\"math\">$$A_i$$</span> and <span class=\"math\">$$B_i$$</span> in our examples) to the nodes of the next (<span class=\"math\">$$B$$</span>) or previous (<span class=\"math\">$$A$$</span>) types respectively.</p>\r\n\r\n<p>[remark: we use the actual types instead of a notation \"<span class=\"math\">$$K$$</span>\" for simplicity. The results that are written below for nodes of type <span class=\"math\">$$A$$</span> or <span class=\"math\">$$B$$</span> stand general]</p>\r\n\r\n<h2>Proof of the equivalence between forward and backward complexities</h2>\r\n\r\n<p>The average of node degree is written <span class=\"math\">$$E[a_i]$$</span> such as:</p>\r\n\r\n<div class=\"math\">$$$\r\n\\begin{align}\r\nE_{i\\in1,...,N_A}[a_i]=\\frac{1}{N_A}\\sum_{i\\in1,...,N_A}{a_i}\r\n\\end{align}\r\n$$$</div>\r\n\r\n<p>Then, we have</p>\r\n\r\n<div class=\"math\">$$$\r\n\\begin{align}\r\nE_{i\\in1,...,N_A}[a_i] \\cdot N_A = N_{A-B} = E_{i\\in1,...,N_B}[b_i'] \\cdot N_B \r\n\\end{align}\r\n$$$</div>\r\n\r\n<p>where <span class=\"math\">$$N_{A-B}$$</span> is the number of edges from the nodes <span class=\"math\">$$A$$</span> to <span class=\"math\">$$B$$</span>. Follows, in short notation: </p>\r\n\r\n<div class=\"math\">$$$\r\n\\begin{align}\r\nE[a_i] = E[b_i'] \\cdot \\frac{N_B} {N_A} \r\n\\quad or \\quad\r\nE[b_i'] = E[a_i] \\cdot \\frac{N_A} {N_B} \r\n\\end{align}\r\n$$$</div>\r\n\r\n<p>The estimation of 'forward complexity' outlined above, for discovering the paths that correspond to the metapath <span class=\"math\">$$A-B-C-D$$</span> is written as:</p>\r\n\r\n<div class=\"math\">$$$\r\n\\begin{equation}\r\nFC_{A-B-C-D} = E[a_i] \\cdot E[b_i] \\cdot E[c_i]\r\n\\end{equation}\r\n$$$</div>\r\n\r\n<p>The estimated backward complexity can be written as:</p>\r\n\r\n<div class=\"math\">$$$\r\n\\begin{align}\r\nBC_{A-B-C-D} = FC_{D-C-B-A} = E[b_i'] \\cdot E[c_i'] \\cdot E[d_i']\r\n\\end{align}\r\n$$$</div>\r\n\r\n<p>Follows:</p>\r\n\r\n<div class=\"math\">$$$\r\n\\begin{align}\r\nBC_{A-B-C-D} &amp;= E[b_i'] \\cdot E[c_i'] \\cdot E[d_i']\\\\\r\n &amp;= E[a_i]\\frac{N_A}{N_B} \\cdot E[b_i]\\frac{N_B}{N_C} \\cdot E[c_i]\\frac{N_C}{N_D}\\\\\r\n &amp;= E[a_i] \\cdot E[b_i]\\ \\cdot E[c_i] \\cdot \\frac{N_A}{N_D}\\\\\r\nBC_{A-B-C-D} &amp;= FC_{A-B-C-D} \\cdot \\frac{N_A}{N_D}\r\n\\end{align}\r\n$$$</div>\r\n\r\n<p>More generally:</p>\r\n\r\n<div class=\"math\">$$$\r\n\\begin{equation}\r\n\\boxed{BC = FC \\cdot \\frac{N_{START}}{N_{END}}}\r\n\\end{equation}\r\n$$$</div>\r\n\r\n<h2>All sequential complexities are made equal</h2>\r\n\r\n<p>Thus, forward and backward complexities are trivially related by the formula above. </p>\r\n\r\n<p><em>Nevertheless</em>, we need to keep in mind that the forward and backward complexity estimates above stand only for a single traversal, starting either from one Node <span class=\"math\">$$A_i$$</span> or <span class=\"math\">$$D_i$$</span> respectively. Thus, if you plan to do these traversals for all the <span class=\"math\">$$N_A \\cdot N_D $$</span> possible pairs (or a randomly selected subset of those) you need to multiply these complexities by the number of starting nodes, <span class=\"math\">$$N_A$$</span> or <span class=\"math\">$$N_D$$</span> in order to get a relevant time estimate. As a result of this computation and under these hypotheses, there is no difference between forward and backward traversal - which is reassuring.</p>\r\n\r\n<h2>Conclusion</h2>\r\n\r\n<ul><li>Forward and Backward Complexity estimates are trivially related through the formula above.</li><li>Time estimates should take into account the number of starting nodes, resulting in no difference between both directions of traversal.</li><li>About join complexity: I believe that the sequential complexity estimates discussed here are good proxies for the process of \"joint\" traversal, when two semi-traversals are joined in the middle in order to reduce computation time.</li></ul>",
      "body_md": "# A note on traversal complexity\r\n\r\n## Intro\r\n\r\nThe question here is to estimate the complexity of graph traversal when following a 'metapath': a specification for which kind of nodes we need to traverse, in which order. This complexity will be useful to get a proxy for computation time when create the features we currently use in _rephetio_ for prediction problems, the DWPC. We mainly look at sequential complexity, which is estimating complexity of traversal from one point to another, in only one direction, without joining two partial traversals in the middle. (even though the latter is almost always a better option)\r\n\r\nWe can reasonably make the hypothesis that sequential complexity is equal to the number of possible paths that must be traversed. You propose above, and I would agree, that we use something along the [mean-field approximation](https://en.wikipedia.org/wiki/Mean_field_theory) and consider that the average number of paths to be traversed when following a Metapath is equal to the multiplication of the average degrees (outward connections) of all node types along the metapath. Doing so, we intrinsically base our analysis on the number of edges in the networks, which gives us some simple, neat properties.\r\n\r\n## Formal definition\r\n\r\nWe consider as an example 4 types of Nodes in our Network, $$A, B, C, D$$, linked with symmetrical edges, and the Metapath joining them $$A-B-C-D$$. We consider these nodes in order, since the difference between 'forward' (starting from $$A$$) and 'backward' (starting from $$D$$) traversal matters - as defined by @dhimmel above. Each node type has respectively $$N_A, N_B, N_C, N_D$$ nodes, noted  $$A_i, B_j,$$... Each of these single nodes has a forward degree, noted $$a_i$$ for instance, and a backward degree, noted with a prime, eg $$b_i'$$. These degrees represent the number of edges that connect the node at stake ($$A_i$$ and $$B_i$$ in our examples) to the nodes of the next ($$B$$) or previous ($$A$$) types respectively.\r\n\r\n[remark: we use the actual types instead of a notation \"$$K$$\" for simplicity. The results that are written below for nodes of type $$A$$ or $$B$$ stand general]\r\n\r\n## Proof of the equivalence between forward and backward complexities\r\n\r\nThe average of node degree is written $$E[a_i]$$ such as:\r\n\r\n$$$\r\n\\begin{align}\r\nE_{i\\in1,...,N_A}[a_i]=\\frac{1}{N_A}\\sum_{i\\in1,...,N_A}{a_i}\r\n\\end{align}\r\n$$$\r\n\r\nThen, we have\r\n\r\n$$$\r\n\\begin{align}\r\nE_{i\\in1,...,N_A}[a_i] \\cdot N_A = N_{A-B} = E_{i\\in1,...,N_B}[b_i'] \\cdot N_B \r\n\\end{align}\r\n$$$\r\n\r\nwhere $$N_{A-B}$$ is the number of edges from the nodes $$A$$ to $$B$$. Follows, in short notation: \r\n\r\n$$$\r\n\\begin{align}\r\nE[a_i] = E[b_i'] \\cdot \\frac{N_B} {N_A} \r\n\\quad or \\quad\r\nE[b_i'] = E[a_i] \\cdot \\frac{N_A} {N_B} \r\n\\end{align}\r\n$$$\r\n\r\nThe estimation of 'forward complexity' outlined above, for discovering the paths that correspond to the metapath $$A-B-C-D$$ is written as:\r\n\r\n$$$\r\n\\begin{equation}\r\nFC_{A-B-C-D} = E[a_i] \\cdot E[b_i] \\cdot E[c_i]\r\n\\end{equation}\r\n$$$\r\n\r\nThe estimated backward complexity can be written as:\r\n\r\n$$$\r\n\\begin{align}\r\nBC_{A-B-C-D} = FC_{D-C-B-A} = E[b_i'] \\cdot E[c_i'] \\cdot E[d_i']\r\n\\end{align}\r\n$$$\r\n\r\nFollows:\r\n\r\n$$$\r\n\\begin{align}\r\nBC_{A-B-C-D} &= E[b_i'] \\cdot E[c_i'] \\cdot E[d_i']\\\\\r\n &= E[a_i]\\frac{N_A}{N_B} \\cdot E[b_i]\\frac{N_B}{N_C} \\cdot E[c_i]\\frac{N_C}{N_D}\\\\\r\n &= E[a_i] \\cdot E[b_i]\\ \\cdot E[c_i] \\cdot \\frac{N_A}{N_D}\\\\\r\nBC_{A-B-C-D} &= FC_{A-B-C-D} \\cdot \\frac{N_A}{N_D}\r\n\\end{align}\r\n$$$\r\n\r\nMore generally:\r\n\r\n$$$\r\n\\begin{equation}\r\n\\boxed{BC = FC \\cdot \\frac{N_{START}}{N_{END}}}\r\n\\end{equation}\r\n$$$\r\n\r\n## All sequential complexities are made equal\r\n\r\nThus, forward and backward complexities are trivially related by the formula above. \r\n\r\n*Nevertheless*, we need to keep in mind that the forward and backward complexity estimates above stand only for a single traversal, starting either from one Node $$A_i$$ or $$D_i$$ respectively. Thus, if you plan to do these traversals for all the $$N_A \\cdot N_D $$ possible pairs (or a randomly selected subset of those) you need to multiply these complexities by the number of starting nodes, $$N_A$$ or $$N_D$$ in order to get a relevant time estimate. As a result of this computation and under these hypotheses, there is no difference between forward and backward traversal - which is reassuring.\r\n\r\n## Conclusion\r\n\r\n+ Forward and Backward Complexity estimates are trivially related through the formula above.\r\n+ Time estimates should take into account the number of starting nodes, resulting in no difference between both directions of traversal.\r\n+ About join complexity: I believe that the sequential complexity estimates discussed here are good proxies for the process of \"joint\" traversal, when two semi-traversals are joined in the middle in order to reduce computation time.",
      "profile": 23,
      "published": "2016-03-23T17:19:48.764749Z",
      "thread": 187,
      "url": "/discussion/estimating-the-complexity-of-hetnet-traversal/187#3"
    },
    {
      "body_html": "<h1>The <a href=\"/u/alizee\" class=\"username\">@alizee</a> theorem of sequential complexity</h1>\r\n\r\n<p><a href=\"/u/alizee\" class=\"username\">@alizee</a>, fantastic proof! The takeaway for me is that if you're doing a sequential traversal from one source to one target, start from the end where there are a greater number of nodes. In situations where we are using sequential complexity to estimate runtime, we only have be consistent on reporting either forward or backward complexity.</p>",
      "body_md": "# The @alizee theorem of sequential complexity\r\n\r\n@alizee, fantastic proof! The takeaway for me is that if you're doing a sequential traversal from one source to one target, start from the end where there are a greater number of nodes. In situations where we are using sequential complexity to estimate runtime, we only have be consistent on reporting either forward or backward complexity.",
      "profile": 17,
      "published": "2016-03-31T00:13:13.028884Z",
      "thread": 187,
      "url": "/discussion/estimating-the-complexity-of-hetnet-traversal/187#4"
    },
    {
      "body_html": "<h1>A formula for estimating joint traversal complexity</h1>\r\n\r\n<p>Rather than perform traversals sequentially, both <a href=\"http://neo4j.com/docs/2.3.3/execution-plans-combining.html#query-plan-node-hash-join\" title=\"Node Hash Join in Neo4j\">Neo4j</a> and <a href=\"https://github.com/dhimmel/hetio/blob/2c135fcd32616d6979972105ba60b003d65c98bd/hetio/pathtools.py#L96\" title=\"hetio.pathtools.paths_between()\">hetio</a> support joint traversal. In joint traversal, paths are expanded upon from both endpoints until meeting at an interior node, where they are joined.</p>\r\n\r\n<p>Using <a href=\"/u/alizee\" class=\"username\">@alizee</a>'s notation <a href=\"#3\">above</a>, we'll consider metapath <span class=\"math\">$$A-B-C-D$$</span>. If we adopt a join index of 2 (joining on <em>C</em>), we compute joint complexity (<em>JC</em>) as:</p>\r\n\r\n<div class=\"math\">$$$\r\nJC_{A-B-C_{join}-D} = \\log _{10} (E[a_i] \\cdot E[b_i] + E[d_i'])\r\n$$$</div>\r\n\r\n<p>Note that <a href=\"/u/alizee\" class=\"username\">@alizee</a> didn't include the log, but for consistency with our reported complexity values, I'm including it. This proposed formula for complexity assumes that joining the traversals is a free operation. In Neo4j a <a href=\"(http://neo4j.com/docs/2.3.3/execution-plans-combining.html#query-plan-node-hash-join\" title=\"Node Hash Join in Neo4j\"><code>NodeHashJoin</code></a> doesn't require any dbhits, but could still add to runtime.</p>\r\n\r\n<p>When the join index is 0, the joint traversal becomes equivalent to backward traversal. When the join index is the target node, the joint traversal becomes equivalent to forward traversal.</p>",
      "body_md": "# A formula for estimating joint traversal complexity\r\n\r\nRather than perform traversals sequentially, both [Neo4j](http://neo4j.com/docs/2.3.3/execution-plans-combining.html#query-plan-node-hash-join \"Node Hash Join in Neo4j\") and [hetio](https://github.com/dhimmel/hetio/blob/2c135fcd32616d6979972105ba60b003d65c98bd/hetio/pathtools.py#L96 \"hetio.pathtools.paths_between()\") support joint traversal. In joint traversal, paths are expanded upon from both endpoints until meeting at an interior node, where they are joined.\r\n\r\nUsing @alizee's notation [above](#3), we'll consider metapath $$A-B-C-D$$. If we adopt a join index of 2 (joining on _C_), we compute joint complexity (_JC_) as:\r\n\r\n$$$\r\nJC_{A-B-C_{join}-D} = \\log _{10} (E[a_i] \\cdot E[b_i] + E[d_i'])\r\n$$$\r\n\r\nNote that @alizee didn't include the log, but for consistency with our reported complexity values, I'm including it. This proposed formula for complexity assumes that joining the traversals is a free operation. In Neo4j a [`NodeHashJoin`]((http://neo4j.com/docs/2.3.3/execution-plans-combining.html#query-plan-node-hash-join \"Node Hash Join in Neo4j\") doesn't require any dbhits, but could still add to runtime.\r\n\r\nWhen the join index is 0, the joint traversal becomes equivalent to backward traversal. When the join index is the target node, the joint traversal becomes equivalent to forward traversal.",
      "profile": 17,
      "published": "2016-03-31T00:37:47.170455Z",
      "thread": 187,
      "url": "/discussion/estimating-the-complexity-of-hetnet-traversal/187#5"
    },
    {
      "body_html": "<h1>Choosing the best join index for Neo4j queries</h1>\r\n\r\n<p>If you don't give Neo4j 2.3.2 any hints where to join, the planner decides for itself. The planner is capable of sequential or join traversal. However, which query plan to use is determined prior to query execution (see <a href=\"https://github.com/neo4j/neo4j/issues/6030\" title=\"GitHub Issue on whether the join index could be adaptively determined during Neo4j query execution\">my issue</a>). You can tell Neo4j which node to join on using a join hint (<code>USING JOIN ON</code> in Cypher). If you specify a terminal node to join on in 2.3.2, a sequential query will be executed, but Neo4j will decide for itself whether to do a forward or backward sequential join.</p>\r\n\r\n<p>Therefore, we wanted to see whether we could use our <a href=\"#5\">estimated join complexities</a> to optimize our Neo4j queries. Therefore we did a parameter sweep where we evaluated all possible join indexes when computing features (<em>DWPCs</em>) for 75 metapaths × 150 compound–disease pairs (<a href=\"https://github.com/dhimmel/learn/blob/92d89c9eaf54704968f7397fac38d3e7a2074ca5/optimize/join-index/sweep.ipynb\">notebook</a>). In total, we performed 66,600 queries.</p>\r\n\r\n<p>We consider three options for specifying the join index:</p>\r\n\r\n<ul><li><strong>midpoint index</strong> — the floor of the metapath length divided by two. Note if there were more diseases than compounds, then <a href=\"#3\">we expect</a> the ceiling would outperform the floor.</li><li><strong>optimal index</strong> — the least complex join index based on <a href=\"#5\">our formula</a> for estimating join complexity.</li><li><strong>no hint</strong> — where we let Neo4j choose the join index. The query planner will choose a join index, although we do not know which index was used.</li></ul>\r\n\r\n<p>For each metapath, we identified the average runtime of the three options (<a href=\"https://github.com/dhimmel/learn/blob/92d89c9eaf54704968f7397fac38d3e7a2074ca5/optimize/join-index/data/index-choice-by-metapath.tsv\">table</a>). While no option was universally fastest for all metapaths, on average the midpoint index was best (0.229 seconds), followed by the optimal index (0.339), and last no hint (0.656). In other words, specifying the midpoint as the join index cut runtime in third compared to not providing any join hint. Interestingly, our optimal complexity estimate performed ~50% worse than the midpoint overall.</p>\r\n\r\n<p>On an individual query level, the midpoint index was the fastest index for 40% of queries, while the optimal index was the fastest index for 29% of the queries. The average rank of the fastest join index according to our complexity estimate was 2.5.</p>\r\n\r\n<p>In conclusion, the optimal join index for a specific query is highly variable. For a given metapath, different compound–disease pairs will prefer different join indexes. However, since we plan to select a join index at the metapath level, the midpoint is the best option thus far.</p>",
      "body_md": "# Choosing the best join index for Neo4j queries\r\n\r\nIf you don't give Neo4j 2.3.2 any hints where to join, the planner decides for itself. The planner is capable of sequential or join traversal. However, which query plan to use is determined prior to query execution (see [my issue](https://github.com/neo4j/neo4j/issues/6030 \"GitHub Issue on whether the join index could be adaptively determined during Neo4j query execution\")). You can tell Neo4j which node to join on using a join hint (`USING JOIN ON` in Cypher). If you specify a terminal node to join on in 2.3.2, a sequential query will be executed, but Neo4j will decide for itself whether to do a forward or backward sequential join.\r\n\r\nTherefore, we wanted to see whether we could use our [estimated join complexities](#5) to optimize our Neo4j queries. Therefore we did a parameter sweep where we evaluated all possible join indexes when computing features (_DWPCs_) for 75 metapaths × 150 compound–disease pairs ([notebook](https://github.com/dhimmel/learn/blob/92d89c9eaf54704968f7397fac38d3e7a2074ca5/optimize/join-index/sweep.ipynb)). In total, we performed 66,600 queries.\r\n\r\nWe consider three options for specifying the join index:\r\n\r\n+ **midpoint index** -- the floor of the metapath length divided by two. Note if there were more diseases than compounds, then [we expect](#3) the ceiling would outperform the floor.\r\n+ **optimal index** -- the least complex join index based on [our formula](#5) for estimating join complexity.\r\n+ **no hint** -- where we let Neo4j choose the join index. The query planner will choose a join index, although we do not know which index was used.\r\n\r\nFor each metapath, we identified the average runtime of the three options ([table](https://github.com/dhimmel/learn/blob/92d89c9eaf54704968f7397fac38d3e7a2074ca5/optimize/join-index/data/index-choice-by-metapath.tsv)). While no option was universally fastest for all metapaths, on average the midpoint index was best (0.229 seconds), followed by the optimal index (0.339), and last no hint (0.656). In other words, specifying the midpoint as the join index cut runtime in third compared to not providing any join hint. Interestingly, our optimal complexity estimate performed ~50% worse than the midpoint overall.\r\n\r\nOn an individual query level, the midpoint index was the fastest index for 40% of queries, while the optimal index was the fastest index for 29% of the queries. The average rank of the fastest join index according to our complexity estimate was 2.5.\r\n\r\nIn conclusion, the optimal join index for a specific query is highly variable. For a given metapath, different compound–disease pairs will prefer different join indexes. However, since we plan to select a join index at the metapath level, the midpoint is the best option thus far.",
      "profile": 17,
      "published": "2016-03-31T02:09:04.916809Z",
      "thread": 187,
      "url": "/discussion/estimating-the-complexity-of-hetnet-traversal/187#6"
    },
    {
      "body_html": "<h1>Complexity poorly estimates runtime</h1>\r\n\r\n<p>We recently performed 22,933,125 <em>DWPC</em> queries (3,775 compound–disease pairs × 1,215 metapaths × 5 hetnets). For these queries, I specified the optimal join index according to our <a href=\"#5\">formula for join complexity</a>. In the future, we will switch to the midpoint join index.</p>\r\n\r\n<p>Both sequential complexity and optimal index join complexity poorly predicted runtime (<a href=\"http://nbviewer.jupyter.org/github/dhimmel/learn/blob/edb3cb68cb62e37ac426e39dc9196cfb279db2e7/optimize/time.ipynb\">notebook</a>). Many of the outlier metapaths appear to contain a <em>Gene→regulates→Gene</em> edge. Since, the target genes for these edges will largely <a href=\"http://thinklab.com/discussion/computing-consensus-transcriptional-profiles-for-lincs-l1000-perturbations/43#7\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d43\">be restricted</a> to the 978 landmark L1000 genes, I suspect mean degree is a underestimating complexity. One idea I've had is to weight the average degree calculation by degree to address this issue. In other words, should we account for the fact that queries are more likely to traverse high degree nodes when estimating complexity?</p>",
      "body_md": "# Complexity poorly estimates runtime\r\n\r\nWe recently performed 22,933,125 _DWPC_ queries (3,775 compound--disease pairs × 1,215 metapaths × 5 hetnets). For these queries, I specified the optimal join index according to our [formula for join complexity](#5). In the future, we will switch to the midpoint join index.\r\n\r\nBoth sequential complexity and optimal index join complexity poorly predicted runtime ([notebook](http://nbviewer.jupyter.org/github/dhimmel/learn/blob/edb3cb68cb62e37ac426e39dc9196cfb279db2e7/optimize/time.ipynb)). Many of the outlier metapaths appear to contain a _Gene→regulates→Gene_ edge. Since, the target genes for these edges will largely [be restricted](http://thinklab.com/discussion/computing-consensus-transcriptional-profiles-for-lincs-l1000-perturbations/43#7) to the 978 landmark L1000 genes, I suspect mean degree is a underestimating complexity. One idea I've had is to weight the average degree calculation by degree to address this issue. In other words, should we account for the fact that queries are more likely to traverse high degree nodes when estimating complexity?",
      "profile": 17,
      "published": "2016-03-31T02:42:11.987669Z",
      "thread": 187,
      "url": "/discussion/estimating-the-complexity-of-hetnet-traversal/187#7"
    },
    {
      "body_html": "<h1>Permission to reuse ADEPTUS</h1>\r\n\r\n<p>Now that our STARGEO <a href=\"http://thinklab.com/discussion/stargeo-expression-signatures-for-disease-using-crowdsourced-geo-annotation/96#10\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d96\">analysis is ready</a>, we're no longer including ADEPTUS in our hetnet. From STARGEO, we extracted differential expression signatures for 49 diseases, so integrating the signatures for the 3 diseases from ADEPTUS no longer made sense.</p>\r\n\r\n<p>However, on August 29, 2015 I emailed the authors requesting permission to reuse the ADEPTUS data as I did not see a license on their <a href=\"http://acgt.cs.tau.ac.il/adeptus/download.html\">website</a>. This was part of a <a href=\"http://thinklab.com/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#14\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d107\">broader effort</a> to try to comply with the copyrights and licenses of our sources.</p>\r\n\r\n<p>Today, David Amar responded stating that we have permission to use the datasets in question. Specifically, he noted that the publication itself <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkv810\" class=\"citation\" data-key=\"10.1093/nar/gkv810\">1</a>]</span> grants permission to use any of their results.</p>",
      "body_md": "# Permission to reuse ADEPTUS\r\n\r\nNow that our STARGEO [analysis is ready](http://thinklab.com/discussion/stargeo-expression-signatures-for-disease-using-crowdsourced-geo-annotation/96#10), we're no longer including ADEPTUS in our hetnet. From STARGEO, we extracted differential expression signatures for 49 diseases, so integrating the signatures for the 3 diseases from ADEPTUS no longer made sense.\r\n\r\nHowever, on August 29, 2015 I emailed the authors requesting permission to reuse the ADEPTUS data as I did not see a license on their [website](http://acgt.cs.tau.ac.il/adeptus/download.html). This was part of a [broader effort](http://thinklab.com/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#14) to try to comply with the copyrights and licenses of our sources.\r\n\r\nToday, David Amar responded stating that we have permission to use the datasets in question. Specifically, he noted that the publication itself [@10.1093/nar/gkv810] grants permission to use any of their results.",
      "profile": 17,
      "published": "2016-03-31T04:55:38.403207Z",
      "thread": 101,
      "url": "/discussion/the-adeptus-resource-for-expression-signatures-of-disease/101#2"
    },
    {
      "body_html": "<p>Our approach for hetnet edge prediction models the relationship between two nodes by extracting degree-weighted path counts (<em>DWPCs</em>) <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1004259\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1004259\">1</a>]</span>. We use <em>DWPCs</em>, each corresponding to a different metapath, as the main features for a logistic regression classifier. Here we will investigate whether <em>DWPCs</em> should be transformed prior to being used as predictors.</p>\r\n\r\n<p>Below, we show the distribution of <em>DWPCs</em> for randomly selected metapaths, stratified by percent of non-zero values (<a href=\"https://github.com/dhimmel/learn/blob/becacbb47bed3346478a4c05beade44c165a22bd/all-features/transform.ipynb\">notebook</a>). We look at three metapaths for each non-zero quintile. These distributions are calculated from all positives (<em>Compound–treats–Disease</em> pairs) but only a small subset of negatives (4 times the # of positives). Since positives tend to be more connected than negatives, we expect the distribution for all compound–disease pairs to be even sparser. </p>\r\n\r\n<p><img src=\"https://github.com/dhimmel/learn/raw/becacbb47bed3346478a4c05beade44c165a22bd/all-features/media/DWPC-distribution.png\" alt=\"Raw DWPC distributions\" title=\"Facet strips note the non-zero quintile, metapath abbreviation, and non-zero percentage\"></p>\r\n\r\n<p>Note that the y-axis (histograms counts) is heavily transformed. The <em>DWPC</em> distribution is zero-inflated. The non-zero portion of the distribution has a long right tail, looking potentially lognormal.</p>\r\n\r\n<p>My concern is that these long-tailed distributions are suboptimal for linear modeling. For example, they lead to very few extremely high predictions at the expense of all other predictions. We <a href=\"http://het.io/disease-genes/browse/disease/?disease=DOID_12236\" title=\"Predictions for primary biliary cirrhosis\">observed this trend</a> when we used a <em>DWPC</em> approach for predictng gene–disease associations.</p>\r\n\r\n<p>This discussion will look into whether transforming <em>DWPCs</em> makes sense.</p>",
      "body_md": "Our approach for hetnet edge prediction models the relationship between two nodes by extracting degree-weighted path counts (_DWPCs_) [@10.1371/journal.pcbi.1004259]. We use _DWPCs_, each corresponding to a different metapath, as the main features for a logistic regression classifier. Here we will investigate whether _DWPCs_ should be transformed prior to being used as predictors.\r\n\r\nBelow, we show the distribution of _DWPCs_ for randomly selected metapaths, stratified by percent of non-zero values ([notebook](https://github.com/dhimmel/learn/blob/becacbb47bed3346478a4c05beade44c165a22bd/all-features/transform.ipynb)). We look at three metapaths for each non-zero quintile. These distributions are calculated from all positives (_Compound--treats--Disease_ pairs) but only a small subset of negatives (4 times the # of positives). Since positives tend to be more connected than negatives, we expect the distribution for all compound--disease pairs to be even sparser. \r\n\r\n![Raw DWPC distributions](https://github.com/dhimmel/learn/raw/becacbb47bed3346478a4c05beade44c165a22bd/all-features/media/DWPC-distribution.png \"Facet strips note the non-zero quintile, metapath abbreviation, and non-zero percentage\")\r\n\r\nNote that the y-axis (histograms counts) is heavily transformed. The _DWPC_ distribution is zero-inflated. The non-zero portion of the distribution has a long right tail, looking potentially lognormal.\r\n\r\nMy concern is that these long-tailed distributions are suboptimal for linear modeling. For example, they lead to very few extremely high predictions at the expense of all other predictions. We [observed this trend](http://het.io/disease-genes/browse/disease/?disease=DOID_12236 \"Predictions for primary biliary cirrhosis\") when we used a _DWPC_ approach for predictng gene--disease associations.\r\n\r\nThis discussion will look into whether transforming _DWPCs_ makes sense.",
      "profile": 17,
      "published": "2016-04-01T21:13:21.147431Z",
      "thread": 193,
      "url": "/discussion/transforming-dwpcs-for-hetnet-edge-prediction/193"
    },
    {
      "body_html": "<h1>Inverse hyperbolic sine transformation</h1>\r\n\r\n<p>One option is to use the inverse hyperbolic sine (IHS) transformation <span class=\"citation\">[<a href=\"/doi/10.2307/2288929\" class=\"citation\" data-key=\"10.2307/2288929\">1</a>, <a href=\"/doi/10.2307/2332539\" class=\"citation\" data-key=\"10.2307/2332539\">2</a>]</span>. The IHS transformation has <a href=\"http://worthwhile.typepad.com/worthwhile_canadian_initi/2011/07/a-rant-on-inverse-hyperbolic-sine-transformations.html\" title=\"A rant on inverse hyperbolic sine transformations\">nice properties</a>. Foremost, it's zero preserving and easy to implement. It has a single parameter, <em>θ</em> that controls to what extent values are pulled towards 0. Here's an R implementation:</p>\r\n\r\n<pre><code class=\"r\">ihs_transform &lt;- function(x, theta = 1) {\r\n  # Inverse Hyperbolic Sine transformation\r\n  return(asinh(theta * x) / theta)\r\n}</code></pre>\r\n\r\n<p>This implementation can be easily ported to Python by replacing <a href=\"https://stat.ethz.ch/R-manual/R-devel/library/base/html/Hyperbolic.html\"><code>asinh</code></a> with <a href=\"http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.arcsinh.html\"><code>numpy.arcsinh</code></a>.</p>\r\n\r\n<p>Now many applications, where the values extend into the natural number range, will do fine with the default <em>θ</em> = 1. However, since <em>DWPCs</em> tend to be very small numbers, the IHS transformation will have a negligible effect unless we increase <em>θ</em>.</p>\r\n\r\n<p>Others have discussed choosing <em>θ</em> to <a href=\"http://stats.stackexchange.com/a/79109/74908\">achieve normality</a> using <a href=\"http://stats.stackexchange.com/a/26373/74908\">maximum likelihood</a>. There is also a recent R package <a href=\"https://cran.r-project.org/web/packages/ihs/index.html\"><code>ihs</code></a> that could be useful.</p>\r\n\r\n<p>So the question becomes, what exactly do we want our transformation to do? Should we base the fitting of <em>θ</em> only on the non-zero <em>DWPCs</em> or on all <em>DWPCs</em>. Should we use an efficient and simple heuristic to fit <em>θ</em> or should we go with a more intense likelihood method?</p>",
      "body_md": "# Inverse hyperbolic sine transformation\r\n\r\nOne option is to use the inverse hyperbolic sine (IHS) transformation [@10.2307/2288929 @10.2307/2332539]. The IHS transformation has [nice properties](http://worthwhile.typepad.com/worthwhile_canadian_initi/2011/07/a-rant-on-inverse-hyperbolic-sine-transformations.html \"A rant on inverse hyperbolic sine transformations\"). Foremost, it's zero preserving and easy to implement. It has a single parameter, _θ_ that controls to what extent values are pulled towards 0. Here's an R implementation:\r\n\r\n```r\r\nihs_transform <- function(x, theta = 1) {\r\n  # Inverse Hyperbolic Sine transformation\r\n  return(asinh(theta * x) / theta)\r\n}\r\n```\r\n\r\nThis implementation can be easily ported to Python by replacing [`asinh`](https://stat.ethz.ch/R-manual/R-devel/library/base/html/Hyperbolic.html) with [`numpy.arcsinh`](http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.arcsinh.html).\r\n\r\nNow many applications, where the values extend into the natural number range, will do fine with the default _θ_ = 1. However, since _DWPCs_ tend to be very small numbers, the IHS transformation will have a negligible effect unless we increase _θ_.\r\n\r\nOthers have discussed choosing _θ_ to [achieve normality](http://stats.stackexchange.com/a/79109/74908) using [maximum likelihood](http://stats.stackexchange.com/a/26373/74908). There is also a recent R package [`ihs`](https://cran.r-project.org/web/packages/ihs/index.html) that could be useful.\r\n\r\nSo the question becomes, what exactly do we want our transformation to do? Should we base the fitting of _θ_ only on the non-zero _DWPCs_ or on all _DWPCs_. Should we use an efficient and simple heuristic to fit _θ_ or should we go with a more intense likelihood method?",
      "profile": 17,
      "published": "2016-04-01T21:35:20.632708Z",
      "thread": 193,
      "url": "/discussion/transforming-dwpcs-for-hetnet-edge-prediction/193#2"
    },
    {
      "body_html": "<p>Overall I'd say arcsin is a fine function. But to achieve your stated goals, I wonder why you didn't just use a log transformation? </p>\r\n\r\n<p>To achieve goal<br>1.zero preserving<br>2. easy to implement</p>\r\n\r\n<p>x -&gt; log(x+1)</p>\r\n\r\n<p>To acheive a third goal<br>3. has a \"theta\" value that could be use to \"pull\" values toward zero</p>\r\n\r\n<p>x-&gt; log(ax+1) / a</p>\r\n\r\n<p>The arsinh, as you know, is a linear derivative of the exponential function. And thus the inverse of this is a derivative of the log. So I guess it's unclear why you chose a derivative rather than the actual.</p>",
      "body_md": "Overall I'd say arcsin is a fine function. But to achieve your stated goals, I wonder why you didn't just use a log transformation? \r\n\r\nTo achieve goal\r\n1.zero preserving\r\n2. easy to implement\r\n\r\nx -> log(x+1)\r\n\r\nTo acheive a third goal\r\n3. has a \"theta\" value that could be use to \"pull\" values toward zero\r\n\r\nx-> log(ax+1) / a\r\n\r\nThe arsinh, as you know, is a linear derivative of the exponential function. And thus the inverse of this is a derivative of the log. So I guess it's unclear why you chose a derivative rather than the actual.",
      "profile": 188,
      "published": "2016-04-02T15:31:03.700466Z",
      "thread": 193,
      "url": "/discussion/transforming-dwpcs-for-hetnet-edge-prediction/193#3"
    },
    {
      "body_html": "<h1>Log versus IHS transformation</h1>\r\n\r\n<p><a href=\"/u/pouyakhankhanian\" class=\"username\">@pouyakhankhanian</a>, originally I stayed away from <code>log1p</code> because because it's <a href=\"https://www.wolframalpha.com/input/?i=log(x+%2B+1)+between+0+and+1\" title=\"log(x + 1) between 0 and 1 · WolframAlpha\">practically linear</a> across the range of our <em>DWPCs</em>. You bring up a good point regarding scaling <em>DWPCs</em> prior to transformation.</p>\r\n\r\n<p>Yesterday, <a href=\"/u/alizee\" class=\"username\">@alizee</a> and I looked into scaling <em>DWPCs</em> before transformation. This would eliminate the need to fit <em>θ</em>: we could use <em>θ</em> = 1 for both the log and IHS transforms. For example, if <code>x</code> is the vector of <em>DWPCs</em> for a single feature, we could transform using:</p>\r\n\r\n<pre><code class=\"r\"># Standard deviation scaling\r\nx_scale = sd(x)\r\n\r\n# Mean absolute deviation scaling\r\nx_scale = mad(x, center = mean(x))\r\n\r\n# Mean scaling\r\nx_scale = mean(x)\r\n\r\n# Scale\r\nx_scaled = x / x_scale\r\n\r\n# Inverse hyperbolic sine transform\r\nasinh(x_scaled)\r\n\r\n# Log transform\r\nlog1p(x_scaled)</code></pre>\r\n\r\n<p>I think we should choose between the log and IHS methods based on which gives better performance. Regarding choosing a derivative rather than the actual, I don't view one as inherently superior, especially since the IHS has better transformation properties than the log, such as handling negatives (although this isn't an issue here).</p>\r\n\r\n<p>I think we should also use performance to choose the scaling method, but with a preference for standard deviation scaling since <a href=\"/u/alizee\" class=\"username\">@alizee</a> thinks it the most versatile method.</p>",
      "body_md": "# Log versus IHS transformation\r\n\r\n@pouyakhankhanian, originally I stayed away from `log1p` because because it's [practically linear](https://www.wolframalpha.com/input/?i=log(x+%2B+1)+between+0+and+1 \"log(x + 1) between 0 and 1 · WolframAlpha\") across the range of our _DWPCs_. You bring up a good point regarding scaling _DWPCs_ prior to transformation.\r\n\r\nYesterday, @alizee and I looked into scaling _DWPCs_ before transformation. This would eliminate the need to fit _θ_: we could use _θ_ = 1 for both the log and IHS transforms. For example, if `x` is the vector of _DWPCs_ for a single feature, we could transform using:\r\n\r\n```r\r\n# Standard deviation scaling\r\nx_scale = sd(x)\r\n\r\n# Mean absolute deviation scaling\r\nx_scale = mad(x, center = mean(x))\r\n\r\n# Mean scaling\r\nx_scale = mean(x)\r\n\r\n# Scale\r\nx_scaled = x / x_scale\r\n\r\n# Inverse hyperbolic sine transform\r\nasinh(x_scaled)\r\n\r\n# Log transform\r\nlog1p(x_scaled)\r\n```\r\n\r\nI think we should choose between the log and IHS methods based on which gives better performance. Regarding choosing a derivative rather than the actual, I don't view one as inherently superior, especially since the IHS has better transformation properties than the log, such as handling negatives (although this isn't an issue here).\r\n\r\nI think we should also use performance to choose the scaling method, but with a preference for standard deviation scaling since @alizee thinks it the most versatile method.",
      "profile": 17,
      "published": "2016-04-02T19:05:58.165957Z",
      "thread": 193,
      "url": "/discussion/transforming-dwpcs-for-hetnet-edge-prediction/193#4"
    },
    {
      "body_html": "<h1>Replacing MSigDB with Pathway Commons</h1>\r\n\r\n<p>Due to <a href=\"http://thinklab.com/discussion/msigdb-licensing/108\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d108\">licensing issues with MSigDB</a>, we've removed MSigDB pathways and switched to Pathway Commons as <a href=\"/u/alexanderpico\" class=\"username\">@alexanderpico</a> <a href=\"#1\">initially suggested</a>. <a href=\"http://www.pathwaycommons.org/pc2/\" title=\"Pathway Commons 2\">Pathway Commons</a> aggregates pathway and binary interaction data from <a href=\"http://www.pathwaycommons.org/pc2/datasources\">many providers</a> <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkq1039\" class=\"citation\" data-key=\"10.1093/nar/gkq1039\">1</a>]</span>.</p>\r\n\r\n<p>Pathway Commons data is <a href=\"https://github.com/dhimmel/integrate/blob/e0d90e389c017dddacd98189d80ec1ffe3223c9b/licenses/custom/PathwayCommons.md\">freely available</a>, but the data is licensed under the terms of each contributing database. For example, Pathway Commons includes KEGG pathways, which have a <a href=\"https://github.com/dhimmel/integrate/blob/e0d90e389c017dddacd98189d80ec1ffe3223c9b/licenses/custom/KEGG.md\">problematic license</a>. Accordingly, we only include pathways from Pathway Commons resources that are openly licensed.</p>\r\n\r\n<p>Specifically, I identified only two appropriate resources from the 8 Pathway Commons resources that contribute pathways (see <a href=\"https://github.com/dhimmel/pathways/blob/1bd2c68853e38297d20f8f885419ff81fc0608a8/merge-resources.ipynb\">notebook</a> cell 7). These resources were <a href=\"http://www.reactome.org/\">Reactome</a> <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkv1351\" class=\"citation\" data-key=\"10.1093/nar/gkv1351\">2</a>]</span> and the <a href=\"http://pid.nci.nih.gov/\">Pathway Interaction Database</a> (PID) <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkn653\" class=\"citation\" data-key=\"10.1093/nar/gkn653\">3</a>]</span>. Reactome is licensed as CC BY, while I believe PID data is in the public domain since it was created by US Government employees. At least the PID publication states, \"All data in PID is freely available, without restriction on use. <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkn653\" class=\"citation\" data-key=\"10.1093/nar/gkn653\">3</a>]</span>\" Since Reactome and PID contributed the <a href=\"#6\">majority of MSigDB pathways</a>, I suspect that we didn't lose much information by abandoning MSigDB.</p>\r\n\r\n<p>Ultimately, our updated compilation of human gene sets (<code>dhimmel/pathways v2.0</code> <span class=\"citation\">[<a href=\"/doi/10.5281/zenodo.48810\" class=\"citation\" data-key=\"10.5281/zenodo.48810\">4</a>]</span>) contains 1,862 human pathways of which 1,341 are from Reactome, 298 are from WikiPathways, and 223 are from the PID.</p>",
      "body_md": "# Replacing MSigDB with Pathway Commons\r\n\r\nDue to [licensing issues with MSigDB](http://thinklab.com/discussion/msigdb-licensing/108), we've removed MSigDB pathways and switched to Pathway Commons as @alexanderpico [initially suggested](#1). [Pathway Commons](http://www.pathwaycommons.org/pc2/ \"Pathway Commons 2\") aggregates pathway and binary interaction data from [many providers](http://www.pathwaycommons.org/pc2/datasources) [@10.1093/nar/gkq1039].\r\n\r\nPathway Commons data is [freely available](https://github.com/dhimmel/integrate/blob/e0d90e389c017dddacd98189d80ec1ffe3223c9b/licenses/custom/PathwayCommons.md), but the data is licensed under the terms of each contributing database. For example, Pathway Commons includes KEGG pathways, which have a [problematic license](https://github.com/dhimmel/integrate/blob/e0d90e389c017dddacd98189d80ec1ffe3223c9b/licenses/custom/KEGG.md). Accordingly, we only include pathways from Pathway Commons resources that are openly licensed.\r\n\r\nSpecifically, I identified only two appropriate resources from the 8 Pathway Commons resources that contribute pathways (see [notebook](https://github.com/dhimmel/pathways/blob/1bd2c68853e38297d20f8f885419ff81fc0608a8/merge-resources.ipynb) cell 7). These resources were [Reactome](http://www.reactome.org/) [@10.1093/nar/gkv1351] and the [Pathway Interaction Database](http://pid.nci.nih.gov/) (PID) [@10.1093/nar/gkn653]. Reactome is licensed as CC BY, while I believe PID data is in the public domain since it was created by US Government employees. At least the PID publication states, \"All data in PID is freely available, without restriction on use. [@10.1093/nar/gkn653]\" Since Reactome and PID contributed the [majority of MSigDB pathways](#6), I suspect that we didn't lose much information by abandoning MSigDB.\r\n\r\nUltimately, our updated compilation of human gene sets (`dhimmel/pathways v2.0` [@10.5281/zenodo.48810]) contains 1,862 human pathways of which 1,341 are from Reactome, 298 are from WikiPathways, and 223 are from the PID.",
      "profile": 17,
      "published": "2016-04-02T22:16:02.980687Z",
      "thread": 72,
      "url": "/discussion/adding-pathway-resources-to-your-network/72#11"
    },
    {
      "body_html": "<h1>Removing MSigDB from the Rephetio project</h1>\r\n\r\n<p>I removed MSigDB from our project, since we haven't been able to resolve the licensing issues. It's been 186 days since we initially contacted the MSigDB team and 53 days since we were told that the IP/Licensing team had been notified and a meeting scheduled. Unfortunately, we can't wait any longer.</p>\r\n\r\n<p>There are a few distinctions that make MSigDB distinct from <a href=\"http://thinklab.com/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#14\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d107\">other resources</a> where permission is pending but we continue to include. MSigDB is the <a href=\"https://github.com/dhimmel/integrate/blob/daefe6e3e9a44b9fdc85cb79cee597927f119559/licenses/README.md\" title=\"Table of licenses for all sources\">only resource</a> with a license that explicitly forbids distribution. Additionally, the MSigDB website requires <a href=\"http://software.broadinstitute.org/gsea/register.jsp\">registration</a>, although accessing the database by URL bypasses registration.</p>\r\n\r\n<p>Registration makes the license into a legally binding agreement. Essentially, the registration acts as a contract, which can place additional restrictions beyond copyright. As an aside, I therefore find it misleading that the <a href=\"http://software.broadinstitute.org/gsea/msigdb\">website</a> states:</p>\r\n\r\n<blockquote><p>Registration is free. Its only purpose is to help us track usage for reports to our funding agencies.</p></blockquote>\r\n\r\n<p>Specifically, <a href=\"https://github.com/dhimmel/integrate/blob/daefe6e3e9a44b9fdc85cb79cee597927f119559/licenses/custom/MSigDB.asciidoc\" title=\"MSigDB License\">the license</a> contains several troubling components. First is a reporting requirement for modifications and bug fixes:</p>\r\n\r\n<blockquote><p>modifications and BUG FIXES shall be provided to MIT promptly upon their creation.</p></blockquote>\r\n\r\n<p>While this reporting requirement only applies to the program, which we don't use, a different reporting requirement applies to database:</p>\r\n\r\n<blockquote><p>As consideration for the licenses granted in this Agreement, LICENSEE agrees to provide … a written evaluation of the PROGRAM and the DATABASE, including a description of its functionality or problems and areas for further improvement in the PROGRAM or the DATABASE.</p></blockquote>\r\n\r\n<p>The license is very clear that distribution is prohibited. In fact, uploading the database to a private cloud service appears to violate the license:</p>\r\n\r\n<blockquote><p>2.2 No Sublicensing or Additional Rights. In no event shall LICENSEE sublicense or distribute, in whole or in part, the PROGRAM, modifications, BUG FIXES, or the DATABASE, without prior permission from MIT. LICENSEE agrees not to allow any non-employee of LICENSEE to access, view, or use the PROGRAM or the DATABASE, unless such person is an independent contractor performing services on behalf of LICENSEE. LICENSEE agrees not to put the PROGRAM or the DATABASE on a network, server, or other similar technology that may be accessed by any individual other than the LICENSEE.</p></blockquote>\r\n\r\n<p>As a result, using MSigDB as part of extensible open science project is not possible.</p>\r\n\r\n<h2>Remedial action</h2>\r\n\r\n<p>I removed MSigDB from our hetnet (<a href=\"https://github.com/dhimmel/integrate/commit/daefe6e3e9a44b9fdc85cb79cee597927f119559\">commit</a>). Our hetnet no longer contains perturbation gene sets, which were from the <code>C2:CGP</code> collection. I <a href=\"http://thinklab.com/discussion/adding-pathway-resources-to-your-network/72#11\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d72\">replaced</a> the MSigDB pathways (<code>C2:CP</code> collection) using <a href=\"http://www.pathwaycommons.org/\">Pathway Commons</a>. Both Pathway Commons and MSigDB include data from Reactome and the PID, but by going through Pathway Commons we were able to release those resources as CC BY and CC0.</p>\r\n\r\n<p>I deleted my GitHub repository, formerly <a href=\"https://github.com/dhimmel/msigdb\"><code>dhimmel/msigdb</code></a>, for converting the database into a single user-friendly TSV. Our <a href=\"http://het.io/disease-genes/downloads/\">website</a> for a previous project contains a hetnet including MSigDB 3.0, which I posted before being aware of the licensing issue. Since this hetnet is the foundation of our prior study <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1004259\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1004259\">1</a>]</span>, taking it offline would be problematic for reproducibility and destructive to science. Therefore, I am not taking down this dataset unless specifically requested.</p>\r\n\r\n<p>Finally, I removed the following two paragraphs from our project report draft. I'll let them serve as a eulogy:</p>\r\n\r\n<blockquote><p>Pathways were extracted by combining human pathways from <a href=\"http://www.wikipathways.org/\">WikiPathways</a> <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkv1024\" class=\"citation\" data-key=\"10.1093/nar/gkv1024\">2</a>, <a href=\"/doi/10.1371/journal.pbio.0060184\" class=\"citation\" data-key=\"10.1371/journal.pbio.0060184\">3</a>]</span> and MSigDB version 5.1 <span class=\"citation\">[<a href=\"/doi/10.1073/pnas.0506580102\" class=\"citation\" data-key=\"10.1073/pnas.0506580102\">4</a>, <a href=\"/doi/10.1093/bioinformatics/btr260\" class=\"citation\" data-key=\"10.1093/bioinformatics/btr260\">5</a>]</span>. Perturbations were extracted from MSigDB. Each perturbation corresponds to a differential expression experiment of a chemical or genetic perturbation.</p><p><em>Perturbation–regulates–Gene</em> edges are from MSigDB <span class=\"citation\">[<a href=\"/doi/10.1073/pnas.0506580102\" class=\"citation\" data-key=\"10.1073/pnas.0506580102\">4</a>, <a href=\"/doi/10.1093/bioinformatics/btr260\" class=\"citation\" data-key=\"10.1093/bioinformatics/btr260\">5</a>]</span>. These edges represent groups of genes that responded in the same direction to a chemical or genetic perturbation. Our previous project found this indiscriminate, automated, and high-throughput method produced gene sets that together were highly informative for predicting disease–gene associations <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1004743\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1004743\">6</a>]</span>.</p></blockquote>\r\n\r\n<h2>Reflections</h2>\r\n\r\n<p>One issue at play is the restrictive licensing of resources that MSigDB integrates. For example, KEGG <a href=\"https://github.com/dhimmel/integrate/blob/daefe6e3e9a44b9fdc85cb79cee597927f119559/licenses/custom/KEGG.md\">requires</a> academic services to obtain a <a href=\"http://www.bioinformatics.jp/docs/subscription_organizational.pdf\">subscription</a>, which stipulates that:</p>\r\n\r\n<blockquote><p>Your Product or Service must not allow Your users to obtain KEGG FTP Data, except in small quantities.</p></blockquote>\r\n\r\n<p>Hence, portions of the MSigDB dataset do have to be licensed to forbid redistribution. Nevertheless, MIT went beyond these upstream restrictions when writing the MSigDB license. First, much of the database could likely be released openly. Second, I find the reporting requirements extreme. It's possible that MIT had a financial motivation when writing the license, as it states:</p>\r\n\r\n<blockquote><p>LICENSEE agrees that neither the PROGRAM nor the DATABASE shall be used as the basis of a commercial software or hardware product</p></blockquote>\r\n\r\n<p>I think the MSigDB team deserves credit for making their comprehensive compilation of gene sets public. However, given the <a href=\"http://grantome.com/grant/NIH/R01-CA121941-10\" title=\"See Related projects on Grantome\">extent of public funding</a> this project has received, I question whether it's ethical for MIT to apply such a problematic license.</p>\r\n\r\n<p>Finally I'm not deflecting responsibility: I'm the one who included a resource whose license forbids redistribution. While scientists are often poorly informed on the legality of data reuse, I think it's important to take responsibility for educating yourself. Going forward I will address licensing issues before using a new dataset to avoid similar problems.</p>",
      "body_md": "# Removing MSigDB from the Rephetio project\r\n\r\nI removed MSigDB from our project, since we haven't been able to resolve the licensing issues. It's been 186 days since we initially contacted the MSigDB team and 53 days since we were told that the IP/Licensing team had been notified and a meeting scheduled. Unfortunately, we can't wait any longer.\r\n\r\nThere are a few distinctions that make MSigDB distinct from [other resources](http://thinklab.com/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#14) where permission is pending but we continue to include. MSigDB is the [only resource](https://github.com/dhimmel/integrate/blob/daefe6e3e9a44b9fdc85cb79cee597927f119559/licenses/README.md \"Table of licenses for all sources\") with a license that explicitly forbids distribution. Additionally, the MSigDB website requires [registration](http://software.broadinstitute.org/gsea/register.jsp), although accessing the database by URL bypasses registration.\r\n\r\nRegistration makes the license into a legally binding agreement. Essentially, the registration acts as a contract, which can place additional restrictions beyond copyright. As an aside, I therefore find it misleading that the [website](http://software.broadinstitute.org/gsea/msigdb) states:\r\n\r\n> Registration is free. Its only purpose is to help us track usage for reports to our funding agencies.\r\n\r\nSpecifically, [the license](https://github.com/dhimmel/integrate/blob/daefe6e3e9a44b9fdc85cb79cee597927f119559/licenses/custom/MSigDB.asciidoc \"MSigDB License\") contains several troubling components. First is a reporting requirement for modifications and bug fixes:\r\n\r\n> modifications and BUG FIXES shall be provided to MIT promptly upon their creation.\r\n\r\nWhile this reporting requirement only applies to the program, which we don't use, a different reporting requirement applies to database:\r\n\r\n> As consideration for the licenses granted in this Agreement, LICENSEE agrees to provide … a written evaluation of the PROGRAM and the DATABASE, including a description of its functionality or problems and areas for further improvement in the PROGRAM or the DATABASE.\r\n\r\nThe license is very clear that distribution is prohibited. In fact, uploading the database to a private cloud service appears to violate the license:\r\n\r\n> 2.2 No Sublicensing or Additional Rights. In no event shall LICENSEE sublicense or distribute, in whole or in part, the PROGRAM, modifications, BUG FIXES, or the DATABASE, without prior permission from MIT. LICENSEE agrees not to allow any non-employee of LICENSEE to access, view, or use the PROGRAM or the DATABASE, unless such person is an independent contractor performing services on behalf of LICENSEE. LICENSEE agrees not to put the PROGRAM or the DATABASE on a network, server, or other similar technology that may be accessed by any individual other than the LICENSEE.\r\n\r\nAs a result, using MSigDB as part of extensible open science project is not possible.\r\n\r\n## Remedial action\r\n\r\nI removed MSigDB from our hetnet ([commit](https://github.com/dhimmel/integrate/commit/daefe6e3e9a44b9fdc85cb79cee597927f119559)). Our hetnet no longer contains perturbation gene sets, which were from the `C2:CGP` collection. I [replaced](http://thinklab.com/discussion/adding-pathway-resources-to-your-network/72#11) the MSigDB pathways (`C2:CP` collection) using [Pathway Commons](http://www.pathwaycommons.org/). Both Pathway Commons and MSigDB include data from Reactome and the PID, but by going through Pathway Commons we were able to release those resources as CC BY and CC0.\r\n\r\nI deleted my GitHub repository, formerly [`dhimmel/msigdb`](https://github.com/dhimmel/msigdb), for converting the database into a single user-friendly TSV. Our [website](http://het.io/disease-genes/downloads/) for a previous project contains a hetnet including MSigDB 3.0, which I posted before being aware of the licensing issue. Since this hetnet is the foundation of our prior study [@10.1371/journal.pcbi.1004259], taking it offline would be problematic for reproducibility and destructive to science. Therefore, I am not taking down this dataset unless specifically requested.\r\n\r\nFinally, I removed the following two paragraphs from our project report draft. I'll let them serve as a eulogy:\r\n\r\n> Pathways were extracted by combining human pathways from [WikiPathways](http://www.wikipathways.org/) [@10.1093/nar/gkv1024 @10.1371/journal.pbio.0060184] and MSigDB version 5.1 [@10.1073/pnas.0506580102 @10.1093/bioinformatics/btr260]. Perturbations were extracted from MSigDB. Each perturbation corresponds to a differential expression experiment of a chemical or genetic perturbation.\r\n\r\n> _Perturbation–regulates–Gene_ edges are from MSigDB [@10.1073/pnas.0506580102 @10.1093/bioinformatics/btr260]. These edges represent groups of genes that responded in the same direction to a chemical or genetic perturbation. Our previous project found this indiscriminate, automated, and high-throughput method produced gene sets that together were highly informative for predicting disease–gene associations [@10.1371/journal.pcbi.1004743].\r\n\r\n## Reflections\r\n\r\nOne issue at play is the restrictive licensing of resources that MSigDB integrates. For example, KEGG [requires](https://github.com/dhimmel/integrate/blob/daefe6e3e9a44b9fdc85cb79cee597927f119559/licenses/custom/KEGG.md) academic services to obtain a [subscription](http://www.bioinformatics.jp/docs/subscription_organizational.pdf), which stipulates that:\r\n\r\n> Your Product or Service must not allow Your users to obtain KEGG FTP Data, except in small quantities.\r\n\r\nHence, portions of the MSigDB dataset do have to be licensed to forbid redistribution. Nevertheless, MIT went beyond these upstream restrictions when writing the MSigDB license. First, much of the database could likely be released openly. Second, I find the reporting requirements extreme. It's possible that MIT had a financial motivation when writing the license, as it states:\r\n\r\n> LICENSEE agrees that neither the PROGRAM nor the DATABASE shall be used as the basis of a commercial software or hardware product\r\n\r\nI think the MSigDB team deserves credit for making their comprehensive compilation of gene sets public. However, given the [extent of public funding](http://grantome.com/grant/NIH/R01-CA121941-10 \"See Related projects on Grantome\") this project has received, I question whether it's ethical for MIT to apply such a problematic license.\r\n\r\nFinally I'm not deflecting responsibility: I'm the one who included a resource whose license forbids redistribution. While scientists are often poorly informed on the legality of data reuse, I think it's important to take responsibility for educating yourself. Going forward I will address licensing issues before using a new dataset to avoid similar problems.",
      "profile": 17,
      "published": "2016-04-03T04:08:21.092646Z",
      "thread": 108,
      "url": "/discussion/msigdb-licensing/108#3"
    },
    {
      "body_html": "<h2>A remark on asinh, log1p and derivatives</h2>\r\n\r\n<p>[The aim of this post is mainly to respond to <a href=\"/u/pouyakhankhanian\" class=\"username\">@pouyakhankhanian</a> regarding his post above] </p>\r\n\r\n<p><a href=\"/u/pouyakhankhanian\" class=\"username\">@pouyakhankhanian</a>: I find your last remark interesting, but unfortunately not quite accurate as currently stated. </p>\r\n\r\n<p>(i) <span class=\"math\">$$\\sinh$$</span> is not a linear derivative of the exponential function, but a simple linear combination of it: <span class=\"math\">$$ \\sinh = \\frac{1}{2}(e^x - e^{-x}) $$</span>.</p>\r\n\r\n<p>(ii) Further, your second sentence has no grounding: the inverse of a derivative of a function <span class=\"math\">$$f$$</span> is definitely not the derivative of the inverse of the same <span class=\"math\">$$f$$</span>... So even if (i) was right, the result wouldn't hold. </p>\r\n\r\n<p>To see more clearly that <span class=\"math\">$$log1p$$</span> and <span class=\"math\">$$asinh$$</span> are not derivatives of each other, we can look at their analytical formulae:</p>\r\n\r\n<div class=\"math\">$$$\r\n\\begin{align}\r\n\\text{log1p}(x) &amp;= \\log(1 + x)\\\\\r\n\\sinh^{-1}(x) &amp;= \\log(x + \\sqrt{1 + x^2})\r\n\\end{align}\r\n$$$</div>\r\n\r\n<p>On a related note, we see here that both functions are very similar. This similarity is even clearer when looking at their derivatives:</p>\r\n\r\n<div class=\"math\">$$$\r\n\\begin{align}\r\n\\text{log1p}'(x) &amp;= \\frac{1}{1 + x}\\\\\r\n(\\sinh^{-1})'(x) &amp;= \\frac{1}{\\sqrt{1 + x^2}}\r\n\\end{align}\r\n$$$</div>\r\n\r\n<p>When <a href=\"https://www.wolframalpha.com/input/?i=Plot%5B%7B1%2FSqrt%5B1+%2B+x%5E2%5D,+(1+%2B+x)%5E(-1)%7D,+%7Bx,+0,+10%7D%5D\">plotted</a>, the trend is clear. The derivative of <span class=\"math\">$$\\text{asinh}$$</span> takes some time to get into the form of <span class=\"math\">$$ \\frac{1}{x} $$</span>, thus expanding the beginning of the range (for <span class=\"math\">$$x \\in 0..2$$</span>) more than its counterpart.</p>",
      "body_md": "## A remark on asinh, log1p and derivatives\r\n\r\n[The aim of this post is mainly to respond to @pouyakhankhanian regarding his post above] \r\n\r\n@pouyakhankhanian: I find your last remark interesting, but unfortunately not quite accurate as currently stated. \r\n\r\n(i) $$\\sinh$$ is not a linear derivative of the exponential function, but a simple linear combination of it: $$ \\sinh = \\frac{1}{2}(e^x - e^{-x}) $$.\r\n\r\n(ii) Further, your second sentence has no grounding: the inverse of a derivative of a function $$f$$ is definitely not the derivative of the inverse of the same $$f$$... So even if (i) was right, the result wouldn't hold. \r\n\r\nTo see more clearly that $$log1p$$ and $$asinh$$ are not derivatives of each other, we can look at their analytical formulae:\r\n\r\n$$$\r\n\\begin{align}\r\n\\text{log1p}(x) &= \\log(1 + x)\\\\\r\n\\sinh^{-1}(x) &= \\log(x + \\sqrt{1 + x^2})\r\n\\end{align}\r\n$$$\r\n\r\nOn a related note, we see here that both functions are very similar. This similarity is even clearer when looking at their derivatives:\r\n\r\n$$$\r\n\\begin{align}\r\n\\text{log1p}'(x) &= \\frac{1}{1 + x}\\\\\r\n(\\sinh^{-1})'(x) &= \\frac{1}{\\sqrt{1 + x^2}}\r\n\\end{align}\r\n$$$\r\n\r\nWhen [plotted](https://www.wolframalpha.com/input/?i=Plot%5B%7B1%2FSqrt%5B1+%2B+x%5E2%5D,+(1+%2B+x)%5E(-1)%7D,+%7Bx,+0,+10%7D%5D), the trend is clear. The derivative of $$\\text{asinh}$$ takes some time to get into the form of $$ \\frac{1}{x} $$, thus expanding the beginning of the range (for $$x \\in 0..2$$) more than its counterpart.",
      "profile": 23,
      "published": "2016-04-04T17:41:01.777748Z",
      "thread": 193,
      "url": "/discussion/transforming-dwpcs-for-hetnet-edge-prediction/193#5"
    },
    {
      "body_html": "<h1>The Prior Problem</h1>\r\n\r\n<p>The last step of our project <em>rephetio</em> is to use our heterogeneous Network <em>hetionet</em> † to create features that can predict missing edges in the Network. As the goal is to repurpose existing drugs to existing diseases, the outcome metaedge we are predicting is <em>treatments</em> (read 'treats' as a verb in our nomenclature). We tackle here a major issue that we call 'self-testing', which makes our Machine Learning approach non-conventional.</p>\r\n\r\n<h2>Self-testing</h2>\r\n\r\n<p>For computational and semantical reasons, we currently fit our machine-learned models and report evaluated performance based on the <strong>training</strong> error. Indeed, our Network architecture, implementation and data currently prevents us from creating separate training and testing sets, e.g. by selectively hiding predictor and predicted edges based on time of apparition. As a result (i) predictive models are trained with features that have intrinsic knowledge of the outcome, exposing us to overfitting of these features (ii) classical performance evaluation measures (like AUROC) lose relevance and applicability.</p>\r\n\r\n<h2>Source-Target Degrees as Features</h2>\r\n\r\n<p>We included source-target degrees as features of our edge prediction problem. The goal of this addition, as explained elsewhere, is to avoid misleading selection of DWPC features that are proxies for this basic topological information. </p>\r\n\r\n<p>In this discussion, we want to focus on treatment degrees. They can be noted <span class=\"math\">$$n_{DtC}(Ci)$$</span> and <span class=\"math\">$$n_{CtD}(D_j)$$</span>, and represent, respectively, the number of diseases treated by a Compound <span class=\"math\">$$C_i$$</span> (source Node) and the number of compounds that treat a Disease <span class=\"math\">$$D_j$$</span> (target Node). We believe that these degrees, as features, crystallizes the problem of self-testing in our edge-prediction problem.</p>\r\n\r\n<h2>Direct contamination</h2>\r\n\r\n<p>These degrees characterize with precision the (bipartite) subnetwork <span class=\"math\">$$\\mathcal{N}_O$$</span> that have only edges from the 'treatment' metaedge and source and target nodes (from the Compound and Disease metanodes). <span class=\"math\">$$\\mathcal{N}_O$$</span> contains no information apart from the actual outcomes we want to predict, and yet it is sufficient to compute all the treatment degrees <span class=\"math\">$$n_{DtC}$$</span> and <span class=\"math\">$$n_{CtD}$$</span>. As a result, outcome observations directly 'contaminates' our features on which the models are trained and evaluated. Because these degrees are so unequally distributed (they follow a <a href=\"https://en.wikipedia.org/wiki/Power_law\">power law</a>), and since these degree features are directly linked to the probability of existence of a treatment, fitting a model with only these two degree features gives misleeading high performance numbers.</p>\r\n\r\n<p>A first pass gave a AUROC above 0.97.</p>\r\n\r\n<h2>Prior knowledge</h2>\r\n\r\n<p>The information encapsulated in these two degree features are solely characterizing the treatment edges, <em>as we use them for fitting and evaluation</em>. We are not tackling here the more general problem of knowledge bias, but the specific issue arising as a result of self-testing: actual outcome observations directly contaminates our features, inducing prior knowledge about their value (positive or negative) before any additional information being integrated in our network.</p>\r\n\r\n<hr>\r\n\r\n<p>† This is the first public use of the name '<em>hetionet</em>' for our heterogeneous network that powers — in particular — the project <em>rephetio</em>. We call it a 'soft disclosure' ;-)</p>",
      "body_md": "# The Prior Problem\r\n\r\nThe last step of our project _rephetio_ is to use our heterogeneous Network _hetionet_ † to create features that can predict missing edges in the Network. As the goal is to repurpose existing drugs to existing diseases, the outcome metaedge we are predicting is *treatments* (read 'treats' as a verb in our nomenclature). We tackle here a major issue that we call 'self-testing', which makes our Machine Learning approach non-conventional.\r\n\r\n## Self-testing\r\n\r\nFor computational and semantical reasons, we currently fit our machine-learned models and report evaluated performance based on the **training** error. Indeed, our Network architecture, implementation and data currently prevents us from creating separate training and testing sets, e.g. by selectively hiding predictor and predicted edges based on time of apparition. As a result (i) predictive models are trained with features that have intrinsic knowledge of the outcome, exposing us to overfitting of these features (ii) classical performance evaluation measures (like AUROC) lose relevance and applicability.\r\n\r\n## Source-Target Degrees as Features\r\n\r\nWe included source-target degrees as features of our edge prediction problem. The goal of this addition, as explained elsewhere, is to avoid misleading selection of DWPC features that are proxies for this basic topological information. \r\n\r\nIn this discussion, we want to focus on treatment degrees. They can be noted $$n_{DtC}(Ci)$$ and $$n_{CtD}(D_j)$$, and represent, respectively, the number of diseases treated by a Compound $$C_i$$ (source Node) and the number of compounds that treat a Disease $$D_j$$ (target Node). We believe that these degrees, as features, crystallizes the problem of self-testing in our edge-prediction problem.\r\n\r\n## Direct contamination\r\n\r\nThese degrees characterize with precision the (bipartite) subnetwork $$\\mathcal{N}_O$$ that have only edges from the 'treatment' metaedge and source and target nodes (from the Compound and Disease metanodes). $$\\mathcal{N}_O$$ contains no information apart from the actual outcomes we want to predict, and yet it is sufficient to compute all the treatment degrees $$n_{DtC}$$ and $$n_{CtD}$$. As a result, outcome observations directly 'contaminates' our features on which the models are trained and evaluated. Because these degrees are so unequally distributed (they follow a [power law](https://en.wikipedia.org/wiki/Power_law)), and since these degree features are directly linked to the probability of existence of a treatment, fitting a model with only these two degree features gives misleeading high performance numbers.\r\n\r\nA first pass gave a AUROC above 0.97.\r\n\r\n## Prior knowledge\r\n\r\nThe information encapsulated in these two degree features are solely characterizing the treatment edges, _as we use them for fitting and evaluation_. We are not tackling here the more general problem of knowledge bias, but the specific issue arising as a result of self-testing: actual outcome observations directly contaminates our features, inducing prior knowledge about their value (positive or negative) before any additional information being integrated in our network.\r\n\r\n---\r\n† This is the first public use of the name '_hetionet_' for our heterogeneous network that powers -- in particular -- the project _rephetio_. We call it a 'soft disclosure' ;-)",
      "profile": 23,
      "published": "2016-04-05T22:49:42.917862Z",
      "thread": 194,
      "url": "/discussion/network-edge-prediction-how-to-deal-with-self-testing/194"
    },
    {
      "body_html": "<h1>Characterizing the Prior</h1>\r\n\r\n<p>Removal of degree features would only obfuscate the problem at hand, since this trivial information about outcomes could be picked up in subtler ways by DWPC features. On the contrary, we found that embracing the concept of prior knowledge presented above could solve both problems of fitting and evaluating our models. We will strive here for more accurate characterization of this prior knowledge.</p>\r\n\r\n<h2>Computing prior probabilities</h2>\r\n\r\n<p>Solely from the two treatment degree features, we can directly compute a probability of a given Compound-Disease couple to be linked by a treatment edge. </p>\r\n\r\n<p>Let us consider a couple <span class=\"math\">$$C_i\\text{-}D_j$$</span> where the treatment degree from the compound <span class=\"math\">$$C_i$$</span> is known, and the Disease <span class=\"math\">$$D_j$$</span> is drawn at random. The probability of this couple to be a treatment is equal to the number of disease <span class=\"math\">$$C_i$$</span> is connected to divided by the total number of diseases it could connect to. This can be written as:</p>\r\n\r\n<div class=\"math\">$$$\r\n\\begin{align}\r\np_t(C_i\\text{-}D) &amp; = \\frac{n_{CtD}(C_i)}{N_{C_i\\text{-}D}} \\\\\r\np_t(C_i\\text{-}D)  &amp; = \\frac{n_{CtD}(C_i)}{N_D}\r\n\\end{align}\r\n$$$</div>\r\n\r\n<p>... where <span class=\"math\">$$N_{C_i\\text{-}D}$$</span> is the total number of possible edges starting from <span class=\"math\">$$C_i$$</span>, equal to <span class=\"math\">$$N_D$$</span> the number of Diseases in the network.</p>\r\n\r\n<p>A similar formula can be derived for the Disease degrees:</p>\r\n\r\n<div class=\"math\">$$$\r\n\\begin{align}\r\np_t(D_j\\text{-}C)  &amp; = \\frac{n_{DtC}(D_j)}{N_C}\r\n\\end{align}\r\n$$$</div>\r\n\r\n<p>And both of these prior probabilities can be combined to get the prior probability of a given treatment edge knowing both degrees:</p>\r\n\r\n<div class=\"math\">$$$\r\n\\begin{equation}\r\n\\boxed{p_{ij} = p_t(C_i\\text{-}D_j) = \\frac{n_{DtC}(C_i) \\cdot n_{DtC}(D_j)}{N_D \\cdot N_C}}\r\n\\end{equation}\r\n$$$</div>\r\n\r\n<p>This probability fully describes the prior knowledge about the outcome that the treatment degrees hold.</p>\r\n\r\n<h2>A Null model</h2>\r\n\r\n<p>A null model can be specified, where the outcome for any source-target couple when predicting presence of a treatment edge is equal to the prior probability <span class=\"math\">$$p_{ij}$$</span> above. This model is denoted <span class=\"math\">$$\\mathcal{M}_{prior}$$</span>.</p>\r\n\r\n<p><span class=\"math\">$$\\mathcal{M}_{prior}$$</span>, when tested on all observations, is expected to have a very high AUROC, because it successfully stratifies the population of potential edges into tiers that have increasing and radically different probabilities of being a true link. For instance, it successfully discriminates the huge majority (85%) of couples that have no chances of being a treatment edge because either the Compound and/or the Disease has a treatment degree equal to zero. </p>\r\n\r\n<hr>\r\n\r\n<h2>AUROC of the null model</h2>\r\n\r\n<p>Just considering this point in the ROC curve, with <span class=\"math\">$$P_0 = P\\left(p_{ij} = 0\\right)$$</span> the proportion of edges that have a priori probability of zero, we can compute (for the fun) a lower bound for the AUROC that will illustrate why the expected performance is high. Every couple with prior probability of zero is ranked lower than any other couple, and each of these couples is actually a True Negative (TN) by design. Since we know that the model will be better than random for the remaining of the range of specificity, the lower bound for the AUROC of this null model is given by a simple linear interpolation based on one point we know in ROC space. </p>\r\n\r\n<p>This point corresponds to classifying only these \"absolute negatives\" that have <span class=\"math\">$$p_{ij} = 0$$</span>, as negatives. We get a sensitivity <span class=\"math\">$$sens = 1$$</span>, since all the real positives are predicted to be positives; and a specificity of </p>\r\n\r\n<div class=\"math\">$$$\r\n\\begin{align}\r\nspe &amp;= \\frac{P_0}{P_0 + (1 - P_0) \\cdot (1 - pre)}\\\\\r\n&amp;= \\frac{P_0}{1 - pre \\cdot (1 - P_0)}\\\\\r\nspe &amp;\\simeq  P_0\r\n\\end{align}\r\n$$$</div>\r\n\r\n<p>with the prevalence <span class=\"math\">$$pre$$</span> being negligible (of the order of <span class=\"math\">$$10^{-4}$$</span>).</p>\r\n\r\n<p>The linear interpolation gives us:</p>\r\n\r\n<div class=\"math\">$$$ \r\n\\begin{align}\r\nauroc_{min}\\left(\\mathcal{M}_{prior}\\right) &amp;= 1 - \\frac{1 - P_0}{2}\r\n\\end{align}\r\n$$$</div>\r\n\r\n<p>With <span class=\"math\">$$P_0 = 0.85 $$</span>, we get <span class=\"math\">$$ \\boxed{auroc_{min}\\left(\\mathcal{M}_{prior}\\right) = 92.5 \\%} $$</span> as a lower bound for the null model performance.</p>\r\n\r\n<hr>\r\n\r\n<p>The measured AUROC of the this null model, on all observations is equal to:</p>\r\n\r\n<div class=\"math\">$$$ auroc\\left(\\mathcal{M}_{prior}\\right)  = 97.9 \\%$$$</div>",
      "body_md": "# Characterizing the Prior\r\n\r\nRemoval of degree features would only obfuscate the problem at hand, since this trivial information about outcomes could be picked up in subtler ways by DWPC features. On the contrary, we found that embracing the concept of prior knowledge presented above could solve both problems of fitting and evaluating our models. We will strive here for more accurate characterization of this prior knowledge.\r\n\r\n## Computing prior probabilities\r\n\r\nSolely from the two treatment degree features, we can directly compute a probability of a given Compound-Disease couple to be linked by a treatment edge. \r\n\r\nLet us consider a couple $$C_i\\text{-}D_j$$ where the treatment degree from the compound $$C_i$$ is known, and the Disease $$D_j$$ is drawn at random. The probability of this couple to be a treatment is equal to the number of disease $$C_i$$ is connected to divided by the total number of diseases it could connect to. This can be written as:\r\n$$$\r\n\\begin{align}\r\np_t(C_i\\text{-}D) & = \\frac{n_{CtD}(C_i)}{N_{C_i\\text{-}D}} \\\\\r\np_t(C_i\\text{-}D)  & = \\frac{n_{CtD}(C_i)}{N_D}\r\n\\end{align}\r\n$$$\r\n... where $$N_{C_i\\text{-}D}$$ is the total number of possible edges starting from $$C_i$$, equal to $$N_D$$ the number of Diseases in the network.\r\n\r\nA similar formula can be derived for the Disease degrees:\r\n$$$\r\n\\begin{align}\r\np_t(D_j\\text{-}C)  & = \\frac{n_{DtC}(D_j)}{N_C}\r\n\\end{align}\r\n$$$\r\n\r\nAnd both of these prior probabilities can be combined to get the prior probability of a given treatment edge knowing both degrees:\r\n$$$\r\n\\begin{equation}\r\n\\boxed{p_{ij} = p_t(C_i\\text{-}D_j) = \\frac{n_{DtC}(C_i) \\cdot n_{DtC}(D_j)}{N_D \\cdot N_C}}\r\n\\end{equation}\r\n$$$\r\n\r\nThis probability fully describes the prior knowledge about the outcome that the treatment degrees hold.\r\n\r\n## A Null model\r\n\r\nA null model can be specified, where the outcome for any source-target couple when predicting presence of a treatment edge is equal to the prior probability $$p_{ij}$$ above. This model is denoted $$\\mathcal{M}_{prior}$$.\r\n\r\n$$\\mathcal{M}_{prior}$$, when tested on all observations, is expected to have a very high AUROC, because it successfully stratifies the population of potential edges into tiers that have increasing and radically different probabilities of being a true link. For instance, it successfully discriminates the huge majority (85%) of couples that have no chances of being a treatment edge because either the Compound and/or the Disease has a treatment degree equal to zero. \r\n\r\n---\r\n\r\n## AUROC of the null model\r\n\r\nJust considering this point in the ROC curve, with $$P_0 = P\\left(p_{ij} = 0\\right)$$ the proportion of edges that have a priori probability of zero, we can compute (for the fun) a lower bound for the AUROC that will illustrate why the expected performance is high. Every couple with prior probability of zero is ranked lower than any other couple, and each of these couples is actually a True Negative (TN) by design. Since we know that the model will be better than random for the remaining of the range of specificity, the lower bound for the AUROC of this null model is given by a simple linear interpolation based on one point we know in ROC space. \r\n\r\nThis point corresponds to classifying only these \"absolute negatives\" that have $$p_{ij} = 0$$, as negatives. We get a sensitivity $$sens = 1$$, since all the real positives are predicted to be positives; and a specificity of \r\n$$$\r\n\\begin{align}\r\nspe &= \\frac{P_0}{P_0 + (1 - P_0) \\cdot (1 - pre)}\\\\\r\n&= \\frac{P_0}{1 - pre \\cdot (1 - P_0)}\\\\\r\nspe &\\simeq  P_0\r\n\\end{align}\r\n$$$\r\nwith the prevalence $$pre$$ being negligible (of the order of $$10^{-4}$$).\r\n\r\nThe linear interpolation gives us:\r\n$$$ \r\n\\begin{align}\r\nauroc_{min}\\left(\\mathcal{M}_{prior}\\right) &= 1 - \\frac{1 - P_0}{2}\r\n\\end{align}\r\n$$$\r\nWith $$P_0 = 0.85 $$, we get $$ \\boxed{auroc_{min}\\left(\\mathcal{M}_{prior}\\right) = 92.5 \\%} $$ as a lower bound for the null model performance.\r\n\r\n---\r\n\r\nThe measured AUROC of the this null model, on all observations is equal to:\r\n$$$ auroc\\left(\\mathcal{M}_{prior}\\right)  = 97.9 \\%$$$",
      "profile": 23,
      "published": "2016-04-07T22:14:58.448547Z",
      "thread": 194,
      "url": "/discussion/network-edge-prediction-how-to-deal-with-self-testing/194#2"
    },
    {
      "body_html": "<h1>Why we self-test</h1>\r\n\r\n<p><a href=\"/u/alizee\" class=\"username\">@alizee</a> mentions that we plan to use self-testing. Here I'll describe what that means, and why we chose this unconventional approach. Self-testing refers to our plan to not withhold any treatments (positives) and non-treatments (negatives) from our hetnet and subsequent model training. In a conventional testing/validation approach, a subset of compound–disease pairs would be entirely removed from the training process. Here are reasons we don't want to withhold compound–disease pairs for testing.</p>\r\n\r\n<p>We only have 755 treatments. Many of the best performing features rely on these edges. For example, we'll look for compounds that bind to similar genes to compounds that treat the target disease. The informativeness of this feature is proportional to how many compounds treat the target disease. Therefore, withholding too many treatments for testing will hurt performance. Conversely, withholding too few treatments will lead to unreliable performance estimates. One solution to this dilemma is cross-validation. However, each fold would require a distinct feature extraction on a distinct hetnet. With our current infrastructure, this would create a major runtime and implementation time bottleneck that would negatively influence our agility.</p>\r\n\r\n<p>So the question becomes whether this investment is warranted by the benefits of a true testing set. The main benefit is accurate assessment of performance. However, in our particular situation, I don't think this a major concern and is probably not even possible. We're focused on making good predictions, not comparing the performance of many different methods. Ultimately the predictions will be unaffected by whether we adopt a formal testing scheme.</p>\r\n\r\n<p>Next, I expect the extent of model-based overfitting to be low. When training our logistic regression model, we use cross-validation to find an optimal level of regularization (a penalty to prevent overfitting). Specifically, we adopt the \"one-standard-error\" rule to choose the optimal λ from deviance <span class=\"citation\">[<a href=\"/doi/10.18637/jss.v033.i01\" class=\"citation\" data-key=\"10.18637/jss.v033.i01\">1</a>]</span>. This is a stringent choice that takes a more conservative regularization strength than the deviance-minimizing value as an extra precaution against overfitting. In our previous project, where we did mask edges from the hetnet for testing, we comment <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1004259\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1004259\">2</a>]</span>:</p>\r\n\r\n<blockquote><p>Importantly, we did not observe any significant degradation of performance from training to testing (<a href=\"https://doi.org/10.1371/journal.pcbi.1004259.g003\">Fig 3A</a>), indicating that our disciplined regularization approach avoided overfitting and that predictions for associations included in the network were not biased by their presence in the network.</p></blockquote>\r\n\r\n<p>And finally, there are two much bigger biases than overfitting. The first is the bias of existing knowledge (study bias), which we realistically cannot control for. The second is the prior probability of treatment based on degrees, as <a href=\"/u/alizee\" class=\"username\">@alizee</a> has discussed. I don't think cross-validation can easily address the prior probability issue. The PREDICT study — which also incorporated treatment information in features used to predict treatments — attempted to address the issue with the following approach <span class=\"citation\">[<a href=\"/doi/10.1038/msb.2011.26\" class=\"citation\" data-key=\"10.1038/msb.2011.26\">3</a>]</span>:</p>\r\n\r\n<blockquote><p>To evaluate our classification scheme, we applied it in a 10‐fold cross‐validation setting. To avoid easy prediction cases, we hid all the associations involved with 10% of the drugs in each iteration, rather than hiding 10% of the associations.</p></blockquote>\r\n\r\n<p>However, if you hide all treatments for a set of drugs, a non-uniform prior based on the treatment-degree of the diseases still exists. Therefore, I think the best way forward will be explicitly model the prior probability of treatment based on degree and proceed intelligently.</p>",
      "body_md": "# Why we self-test\r\n\r\n@alizee mentions that we plan to use self-testing. Here I'll describe what that means, and why we chose this unconventional approach. Self-testing refers to our plan to not withhold any treatments (positives) and non-treatments (negatives) from our hetnet and subsequent model training. In a conventional testing/validation approach, a subset of compound--disease pairs would be entirely removed from the training process. Here are reasons we don't want to withhold compound--disease pairs for testing.\r\n\r\nWe only have 755 treatments. Many of the best performing features rely on these edges. For example, we'll look for compounds that bind to similar genes to compounds that treat the target disease. The informativeness of this feature is proportional to how many compounds treat the target disease. Therefore, withholding too many treatments for testing will hurt performance. Conversely, withholding too few treatments will lead to unreliable performance estimates. One solution to this dilemma is cross-validation. However, each fold would require a distinct feature extraction on a distinct hetnet. With our current infrastructure, this would create a major runtime and implementation time bottleneck that would negatively influence our agility.\r\n\r\nSo the question becomes whether this investment is warranted by the benefits of a true testing set. The main benefit is accurate assessment of performance. However, in our particular situation, I don't think this a major concern and is probably not even possible. We're focused on making good predictions, not comparing the performance of many different methods. Ultimately the predictions will be unaffected by whether we adopt a formal testing scheme.\r\n\r\nNext, I expect the extent of model-based overfitting to be low. When training our logistic regression model, we use cross-validation to find an optimal level of regularization (a penalty to prevent overfitting). Specifically, we adopt the \"one-standard-error\" rule to choose the optimal λ from deviance [@10.18637/jss.v033.i01]. This is a stringent choice that takes a more conservative regularization strength than the deviance-minimizing value as an extra precaution against overfitting. In our previous project, where we did mask edges from the hetnet for testing, we comment [@10.1371/journal.pcbi.1004259]:\r\n\r\n> Importantly, we did not observe any significant degradation of performance from training to testing ([Fig 3A](https://doi.org/10.1371/journal.pcbi.1004259.g003)), indicating that our disciplined regularization approach avoided overfitting and that predictions for associations included in the network were not biased by their presence in the network.\r\n\r\nAnd finally, there are two much bigger biases than overfitting. The first is the bias of existing knowledge (study bias), which we realistically cannot control for. The second is the prior probability of treatment based on degrees, as @alizee has discussed. I don't think cross-validation can easily address the prior probability issue. The PREDICT study -- which also incorporated treatment information in features used to predict treatments -- attempted to address the issue with the following approach [@10.1038/msb.2011.26]:\r\n\r\n> To evaluate our classification scheme, we applied it in a 10‐fold cross‐validation setting. To avoid easy prediction cases, we hid all the associations involved with 10% of the drugs in each iteration, rather than hiding 10% of the associations.\r\n\r\nHowever, if you hide all treatments for a set of drugs, a non-uniform prior based on the treatment-degree of the diseases still exists. Therefore, I think the best way forward will be explicitly model the prior probability of treatment based on degree and proceed intelligently.",
      "profile": 17,
      "published": "2016-04-14T06:31:03.552549Z",
      "thread": 194,
      "url": "/discussion/network-edge-prediction-how-to-deal-with-self-testing/194#4"
    },
    {
      "body_html": "<h1>Incorporating the prior in fitting and testing</h1>\r\n\r\n<h2>Using <span class=\"math\">$$\\mathcal{M}_{prior}$$</span> as baseline</h2>\r\n\r\n<p>The performance achieved by the null model <span class=\"math\">$$\\mathcal{M}_{prior}$$</span> should serve as a baseline, thus mitigating the lack of meaning of the AUROC: we'll use <span class=\"math\">$$auroc(\\mathcal{M}_{prior})$$</span> as a minimum value to improve on (instead of the usual <span class=\"math\">$$auroc(\\mathcal{M}_{random})= 0.5$$</span>).</p>\r\n\r\n<h2>Using <span class=\"math\">$$p_{ij}$$</span> as covariate</h2>\r\n\r\n<p>Nevertheless, we also want to include as a covariate in our models this prior probability, in order to isolate the prior knowledge about the outcome it describes, and avoid selection of DWPC features that 'tag' it. </p>\r\n\r\n<p>In the framework of generalized linear regressions I propose to directly take the linear predictor that corresponds to the computed prior probability. In the case of logistic regression, the feature to use as covariate in the model would be equal to:</p>\r\n\r\n<div class=\"math\">$$$\r\n\\begin{align}\r\n\\boxed{f_{prior}\\left(C_i\\text{-}D_j\\right) = \\text{logit}\\left(p_{ij}\\right)\\strut}\r\n\\end{align}\r\n$$$</div>\r\n\r\n<h2>Proposed procedure</h2>\r\n\r\n<p>We propose an incremental approach to the fitting and evaluation of the models, going through different iteration:</p>\r\n\r\n<div class=\"math\">$$$\r\n\\begin{align}\r\n\\text{1.} &amp;&amp; \\mathcal{M}_{prior}: \\; &amp;\\text{the null model described above} \\\\\r\n\\text{2.} &amp;&amp; \\mathcal{M}_{deg}: \\; &amp;\\delta_{CtD}  \\sim f_{prior} + degrees \\\\\r\n\\text{3.} &amp;&amp; \\mathcal{M}_{perm}: \\; &amp;\\delta_{CtD}  \\sim f_{prior} + degrees + DWPC_{permuted} \\\\\r\n\\text{4.} &amp;&amp; \\mathcal{M}_{real}: \\; &amp;\\delta_{CtD}  \\sim f_{prior} + degrees + DWPC_{real} \\\\\r\n\\end{align}\r\n$$$</div>\r\n\r\n<p>where we use the R formula notation to specify outcomes and features of the model;<br>where <span class=\"math\">$$\\delta_{CtD} = \\delta_{DtC}$$</span> is the outcome variable that denotes the presence of a treatment edge between a given couple of Compound and Disease; <br>where '<span class=\"math\">$$degrees$$</span>' are all the source/target degree features; and<br>where the <span class=\"math\">$$DWPC_{permuted}$$</span> and <span class=\"math\">$$DWPC_{real}$$</span> are the all DWPC features computed respectively from the degree-preserving permuted network <span class=\"math\">$$ \\mathcal{N}_{permuted} $$</span> (see ***) and the full network <span class=\"math\">$$\\mathcal{N}$$</span>.</p>\r\n\r\n<p>We expect <span class=\"math\">$$ \\mathcal{M}_{deg} $$</span> to bring no additional predictive value than the null model <span class=\"math\">$$ \\mathcal{M}_{prior} $$</span>, since the latter withholds already the best information possible at the atomicity of the source/target degrees level. For <span class=\"math\">$$\\mathcal{M}_{perm}$$</span> and <span class=\"math\">$$\\mathcal{M}_{real}$$</span>, the degrees might be useful in conjunction with the DWPC features.</p>\r\n\r\n<p>Testing of new or fitted data will be carried out using an arbitrary value for the prior in the models, e.g. an estimation of the prevalence.</p>\r\n\r\n<h2>Redefining the training &amp; testing sets</h2>\r\n\r\n<p>Finally, we propose to fit the models above only on couples that have a non-null prior probability of being a treatment edge, i.e. couples that verify both:</p>\r\n\r\n<div class=\"math\">$$$\r\n\\left\\{ \r\n\\begin{array}{c}\r\nn_{CtD}(C_i) &gt; 0 \\\\\r\nn_{DtC}(D_j) &gt; 0\r\n\\end{array}\r\n\\right. \r\n$$$</div>\r\n\r\n<p>Indeed, when the prior probability <span class=\"math\">$$p_{ij}$$</span> is equal to zero, the points do not theoretically contribute to the fitting, whereas they practically induce complications (lack of convergence, problem with the numerical encoding of <span class=\"math\">$$-\\infty$$</span> ... ). Moreover, it gives us the opportunity to report results for the couples from the complementary set (of the couples that have a null prior), which seems quite independent from the set used for training.</p>\r\n\r\n<p>On this smaller training set, the performance of the prior alone as measured by the ROC decreases since we removed obvious negatives:</p>\r\n\r\n<div class=\"math\">$$$ auroc\\left(\\mathcal{M}_{prior}\\right)  = 85.1 \\%$$$</div>\r\n\r\n<p>... while the area under the precision recall curve is exactly the same: <span class=\"math\">$$auprc\\left(\\mathcal{M}_{prior}\\right) = 0.16$$</span>.</p>",
      "body_md": "# Incorporating the prior in fitting and testing\r\n\r\n## Using $$\\mathcal{M}_{prior}$$ as baseline\r\n\r\nThe performance achieved by the null model $$\\mathcal{M}_{prior}$$ should serve as a baseline, thus mitigating the lack of meaning of the AUROC: we'll use $$auroc(\\mathcal{M}_{prior})$$ as a minimum value to improve on (instead of the usual $$auroc(\\mathcal{M}_{random})= 0.5$$).\r\n\r\n## Using $$p_{ij}$$ as covariate\r\n\r\nNevertheless, we also want to include as a covariate in our models this prior probability, in order to isolate the prior knowledge about the outcome it describes, and avoid selection of DWPC features that 'tag' it. \r\n\r\nIn the framework of generalized linear regressions I propose to directly take the linear predictor that corresponds to the computed prior probability. In the case of logistic regression, the feature to use as covariate in the model would be equal to:\r\n$$$\r\n\\begin{align}\r\n\\boxed{f_{prior}\\left(C_i\\text{-}D_j\\right) = \\text{logit}\\left(p_{ij}\\right)\\strut}\r\n\\end{align}\r\n$$$\r\n\r\n## Proposed procedure\r\n\r\nWe propose an incremental approach to the fitting and evaluation of the models, going through different iteration:\r\n\r\n$$$\r\n\\begin{align}\r\n\\text{1.} && \\mathcal{M}_{prior}: \\; &\\text{the null model described above} \\\\\r\n\\text{2.} && \\mathcal{M}_{deg}: \\; &\\delta_{CtD}  \\sim f_{prior} + degrees \\\\\r\n\\text{3.} && \\mathcal{M}_{perm}: \\; &\\delta_{CtD}  \\sim f_{prior} + degrees + DWPC_{permuted} \\\\\r\n\\text{4.} && \\mathcal{M}_{real}: \\; &\\delta_{CtD}  \\sim f_{prior} + degrees + DWPC_{real} \\\\\r\n\\end{align}\r\n$$$\r\n\r\nwhere we use the R formula notation to specify outcomes and features of the model;\r\nwhere $$\\delta_{CtD} = \\delta_{DtC}$$ is the outcome variable that denotes the presence of a treatment edge between a given couple of Compound and Disease; \r\nwhere '$$degrees$$' are all the source/target degree features; and\r\nwhere the $$DWPC_{permuted}$$ and $$DWPC_{real}$$ are the all DWPC features computed respectively from the degree-preserving permuted network $$ \\mathcal{N}_{permuted} $$ (see ***) and the full network $$\\mathcal{N}$$.\r\n\r\nWe expect $$ \\mathcal{M}_{deg} $$ to bring no additional predictive value than the null model $$ \\mathcal{M}_{prior} $$, since the latter withholds already the best information possible at the atomicity of the source/target degrees level. For $$\\mathcal{M}_{perm}$$ and $$\\mathcal{M}_{real}$$, the degrees might be useful in conjunction with the DWPC features.\r\n\r\nTesting of new or fitted data will be carried out using an arbitrary value for the prior in the models, e.g. an estimation of the prevalence.\r\n\r\n## Redefining the training & testing sets\r\n\r\nFinally, we propose to fit the models above only on couples that have a non-null prior probability of being a treatment edge, i.e. couples that verify both:\r\n$$$\r\n\\left\\{ \r\n\\begin{array}{c}\r\nn_{CtD}(C_i) > 0 \\\\\r\nn_{DtC}(D_j) > 0\r\n\\end{array}\r\n\\right. \r\n$$$\r\n\r\nIndeed, when the prior probability $$p_{ij}$$ is equal to zero, the points do not theoretically contribute to the fitting, whereas they practically induce complications (lack of convergence, problem with the numerical encoding of $$-\\infty$$ ... ). Moreover, it gives us the opportunity to report results for the couples from the complementary set (of the couples that have a null prior), which seems quite independent from the set used for training.\r\n\r\nOn this smaller training set, the performance of the prior alone as measured by the ROC decreases since we removed obvious negatives:\r\n$$$ auroc\\left(\\mathcal{M}_{prior}\\right)  = 85.1 \\%$$$\r\n\r\n... while the area under the precision recall curve is exactly the same: $$auprc\\left(\\mathcal{M}_{prior}\\right) = 0.16$$.",
      "profile": 23,
      "published": "2016-04-07T23:43:24.684141Z",
      "thread": 194,
      "url": "/discussion/network-edge-prediction-how-to-deal-with-self-testing/194#3"
    },
    {
      "body_html": "<p>Extracting LD from 1000 genomes data is not straightforward. Here is a rough outline of the solution I came up with. I used Plink1.9 <a href=\"https://www.cog-genomics.org/plink2/\">https://www.cog-genomics.org/plink2/</a> and VCFTools. It will require some tweaking for specific applications.</p>\r\n\r\n<h1>Step 0: Download 1000 genomes data and remove duplicate SNP IDs</h1>\r\n\r\n<h1></h1>\r\n\r\n<h1>Step 1: use vcftools to generate a population specific tped file</h1>\r\n\r\n<pre><code class=\"no-highlight hljs\">vcftools --gzvcf &lt;vcf_file&gt; --plink-tped --keep &lt;samples.txt&gt; --out &lt;tped_fh&gt;</code></pre>\r\n\r\n<p>where &lt;samples.txt&gt; is the location of a text file with a single column of sample IDs</p>\r\n\r\n<h1>Step 2: transpose the tped file (more efficient than creating a ped file originally)</h1>\r\n\r\n<pre><code class=\"no-highlight hljs\">plink --tfile &lt;tped_fh&gt; --recode --threads &lt;num_threads&gt; --no-sex --no-pheno --out &lt;ped_fh&gt;</code></pre>\r\n\r\n<h1>Step 3: use Plink1.9 to pull out an LD matrix</h1>\r\n\r\n<pre><code class=\"no-highlight hljs\">plink --file &lt;vcf_file&gt; --allow-no-sex --r2 --threads &lt;num_threads&gt; --ld-window-r2 &lt;window&gt; --chr &lt;chromosome&gt; --ld-snps &lt;snp_string&gt; --out &lt;out_name&gt;</code></pre>\r\n\r\n<p>where &lt;snp_string&gt; is a comma separated list of RS IDs</p>",
      "body_md": "Extracting LD from 1000 genomes data is not straightforward. Here is a rough outline of the solution I came up with. I used Plink1.9 <https://www.cog-genomics.org/plink2/> and VCFTools. It will require some tweaking for specific applications.\r\n#\r\nStep 0: Download 1000 genomes data and remove duplicate SNP IDs\r\n#\r\n#\r\nStep 1: use vcftools to generate a population specific tped file\r\n\r\n```\r\nvcftools --gzvcf <vcf_file> --plink-tped --keep <samples.txt> --out <tped_fh>\r\n```\r\nwhere <samples.txt> is the location of a text file with a single column of sample IDs\r\n#\r\nStep 2: transpose the tped file (more efficient than creating a ped file originally)\r\n\r\n```\r\nplink --tfile <tped_fh> --recode --threads <num_threads> --no-sex --no-pheno --out <ped_fh>\r\n```\r\n#\r\nStep 3: use Plink1.9 to pull out an LD matrix\r\n\r\n```\r\nplink --file <vcf_file> --allow-no-sex --r2 --threads <num_threads> --ld-window-r2 <window> --chr <chromosome> --ld-snps <snp_string> --out <out_name>\r\n```\r\nwhere <snp_string> is a comma separated list of RS IDs",
      "profile": 224,
      "published": "2016-04-08T13:55:13.834889Z",
      "thread": 71,
      "url": "/discussion/calculating-genomic-windows-for-gwas-lead-snps/71#8"
    },
    {
      "body_html": "<h1>Cheng et al 2014</h1>\r\n\r\n<p>A 2014 study titled \"Systematic evaluation of connectivity map for disease indications\" compiled 890 indications between 152 drugs and 145 diseases <span class=\"citation\">[<a href=\"/doi/10.1186/s13073-014-0095-1\" class=\"citation\" data-key=\"10.1186/s13073-014-0095-1\">1</a>]</span>. They compiled the indications from FAERS and <a href=\"http://citeline.com/products/pharmaprojects/\">Pharmaprojects</a>. The indications are available as free text in Table S2 of the supplementary word document. I copied Table S2 into a TSV <a href=\"https://gist.github.com/dhimmel/4977c3f538fb215d9c49c9ccd66a41d4\" title=\"GitHub Gist: Catalog of treatments from Table S2 of Cheng et al 2014\">available here</a>.</p>",
      "body_md": "# Cheng et al 2014\r\n\r\nA 2014 study titled \"Systematic evaluation of connectivity map for disease indications\" compiled 890 indications between 152 drugs and 145 diseases [@10.1186/s13073-014-0095-1]. They compiled the indications from FAERS and [Pharmaprojects](http://citeline.com/products/pharmaprojects/). The indications are available as free text in Table S2 of the supplementary word document. I copied Table S2 into a TSV [available here](https://gist.github.com/dhimmel/4977c3f538fb215d9c49c9ccd66a41d4 \"GitHub Gist: Catalog of treatments from Table S2 of Cheng et al 2014\").",
      "profile": 17,
      "published": "2016-04-10T03:18:34.097548Z",
      "thread": 21,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#26"
    },
    {
      "body_html": "<p>Have you considered the <a href=\"http://mentha.uniroma2.it/about.php\">Mentha PPI database</a>? </p>\r\n\r\n<ul><li>It combines protein-protein interaction data from most databases.</li><li>It assigns a reliability score to each interaction (like iRefIndex). </li><li>It uses the psicquic protocol and MITAB 2.5 format, so it compatible with all <a href=\"http://www.ebi.ac.uk/Tools/webservices/psicquic/registry/registry?action=STATUS\">other protein interaction databases</a>.</li><li>Over the last few years, it has been <a href=\"http://mentha.uniroma2.it/download.php\">regularly updated</a>. </li><li>It uses HGNC Gene IDs and UniProt accessions.</li></ul>",
      "body_md": "Have you considered the [Mentha PPI database](http://mentha.uniroma2.it/about.php)? \r\n\r\n  - It combines protein-protein interaction data from most databases.\r\n  - It assigns a reliability score to each interaction (like iRefIndex). \r\n  - It uses the psicquic protocol and MITAB 2.5 format, so it compatible with all [other protein interaction databases](http://www.ebi.ac.uk/Tools/webservices/psicquic/registry/registry?action=STATUS).\r\n  - Over the last few years, it has been [regularly updated](http://mentha.uniroma2.it/download.php). \r\n  - It uses HGNC Gene IDs and UniProt accessions.",
      "profile": 226,
      "published": "2016-04-11T03:03:22.135132Z",
      "thread": 85,
      "url": "/discussion/creating-a-catalog-of-protein-interactions/85#10"
    },
    {
      "body_html": "<p><a href=\"/u/ostrokach\" class=\"username\">@ostrokach</a>, thanks for letting us know about <a href=\"http://mentha.uniroma2.it/\">Mentha</a> <span class=\"citation\">[<a href=\"/doi/10.1038/nmeth.2561\" class=\"citation\" data-key=\"10.1038/nmeth.2561\">1</a>]</span>. We've moved past the network construction stage on this project but will keep Mentha in mind for the future.</p>\r\n\r\n<p>I downloaded the latest release (<code>2016-04-11.zip</code>). Here are a few select rows from the top:</p>\r\n\r\n<table class=\"table markdown-table\"><thead><tr><th>Protein A</th><th>Gene A</th><th>Taxon A</th><th>Protein B</th><th>Gene B</th><th>Taxon B</th><th>Score</th><th>PMID</th></tr></thead><tbody><tr><td>Q86UX7</td><td>FERMT3</td><td>9606</td><td>Q06830</td><td>PRDX1</td><td>9606</td><td>0.126</td><td>26496610</td></tr><tr><td>Q9LK77</td><td>Q9LK77</td><td>3702</td><td>A0A088QD33</td><td>OEC103 {ECO:0000313|EMBL:AIN81148.1}</td><td>62715</td><td>0.236</td><td>25211078</td></tr><tr><td>Q96QD9</td><td>FYTTD1</td><td>9606</td><td>Q86UX7</td><td>FERMT3</td><td>9606</td><td>0.126</td><td>26496610</td></tr><tr><td>F4J5N9</td><td>BZIP24 {ECO:0000313|EMBL:AEE78868.1}</td><td>3702</td><td>A0A088QD42</td><td>OEC112 {ECO:0000313|EMBL:AIN81158.1}</td><td>62715</td><td>0.236</td><td>25211078</td></tr><tr><td>Q9Y5Q8</td><td>GTF3C5</td><td>9606</td><td>Q86U86</td><td>PBRM1</td><td>9606</td><td>0.155</td><td>22939629 26344197</td></tr></tbody></table>\r\n\r\n<p>Two features that caught my eye are the reliability scores and the PubMed IDs of the original source(s). It looks like the file format could be a little bit cleaner (see for example <code>{ECO:0000313|EMBL:AEE78868.1}</code>). Also I didn't see a license on the Mentha website. Going forward I plan to avoid resources without an <a href=\"http://opendefinition.org/licenses/\" title=\"Conformant Licenses: Open Definition\">open license</a> — something we <a href=\"http://thinklab.com/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#14\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d107\">learned the importance</a> of during this project.</p>",
      "body_md": "@ostrokach, thanks for letting us know about [Mentha](http://mentha.uniroma2.it/) [@10.1038/nmeth.2561]. We've moved past the network construction stage on this project but will keep Mentha in mind for the future.\r\n\r\nI downloaded the latest release (`2016-04-11.zip`). Here are a few select rows from the top:\r\n\r\n| Protein A | Gene A | Taxon A | Protein B | Gene B | Taxon B | Score | PMID |\r\n|-----------|--------------|---------|------------|---------------|---------|-------|-------------------|\r\n| Q86UX7 | FERMT3 | 9606 | Q06830 | PRDX1 | 9606 | 0.126 | 26496610 |\r\n| Q9LK77 | Q9LK77 | 3702 | A0A088QD33 | OEC103 {ECO:0000313\\|EMBL:AIN81148.1} | 62715 | 0.236 | 25211078 |\r\n| Q96QD9 | FYTTD1 | 9606 | Q86UX7 | FERMT3 | 9606 | 0.126 | 26496610 |\r\n| F4J5N9 | BZIP24 {ECO:0000313\\|EMBL:AEE78868.1} | 3702 | A0A088QD42 | OEC112 {ECO:0000313\\|EMBL:AIN81158.1} | 62715 | 0.236 | 25211078 |\r\n| Q9Y5Q8 | GTF3C5 | 9606 | Q86U86 | PBRM1 | 9606 | 0.155 | 22939629 26344197 |\r\n\r\nTwo features that caught my eye are the reliability scores and the PubMed IDs of the original source(s). It looks like the file format could be a little bit cleaner (see for example `{ECO:0000313|EMBL:AEE78868.1}`). Also I didn't see a license on the Mentha website. Going forward I plan to avoid resources without an [open license](http://opendefinition.org/licenses/ \"Conformant Licenses: Open Definition\") -- something we [learned the importance](http://thinklab.com/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#14) of during this project.",
      "profile": 17,
      "published": "2016-04-11T05:56:08.147984Z",
      "thread": 85,
      "url": "/discussion/creating-a-catalog-of-protein-interactions/85#11"
    },
    {
      "body_html": "<p>We're planning to release our first project report soon, which will cover the completed hetnet. We'd like to assess the amount of user contribution and content creation the <em>Thinklab</em> venue has facilitated thus far. With the new <a href=\"http://thinklab.com/discussion/discussion-summary-statistics-for-illustrating-project-impact/191\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d191\">project export feature</a>, we can now perform custom analytics.</p>\r\n\r\n<p>Using today's export, I calculated some basic stats (<a href=\"https://github.com/dhimmel/thinklytics/blob/a3403caa68e37102d53722de38dfba7e98adf2d7/rephetio-stats.ipynb\">notebook</a>). The <a href=\"http://thinklab.com/p/rephetio/discussion\">67 discussions</a> in our project (comments + notes) contain:</p>\r\n\r\n<ul><li>403 comments and 133 notes containing 662,501 characters</li><li>contributions from 40 users who have written a comment or note</li><li>248 unique DOI references to 39 different DOI registrants</li></ul>\r\n\r\n<p><a href=\"/u/alizee\" class=\"username\">@alizee</a> is creating a visualization of user contribution over time, so stay tuned.</p>",
      "body_md": "We're planning to release our first project report soon, which will cover the completed hetnet. We'd like to assess the amount of user contribution and content creation the _Thinklab_ venue has facilitated thus far. With the new [project export feature](http://thinklab.com/discussion/discussion-summary-statistics-for-illustrating-project-impact/191), we can now perform custom analytics.\r\n\r\nUsing today's export, I calculated some basic stats ([notebook](https://github.com/dhimmel/thinklytics/blob/a3403caa68e37102d53722de38dfba7e98adf2d7/rephetio-stats.ipynb)). The [67 discussions](http://thinklab.com/p/rephetio/discussion) in our project (comments + notes) contain:\r\n\r\n+ 403 comments and 133 notes containing 662,501 characters\r\n+ contributions from 40 users who have written a comment or note\r\n+ 248 unique DOI references to 39 different DOI registrants\r\n\r\n@alizee is creating a visualization of user contribution over time, so stay tuned.",
      "profile": 17,
      "published": "2016-04-11T22:43:07.368630Z",
      "thread": 200,
      "url": "/discussion/measuring-user-contribution-and-content-creation/200"
    },
    {
      "body_html": "<p>We adapted one of the graph shown in the <a href=\"http://thinklab.com/discussion/discussion-summary-statistics-for-illustrating-project-impact/191#10\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d191\">thinklytics dicussion</a> for the scope of the <em>rephetio</em> project (R-<a href=\"https://github.com/antoine-lizee/thinklytics/blob/master/R-Code/03-singleProject.R\">script</a>), as well as the few numbers that <a href=\"/u/dhimmel\" class=\"username\">@dhimmel</a> shared above(R-<a href=\"https://github.com/antoine-lizee/thinklytics/blob/master/R-Code/02-description.R\">script</a>).</p>\r\n\r\n<p>We include in the graph only the 33 users that have written more than 500 characters, highlighting the 8 top contributors with more than 5 thousands.</p>\r\n\r\n<p><img src=\"https://github.com/antoine-lizee/thinklytics/raw/master/Output/03-evoCumProfiles.png\" alt=\"Evolution of individual contributions\" title=\"Evolution of individual contributions\"></p>",
      "body_md": "We adapted one of the graph shown in the [thinklytics dicussion](http://thinklab.com/discussion/discussion-summary-statistics-for-illustrating-project-impact/191#10) for the scope of the _rephetio_ project (R-[script](https://github.com/antoine-lizee/thinklytics/blob/master/R-Code/03-singleProject.R)), as well as the few numbers that @dhimmel shared above(R-[script](https://github.com/antoine-lizee/thinklytics/blob/master/R-Code/02-description.R)).\r\n\r\nWe include in the graph only the 33 users that have written more than 500 characters, highlighting the 8 top contributors with more than 5 thousands.\r\n\r\n![Evolution of individual contributions](https://github.com/antoine-lizee/thinklytics/raw/master/Output/03-evoCumProfiles.png \"Evolution of individual contributions\")",
      "profile": 23,
      "published": "2016-04-12T05:44:20.935980Z",
      "thread": 200,
      "url": "/discussion/measuring-user-contribution-and-content-creation/200#2"
    },
    {
      "body_html": "<p>We also created the stream chart that shows instantaneous instead of cumulative contribution over time. I removed the main project owner, <a href=\"/u/dhimmel\" class=\"username\">@dhimmel</a>, to highlight the patterns outside of his massive contribution. These counts are not transformed.</p>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/antoine-lizee/thinklytics/master/Output/03-evoStreamProfiles.png\" alt=\"User contribution stream chart\"></p>",
      "body_md": "We also created the stream chart that shows instantaneous instead of cumulative contribution over time. I removed the main project owner, @dhimmel, to highlight the patterns outside of his massive contribution. These counts are not transformed.\r\n\r\n![User contribution stream chart](https://raw.githubusercontent.com/antoine-lizee/thinklytics/master/Output/03-evoStreamProfiles.png)",
      "profile": 23,
      "published": "2016-04-15T00:46:13.213603Z",
      "thread": 200,
      "url": "/discussion/measuring-user-contribution-and-content-creation/200#3"
    },
    {
      "body_html": "<p>We discussed in a different <a href=\"http://thinklab.com/discussion/network-edge-prediction-how-to-deal-with-self-testing/194\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d194\">thread</a> why we believe that including the prior is a correct way of mitigating the problems posed by self-testing. In one of the <a href=\"http://thinklab.com/discussion/network-edge-prediction-how-to-deal-with-self-testing/194#2\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d194\">comments</a>, I derived a formula for the prior probability <span class=\"math\">$$p_{ij}$$</span> of a certain couple <span class=\"math\">$$C_i\\text{-}D_j$$</span> to have a treatment edge, only based on the source/target degrees <span class=\"math\">$$n_{CtD}(C_i)$$</span> and <span class=\"math\">$$n_{DtC}(D_j)$$</span>. Unfortunately, this formula is wrong. </p>\r\n\r\n<p>Here, we want to discuss why the formula is wrong, find a more accurate analytical formula, and explore ways to estimate programmatically this prior probability.</p>\r\n\r\n<h2>The previous formula</h2>\r\n\r\n<p>The reasoning was flawed at the point where we combined the two correct \"asymmetrical\"  probabilities into our final prior probability <span class=\"math\">$$p_{ij}$$</span>. Indeed, the \"asymmetrical\" probability of a source node <span class=\"math\">$$C_i$$</span> connecting a target node drawn at random can be accurately expressed as the ratio of the degree <span class=\"math\">$$n_{CtD}(C_i)$$</span> of this node over the total number of target nodes <span class=\"math\">$$N_D$$</span>. Nevertheless, multiplying the two probabilities  <span class=\"math\">$$p_t(C_i\\text{-}D)  = \\frac{n_{CtD}(C_i)}{N_D}$$</span> and  <span class=\"math\">$$p_t(D_j\\text{-}C)  = \\frac{n_{DtC}(D_j)}{N_D}$$</span> is <em>not</em> equal to the prior probability <span class=\"math\">$$p_{ij}$$</span>, but merely the joint probability of <span class=\"math\">$$C_i$$</span> to treat a disease drawn at random, and <span class=\"math\">$$D_j$$</span> to be treated by a Compound drawn at random.</p>\r\n\r\n<p>This formula appeared obviously wrong since the sum of the prior probabilities over all potential is not equal to anything meaningful, and certainly not the expected number of treatments <span class=\"math\">$$N_T$$</span>:</p>\r\n\r\n<div class=\"math\">$$$\r\n\\begin{align}\r\n\\sum_{i,j \\in 1..N_C \\times 1..N_D} p_{ij} &amp;= \\sum_{i,j \\in 1..N_C \\times 1..N_D} \\frac{n_{CtD}(C_i) \\cdot n_{DtC}(D_j)}{N_D \\cdot N_C}\\\\\r\n&amp;= \\frac{\\left( \\sum_{i \\in 1..N_C}n_{CtD}(C_i) \\right) \\cdot \\left( \\sum_{j \\in 1..N_D}n_{DtC}(D_j) \\right)}{N_D \\cdot N_C}\\\\\r\n&amp;= \\frac{N_T^2}{N_D \\cdot N_C} \\ne N_T\r\n\\end{align}\r\n$$$</div>\r\n\r\n<h2>Prior probability estimation is a hard problem</h2>\r\n\r\n<p>Deriving the prior probability <span class=\"math\">$$p_{ij}$$</span> from the individual source/target degrees is harder than we first thought. To grasp the complexity of the problem, we can see that the prior probability for one particular couple <span class=\"math\">$$C_i\\text-D_j$$</span> is not only dependant on the degrees of the two nodes <span class=\"math\">$$C_i$$</span> and <span class=\"math\">$$D_j$$</span>, but on the degrees of all the Compound and Disease nodes. As a result, it is complicated (maybe impossible) to derive an exact analytical solution, and it might be desirable to strive for a 'mean-field approximation' solution.</p>\r\n\r\n<p>In the meantime, we can brute-force the estimation of the prior probability, based on many random source/target subnetworks that have the same degrees than ours.</p>",
      "body_md": "We discussed in a different [thread](http://thinklab.com/discussion/network-edge-prediction-how-to-deal-with-self-testing/194) why we believe that including the prior is a correct way of mitigating the problems posed by self-testing. In one of the [comments](http://thinklab.com/discussion/network-edge-prediction-how-to-deal-with-self-testing/194#2), I derived a formula for the prior probability $$p_{ij}$$ of a certain couple $$C_i\\text{-}D_j$$ to have a treatment edge, only based on the source/target degrees $$n_{CtD}(C_i)$$ and $$n_{DtC}(D_j)$$. Unfortunately, this formula is wrong. \r\n\r\nHere, we want to discuss why the formula is wrong, find a more accurate analytical formula, and explore ways to estimate programmatically this prior probability.\r\n\r\n## The previous formula\r\n\r\nThe reasoning was flawed at the point where we combined the two correct \"asymmetrical\"  probabilities into our final prior probability $$p_{ij}$$. Indeed, the \"asymmetrical\" probability of a source node $$C_i$$ connecting a target node drawn at random can be accurately expressed as the ratio of the degree $$n_{CtD}(C_i)$$ of this node over the total number of target nodes $$N_D$$. Nevertheless, multiplying the two probabilities  $$p_t(C_i\\text{-}D)  = \\frac{n_{CtD}(C_i)}{N_D}$$ and  $$p_t(D_j\\text{-}C)  = \\frac{n_{DtC}(D_j)}{N_D}$$ is *not* equal to the prior probability $$p_{ij}$$, but merely the joint probability of $$C_i$$ to treat a disease drawn at random, and $$D_j$$ to be treated by a Compound drawn at random.\r\n\r\nThis formula appeared obviously wrong since the sum of the prior probabilities over all potential is not equal to anything meaningful, and certainly not the expected number of treatments $$N_T$$:\r\n$$$\r\n\\begin{align}\r\n\\sum_{i,j \\in 1..N_C \\times 1..N_D} p_{ij} &= \\sum_{i,j \\in 1..N_C \\times 1..N_D} \\frac{n_{CtD}(C_i) \\cdot n_{DtC}(D_j)}{N_D \\cdot N_C}\\\\\r\n&= \\frac{\\left( \\sum_{i \\in 1..N_C}n_{CtD}(C_i) \\right) \\cdot \\left( \\sum_{j \\in 1..N_D}n_{DtC}(D_j) \\right)}{N_D \\cdot N_C}\\\\\r\n&= \\frac{N_T^2}{N_D \\cdot N_C} \\ne N_T\r\n\\end{align}\r\n$$$\r\n\r\n\r\n## Prior probability estimation is a hard problem\r\n\r\nDeriving the prior probability $$p_{ij}$$ from the individual source/target degrees is harder than we first thought. To grasp the complexity of the problem, we can see that the prior probability for one particular couple $$C_i\\text-D_j$$ is not only dependant on the degrees of the two nodes $$C_i$$ and $$D_j$$, but on the degrees of all the Compound and Disease nodes. As a result, it is complicated (maybe impossible) to derive an exact analytical solution, and it might be desirable to strive for a 'mean-field approximation' solution.\r\n\r\nIn the meantime, we can brute-force the estimation of the prior probability, based on many random source/target subnetworks that have the same degrees than ours.",
      "profile": 23,
      "published": "2016-04-15T01:44:45.425274Z",
      "thread": 201,
      "url": "/discussion/network-edge-prediction-estimating-the-prior/201"
    },
    {
      "body_html": "<p>What about disease-disease co-occurrence data mined from electronic health records (EHR)? I know of two studies in particular, but there are probably more: </p>\r\n\r\n<p>Human Disease Network (HuDiNe) <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1000353\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1000353\">1</a>]</span></p>\r\n\r\n<ul><li>Data compiled from Medicare claims of mostly elderly Americans. </li><li>Diseases are mapped to ICD9.</li><li>Data can be downloaded from the <a href=\"http://barabasilab.neu.edu/projects/hudine/resource/data/data.html\">HuDiNe website</a>.</li><li>License: Free for academic use only.</li></ul>\r\n\r\n<p>Stanford Translational Research Integrated Database Environment (STRIDE) <span class=\"citation\">[<a href=\"/doi/10.1038/sdata.2014.32\" class=\"citation\" data-key=\"10.1038/sdata.2014.32\">2</a>]</span></p>\r\n\r\n<ul><li>Data was mined from EHR at Stanford hospitals.</li><li>Diseases are mapped to UMLS CUI.</li><li>Data can be downloaded from <a href=\"http://datadryad.org/pages/policies\">dryad</a>.</li><li>License: Looks like something liberal.</li></ul>",
      "body_md": "What about disease-disease co-occurrence data mined from electronic health records (EHR)? I know of two studies in particular, but there are probably more: \r\n\r\nHuman Disease Network (HuDiNe) [@10.1371/journal.pcbi.1000353]\r\n\r\n- Data compiled from Medicare claims of mostly elderly Americans. \r\n- Diseases are mapped to ICD9.\r\n- Data can be downloaded from the [HuDiNe website](http://barabasilab.neu.edu/projects/hudine/resource/data/data.html).\r\n- License: Free for academic use only.\r\n\r\nStanford Translational Research Integrated Database Environment (STRIDE) [@10.1038/sdata.2014.32]\r\n\r\n- Data was mined from EHR at Stanford hospitals.\r\n- Diseases are mapped to UMLS CUI.\r\n- Data can be downloaded from [dryad](http://datadryad.org/pages/policies).\r\n- License: Looks like something liberal.",
      "profile": 226,
      "published": "2016-04-15T20:54:32.937525Z",
      "thread": 22,
      "url": "/discussion/suggestions-for-additional-information-types/22#7"
    },
    {
      "body_html": "<h1>Estimating the prior probability of treatment via permutation</h1>\r\n\r\n<p><strong>Problem:</strong> We have a bipartite graph of compounds and diseases connected by treatment edges. Using only node degrees, what is the probability that a given compound treats a given disease.</p>\r\n\r\n<p>Since compounds with the same degree are equivalent and diseases with the same degree are equivalent, we can focus on estimating the probability of a degree pair (<span class=\"math\">$$n_{CtD}(C_i)-n_{DtC}(D_i)$$</span>) rather than a specific compound–disease pair.</p>\r\n\r\n<p>Since we struggled to solve this problem analytically, we applied a brute-force permutation approach. Specifically, we took our empiric bipartite treatment network (<a href=\"https://thinklab.com/discussion/announcing-pharmacotherapydb-the-open-catalog-of-drug-therapies-for-disease/182#1\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d182\"><em>PharmacotherapyDB</em></a>) with 755 treatments and derived 744,975 permuted networks.</p>\r\n\r\n<p>We applied the <code>XSwap</code> method of permutation <span class=\"citation\">[<a href=\"/doi/10.1137/1.9781611972795.67\" class=\"citation\" data-key=\"10.1137/1.9781611972795.67\">1</a>, <a href=\"/discussion/permuting-hetnets-and-implementing-randomized-edge-swaps-in-cypher/136\" class=\"citation\" data-key=\"10.15363/thinklab.d136\">2</a>, <a href=\"/doi/10.1371/journal.pcbi.1004259\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1004259\">3</a>]</span> and found 2,265 (755 × 3) attempted swaps provided sufficient randomization <span class=\"citation\">[<a href=\"/discussion/assessing-the-effectiveness-of-our-hetnet-permutations/178\" class=\"citation\" data-key=\"10.15363/thinklab.d178\">4</a>]</span> (<a href=\"https://github.com/dhimmel/learn/blob/0aae784e83cfaa00d1269f17a3bd3a6c6fdc3a0a/prior/1-prior.ipynb\">notebook</a>). After every permutation, we recorded the percent of possible edges that were present for each degree pair. The final result is the prior probability that each degree pair exists (<a href=\"https://github.com/dhimmel/learn/blob/0aae784e83cfaa00d1269f17a3bd3a6c6fdc3a0a/prior/data/degree-prior.tsv\">table</a>).</p>\r\n\r\n<p><strong>Challenge:</strong> Can <a href=\"/u/alizee\" class=\"username\">@alizee</a> or others derive a general formula from these probabilities? For context here are the <a href=\"https://github.com/dhimmel/learn/blob/0aae784e83cfaa00d1269f17a3bd3a6c6fdc3a0a/summary/compounds.tsv\">compounds</a> and <a href=\"https://github.com/dhimmel/learn/blob/0aae784e83cfaa00d1269f17a3bd3a6c6fdc3a0a/summary/diseases.tsv\">diseases</a> — note that nodes with zero treatments (zero-prior nodes) can be ignored. The pseudo-linear relationship we see between our permuted prior probabilities and our faulty theoretic prior probabilities (scaled <span class=\"math\">$$n_{CtD}(C_i) \\cdot n_{DtC}(D_j)$$</span>) after logit transformation suggests an analytic solution may exist:</p>\r\n\r\n<p><img src=\"https://github.com/dhimmel/learn/blob/0aae784e83cfaa00d1269f17a3bd3a6c6fdc3a0a/prior/viz/scatter-theoretic-v-perm.png?raw=true\" alt=\"permuted versus theoretic prior probabilities\"></p>\r\n\r\n<p>Another interesting visualization shows the log-transformed probability of an edge existing for all compound–disease degree pairs (<a href=\"https://github.com/dhimmel/learn/blob/0aae784e83cfaa00d1269f17a3bd3a6c6fdc3a0a/prior/2-prior-viz.ipynb\">notebook</a>). The top facet shows the prevalence in the unpermuted treatment network:</p>\r\n\r\n<p><img src=\"https://github.com/dhimmel/learn/blob/0aae784e83cfaa00d1269f17a3bd3a6c6fdc3a0a/prior/viz/log-prob-tiled-empiric-v-perm.png?raw=true\" alt=\"compound versus disease degree permuted log priors\"></p>\r\n\r\n<p>Now here is the same plot without transforming the probabilities. Notice the high probabilities for the existence of treatment between high degree compounds and diseases.</p>\r\n\r\n<p><img src=\"https://github.com/dhimmel/learn/blob/0aae784e83cfaa00d1269f17a3bd3a6c6fdc3a0a/prior/viz/prob-tiled-empiric-v-perm.png?raw=true\" alt=\"compound versus disease degree permuted priors\"></p>\r\n\r\n<p>The incredible disparity in prior probabilities based on treatment degree reinforces the need to explicitly model the phenomenon.</p>",
      "body_md": "# Estimating the prior probability of treatment via permutation\r\n\r\n**Problem:** We have a bipartite graph of compounds and diseases connected by treatment edges. Using only node degrees, what is the probability that a given compound treats a given disease.\r\n\r\nSince compounds with the same degree are equivalent and diseases with the same degree are equivalent, we can focus on estimating the probability of a degree pair ($$n_{CtD}(C_i)-n_{DtC}(D_i)$$) rather than a specific compound--disease pair.\r\n\r\nSince we struggled to solve this problem analytically, we applied a brute-force permutation approach. Specifically, we took our empiric bipartite treatment network ([_PharmacotherapyDB_](https://thinklab.com/discussion/announcing-pharmacotherapydb-the-open-catalog-of-drug-therapies-for-disease/182#1)) with 755 treatments and derived 744,975 permuted networks.\r\n\r\nWe applied the `XSwap` method of permutation [@10.1137/1.9781611972795.67 @10.15363/thinklab.d136 @10.1371/journal.pcbi.1004259] and found 2,265 (755 × 3) attempted swaps provided sufficient randomization [@10.15363/thinklab.d178] ([notebook](https://github.com/dhimmel/learn/blob/0aae784e83cfaa00d1269f17a3bd3a6c6fdc3a0a/prior/1-prior.ipynb)). After every permutation, we recorded the percent of possible edges that were present for each degree pair. The final result is the prior probability that each degree pair exists ([table](https://github.com/dhimmel/learn/blob/0aae784e83cfaa00d1269f17a3bd3a6c6fdc3a0a/prior/data/degree-prior.tsv)).\r\n\r\n**Challenge:** Can @alizee or others derive a general formula from these probabilities? For context here are the [compounds](https://github.com/dhimmel/learn/blob/0aae784e83cfaa00d1269f17a3bd3a6c6fdc3a0a/summary/compounds.tsv) and [diseases](https://github.com/dhimmel/learn/blob/0aae784e83cfaa00d1269f17a3bd3a6c6fdc3a0a/summary/diseases.tsv) -- note that nodes with zero treatments (zero-prior nodes) can be ignored. The pseudo-linear relationship we see between our permuted prior probabilities and our faulty theoretic prior probabilities (scaled $$n_{CtD}(C_i) \\cdot n_{DtC}(D_j)$$) after logit transformation suggests an analytic solution may exist:\r\n\r\n![permuted versus theoretic prior probabilities](https://github.com/dhimmel/learn/blob/0aae784e83cfaa00d1269f17a3bd3a6c6fdc3a0a/prior/viz/scatter-theoretic-v-perm.png?raw=true)\r\n\r\nAnother interesting visualization shows the log-transformed probability of an edge existing for all compound--disease degree pairs ([notebook](https://github.com/dhimmel/learn/blob/0aae784e83cfaa00d1269f17a3bd3a6c6fdc3a0a/prior/2-prior-viz.ipynb)). The top facet shows the prevalence in the unpermuted treatment network:\r\n\r\n![compound versus disease degree permuted log priors](https://github.com/dhimmel/learn/blob/0aae784e83cfaa00d1269f17a3bd3a6c6fdc3a0a/prior/viz/log-prob-tiled-empiric-v-perm.png?raw=true)\r\n\r\nNow here is the same plot without transforming the probabilities. Notice the high probabilities for the existence of treatment between high degree compounds and diseases.\r\n\r\n![compound versus disease degree permuted priors](https://github.com/dhimmel/learn/blob/0aae784e83cfaa00d1269f17a3bd3a6c6fdc3a0a/prior/viz/prob-tiled-empiric-v-perm.png?raw=true)\r\n\r\nThe incredible disparity in prior probabilities based on treatment degree reinforces the need to explicitly model the phenomenon.",
      "profile": 17,
      "published": "2016-04-15T21:31:50.795312Z",
      "thread": 201,
      "url": "/discussion/network-edge-prediction-estimating-the-prior/201#2"
    },
    {
      "body_html": "<p>Thanks <a href=\"/u/ostrokach\" class=\"username\">@ostrokach</a>. While <a href=\"#7\">these resources</a> won't make it into the current project (corresponding to Hetionet v1.0), your suggestions will help us in the future.</p>\r\n\r\n<h2>HuDiNe</h2>\r\n\r\n<p>Regarding HuDiNe, I know I looked into the resource in the past. Specifically, I mention it in my Qualifying Exam proposal in 2013 <span class=\"citation\">[<a href=\"/doi/10.6084/m9.figshare.3180142.v1\" class=\"citation\" data-key=\"10.6084/m9.figshare.3180142.v1\">1</a>]</span>:</p>\r\n\r\n<blockquote><p>Disease comorbidity will be extracted from <a href=\"http://hudine.neu.edu/\">HuDiNe</a>, a resource of co-occurring diseases constructed from Medicare claims spanning three years and 13 million patients <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1000353\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1000353\">2</a>]</span>. FDR adjusted p-values from the φ comorbidity statistic — a conservative metric optimized for prevalent diseases — will determine disease-disease edges.</p></blockquote>\r\n\r\n<p>If I remember correctly, I didn't end up including comorbidity relationships from HuDiNe because I had trouble picking an inclusion threshold: prevalent diseases were always comorbid with each other, while rare diseases never had comorbidities. However, as the HuDiNe paper explains they include two measures of comorbidity <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1000353\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1000353\">2</a>]</span>:</p>\r\n\r\n<blockquote><p>We will use two comorbidity measures to quantify the distance between two diseases: The Relative Risk (RR) and φ-correlation (φ). … For example, <em>RR</em> overestimates relationships involving rare diseases and underestimates the comorbidity between highly prevalent illnesses, whereas φ accurately discriminates comorbidities between pairs of diseases of similar prevalence but underestimates the comorbidity between rare and common diseases (see <a href=\"https://doi.org/10.1371/journal.pcbi.1000353.s001\">SM Box 1</a>).</p></blockquote>\r\n\r\n<p>Hence, I think it may make sense for us to revisit this dataset and attempt to pick a threshold that combines information from several <a href=\"http://barabasilab.neu.edu/projects/hudine/resource/data/data.html\" title=\"HuDiNe Data Download Page\">available columns</a> including:</p>\r\n\r\n<ul><li>Relative Risk 99% Conf. Interval</li><li>Phi-correlation</li><li>t-test value</li></ul>\r\n\r\n<p>Intuitively, comorbidity is an ideal relationship for our approach because it's high throughput and could potentially offer orthogonal information to existing relationships.</p>\r\n\r\n<h2>STRIDE</h2>\r\n\r\n<p>STRIDE is definitely be of interest <span class=\"citation\">[<a href=\"/doi/10.1038/sdata.2014.32\" class=\"citation\" data-key=\"10.1038/sdata.2014.32\">3</a>]</span>. The data is licensed as CC0 <span class=\"citation\">[<a href=\"/doi/10.5061/dryad.jp917\" class=\"citation\" data-key=\"10.5061/dryad.jp917\">4</a>]</span>, which is ideal. However, a good deal of exploratory analysis would be needed to determine a processing pipeline and see what types of cooccurring concepts have meaning.</p>",
      "body_md": "Thanks @ostrokach. While [these resources](#7) won't make it into the current project (corresponding to Hetionet v1.0), your suggestions will help us in the future.\r\n\r\n## HuDiNe\r\n\r\nRegarding HuDiNe, I know I looked into the resource in the past. Specifically, I mention it in my Qualifying Exam proposal in 2013 [@10.6084/m9.figshare.3180142.v1]:\r\n\r\n> Disease comorbidity will be extracted from [HuDiNe](http://hudine.neu.edu/), a resource of co-occurring diseases constructed from Medicare claims spanning three years and 13 million patients [@10.1371/journal.pcbi.1000353]. FDR adjusted p-values from the φ comorbidity statistic -- a conservative metric optimized for prevalent diseases -- will determine disease-disease edges.\r\n\r\nIf I remember correctly, I didn't end up including comorbidity relationships from HuDiNe because I had trouble picking an inclusion threshold: prevalent diseases were always comorbid with each other, while rare diseases never had comorbidities. However, as the HuDiNe paper explains they include two measures of comorbidity [@10.1371/journal.pcbi.1000353]:\r\n\r\n> We will use two comorbidity measures to quantify the distance between two diseases: The Relative Risk (RR) and φ-correlation (φ). … For example, _RR_ overestimates relationships involving rare diseases and underestimates the comorbidity between highly prevalent illnesses, whereas φ accurately discriminates comorbidities between pairs of diseases of similar prevalence but underestimates the comorbidity between rare and common diseases (see [SM Box 1](https://doi.org/10.1371/journal.pcbi.1000353.s001)).\r\n\r\nHence, I think it may make sense for us to revisit this dataset and attempt to pick a threshold that combines information from several [available columns](http://barabasilab.neu.edu/projects/hudine/resource/data/data.html \"HuDiNe Data Download Page\") including:\r\n\r\n+ Relative Risk 99% Conf. Interval\r\n+ Phi-correlation\r\n+ t-test value\r\n\r\nIntuitively, comorbidity is an ideal relationship for our approach because it's high throughput and could potentially offer orthogonal information to existing relationships.\r\n\r\n## STRIDE\r\n\r\nSTRIDE is definitely be of interest [@10.1038/sdata.2014.32]. The data is licensed as CC0 [@10.5061/dryad.jp917], which is ideal. However, a good deal of exploratory analysis would be needed to determine a processing pipeline and see what types of cooccurring concepts have meaning.",
      "profile": 17,
      "published": "2016-04-16T19:39:04.496320Z",
      "thread": 22,
      "url": "/discussion/suggestions-for-additional-information-types/22#8"
    },
    {
      "body_html": "<h1>The relationship between midpoint-join runtime and complexity</h1>\r\n\r\n<p>We performed a similar <em>DWPC</em> computation to <a href=\"#7\">before</a> but switched to specifying midpoint join indexes rather than our \"optimal\" join indexes. This time we computed 27,308,958 <em>DWPCs</em> for 3,775 compound–disease pairs × 1,206 metapaths × 6 hetnets. However, a <a href=\"https://github.com/dhimmel/learn/issues/1\" title=\"Failing feature extraction queries due to py2neo's socket timeout\">bug was introduced</a> which made queries taking over 30 seconds fail silently. Therefore, the following features are incomplete and will be prone to runtime underestimation: <em>CbGeAeGaD</em>, <em>CdG&lt;rGeAlD</em>, <em>CdGeAeGaD</em>, <em>CdGeAeGdD</em>, <em>CdGeAeGuD</em>, <em>CuG&lt;rGeAlD</em>, <em>CuGeAeGaD</em>, <em>CuGeAeGdD</em>, <em>CuGeAeGuD</em>, <em>CuGeAuGaD</em>.</p>\r\n\r\n<p><a href=\"/u/alizee\" class=\"username\">@alizee</a>: the switch to midpoint join led to a stronger association between sequential complexity and runtime (<a href=\"http://nbviewer.jupyter.org/github/dhimmel/learn/blob/6775c34768b5d4aaa0f386351a3ffec115548b2f/optimize/time.ipynb\">notebook</a>). Interestingly, switch also improved the fit between optimal complexity and runtime.</p>",
      "body_md": "# The relationship between midpoint-join runtime and complexity\r\n\r\nWe performed a similar _DWPC_ computation to [before](#7) but switched to specifying midpoint join indexes rather than our \"optimal\" join indexes. This time we computed 27,308,958 _DWPCs_ for 3,775 compound–disease pairs × 1,206 metapaths × 6 hetnets. However, a [bug was introduced](https://github.com/dhimmel/learn/issues/1 \"Failing feature extraction queries due to py2neo's socket timeout\") which made queries taking over 30 seconds fail silently. Therefore, the following features are incomplete and will be prone to runtime underestimation: _CbGeAeGaD_, _CdG<rGeAlD_, _CdGeAeGaD_, _CdGeAeGdD_, _CdGeAeGuD_, _CuG<rGeAlD_, _CuGeAeGaD_, _CuGeAeGdD_, _CuGeAeGuD_, _CuGeAuGaD_.\r\n\r\n@alizee: the switch to midpoint join led to a stronger association between sequential complexity and runtime ([notebook](http://nbviewer.jupyter.org/github/dhimmel/learn/blob/6775c34768b5d4aaa0f386351a3ffec115548b2f/optimize/time.ipynb)). Interestingly, switch also improved the fit between optimal complexity and runtime.",
      "profile": 17,
      "published": "2016-04-16T04:40:09.487281Z",
      "thread": 187,
      "url": "/discussion/estimating-the-complexity-of-hetnet-traversal/187#8"
    },
    {
      "body_html": "<h1>Transformation sweep</h1>\r\n\r\n<p>I ran a parameter sweep of transformation options on the all-features dataset (<a href=\"https://github.com/dhimmel/learn/blob/dd860cd19951886f3f82b04487d2faf6fa297e1b/all-features/7-transform.ipynb\">notebook</a>). The sweep used 10-fold cross-validation to select the regularization strength and summarized results over 10 cross-validation random seeds. \"All features\" refers to the dataset that includes all metapaths as features but covers only a portion of compound–disease pairs. These findings are not guaranteed to hold for the all-observations dataset, which we use for predictions.</p>\r\n\r\n<p>The performance results (<a href=\"https://github.com/dhimmel/learn/blob/dd860cd19951886f3f82b04487d2faf6fa297e1b/all-features/data/transformation-sweep.tsv\">table</a>) show that it's important to transform degrees and <em>DWPCs</em>. As <a href=\"/u/alizee\" class=\"username\">@alizee</a> hypothesized, transforming with log1p versus asinh didn't make a big difference. If for simplicity we choose the same transformation for degrees and <em>DWPCs</em>, then asinh ranked higher than log1p. Regarding <em>DWPC</em> scaling, the mean appears to perform better than the standard deviation although the difference is small. I know <a href=\"/u/alizee\" class=\"username\">@alizee</a> thinks the mean is less standard for this purpose, but it seems intuitive for <em>DWPCs</em> which start at zero.</p>\r\n\r\n<p>Hence, I plan to proceed by asinh transforming degrees and mean scaling and asinh transforming <em>DWPCs</em>.</p>",
      "body_md": "# Transformation sweep\r\n\r\nI ran a parameter sweep of transformation options on the all-features dataset ([notebook](https://github.com/dhimmel/learn/blob/dd860cd19951886f3f82b04487d2faf6fa297e1b/all-features/7-transform.ipynb)). The sweep used 10-fold cross-validation to select the regularization strength and summarized results over 10 cross-validation random seeds. \"All features\" refers to the dataset that includes all metapaths as features but covers only a portion of compound--disease pairs. These findings are not guaranteed to hold for the all-observations dataset, which we use for predictions.\r\n\r\nThe performance results ([table](https://github.com/dhimmel/learn/blob/dd860cd19951886f3f82b04487d2faf6fa297e1b/all-features/data/transformation-sweep.tsv)) show that it's important to transform degrees and _DWPCs_. As @alizee hypothesized, transforming with log1p versus asinh didn't make a big difference. If for simplicity we choose the same transformation for degrees and _DWPCs_, then asinh ranked higher than log1p. Regarding _DWPC_ scaling, the mean appears to perform better than the standard deviation although the difference is small. I know @alizee thinks the mean is less standard for this purpose, but it seems intuitive for _DWPCs_ which start at zero.\r\n\r\nHence, I plan to proceed by asinh transforming degrees and mean scaling and asinh transforming _DWPCs_.",
      "profile": 17,
      "published": "2016-04-16T17:46:03.774230Z",
      "thread": 193,
      "url": "/discussion/transforming-dwpcs-for-hetnet-edge-prediction/193#6"
    },
    {
      "body_html": "<p>We have finished building the hetnet for Project Rephetio, which we've named <a href=\"https://github.com/dhimmel/hetionet\">Hetionet</a>. As we gear up for the version 1.0 release, we'd like to provide statistics and visualizations to help users appreciate the network. Here we'll discuss ways to communicate hetnet topology and showcase our current visualizations.</p>\r\n\r\n<p>Here are some points to keep in mind:</p>\r\n\r\n<ul><li>the hetnet, which consists of 47,031 nodes of 11 types and 2,250,197 edges of 24 types, will break most existing visualization software</li><li>we prefer approaches that are automatable: we're looking for sustainable and versatile solutions</li></ul>",
      "body_md": "We have finished building the hetnet for Project Rephetio, which we've named [Hetionet](https://github.com/dhimmel/hetionet). As we gear up for the version 1.0 release, we'd like to provide statistics and visualizations to help users appreciate the network. Here we'll discuss ways to communicate hetnet topology and showcase our current visualizations.\r\n\r\nHere are some points to keep in mind:\r\n\r\n+ the hetnet, which consists of 47,031 nodes of 11 types and 2,250,197 edges of 24 types, will break most existing visualization software\r\n+ we prefer approaches that are automatable: we're looking for sustainable and versatile solutions",
      "profile": 17,
      "published": "2016-04-16T21:39:16.725202Z",
      "thread": 202,
      "url": "/discussion/describing-hetionet-v10-through-visualization-and-statistics/202"
    },
    {
      "body_html": "<h1>Metagraph</h1>\r\n\r\n<p>A metagaph is the graph of types in a hetnet. In Neo4j speak, metagraphs are often referred to as \"data models\". Another synonymous term is \"network schema\". Here is the metagraph for Hetionet v1.0:</p>\r\n\r\n<p><img src=\"https://github.com/dhimmel/rephetio/blob/103054a2bc3f86998fed6cb3753d1ecdb5cbe1e7/figure/metagraph.png?raw=true\" alt=\"Hetionet v1.0 Metagraph\" title=\"Hetionet v1.0 Metagraph\"></p>\r\n\r\n<p>Metagraphs show what types of entities and relationships are included in the network. However by design, they don't provide any information on the actual nodes or edges.</p>",
      "body_md": "# Metagraph\r\n\r\nA metagaph is the graph of types in a hetnet. In Neo4j speak, metagraphs are often referred to as \"data models\". Another synonymous term is \"network schema\". Here is the metagraph for Hetionet v1.0:\r\n\r\n![Hetionet v1.0 Metagraph](https://github.com/dhimmel/rephetio/blob/103054a2bc3f86998fed6cb3753d1ecdb5cbe1e7/figure/metagraph.png?raw=true \"Hetionet v1.0 Metagraph\")\r\n\r\nMetagraphs show what types of entities and relationships are included in the network. However by design, they don't provide any information on the actual nodes or edges.",
      "profile": 17,
      "published": "2016-04-18T13:24:16.554453Z",
      "thread": 202,
      "url": "/discussion/describing-hetionet-v10-through-visualization-and-statistics/202#2"
    },
    {
      "body_html": "<h1>Circular metanode layout</h1>\r\n\r\n<p>One of our primary methods for showing the actual hetnet has been a layout which groups nodes by their type. For each metanode, nodes are laid out in circles. Edges are colored by their type. Here is the circular metanode layout for Hetionet:</p>\r\n\r\n<p><img src=\"https://github.com/dhimmel/rephetio/blob/103054a2bc3f86998fed6cb3753d1ecdb5cbe1e7/figure/hetionet-v1.0-labeled.png?raw=true\" alt=\"Hetionet v1.0 Circular Metanode Layout\" title=\"Hetionet v1.0\"></p>\r\n\r\n<p>This method of visualization gives users a bird's eye view of the hetnet. It begins to show certain summary statistics, such as the number of nodes per metanode. It also weakly illustrates whether a metaedge is concentrated to a few high degree nodes or is well dispersed. However, this visualization is primarily meant to be aesthetic and generally accessible.</p>\r\n\r\n<p>In the past, we've received positive feedback on the circular metanode layout. This <a href=\"https://doi.org/10.1371/journal.pcbi.1004259.g001\" title=\"Fig 1. Heterogeneous network integrates diverse information domains\">visualization for our previous project</a> took 2nd place for the most aesthetically pleasing network visualization in the <a href=\"http://www.cytoscape.org/cy32_launch_challenge.html\">Cytoscape 3.2 Launch Challenge</a>.</p>\r\n\r\n<h2>Methods</h2>\r\n\r\n<p>We create this visualization in <a href=\"http://www.cytoscape.org/\">Cytoscape</a> <span class=\"citation\">[<a href=\"/doi/10.1101/gr.1239303\" class=\"citation\" data-key=\"10.1101/gr.1239303\">1</a>, <a href=\"/doi/10.1093/bioinformatics/btq675\" class=\"citation\" data-key=\"10.1093/bioinformatics/btq675\">2</a>]</span> — a Java-based desktop application for network visualization with strong adoption in biology (current version 3.3.0). Creating this visualization is labor intensive and frustrating, since our hetnets push Cytoscape to its limits.</p>\r\n\r\n<p>To make the visualization possible, we limit the number of edges per type to 5,000 (by setting <a href=\"https://github.com/dhimmel/hetio/blob/2c135fcd32616d6979972105ba60b003d65c98bd/hetio/readwrite.py#L260\" title=\"hetio.readwrite.write_sif\"><code>max_edges = 5000</code></a>). One side effect is that Cytoscape only shows the subset of nodes connected by the selected edge subset. Hence, the visualization moderately reflects the number of nodes per metanode and poorly reflects the number of edges per metaedge.</p>",
      "body_md": "# Circular metanode layout\r\n\r\nOne of our primary methods for showing the actual hetnet has been a layout which groups nodes by their type. For each metanode, nodes are laid out in circles. Edges are colored by their type. Here is the circular metanode layout for Hetionet:\r\n\r\n![Hetionet v1.0 Circular Metanode Layout](https://github.com/dhimmel/rephetio/blob/103054a2bc3f86998fed6cb3753d1ecdb5cbe1e7/figure/hetionet-v1.0-labeled.png?raw=true \"Hetionet v1.0\")\r\n\r\nThis method of visualization gives users a bird's eye view of the hetnet. It begins to show certain summary statistics, such as the number of nodes per metanode. It also weakly illustrates whether a metaedge is concentrated to a few high degree nodes or is well dispersed. However, this visualization is primarily meant to be aesthetic and generally accessible.\r\n\r\nIn the past, we've received positive feedback on the circular metanode layout. This [visualization for our previous project](https://doi.org/10.1371/journal.pcbi.1004259.g001 \"Fig 1. Heterogeneous network integrates diverse information domains\") took 2nd place for the most aesthetically pleasing network visualization in the [Cytoscape 3.2 Launch Challenge](http://www.cytoscape.org/cy32_launch_challenge.html).\r\n\r\n## Methods\r\n\r\nWe create this visualization in [Cytoscape](http://www.cytoscape.org/) [@10.1101/gr.1239303 @10.1093/bioinformatics/btq675] -- a Java-based desktop application for network visualization with strong adoption in biology (current version 3.3.0). Creating this visualization is labor intensive and frustrating, since our hetnets push Cytoscape to its limits.\r\n\r\nTo make the visualization possible, we limit the number of edges per type to 5,000 (by setting [`max_edges = 5000`](https://github.com/dhimmel/hetio/blob/2c135fcd32616d6979972105ba60b003d65c98bd/hetio/readwrite.py#L260 \"hetio.readwrite.write_sif\")). One side effect is that Cytoscape only shows the subset of nodes connected by the selected edge subset. Hence, the visualization moderately reflects the number of nodes per metanode and poorly reflects the number of edges per metaedge.",
      "profile": 17,
      "published": "2016-04-18T14:00:26.867435Z",
      "thread": 202,
      "url": "/discussion/describing-hetionet-v10-through-visualization-and-statistics/202#3"
    },
    {
      "body_html": "<h1>Metapath counts by metanode pairs</h1>\r\n\r\n<p>This is a new visualization we're trying out that is based solely on the <a href=\"#2\">metagraph</a>. The plot shows the number of metapaths (types of paths) that connect a source and target metanode for a given length. The Length 1 condition shows the number of metaedges connecting two nodes. The longer lengths help show the combinatoric explosion in types of connectivity on the hetnet. Here's the graph for Hetionet v1.0 (<a href=\"https://github.com/dhimmel/integrate/blob/b53b835dbac09101c2036328b3fc72644fcb71bc/viz/auto/2-netviz.ipynb\">notebook</a>):</p>\r\n\r\n<p><img src=\"https://github.com/dhimmel/integrate/blob/095747de9efe42a9ed1e23ab18db282e14705cd9/viz/auto/figure/metapath-counts.png?raw=true\" alt=\"Hetionet Metapath Counts\" title=\"Hetionet v1.0 Metapath Counts by Metanode Pairs\"></p>",
      "body_md": "# Metapath counts by metanode pairs\r\n\r\nThis is a new visualization we're trying out that is based solely on the [metagraph](#2). The plot shows the number of metapaths (types of paths) that connect a source and target metanode for a given length. The Length 1 condition shows the number of metaedges connecting two nodes. The longer lengths help show the combinatoric explosion in types of connectivity on the hetnet. Here's the graph for Hetionet v1.0 ([notebook](https://github.com/dhimmel/integrate/blob/b53b835dbac09101c2036328b3fc72644fcb71bc/viz/auto/2-netviz.ipynb)):\r\n\r\n![Hetionet Metapath Counts](https://github.com/dhimmel/integrate/blob/095747de9efe42a9ed1e23ab18db282e14705cd9/viz/auto/figure/metapath-counts.png?raw=true \"Hetionet v1.0 Metapath Counts by Metanode Pairs\")",
      "profile": 17,
      "published": "2016-04-18T14:09:45.060264Z",
      "thread": 202,
      "url": "/discussion/describing-hetionet-v10-through-visualization-and-statistics/202#4"
    },
    {
      "body_html": "<h1>Chord diagram of edges per type</h1>\r\n\r\n<p><a href=\"https://en.wikipedia.org/wiki/Chord_diagram\">Chord diagrams</a>, also called radial network diagrams, consist of nodes laid out as segments in a circle and edges as chords connecting the segments. In our example, metanodes are laid out on along the perimeter with chords corresponding to metaedges:</p>\r\n\r\n<p><img src=\"https://github.com/dhimmel/integrate/blob/095747de9efe42a9ed1e23ab18db282e14705cd9/viz/auto/figure/edge-chord-diagram.png?raw=true\" alt=\"Hetionet Chord Diagram\" title=\"Hetionet v1.0 Chord Diagram\"></p>\r\n\r\n<p>Note that we transform sqaure root transformed the edge count for each metaedge, represented with chord width. The segment width for metanodes does not correspond to the proportion of total nodes which may be slightly confusing.</p>\r\n\r\n<p>Chord diagrams were popularized by the <a href=\"http://circos.ca/\">Circos</a> app <span class=\"citation\">[<a href=\"/doi/10.1101/gr.092759.109\" class=\"citation\" data-key=\"10.1101/gr.092759.109\">1</a>]</span>. We created our visualization the the R <a href=\"https://github.com/jokergoo/circlize\"><code>circlize</code></a> package <span class=\"citation\">[<a href=\"/doi/10.1093/bioinformatics/btu393\" class=\"citation\" data-key=\"10.1093/bioinformatics/btu393\">2</a>]</span> (<a href=\"https://github.com/dhimmel/integrate/blob/b53b835dbac09101c2036328b3fc72644fcb71bc/viz/auto/2-netviz.ipynb\">notebook</a>).</p>\r\n\r\n<h2>Chord diagram of edges?</h2>\r\n\r\n<p>Another option is to explore a chord diagram showing actual edges (see Fig. 13B in <span class=\"citation\">[<a href=\"/doi/10.1109/tvcg.2006.147\" class=\"citation\" data-key=\"10.1109/tvcg.2006.147\">3</a>]</span>). I'm hesitant to invest time here, but let us know if you think a chord diagram of edges is promising.</p>",
      "body_md": "# Chord diagram of edges per type\r\n\r\n[Chord diagrams](https://en.wikipedia.org/wiki/Chord_diagram), also called radial network diagrams, consist of nodes laid out as segments in a circle and edges as chords connecting the segments. In our example, metanodes are laid out on along the perimeter with chords corresponding to metaedges:\r\n\r\n![Hetionet Chord Diagram](https://github.com/dhimmel/integrate/blob/095747de9efe42a9ed1e23ab18db282e14705cd9/viz/auto/figure/edge-chord-diagram.png?raw=true \"Hetionet v1.0 Chord Diagram\")\r\n\r\nNote that we transform sqaure root transformed the edge count for each metaedge, represented with chord width. The segment width for metanodes does not correspond to the proportion of total nodes which may be slightly confusing.\r\n\r\nChord diagrams were popularized by the [Circos](http://circos.ca/) app [@10.1101/gr.092759.109]. We created our visualization the the R [`circlize`](https://github.com/jokergoo/circlize) package [@10.1093/bioinformatics/btu393] ([notebook](https://github.com/dhimmel/integrate/blob/b53b835dbac09101c2036328b3fc72644fcb71bc/viz/auto/2-netviz.ipynb)).\r\n\r\n## Chord diagram of edges?\r\n\r\nAnother option is to explore a chord diagram showing actual edges (see Fig. 13B in [@10.1109/tvcg.2006.147]). I'm hesitant to invest time here, but let us know if you think a chord diagram of edges is promising.",
      "profile": 17,
      "published": "2016-04-18T14:30:25.332207Z",
      "thread": 202,
      "url": "/discussion/describing-hetionet-v10-through-visualization-and-statistics/202#5"
    },
    {
      "body_html": "<h1>Hive plots</h1>\r\n\r\n<p>Martin Krzywinski — creator of Circos which led to the technology for making our <a href=\"#5\">chord diagram</a> — also created a type of visualization called a <a href=\"http://www.hiveplot.net/\">hive plot</a> <span class=\"citation\">[<a href=\"/doi/10.1093/bib/bbr069\" class=\"citation\" data-key=\"10.1093/bib/bbr069\">1</a>]</span>. Hive plots lay nodes out along lines which extend radially from a center point. Edges are drawn as curved lines between nodes. The most mature method for generating hive plots looks to be the <a href=\"https://www.bcgsc.ca/wiki/display/jhive/home\" title=\"jhive - A Java GUI for Hive Plots\"><code>jhive</code></a> Java application.</p>",
      "body_md": "# Hive plots\r\n\r\nMartin Krzywinski -- creator of Circos which led to the technology for making our [chord diagram](#5) -- also created a type of visualization called a [hive plot](http://www.hiveplot.net/) [@10.1093/bib/bbr069]. Hive plots lay nodes out along lines which extend radially from a center point. Edges are drawn as curved lines between nodes. The most mature method for generating hive plots looks to be the [`jhive`](https://www.bcgsc.ca/wiki/display/jhive/home \"jhive - A Java GUI for Hive Plots\") Java application.",
      "profile": 17,
      "published": "2016-04-18T14:43:43.077703Z",
      "thread": 202,
      "url": "/discussion/describing-hetionet-v10-through-visualization-and-statistics/202#6"
    },
    {
      "body_html": "<h1>Hetionet v1.0 type nomenclature</h1>\r\n\r\n<p>We've settled on a final type nomenclature for Hetionet v1.0 (our hetnet for this project). See the following tables:</p>\r\n\r\n<ul><li><a href=\"https://github.com/dhimmel/hetionet/blob/31e6cb3162dee8a06085709e730380b52278e32a/hetnet/neo4j/labels.tsv\" title=\"Hetionet v1.0 metanode table\"><strong>Metanodes</strong></a> where <code>metanode</code> is the primary name, <code>abbreviation</code> is the 1–2 letter abbreviation, and <code>label</code> is the Neo4j node label.</li><li><a href=\"https://github.com/dhimmel/hetionet/blob/31e6cb3162dee8a06085709e730380b52278e32a/describe/edges/metaedge-styles.tsv\" title=\"Hetionet v1.0 styled metaedge table\"><strong>Metaedges</strong></a> where <code>metaedge</code> is the primary name, <code>unicode_metaedge</code> is a styled version of the primary name, <code>standard_metaedge</code> is the primary edge orientation, and <code>inverted</code> indicates the non-primary edge orientation. The remaining columns are <code>abbreviation</code>, <code>standard_abbreviation</code>, <code>source</code>, and <code>target</code>. </li><li><a href=\"https://github.com/dhimmel/hetionet/blob/31e6cb3162dee8a06085709e730380b52278e32a/hetnet/neo4j/types.tsv\" title=\"Hetionet v1.0 Neo4j relationship type mapping\"><strong>Neo4j relationship types</strong></a> where <code>metaedge</code> is the primary name, <code>rel_type</code> is Neo4j relationship type, and <code>direction</code> notes whether edges are bidirectional (<code>both</code>) or directed (<code>forward</code> or <code>backward</code>).</li></ul>\r\n\r\n<h2>Neo4j type nomenclature</h2>\r\n\r\n<p>We conform to the Neo4j style of CamelCase labels and ALL_CAPS relationship types. In addition, Neo4j relationship types are appended with metaedge standard abbreviations. This adds source/target-metanode awareness to relationship types and <a href=\"https://thinklab.com/discussion/using-the-neo4j-graph-database-for-hetnets/112#6\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d112\">enables optimized queries</a>.</p>",
      "body_md": "# Hetionet v1.0 type nomenclature\r\n\r\nWe've settled on a final type nomenclature for Hetionet v1.0 (our hetnet for this project). See the following tables:\r\n\r\n+ [**Metanodes**](https://github.com/dhimmel/hetionet/blob/31e6cb3162dee8a06085709e730380b52278e32a/hetnet/neo4j/labels.tsv \"Hetionet v1.0 metanode table\") where `metanode` is the primary name, `abbreviation` is the 1--2 letter abbreviation, and `label` is the Neo4j node label.\r\n+ [**Metaedges**](https://github.com/dhimmel/hetionet/blob/31e6cb3162dee8a06085709e730380b52278e32a/describe/edges/metaedge-styles.tsv \"Hetionet v1.0 styled metaedge table\") where `metaedge` is the primary name, `unicode_metaedge` is a styled version of the primary name, `standard_metaedge` is the primary edge orientation, and `inverted` indicates the non-primary edge orientation. The remaining columns are `abbreviation`, `standard_abbreviation`, `source`, and `target`. \r\n+ [**Neo4j relationship types**](https://github.com/dhimmel/hetionet/blob/31e6cb3162dee8a06085709e730380b52278e32a/hetnet/neo4j/types.tsv \"Hetionet v1.0 Neo4j relationship type mapping\") where `metaedge` is the primary name, `rel_type` is Neo4j relationship type, and `direction` notes whether edges are bidirectional (`both`) or directed (`forward` or `backward`).\r\n\r\n## Neo4j type nomenclature\r\n\r\nWe conform to the Neo4j style of CamelCase labels and ALL_CAPS relationship types. In addition, Neo4j relationship types are appended with metaedge standard abbreviations. This adds source/target-metanode awareness to relationship types and [enables optimized queries](https://thinklab.com/discussion/using-the-neo4j-graph-database-for-hetnets/112#6).",
      "profile": 17,
      "published": "2016-04-18T17:02:02.916266Z",
      "thread": 162,
      "url": "/discussion/data-nomenclature-naming-and-abbreviating-our-network-types/162#8"
    }
  ],
  "documents": [
    {
      "body_html": "<h1 id=\"abstract\">Abstract</h1>\r\n\r\n<p>This project aims to predict new therapeutic indications for small molecules. We will focus on repurposing drugs for well-studied complex human diseases, relying on recently-available high-throughput data sources. The approach is integrative, seeking to combine multiple information domains through heterogeneous networks and modern machine learning techniques.</p>\r\n\r\n<h1 id=\"objectives\">Objectives</h1>\r\n\r\n<ol><li><p><strong>Create an open resource for integrative drug repurposing.</strong> <br>A slew of bioinformatics resources have recently come online. Yet, these resources frequently rely on different vocabularies and require cleaning. We plan to release a network capturing a systems biology perspective of drug efficacy. Standardized vocabularies will form a network template and will be connected by the results of high-throughput experimentation. Structuring the resource as a network ensures the data is processed and reusable. The complete resource will be posted to our <a href=\"http://het.io\">online portal for heterogeneous data integration</a> under a <a href=\"https://creativecommons.org/licenses/by/4.0/\">CC-BY license</a> and will provide a goto dataset for researchers implementing systems/network/computational pharmacology analyses.</p></li><li><p><strong>Compare and identify influential mechanisms of drug efficacy.</strong> <br>The mechanism of action is poorly understood for many prevalent small molecule therapies. Discovering mechanisms through target identification has proven difficult. High-throughput drug and disease signatures offer an alternative approach for investigating a compound's mechanism of action. We plan to identify influential network connections that underlie drug efficacy. Greater insight into how existing drugs work will help us understand current treatments and predict future treatments. More immediately, our results will compare the informativeness of data sources, providing a solid foundation for computational drug repurposing.</p></li><li><p><strong>Predict probabilities for each small molecule's efficacy in treating each complex disease.</strong> <br>Serendipity was responsible for the discovery of many breakthrough small molecule therapies <span class=\"citation\">[<a href=\"http://www.dialogues-cns.com/publication/the-role-of-serendipity-in-drug-discovery/\" class=\"citation\" data-key=\"Ban06\">1</a>]</span>. Additionally, small molecules frequently treat multiple distinct diseases <span class=\"citation\">[<a href=\"/doi/10.1016/j.drudis.2012.08.005\" class=\"citation\" data-key=\"10.1016/j.drudis.2012.08.005\">2</a>]</span>. Rational systems-based drug repurposing overcomes the inefficiency and unreliability of serendipity, while capturing the benefits of polypharmacology and network pharmacology. From the integrative network, we will predict the probability that a given small molecule will treat a given complex disease. Our predictions will provide pharmacologists with evidence-based drug leads, which could develop into novel approved uses for existing drug.</p></li></ol>\r\n\r\n\r\n\r\n<h1 id=\"background\">Background</h1>\r\n\r\n<p>Pharmaceutical companies seeking to bring a novel therapeutic compound to market face a single digit success rate, price tag in the billions, and duration spanning decades <span class=\"citation\">[<a href=\"/doi/10.1038/nrd1468\" class=\"citation\" data-key=\"10.1038/nrd1468\">3</a>]</span>. The trend in research efficiency is equally grim: the cost of developing a new drug has increased exponentially, doubling approximately every nine years since 1950 <span class=\"citation\">[<a href=\"/doi/10.6084/m9.figshare.937004\" class=\"citation\" data-key=\"10.6084/m9.figshare.937004\">4</a>]</span>.</p>\r\n\r\n<p>Since the 90&#8217;s, the prevailing model of drug discovery has focused on identifying compounds that target a single protein with maximum specificity. Through a molecular, reductionist approach to understanding disease, a plausible target is selected. Drugs are then designed to modulate the target or small molecules with a strong target affinity are identified using high throughput screens. However, overwhelming evidence suggests that the potential of the 'one drug, one target, one disease' approach is limited. Biological systems are characterized by phenotypic robustness: knockout experiments in model organisms reveal that less than one fifth of genes are essential for survival <span class=\"citation\">[<a href=\"/doi/10.1038/nchembio.118\" class=\"citation\" data-key=\"10.1038/nchembio.118\">5</a>]</span>. Similarly, pathology may represent a resilient homeostatic state, resistant to disruptions of a single protein. Approved small molecules affect on average 2.7 known targets, and when accounting for speculative targets that number jumps to 6.3 <span class=\"citation\">[<a href=\"/doi/10.1038/nbt0908-983\" class=\"citation\" data-key=\"10.1038/nbt0908-983\">6</a>]</span>. This promiscuity can play an important role in drug efficacy as exemplified by clozapine which remains the preeminent anti-psychotic drug over compounds engineered to bind a subset of its dozen-plus targets <span class=\"citation\">[<a href=\"/doi/10.1038/nrd1346\" class=\"citation\" data-key=\"10.1038/nrd1346\">7</a>]</span>.</p>\r\n\r\n<p>Uncovering disease therapies that rely on multiple mechanisms, known as <strong>polypharmacology</strong>, requires escaping the limitations of the 'magic bullet' paradigm in favor of a 'magic 00 buckshot' understanding of drug efficacy. An approach called <strong>network pharmacology</strong> seeks to characterize the multitude of corruptions embodying a pathology. With that knowledge, drugs are selected to restore a normal state. Network pharmacology encompasses polypharmacology by evaluating drugs which intervene at multiple points to achieve healthy homeostasis.</p>\r\n\r\n<p>Drug <strong>repurposing</strong> &#8212; identifying novel uses for existing therapeutics &#8212; avoids many pitfalls and challenges of designing drugs from scratch. FDA approved drugs have undergone extensive toxicology profiling during development and safety evaluation in Phase III clinical trials. Given ample time on the market, post-marketing trials and adverse event reporting uncover potential flaws that could lead to withdrawal. The wealth of information surrounding approved drugs creates a favorable outcome for repurposed compounds compared to new molecular entities: time to approval is cut in half to as low as three years <span class=\"citation\">[<a href=\"/doi/10.1038/nrd1468\" class=\"citation\" data-key=\"10.1038/nrd1468\">3</a>]</span>; the success rate of advancing from phase II trials to approval increases from 10 to 25 percent <span class=\"citation\">[<a href=\"http://cen.acs.org/articles/90/i40/Drug-Repurposing.html\" class=\"citation\" data-key=\"Thayer12\">8</a>]</span>; and the average development cost for successful drugs plummets from 1.3 billion to as low as 8.4 million dollars <span class=\"citation\">[<a href=\"http://www.ddw-online.com/media/32/2226/drug-repositioning.pdf\" class=\"citation\" data-key=\"Persidis11\">9</a>]</span>.</p>\r\n\r\n<p>Between 1999 and 2008, more first-in-class small-molecule drugs were discovered with phenotypic screening than target-centric approaches, despite preferential investment towards the later <span class=\"citation\">[<a href=\"/doi/10.1038/nrd3480\" class=\"citation\" data-key=\"10.1038/nrd3480\">10</a>]</span>. The advent of omics-technologies has enabled the quantification of several intermediate phenotypes between a disease's (or drug's) molecular basis and clinical manifestation. Intermediate phenotypes include transcriptional profiles, biological pathways, and genetic susceptibility markers. Traditional phenotypic approaches have focused on <em>in vivo</em> screening to identify compounds that alter a primary clinical indicator. <em>In silico</em> screening that instead relies on intermediate phenotypes offers a less costly and time-consuming way forward. Such approaches are easily amenable to leveraging repurposing, polypharmacology, and network pharmacology.</p>\r\n\r\n<p>We propose an integrative method for repurposing approved small molecules to treat additional complex diseases. The approach relies on characterizing the effect of compounds and diseases using high-throughput resources &#8212; many of which provide intermediate-phenotypic profiles for compounds and diseases &#8212; and, from this information, calculating features that describe specific aspects of a compound-disease relationship. From these features, a machine learning approach identifies the influential mechanisms behind drug efficacy and predicts additional indications for existing drugs.</p>\r\n\r\n<p>We chose to focus on complex diseases because they frequently exhibit:</p>\r\n\r\n<ul><li>poorly understood molecular bases</li><li>modest efficacy of approved therapies</li><li>multifactorial etiologies that highlight multiple modalities for intervention</li><li>good coverage in high-throughput bioinformatic resources</li></ul>\r\n\r\n<p>We chose to focus on small molecules because they exhibit:</p>\r\n\r\n<ul><li>known structures</li><li>greater data-availability than biologics</li><li>promiscuous binding, which enables polypharmacology</li><li>incomplete target knowledge, which can be overcome with phenotypic profiling</li></ul>\r\n\r\n\r\n\r\n\r\n\r\n<h1 id=\"research-plan\">Research Plan</h1>\r\n\r\n<h2 id=\"part-1-resource-construction\">Part 1. Resource Construction</h2>\r\n\r\n<p>First, we will construct a resource that encodes a systems perspective of pathogenesis and pharmacology. We will structure the resource as a network where entities (nodes) are connected by their relationships (edges). Nodes and edges belong to predefined types &#8212; respectively called metanodes (<a href=\"#metanodes\">Table 1</a>) and metaedges (<a href=\"#metaedges\">Table 2</a>). The schematic view showing how types relate is called a metagraph (<a href=\"#metagraph\">Figure 1</a>). </p>\r\n\r\n<a name=\"metanodes\"></a><div class=\"figure\" figure-id=\"metanodes\">\n            <div class=\"signature-3\">\n                <div class=\"figure-title\">Table 1. Metanodes</div>\n                <div class=\"figure-description\"><p>The network will consist of the following node types. Domain-specific vocabularies provide standardized terminologies for each node type.</p></div>\n            </div>\n        <div class=\"figure-content\"><table class=\"table markdown-table\"><thead><tr><th>Type</th><th>Resource</th><th>Cite</th></tr></thead><tbody><tr><td>Compound</td><td><a href=\"http://www.drugbank.ca/\">DrugBank</a></td><td><span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkt1068\" class=\"citation\" data-key=\"10.1093/nar/gkt1068\">11</a>]</span></td></tr><tr><td>Disease</td><td><a href=\"http://disease-ontology.org/\">Disease Ontology</a></td><td><span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkr972\" class=\"citation\" data-key=\"10.1093/nar/gkr972\">12</a>]</span></td></tr><tr><td>Gene</td><td><a href=\"//www.ncbi.nlm.nih.gov/gene\">Entrez Gene</a></td><td><span class=\"citation\">[<a href=\"/doi/10.1093/nar/gki031\" class=\"citation\" data-key=\"10.1093/nar/gki031\">13</a>]</span></td></tr><tr><td>Anatomy</td><td><a href=\"https://uberon.github.io/\">Uberon</a></td><td><span class=\"citation\">[<a href=\"/doi/10.1186/gb-2012-13-1-r5\" class=\"citation\" data-key=\"10.1186/gb-2012-13-1-r5\">14</a>]</span></td></tr><tr><td>Cellular Component</td><td><a href=\"http://geneontology.org/\">Gene Ontology</a></td><td><span class=\"citation\">[<a href=\"/doi/10.1093/nar/gku1179\" class=\"citation\" data-key=\"10.1093/nar/gku1179\">15</a>, <a href=\"/doi/10.1038/75556\" class=\"citation\" data-key=\"10.1038/75556\">16</a>]</span></td></tr><tr><td>Molecular Function</td><td><a href=\"http://geneontology.org/\">Gene Ontology</a></td><td><span class=\"citation\">[<a href=\"/doi/10.1093/nar/gku1179\" class=\"citation\" data-key=\"10.1093/nar/gku1179\">15</a>, <a href=\"/doi/10.1038/75556\" class=\"citation\" data-key=\"10.1038/75556\">16</a>]</span></td></tr><tr><td>Biological Process</td><td><a href=\"http://geneontology.org/\">Gene Ontology</a></td><td><span class=\"citation\">[<a href=\"/doi/10.1093/nar/gku1179\" class=\"citation\" data-key=\"10.1093/nar/gku1179\">15</a>, <a href=\"/doi/10.1038/75556\" class=\"citation\" data-key=\"10.1038/75556\">16</a>]</span></td></tr><tr><td>Perturbation Gene Set</td><td><a href=\"https://www.broadinstitute.org/gsea/msigdb/index.jsp\">Molecular Signatures Database</a> (MSigDB 5.0)</td><td><span class=\"citation\">[<a href=\"/doi/10.1093/bioinformatics/btr260\" class=\"citation\" data-key=\"10.1093/bioinformatics/btr260\">17</a>]</span></td></tr><tr><td>Pathway</td><td><a href=\"https://www.broadinstitute.org/gsea/msigdb/index.jsp\">Molecular Signatures Database</a> (MSigDB 5.0)</td><td><span class=\"citation\">[<a href=\"/doi/10.1093/bioinformatics/btr260\" class=\"citation\" data-key=\"10.1093/bioinformatics/btr260\">17</a>]</span></td></tr><tr><td>Pathway</td><td><a href=\"//www.wikipathways.org/index.php/WikiPathways\">WikiPathways</a></td><td><span class=\"citation\">[<a href=\"/doi/10.1371/journal.pbio.0060184\" class=\"citation\" data-key=\"10.1371/journal.pbio.0060184\">18</a>]</span></td></tr><tr><td>Side Effect</td><td><a href=\"http://www.nlm.nih.gov/pubs/factsheets/umlsmeta.html\">UMLS Metathesaurus</a></td><td><span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkh061\" class=\"citation\" data-key=\"10.1093/nar/gkh061\">19</a>]</span></td></tr><tr><td>Symptom</td><td><a href=\"//www.nlm.nih.gov/mesh/\">MeSH</a></td><td><span class=\"citation\">[<a href=\"/doi/10.1001/jama.1994.03510380059038\" class=\"citation\" data-key=\"10.1001/jama.1994.03510380059038\">20</a>]</span></td></tr></tbody></table></div></div>\r\n\r\n<a name=\"metaedges\"></a><div class=\"figure\" figure-id=\"metaedges\">\n            <div class=\"signature-3\">\n                <div class=\"figure-title\">Table 2. Metaedges</div>\n                <div class=\"figure-description\"><p>The network will consist of the following edge types. High-throughput bioinformatics resources provide the necessary information for connecting nodes.</p></div>\n            </div>\n        <div class=\"figure-content\"><table class=\"table markdown-table\"><thead><tr><th>Source</th><th>Target</th><th>Type</th><th>Resource</th><th>Cite</th></tr></thead><tbody><tr><td>Compound</td><td>Disease</td><td>Indication</td><td><a href=\"http://knowledgemap.mc.vanderbilt.edu/research/content/MEDI\">MEDI</a></td><td><span class=\"citation\">[<a href=\"/doi/10.1136/amiajnl-2012-001431\" class=\"citation\" data-key=\"10.1136/amiajnl-2012-001431\">21</a>]</span></td></tr><tr><td>Compound</td><td>Disease</td><td>Indication</td><td><a href=\"http://ftp.ncbi.nlm.nih.gov/pub/lu/LabeledIn/\">LabeledIn</a></td><td><span class=\"citation\">[<a href=\"/doi/10.1016/j.jbi.2014.08.004\" class=\"citation\" data-key=\"10.1016/j.jbi.2014.08.004\">22</a>, <a href=\"/doi/10.1093/database/bav016\" class=\"citation\" data-key=\"10.1093/database/bav016\">23</a>]</span></td></tr><tr><td>Compound</td><td>Disease</td><td>Indication</td><td>ehrlink</td><td><span class=\"citation\">[<a href=\"/doi/10.1136/amiajnl-2012-000852\" class=\"citation\" data-key=\"10.1136/amiajnl-2012-000852\">24</a>]</span></td></tr><tr><td>Compound</td><td>Disease</td><td>Indication</td><td>PREDICT</td><td><span class=\"citation\">[<a href=\"/doi/10.1021/ci100050t\" class=\"citation\" data-key=\"10.1021/ci100050t\">25</a>]</span></td></tr><tr><td>Compound</td><td>Compound</td><td>Similarity</td><td>Dice index of ECFPs</td><td><span class=\"citation\">[<a href=\"/doi/10.1038/msb.2011.26\" class=\"citation\" data-key=\"10.1038/msb.2011.26\">26</a>]</span></td></tr><tr><td>Compound</td><td>Gene</td><td>Binding</td><td><a href=\"//www.bindingdb.org/bind/index.jsp\">BindingDB</a></td><td><span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkl999\" class=\"citation\" data-key=\"10.1093/nar/gkl999\">27</a>]</span></td></tr><tr><td>Compound</td><td>Gene</td><td>Target</td><td><a href=\"http://www.drugbank.ca/\">DrugBank</a></td><td><span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkt1068\" class=\"citation\" data-key=\"10.1093/nar/gkt1068\">11</a>]</span></td></tr><tr><td>Compound</td><td>Gene</td><td>Expression</td><td><a href=\"http://www.lincscloud.org/l1000/\">LINCS</a></td><td></td></tr><tr><td>Compound</td><td>Side Effect</td><td>Causation</td><td><a href=\"https://www.pharmgkb.org/downloads/\">OFFSIDES</a></td><td><span class=\"citation\">[<a href=\"/doi/10.1126/scitranslmed.3003377\" class=\"citation\" data-key=\"10.1126/scitranslmed.3003377\">28</a>]</span></td></tr><tr><td>Compound</td><td>Side Effect</td><td>Causation</td><td><a href=\"http://sideeffects.embl.de/\">SIDER 4</a></td><td><span class=\"citation\">[<a href=\"/doi/10.1038/msb.2009.98\" class=\"citation\" data-key=\"10.1038/msb.2009.98\">29</a>]</span></td></tr><tr><td>Disease</td><td>Gene</td><td>Variation</td><td><a href=\"https://www.ebi.ac.uk/gwas/\">GWAS Catalog</a></td><td><span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkt1229\" class=\"citation\" data-key=\"10.1093/nar/gkt1229\">30</a>]</span></td></tr><tr><td>Disease</td><td>Gene</td><td>Association</td><td><a href=\"http://diseases.jensenlab.org/Search\">DISEASES</a></td><td><span class=\"citation\">[<a href=\"/doi/10.1016/j.ymeth.2014.11.020\" class=\"citation\" data-key=\"10.1016/j.ymeth.2014.11.020\">31</a>]</span></td></tr><tr><td>Disease</td><td>Gene</td><td>Association</td><td><a href=\"http://www.disgenet.org/web/DisGeNET/menu/home\">DisGeNET</a></td><td><span class=\"citation\">[<a href=\"/doi/10.1093/database/bav028\" class=\"citation\" data-key=\"10.1093/database/bav028\">32</a>]</span></td></tr><tr><td>Disease</td><td>Gene</td><td>Association</td><td><a href=\"http://doa.nubic.northwestern.edu/pages/search.php\">DOAF</a></td><td><span class=\"citation\">[<a href=\"/doi/10.1371/journal.pone.0049686\" class=\"citation\" data-key=\"10.1371/journal.pone.0049686\">33</a>]</span></td></tr><tr><td>Disease</td><td>Gene</td><td>Expression</td><td><a href=\"http://dev.stargeo.io/\">STAR-GEO</a></td><td><span class=\"citation\">[<a href=\"/doi/10.1093/nar/30.1.207\" class=\"citation\" data-key=\"10.1093/nar/30.1.207\">34</a>]</span></td></tr><tr><td>Disease</td><td>Symptom</td><td>Causation</td><td><a href=\"//www.nlm.nih.gov/pubs/factsheets/medline.html\">MEDLINE</a> Cooccurrence</td><td></td></tr><tr><td>Disease</td><td>Anatomy</td><td>Localization</td><td><a href=\"//www.nlm.nih.gov/pubs/factsheets/medline.html\">MEDLINE</a> Cooccurrence</td><td></td></tr><tr><td>Disease</td><td>Disease</td><td>Similarity</td><td><a href=\"//www.nlm.nih.gov/pubs/factsheets/medline.html\">MEDLINE</a> Cooccurrence</td><td></td></tr><tr><td>Gene</td><td>Gene</td><td>Interaction</td><td><a href=\"http://interactome.dfci.harvard.edu/H_sapiens/index.php?page=download\">Human Interactome Project</a></td><td><span class=\"citation\">[<a href=\"/doi/10.1016/j.cell.2014.10.050\" class=\"citation\" data-key=\"10.1016/j.cell.2014.10.050\">35</a>]</span></td></tr><tr><td>Gene</td><td>Gene</td><td>Interaction</td><td><a href=\"http://www.sciencemag.org/content/347/6224/1257601/suppl/DC1\">The Incomplete Interactome</a></td><td><span class=\"citation\">[<a href=\"/doi/10.1126/science.1257601\" class=\"citation\" data-key=\"10.1126/science.1257601\">36</a>]</span></td></tr><tr><td>Gene</td><td>Gene</td><td>Evolution</td><td><a href=\"http://csb.pitt.edu/erc_analysis/Methods.php\">Evolutionary Rate Covariation</a></td><td><span class=\"citation\">[<a href=\"/doi/10.1371/journal.pgen.1004967\" class=\"citation\" data-key=\"10.1371/journal.pgen.1004967\">37</a>]</span></td></tr><tr><td>Gene</td><td>Pathway</td><td>Membership</td><td><a href=\"http://www.wikipathways.org/index.php/WikiPathways\">WikiPathways</a></td><td><span class=\"citation\">[<a href=\"/doi/10.1371/journal.pbio.0060184\" class=\"citation\" data-key=\"10.1371/journal.pbio.0060184\">18</a>]</span></td></tr><tr><td>Gene</td><td>Pathway</td><td>Membership</td><td><a href=\"http://www.broadinstitute.org/gsea/msigdb/index.jsp\">MSigDB 5.0</a></td><td><span class=\"citation\">[<a href=\"/doi/10.1093/bioinformatics/btr260\" class=\"citation\" data-key=\"10.1093/bioinformatics/btr260\">17</a>]</span></td></tr><tr><td>Gene</td><td>Perturbation Gene Set</td><td>Regulation</td><td><a href=\"http://www.broadinstitute.org/gsea/msigdb/index.jsp\">MSigDB 5.0</a></td><td><span class=\"citation\">[<a href=\"/doi/10.1093/bioinformatics/btr260\" class=\"citation\" data-key=\"10.1093/bioinformatics/btr260\">17</a>]</span></td></tr><tr><td>Gene</td><td>Biological Process</td><td>Membership</td><td><a href=\"http://geneontology.org/page/download-annotations\">Gene Ontology annotations</a></td><td><span class=\"citation\">[<a href=\"/doi/10.1093/nar/gku1179\" class=\"citation\" data-key=\"10.1093/nar/gku1179\">15</a>, <a href=\"/doi/10.1038/75556\" class=\"citation\" data-key=\"10.1038/75556\">16</a>]</span></td></tr><tr><td>Gene</td><td>Molecular Function</td><td>Membership</td><td><a href=\"http://geneontology.org/page/download-annotations\">Gene Ontology annotations</a></td><td><span class=\"citation\">[<a href=\"/doi/10.1093/nar/gku1179\" class=\"citation\" data-key=\"10.1093/nar/gku1179\">15</a>, <a href=\"/doi/10.1038/75556\" class=\"citation\" data-key=\"10.1038/75556\">16</a>]</span></td></tr><tr><td>Gene</td><td>Cellular Component</td><td>Membership</td><td><a href=\"http://geneontology.org/page/download-annotations\">Gene Ontology annotations</a></td><td><span class=\"citation\">[<a href=\"/doi/10.1093/nar/gku1179\" class=\"citation\" data-key=\"10.1093/nar/gku1179\">15</a>, <a href=\"/doi/10.1038/75556\" class=\"citation\" data-key=\"10.1038/75556\">16</a>]</span></td></tr><tr><td>Gene</td><td>Anatomy</td><td>Expression</td><td><a href=\"http://bgee.unil.ch/\">Bgee</a></td><td><span class=\"citation\">[<a href=\"/doi/10.1007/978-3-540-69828-9_12\" class=\"citation\" data-key=\"10.1007/978-3-540-69828-9_12\">38</a>]</span></td></tr><tr><td>Gene</td><td>Anatomy</td><td>Expression</td><td><a href=\"http://tissues.jensenlab.org/Search\">TISSUES</a></td><td><span class=\"citation\">[<a href=\"/doi/10.7717/peerj.1054\" class=\"citation\" data-key=\"10.7717/peerj.1054\">39</a>]</span></td></tr></tbody></table></div></div>\r\n\r\n<p>Each node type will be populated using a domain-specific vocabulary (<a href=\"#metanodes\">Table 1</a>). Controlled vocabularies provide a backbone for data integration, ensure entities are conceptually unique, and enable easy annotation for future users. Edges will be extracted from high-throughput bioinformatics resources (<a href=\"#metaedges\">Table 2</a>). We aim to incorporate resources that are high-throughput, high-quality, and publicly-available. When possible, systematic resources that circumvent knowledge biases will be employed.</p>\r\n\r\n<a name=\"metagraph\"></a><div class=\"figure\" figure-id=\"metagraph\"><div class=\"figure-content\"><img src=\"http://think-lab.s3.amazonaws.com/m/figures/3.png\"></div>\n            <div class=\"signature-1\">\n                <div class=\"figure-title\">Figure 1. Metagraph of the heterogeneous network</div>\n                <div class=\"figure-description\"><p>A schematic view of the node and edge types composing the network.</p></div>\n            </div>\n        </div>\r\n\r\n<p>We are <a href=\"http://thinklab.org/p/rephetio/how-should-we-construct-a-catalog-of-drug-indications/21\">currently exploring</a> various resources to provide a high-throughput catalog of indications. Feedback here would be appreciated.</p>\r\n\r\n<h2 id=\"part-2-discovering-mechanisms-of-drug-efficacy\">Part 2. Discovering Mechanisms of Drug Efficacy</h2>\r\n\r\n<p>Features describe the relationship between a compound and disease. Each feature measures a certain aspect of a compound-disease relationship: for example, whether the compound targets a susceptibility gene of the disease or whether the compound downregulates genes that are overexpressed in the disease state. Features that distinguish therapeutic from untherapeutic compound-disease pairs represent mechanisms of drug efficacy. We refer to the discriminatory power of each feature as its performance. The performance of each feature indicates its pharmacological importance. And by comparing performance across features, we can contrast the informativeness of orthogonal domains of information. Finally, features describing the same general relationship but based on different data sources can identify the most informative resource or technology out of many.</p>\r\n\r\n<p>Features will be computed from the network. Each feature will measure the prevalence of a specific type of path between a compound and disease. This approach was initially developed for social network analysis <span class=\"citation\">[<a href=\"/doi/10.1109/ASONAM.2011.112\" class=\"citation\" data-key=\"10.1109/ASONAM.2011.112\">40</a>]</span>, and later adapted by us for predicting disease-associated genes <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1004259\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1004259\">41</a>]</span>. Briefly, the method identifies all paths between a source and target node that follow a specified type of path (metapath). The contribution of each path is weighted by its specificity: paths through high-degree nodes, which are likely to be less informative, are downweighted. The sum of the weighted paths results in a value of 0 or greater, where 0 indicates no connectivity. We plan to use the <a href=\"http://het.io/hnep\"><em>degree-weighted path count</em> metric</a> for computing features <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1004259\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1004259\">41</a>]</span>. The interpretation of a specific feature depends on its corresponding metapath. <a href=\"#metapaths\">Table 3</a> provides example metapaths and describes their pharmacological significance.</p>\r\n\r\n<a name=\"metapaths\"></a><div class=\"figure\" figure-id=\"metapaths\">\n            <div class=\"signature-3\">\n                <div class=\"figure-title\">Table 3. The interpretation of features for select metapaths</div>\n                <div class=\"figure-description\"><p>Features measure the prevalence of a specific metapath between the source compound and target disease. Metapaths are abbreviated using the first letter of each metanode (uppercase) and metaedge (lowercase). Refer to <a href=\"#metagraph\">Figure 1</a> for metanode and metaedge names.</p></div>\n            </div>\n        <div class=\"figure-content\"><table class=\"table markdown-table\"><thead><tr><th>Metapath</th><th>Measures the extent that ...</th></tr></thead><tbody><tr><td><em>CuGdD</em></td><td>genes downregulated by the disease are upregulated by the compound</td></tr><tr><td><em>CtGaD</em></td><td>the compound targets genes associated with the disease</td></tr><tr><td><em>CtGiGaD</em></td><td>the compound targets genes that interact with genes associated with the disease</td></tr><tr><td><em>CcScCiD</em></td><td>the compound shares side effects with compounds indicated for the disease</td></tr><tr><td><em>CtGeTlD</em></td><td>the compound targets genes that are expressed in tissues affected by the disease</td></tr><tr><td><em>CiDmPmD</em></td><td>the compound is indicated for diseases with the same pathophysiology as the target disease</td></tr></tbody></table></div></div>\r\n\r\n<p>Metapath-based approaches have several advantages for predictive data integration including <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1004259\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1004259\">41</a>]</span>:</p>\r\n\r\n<blockquote><ul><li>versatility (most biological phenomena are decomposable into entities connected by relationships)</li><li>scalability (no theoretical limit to metagraph complexity or graph size)</li><li>efficiency (low marginal cost to including an additional network component)</li></ul></blockquote>\r\n\r\n<p>Harnessing these advantages, we hope to evaluate and compare a diverse and broad set of potential mechanisms of efficacy.</p>\r\n\r\n<h2 id=\"part-3-predicting-probabilities-of-efficacy\">Part 3. Predicting Probabilities of Efficacy</h2>\r\n\r\n<p>We plan to predict probabilities of efficacy for compound-disease pairs using heterogeneous network edge prediction <span class=\"citation\">[<a href=\"/doi/10.1109/ASONAM.2011.112\" class=\"citation\" data-key=\"10.1109/ASONAM.2011.112\">40</a>, <a href=\"/doi/10.1371/journal.pcbi.1004259\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1004259\">41</a>]</span>. This approach trains a model from the network-based features and can return a probability of efficacy for any compound-disease pair. Previously, our implementation relied on regularized logistic regression, but <a href=\"https://topepo.github.io/caret/index.html\">modern software packages</a> will allow us to rigorously evaluate a broad range of machine learning algorithms.</p>\r\n\r\n<p><a name=\"open-science\"></a></p>\r\n\r\n<h1 id=\"open-science\">Open science</h1>\r\n\r\n<p>We are committed to a transparent, freely available, reusable, and reproducible scientific process and believe open science can revolutionize medicine <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pbio.1002164\" class=\"citation\" data-key=\"10.1371/journal.pbio.1002164\">42</a>]</span>. To this end, we will release all project related code on <a href=\"https://github.com/dhimmel\">GitHub</a>. Datasets that are too large for GitHub will be published on <a href=\"http://figshare.com/authors/Daniel_Himmelstein/523637\">figshare</a>. All original materials will be released under <a href=\"https://creativecommons.org/licenses/by/4.0/\">CC-BY</a> (requires attribution) or <a href=\"https://creativecommons.org/publicdomain/zero/1.0/\">CC-0</a> (public domain) licences. Derivatives of restrictively-licensed works will be released under the most permissive option available. Our analyses will be made available in real-time using <a href=\"https://pages.github.com/\">GitHub pages</a> to host <a href=\"http://rmarkdown.rstudio.com/\">R Markdown</a> documents and <a href=\"http://ipython.org/notebook.html\">IPython Notebooks</a>. Finally, we plan to follow <a href=\"http://dx.doi.org/10.1371/journal.pcbi.1003285\">10 proposed rules</a> for reproducible research in computational biology <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1003285\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1003285\">43</a>]</span>. </p>\r\n\r\n<h1 id=\"thoughts\">Thoughts?</h1>\r\n\r\n<p>Share your ideas by commenting on an existing discussion or by starting a new thread. Our goal in joining ThinkLab is to generate as much interaction as possible.</p>\r\n\r\n<h1 id=\"team-resources\">Team &amp; Resources</h1>\r\n\r\n<p><a href=\"http://dhimmel.com\">Daniel Himmelstein</a> is a PhD candidate in the <em>Biological &amp; Medical Informatics</em> program at UCSF. Daniel works in the <a href=\"http://baranzinilab.ucsf.edu/\">Sergio Baranzini Lab</a> whose mission is to apply cutting-edge bioinformatic approaches to genomic data, with a focus on multiple sclerosis. Dr. Baranzini has extensive experience with data integration, genomic profiling, and disease bioinformatics.</p>\r\n\r\n<p>UCSF and the surrounding Bay Area are hotspots for drug development and data analytics. The team has access to <a href=\"http://qb3.org/\">QB3</a> resources, which include a <a href=\"http://qb3.ucsf.edu/computing/cluster.html\">computing cluster</a> and <a href=\"https://smdc.ucsf.edu/\">small molecule discovery center</a>.</p>\r\n\r\n<p>This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant Number 1144247. We would like to thank <a href=\"https://www.browserstack.com/\">BrowserStack</a> for providing cross browser testing to help us <em>compatibly</em> share our research.</p>",
      "body_md": "# Abstract\r\n\r\nThis project aims to predict new therapeutic indications for small molecules. We will focus on repurposing drugs for well-studied complex human diseases, relying on recently-available high-throughput data sources. The approach is integrative, seeking to combine multiple information domains through heterogeneous networks and modern machine learning techniques.\r\n\r\n# Objectives\r\n\r\n1. **Create an open resource for integrative drug repurposing.** \r\nA slew of bioinformatics resources have recently come online. Yet, these resources frequently rely on different vocabularies and require cleaning. We plan to release a network capturing a systems biology perspective of drug efficacy. Standardized vocabularies will form a network template and will be connected by the results of high-throughput experimentation. Structuring the resource as a network ensures the data is processed and reusable. The complete resource will be posted to our [online portal for heterogeneous data integration](http://het.io) under a [CC-BY license](https://creativecommons.org/licenses/by/4.0/) and will provide a goto dataset for researchers implementing systems/network/computational pharmacology analyses.\r\n\r\n2. **Compare and identify influential mechanisms of drug efficacy.** \r\nThe mechanism of action is poorly understood for many prevalent small molecule therapies. Discovering mechanisms through target identification has proven difficult. High-throughput drug and disease signatures offer an alternative approach for investigating a compound's mechanism of action. We plan to identify influential network connections that underlie drug efficacy. Greater insight into how existing drugs work will help us understand current treatments and predict future treatments. More immediately, our results will compare the informativeness of data sources, providing a solid foundation for computational drug repurposing.\r\n\r\n3. **Predict probabilities for each small molecule's efficacy in treating each complex disease.** \r\nSerendipity was responsible for the discovery of many breakthrough small molecule therapies [@Ban06]. Additionally, small molecules frequently treat multiple distinct diseases [@10.1016/j.drudis.2012.08.005]. Rational systems-based drug repurposing overcomes the inefficiency and unreliability of serendipity, while capturing the benefits of polypharmacology and network pharmacology. From the integrative network, we will predict the probability that a given small molecule will treat a given complex disease. Our predictions will provide pharmacologists with evidence-based drug leads, which could develop into novel approved uses for existing drug.\r\n\r\n[@Ban06]: http://www.dialogues-cns.com/publication/the-role-of-serendipity-in-drug-discovery/ \"Ban TA (2006) The role of serendipity in drug discovery. Dialogues in clinical neuroscience 8.3: 335.\"\r\n\r\n# Background\r\n\r\nPharmaceutical companies seeking to bring a novel therapeutic compound to market face a single digit success rate, price tag in the billions, and duration spanning decades [@10.1038/nrd1468]. The trend in research efficiency is equally grim: the cost of developing a new drug has increased exponentially, doubling approximately every nine years since 1950 [@10.6084/m9.figshare.937004].\r\n\r\nSince the 90’s, the prevailing model of drug discovery has focused on identifying compounds that target a single protein with maximum specificity. Through a molecular, reductionist approach to understanding disease, a plausible target is selected. Drugs are then designed to modulate the target or small molecules with a strong target affinity are identified using high throughput screens. However, overwhelming evidence suggests that the potential of the 'one drug, one target, one disease' approach is limited. Biological systems are characterized by phenotypic robustness: knockout experiments in model organisms reveal that less than one fifth of genes are essential for survival [@10.1038/nchembio.118]. Similarly, pathology may represent a resilient homeostatic state, resistant to disruptions of a single protein. Approved small molecules affect on average 2.7 known targets, and when accounting for speculative targets that number jumps to 6.3 [@10.1038/nbt0908-983]. This promiscuity can play an important role in drug efficacy as exemplified by clozapine which remains the preeminent anti-psychotic drug over compounds engineered to bind a subset of its dozen-plus targets [@10.1038/nrd1346].\r\n\r\nUncovering disease therapies that rely on multiple mechanisms, known as **polypharmacology**, requires escaping the limitations of the 'magic bullet' paradigm in favor of a 'magic 00 buckshot' understanding of drug efficacy. An approach called **network pharmacology** seeks to characterize the multitude of corruptions embodying a pathology. With that knowledge, drugs are selected to restore a normal state. Network pharmacology encompasses polypharmacology by evaluating drugs which intervene at multiple points to achieve healthy homeostasis.\r\n\r\nDrug **repurposing** -- identifying novel uses for existing therapeutics -- avoids many pitfalls and challenges of designing drugs from scratch. FDA approved drugs have undergone extensive toxicology profiling during development and safety evaluation in Phase III clinical trials. Given ample time on the market, post-marketing trials and adverse event reporting uncover potential flaws that could lead to withdrawal. The wealth of information surrounding approved drugs creates a favorable outcome for repurposed compounds compared to new molecular entities: time to approval is cut in half to as low as three years [@10.1038/nrd1468]; the success rate of advancing from phase II trials to approval increases from 10 to 25 percent [@Thayer12]; and the average development cost for successful drugs plummets from 1.3 billion to as low as 8.4 million dollars [@Persidis11].\r\n\r\nBetween 1999 and 2008, more first-in-class small-molecule drugs were discovered with phenotypic screening than target-centric approaches, despite preferential investment towards the later [@10.1038/nrd3480]. The advent of omics-technologies has enabled the quantification of several intermediate phenotypes between a disease's (or drug's) molecular basis and clinical manifestation. Intermediate phenotypes include transcriptional profiles, biological pathways, and genetic susceptibility markers. Traditional phenotypic approaches have focused on *in vivo* screening to identify compounds that alter a primary clinical indicator. *In silico* screening that instead relies on intermediate phenotypes offers a less costly and time-consuming way forward. Such approaches are easily amenable to leveraging repurposing, polypharmacology, and network pharmacology.\r\n\r\nWe propose an integrative method for repurposing approved small molecules to treat additional complex diseases. The approach relies on characterizing the effect of compounds and diseases using high-throughput resources -- many of which provide intermediate-phenotypic profiles for compounds and diseases -- and, from this information, calculating features that describe specific aspects of a compound-disease relationship. From these features, a machine learning approach identifies the influential mechanisms behind drug efficacy and predicts additional indications for existing drugs.\r\n\r\nWe chose to focus on complex diseases because they frequently exhibit:\r\n\r\n+ poorly understood molecular bases\r\n+ modest efficacy of approved therapies\r\n+ multifactorial etiologies that highlight multiple modalities for intervention\r\n+ good coverage in high-throughput bioinformatic resources\r\n\r\nWe chose to focus on small molecules because they exhibit:\r\n\r\n+ known structures\r\n+ greater data-availability than biologics\r\n+ promiscuous binding, which enables polypharmacology\r\n+ incomplete target knowledge, which can be overcome with phenotypic profiling\r\n\r\n[@Thayer12]: http://cen.acs.org/articles/90/i40/Drug-Repurposing.html \"Thayer AM (2012) Drug Repurposing. Chemical & Engineering News, 90(40), 15-25\"\r\n\r\n[@Persidis11]: http://www.ddw-online.com/media/32/2226/drug-repositioning.pdf \"Persidis, A. (2011). The benefits of drug repositioning. Drug Discovery, 9.\"\r\n\r\n# Research Plan\r\n\r\n## Part 1. Resource Construction\r\n\r\nFirst, we will construct a resource that encodes a systems perspective of pathogenesis and pharmacology. We will structure the resource as a network where entities (nodes) are connected by their relationships (edges). Nodes and edges belong to predefined types -- respectively called metanodes ([Table 1](#metanodes)) and metaedges ([Table 2](#metaedges)). The schematic view showing how types relate is called a metagraph ([Figure 1](#metagraph)). \r\n\r\n[:table](metanodes)\r\n\r\n[:table](metaedges)\r\n\r\nEach node type will be populated using a domain-specific vocabulary ([Table 1](#metanodes)). Controlled vocabularies provide a backbone for data integration, ensure entities are conceptually unique, and enable easy annotation for future users. Edges will be extracted from high-throughput bioinformatics resources ([Table 2](#metaedges)). We aim to incorporate resources that are high-throughput, high-quality, and publicly-available. When possible, systematic resources that circumvent knowledge biases will be employed.\r\n\r\n[:figure](metagraph)\r\n\r\nWe are [currently exploring](http://thinklab.org/p/rephetio/how-should-we-construct-a-catalog-of-drug-indications/21) various resources to provide a high-throughput catalog of indications. Feedback here would be appreciated.\r\n\r\n## Part 2. Discovering Mechanisms of Drug Efficacy\r\n\r\nFeatures describe the relationship between a compound and disease. Each feature measures a certain aspect of a compound-disease relationship: for example, whether the compound targets a susceptibility gene of the disease or whether the compound downregulates genes that are overexpressed in the disease state. Features that distinguish therapeutic from untherapeutic compound-disease pairs represent mechanisms of drug efficacy. We refer to the discriminatory power of each feature as its performance. The performance of each feature indicates its pharmacological importance. And by comparing performance across features, we can contrast the informativeness of orthogonal domains of information. Finally, features describing the same general relationship but based on different data sources can identify the most informative resource or technology out of many.\r\n\r\nFeatures will be computed from the network. Each feature will measure the prevalence of a specific type of path between a compound and disease. This approach was initially developed for social network analysis [@10.1109/ASONAM.2011.112], and later adapted by us for predicting disease-associated genes [@10.1371/journal.pcbi.1004259]. Briefly, the method identifies all paths between a source and target node that follow a specified type of path (metapath). The contribution of each path is weighted by its specificity: paths through high-degree nodes, which are likely to be less informative, are downweighted. The sum of the weighted paths results in a value of 0 or greater, where 0 indicates no connectivity. We plan to use the [*degree-weighted path count* metric](http://het.io/hnep) for computing features [@10.1371/journal.pcbi.1004259]. The interpretation of a specific feature depends on its corresponding metapath. [Table 3](#metapaths) provides example metapaths and describes their pharmacological significance.\r\n\r\n[:table](metapaths)\r\n\r\nMetapath-based approaches have several advantages for predictive data integration including [@10.1371/journal.pcbi.1004259]:\r\n\r\n> - versatility (most biological phenomena are decomposable into entities connected by relationships)\r\n> - scalability (no theoretical limit to metagraph complexity or graph size)\r\n> - efficiency (low marginal cost to including an additional network component)\r\n\r\nHarnessing these advantages, we hope to evaluate and compare a diverse and broad set of potential mechanisms of efficacy.\r\n\r\n## Part 3. Predicting Probabilities of Efficacy\r\n\r\nWe plan to predict probabilities of efficacy for compound-disease pairs using heterogeneous network edge prediction [@10.1109/ASONAM.2011.112 @10.1371/journal.pcbi.1004259]. This approach trains a model from the network-based features and can return a probability of efficacy for any compound-disease pair. Previously, our implementation relied on regularized logistic regression, but [modern software packages](https://topepo.github.io/caret/index.html) will allow us to rigorously evaluate a broad range of machine learning algorithms.\r\n\r\n<a name=\"open-science\"></a>\r\n# Open science\r\n\r\nWe are committed to a transparent, freely available, reusable, and reproducible scientific process and believe open science can revolutionize medicine [@10.1371/journal.pbio.1002164]. To this end, we will release all project related code on [GitHub](https://github.com/dhimmel). Datasets that are too large for GitHub will be published on [figshare](http://figshare.com/authors/Daniel_Himmelstein/523637). All original materials will be released under [CC-BY](https://creativecommons.org/licenses/by/4.0/) (requires attribution) or [CC-0](https://creativecommons.org/publicdomain/zero/1.0/) (public domain) licences. Derivatives of restrictively-licensed works will be released under the most permissive option available. Our analyses will be made available in real-time using [GitHub pages](https://pages.github.com/) to host [R Markdown](http://rmarkdown.rstudio.com/) documents and [IPython Notebooks](http://ipython.org/notebook.html). Finally, we plan to follow [10 proposed rules](http://dx.doi.org/10.1371/journal.pcbi.1003285) for reproducible research in computational biology [@10.1371/journal.pcbi.1003285]. \r\n\r\n# Thoughts?\r\nShare your ideas by commenting on an existing discussion or by starting a new thread. Our goal in joining ThinkLab is to generate as much interaction as possible.\r\n\r\n# Team & Resources\r\n\r\n[Daniel Himmelstein](http://dhimmel.com) is a PhD candidate in the *Biological & Medical Informatics* program at UCSF. Daniel works in the [Sergio Baranzini Lab](http://baranzinilab.ucsf.edu/) whose mission is to apply cutting-edge bioinformatic approaches to genomic data, with a focus on multiple sclerosis. Dr. Baranzini has extensive experience with data integration, genomic profiling, and disease bioinformatics.\r\n\r\nUCSF and the surrounding Bay Area are hotspots for drug development and data analytics. The team has access to [QB3](http://qb3.org/) resources, which include a [computing cluster](http://qb3.ucsf.edu/computing/cluster.html) and [small molecule discovery center](https://smdc.ucsf.edu/).\r\n\r\nThis material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant Number 1144247. We would like to thank [BrowserStack](https://www.browserstack.com/) for providing cross browser testing to help us *compatibly* share our research.",
      "doc_published": "2015-01-12T22:40:57Z",
      "doi": "10.15363/thinklab.a5",
      "intro_html": "",
      "intro_md": "",
      "title": "",
      "topic_field": "",
      "url": "/p/rephetio/proposal",
      "views": 810
    },
    {
      "body_html": "<h1>Project components</h1>\r\n\r\n<h2>Hetnets</h2>\r\n\r\n<p>Hetnets are networks with multiple node and edge types <span class=\"citation\">[<a href=\"/discussion/renaming-heterogeneous-networks-to-a-more-concise-and-catchy-term/104\" class=\"citation\" data-key=\"10.15363/thinklab.d104\">1</a>]</span>. Hetnets excel at data integration and are a versatile and intuitive data structure. While specific incarnations of hetnets have long existed, such as bipartite or property graphs, general algorithms that accommodate the multiple types are a recent development <span class=\"citation\">[<a href=\"/doi/10.2200/S00433ED1V01Y201207DMK005\" class=\"citation\" data-key=\"10.2200/S00433ED1V01Y201207DMK005\">2</a>]</span>. Our research tries to predict edges on hetnets, using an algorithm originally developed for social network analysis <span class=\"citation\">[<a href=\"/doi/10.1109/ASONAM.2011.112\" class=\"citation\" data-key=\"10.1109/ASONAM.2011.112\">3</a>]</span>. Previously, we predicted disease–gene associations <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1004259\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1004259\">4</a>]</span>. Here, we predict repurposing drugs.</p>\r\n\r\n<h2>Network construction</h2>\r\n\r\n<p>We've constructed a state of the art hetnet for drug repurposing called <a href=\"https://github.com/dhimmel/hetionet\">Hetionet</a>. The network contains 47,031 nodes of 11 types and 2,250,197 edges of 24 types. The schema is shown in the metagraph below:</p>\r\n\r\n<p><img src=\"https://github.com/dhimmel/rephetio/blob/103054a2bc3f86998fed6cb3753d1ecdb5cbe1e7/figure/metagraph.png?raw=true\" alt=\"\"></p>\r\n\r\n<p>Nodes are identified using standardized terminologies to facilitate integration and prevent duplication. Edges are integrated from high-throughput databases, which were chosen for their quality, reusability, throughput, and relevance to pharmacology. We thank the community for <a href=\"http://thinklab.com/d/22\">helping us</a> identify the most appropriate resources.</p>\r\n\r\n<p>In general, we've dedicated a single GitHub repository to each resource. Versioning is accomplished using commit specific URLs. We expect several of these repositories to be helpful outside of this project. Examples include our analysis of LINCS L1000 <span class=\"citation\">[<a href=\"/discussion/computing-consensus-transcriptional-profiles-for-lincs-l1000-perturbations/43\" class=\"citation\" data-key=\"10.15363/thinklab.d43\">5</a>]</span>.</p>\r\n\r\n<h2>Indications catalog</h2>\r\n\r\n<p>Our approach requires a catalog of indications (compound–diseases treatments) for training and testing. Unfortunately, there was no open and structured catalog of indications, so we <a href=\"http://thinklab.com/d/21#21\">created our own</a> by combining four resources. We are <a href=\"http://thinklab.com/d/95\">now having physicians</a> curate the automated compilation to separate disease-modifying indications from symptomatic and non indications.</p>\r\n\r\n<h2>Neo4j</h2>\r\n\r\n<p>We use neo4j to store and operate on our hetnet <span class=\"citation\">[<a href=\"/discussion/using-the-neo4j-graph-database-for-hetnets/112\" class=\"citation\" data-key=\"10.15363/thinklab.d112\">6</a>]</span>. Neo4j is a powerful graph database. In addition our <a href=\"https://github.com/dhimmel/hetio\">hetio</a> python package provides an additional layer of functionality. Our project has led to the first public examples of duplicate node exclusion <span class=\"citation\">[<a href=\"/discussion/path-exclusion-conditions/134\" class=\"citation\" data-key=\"10.15363/thinklab.d134\">7</a>]</span> and network permutation <span class=\"citation\">[<a href=\"/discussion/permuting-hetnets-and-implementing-randomized-edge-swaps-in-cypher/136\" class=\"citation\" data-key=\"10.15363/thinklab.d136\">8</a>]</span> in the cypher query language.</p>\r\n\r\n<h2>Open science</h2>\r\n\r\n<p>We're <a href=\"http://thinklab.com/d/23\">committed</a> to making this project as useful to the community as possible. Therefore, the project is entirely open notebook. We strive to share all outputs upon their creation, under permissive open licenses such as CC0 or CC-BY. Furthermore, we have devoted considerable effort to handling data copyright complications <span class=\"citation\">[<a href=\"/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107\" class=\"citation\" data-key=\"10.15363/thinklab.d107\">9</a>]</span>. Much of this effort has been to save our downstream users the hassle. In other words, were we not compiling an open resource our legal burden would have been much diminished.</p>\r\n\r\n<h2>Mechanisms of efficacy</h2>\r\n\r\n<p>Stay tuned for our investigation into which data sources are informative of drug efficacy.</p>\r\n\r\n<h2>Drug repurposing predictions</h2>\r\n\r\n<p>Stay tuned for our predictions of the probability that a given small molecule treats a given complex disease.</p>",
      "body_md": "# Project components\r\n\r\n## Hetnets\r\n\r\nHetnets are networks with multiple node and edge types [@10.15363/thinklab.d104]. Hetnets excel at data integration and are a versatile and intuitive data structure. While specific incarnations of hetnets have long existed, such as bipartite or property graphs, general algorithms that accommodate the multiple types are a recent development [@10.2200/S00433ED1V01Y201207DMK005]. Our research tries to predict edges on hetnets, using an algorithm originally developed for social network analysis [@10.1109/ASONAM.2011.112]. Previously, we predicted disease--gene associations [@10.1371/journal.pcbi.1004259]. Here, we predict repurposing drugs.\r\n\r\n## Network construction\r\n\r\nWe've constructed a state of the art hetnet for drug repurposing called [Hetionet](https://github.com/dhimmel/hetionet). The network contains 47,031 nodes of 11 types and 2,250,197 edges of 24 types. The schema is shown in the metagraph below:\r\n\r\n![](https://github.com/dhimmel/rephetio/blob/103054a2bc3f86998fed6cb3753d1ecdb5cbe1e7/figure/metagraph.png?raw=true)\r\n\r\nNodes are identified using standardized terminologies to facilitate integration and prevent duplication. Edges are integrated from high-throughput databases, which were chosen for their quality, reusability, throughput, and relevance to pharmacology. We thank the community for [helping us](http://thinklab.com/d/22) identify the most appropriate resources.\r\n\r\nIn general, we've dedicated a single GitHub repository to each resource. Versioning is accomplished using commit specific URLs. We expect several of these repositories to be helpful outside of this project. Examples include our analysis of LINCS L1000 [@10.15363/thinklab.d43].\r\n\r\n## Indications catalog\r\n\r\nOur approach requires a catalog of indications (compound--diseases treatments) for training and testing. Unfortunately, there was no open and structured catalog of indications, so we [created our own](http://thinklab.com/d/21#21) by combining four resources. We are [now having physicians](http://thinklab.com/d/95) curate the automated compilation to separate disease-modifying indications from symptomatic and non indications.\r\n\r\n## Neo4j\r\n\r\nWe use neo4j to store and operate on our hetnet [@10.15363/thinklab.d112]. Neo4j is a powerful graph database. In addition our [hetio](https://github.com/dhimmel/hetio) python package provides an additional layer of functionality. Our project has led to the first public examples of duplicate node exclusion [@10.15363/thinklab.d134] and network permutation [@10.15363/thinklab.d136] in the cypher query language.\r\n\r\n## Open science\r\n\r\nWe're [committed](http://thinklab.com/d/23) to making this project as useful to the community as possible. Therefore, the project is entirely open notebook. We strive to share all outputs upon their creation, under permissive open licenses such as CC0 or CC-BY. Furthermore, we have devoted considerable effort to handling data copyright complications [@10.15363/thinklab.d107]. Much of this effort has been to save our downstream users the hassle. In other words, were we not compiling an open resource our legal burden would have been much diminished.\r\n\r\n## Mechanisms of efficacy\r\n\r\nStay tuned for our investigation into which data sources are informative of drug efficacy.\r\n\r\n## Drug repurposing predictions\r\n\r\nStay tuned for our predictions of the probability that a given small molecule treats a given complex disease.",
      "doc_published": "2016-01-06T02:39:22.551877Z",
      "doi": "10.15363/thinklab.a9",
      "intro_html": "",
      "intro_md": "",
      "title": "",
      "topic_field": "",
      "url": "/p/rephetio",
      "views": 1
    }
  ],
  "notes": [
    {
      "added": "2015-01-23T05:05:37.375664Z",
      "body_html": "<p>I don't see the \"attached reference\". Can you link to it or add a citation? Check out the <a href=\"http://thinklab.com/help/writing-in-markdown\">markdown syntax</a> offered by ThinkLab.</p>",
      "body_md": "I don't see the \"attached reference\". Can you link to it or add a citation? Check out the [markdown syntax](http://thinklab.com/help/writing-in-markdown) offered by ThinkLab.",
      "comment": 38,
      "profile": 17,
      "url": "/discussion/seeking-an-open-source-implementation-of-the-gerstein-sonnhammer-chothia-algorithm/25#note-15"
    },
    {
      "added": "2015-01-23T19:27:36.905732Z",
      "body_html": "<p>I'm talking about the reference you provided, whose <a href=\"https://pdf.yt/d/Sx3jMbr8vANgxAej/download\">link</a> is in the code.</p>",
      "body_md": "I'm talking about the reference you provided, whose [link](https://pdf.yt/d/Sx3jMbr8vANgxAej/download) is in the code.",
      "comment": 38,
      "profile": 23,
      "url": "/discussion/seeking-an-open-source-implementation-of-the-gerstein-sonnhammer-chothia-algorithm/25#note-21"
    },
    {
      "added": "2015-01-25T17:28:47.470401Z",
      "body_html": "<p>Okay, I made some code organization and documentation changes. My first github fork and <a href=\"https://github.com/antoine-lizee/R-GSC/pull/1\">pull request</a> (:</p>",
      "body_md": "Okay, I made some code organization and documentation changes. My first github fork and [pull request](https://github.com/antoine-lizee/R-GSC/pull/1) (:",
      "comment": 45,
      "profile": 17,
      "url": "/discussion/seeking-an-open-source-implementation-of-the-gerstein-sonnhammer-chothia-algorithm/25#note-26"
    },
    {
      "added": "2015-01-26T23:18:48.088353Z",
      "body_html": "<p>Sorry, I saw your request too late to make a merge efficient, but I implemented your small formal changes that improve readability. Thanks!</p>",
      "body_md": "Sorry, I saw your request too late to make a merge efficient, but I implemented your small formal changes that improve readability. Thanks!",
      "comment": 45,
      "profile": 23,
      "url": "/discussion/seeking-an-open-source-implementation-of-the-gerstein-sonnhammer-chothia-algorithm/25#note-30"
    },
    {
      "added": "2015-02-11T03:49:38.622777Z",
      "body_html": "<p>It seems appropriate that some of these things be put in the research plan. For example, that you plan to follow the 10 rules of reproducible computational research.</p>",
      "body_md": "It seems appropriate that some of these things be put in the research plan. For example, that you plan to follow the 10 rules of reproducible computational research.",
      "comment": 49,
      "profile": 2,
      "url": "/discussion/enabling-reproducibility-and-reuse/23#note-31"
    },
    {
      "added": "2015-02-17T00:28:30.913847Z",
      "body_html": "<span><strong>markdown error</strong>: <a href=\"/u/jspauld\" class=\"username\">@jspauld</a>, give me my bold! See SPL-X bullet.</span>",
      "body_md": "**markdown error**: @jspauld, give me my bold! See SPL-X bullet.",
      "comment": 29,
      "profile": 17,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#note-32"
    },
    {
      "added": "2015-02-17T00:57:39.879533Z",
      "body_html": "<p>Yep, will fix.</p>",
      "body_md": "Yep, will fix.",
      "comment": 29,
      "profile": 2,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#note-33"
    },
    {
      "added": "2015-02-17T13:52:36.308343Z",
      "body_html": "<p>Thanks! For things that require significant cluster usage I haven't yet found a cost effective way to do this either outside of a grant from one of the cloud providers. I wonder if you could apply to amazon or google for access to compute instances for this purpose.</p>",
      "body_md": "Thanks! For things that require significant cluster usage I haven't yet found a cost effective way to do this either outside of a grant from one of the cloud providers. I wonder if you could apply to amazon or google for access to compute instances for this purpose.",
      "comment": 57,
      "profile": 22,
      "url": "/discussion/enabling-reproducibility-and-reuse/23#note-38"
    },
    {
      "added": "2015-02-27T22:53:29.690324Z",
      "body_html": "<p>The authors do not plan on releasing the resource-specific indication data for the current MEDI database. However, they will consider doing so for future releases.</p>",
      "body_md": "The authors do not plan on releasing the resource-specific indication data for the current MEDI database. However, they will consider doing so for future releases.",
      "comment": 63,
      "profile": 17,
      "url": "/discussion/medi-indications-data-discrepancy-in-resource-specific-counts/31#note-54"
    },
    {
      "added": "2015-03-18T22:52:13.592863Z",
      "body_html": "<span><a href=\"/u/b_good\" class=\"username\">@b_good</a>, thanks for the suggestion. I am excited about any venues where we can publish the data to increase its reuse. Once we complete the data integration stage, I will touch base with you — it's still not entirely clear to me what exactly we would upload.</span>",
      "body_md": "@b_good, thanks for the suggestion. I am excited about any venues where we can publish the data to increase its reuse. Once we complete the data integration stage, I will touch base with you -- it's still not entirely clear to me what exactly we would upload.",
      "comment": 82,
      "profile": 17,
      "url": "/discussion/enabling-reproducibility-and-reuse/23#note-57"
    },
    {
      "added": "2015-03-19T23:42:23.213598Z",
      "body_html": "<p>Here's a <a href=\"http://thinklab.com/discussion/tissue-node/41\">link to the new discussion</a> (Venkat, it might be a good idea to edit your post to add this link)</p>",
      "body_md": "Here's a [link to the new discussion](http://thinklab.com/discussion/tissue-node/41) (Venkat, it might be a good idea to edit your post to add this link)",
      "comment": 85,
      "profile": 2,
      "url": "/discussion/suggestions-for-additional-information-types/22#note-58"
    },
    {
      "added": "2015-03-20T02:37:46.580338Z",
      "body_html": "<span><a href=\"/u/vsmalladi\" class=\"username\">@vsmalladi</a>, you may consider switching the associated publication to the Uberon paper <span class=\"citation\">[<a href=\"/doi/10.1186/gb-2012-13-1-r5\" class=\"citation\" data-key=\"10.1186/gb-2012-13-1-r5\">1</a>]</span> and move the ENCODE use-case paper <span class=\"citation\">[<a href=\"/doi/10.1093/database/bav010\" class=\"citation\" data-key=\"10.1093/database/bav010\">2</a>]</span> as an <a href=\"http://thinklab.com/help/writing-in-markdown\">inline citation</a>.</span>",
      "body_md": "@vsmalladi, you may consider switching the associated publication to the Uberon paper [@10.1186/gb-2012-13-1-r5] and move the ENCODE use-case paper [@10.1093/database/bav010] as an [inline citation](http://thinklab.com/help/writing-in-markdown).",
      "comment": 84,
      "profile": 17,
      "url": "/discussion/tissue-node/41#note-59"
    },
    {
      "added": "2015-03-20T02:38:37.568436Z",
      "body_html": "<span><a href=\"/u/jspauld\" class=\"username\">@jspauld</a>, see the citation display failure in the previous note.</span>",
      "body_md": "@jspauld, see the citation display failure in the previous note.",
      "comment": 84,
      "profile": 17,
      "url": "/discussion/tissue-node/41#note-60"
    },
    {
      "added": "2015-03-20T04:07:52.432555Z",
      "body_html": "<span><a href=\"/u/dhimmel\" class=\"username\">@dhimmel</a> Yeah, the inline citations don't work in the notes right now. I suspect it will be rare that people will try to use them in notes given that most of the substantive conversation is intended to take place in comments. So, for the time being I think I'll wait and see how much demand there will be. [<strong>Edit:</strong> After speaking with <a href=\"/u/dhimmel\" class=\"username\">@dhimmel</a> offline he has convinced me this was a bug. Inline note citations now work!]</span>",
      "body_md": "@dhimmel Yeah, the inline citations don't work in the notes right now. I suspect it will be rare that people will try to use them in notes given that most of the substantive conversation is intended to take place in comments. So, for the time being I think I'll wait and see how much demand there will be. [**Edit:** After speaking with @dhimmel offline he has convinced me this was a bug. Inline note citations now work!]",
      "comment": 84,
      "profile": 2,
      "url": "/discussion/tissue-node/41#note-61"
    },
    {
      "added": "2015-03-20T04:09:55.058400Z",
      "body_html": "<p>In support of the semantic web, you should check out ThinkLab's awesome <a href=\"http://thinklab.com/help/writing-in-markdown\">markdown syntax</a>, which includes easy citation and hyperlink capabilities.</p>",
      "body_md": "In support of the semantic web, you should check out ThinkLab's awesome [markdown syntax](http://thinklab.com/help/writing-in-markdown), which includes easy citation and hyperlink capabilities.",
      "comment": 83,
      "profile": 17,
      "url": "/discussion/enabling-reproducibility-and-reuse/23#note-62"
    },
    {
      "added": "2015-03-20T04:11:01.433966Z",
      "body_html": "<p>With regard to the associated publication — at the present moment this cannot be changed by the user. <a href=\"/u/vsmalladi\" class=\"username\">@vsmalladi</a>, if you agree it makes sense to change it please just let me know and I'll take care of it.</p>",
      "body_md": "With regard to the associated publication -- at the present moment this cannot be changed by the user. @vsmalladi, if you agree it makes sense to change it please just let me know and I'll take care of it.",
      "comment": 84,
      "profile": 2,
      "url": "/discussion/tissue-node/41#note-63"
    },
    {
      "added": "2015-03-20T04:48:01.252427Z",
      "body_html": "<span><a href=\"/u/jspauld\" class=\"username\">@jspauld</a> yes please update the citation</span>",
      "body_md": "@jspauld yes please update the citation",
      "comment": 84,
      "profile": 35,
      "url": "/discussion/tissue-node/41#note-64"
    },
    {
      "added": "2015-03-20T05:47:50.995793Z",
      "body_html": "<p>Updated. If you'd like to do an inline citation to the original DOI you were referencing just insert the following in your markdown: <code>[@10.1093/database/bav010]</code></p>",
      "body_md": "Updated. If you'd like to do an inline citation to the original DOI you were referencing just insert the following in your markdown: `[@10.1093/database/bav010]`",
      "comment": 84,
      "profile": 2,
      "url": "/discussion/tissue-node/41#note-66"
    },
    {
      "added": "2015-03-20T06:31:04.885382Z",
      "body_html": "<p>FYI, it looks like you mentioned the wrong person here. I'm assuming you meant <a href=\"/u/dhimmel\" class=\"username\">@dhimmel</a> .. Any idea how that happened?</p>",
      "body_md": "FYI, it looks like you mentioned the wrong person here. I'm assuming you meant @dhimmel .. Any idea how that happened?",
      "comment": 88,
      "profile": 2,
      "url": "/discussion/tissue-node/41#note-67"
    },
    {
      "added": "2015-03-20T15:39:36.836092Z",
      "body_html": "<span><a href=\"/u/jspauld\" class=\"username\">@jspauld</a> I believe this was just user error. </span>",
      "body_md": "@jspauld I believe this was just user error. ",
      "comment": 88,
      "profile": 35,
      "url": "/discussion/tissue-node/41#note-68"
    },
    {
      "added": "2015-03-29T19:04:10.713623Z",
      "body_html": "<p>Saw this paper <span class=\"citation\">[<a href=\"/doi/10.1371/journal.pcbi.1004068\" class=\"citation\" data-key=\"10.1371/journal.pcbi.1004068\">1</a>]</span> which explores when disease/compound expression profiles are reliable therapeutic indicators, specifically with regards to lung cancer.</p>",
      "body_md": "Saw this paper [@10.1371/journal.pcbi.1004068] which explores when disease/compound expression profiles are reliable therapeutic indicators, specifically with regards to lung cancer.",
      "comment": 93,
      "profile": 17,
      "url": "/discussion/computing-consensus-transcriptional-profiles-for-lincs-l1000-perturbations/43#note-69"
    },
    {
      "added": "2015-04-02T01:20:01.294752Z",
      "body_html": "<p>The markdown error has been fixed.</p>",
      "body_md": "The markdown error has been fixed.",
      "comment": 29,
      "profile": 2,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#note-71"
    },
    {
      "added": "2015-04-02T21:47:12.107325Z",
      "body_html": "<p>For future reference it's probably better to wait until you have more to say here before you post this. The project's followers probably don't need to know what you are planning to post here (until you actually post it!) What do you think? And yes, 'draft mode' is coming soon!</p>",
      "body_md": "For future reference it's probably better to wait until you have more to say here before you post this. The project's followers probably don't need to know what you are planning to post here (until you actually post it!) What do you think? And yes, 'draft mode' is coming soon!",
      "comment": 97,
      "profile": 2,
      "url": "/discussion/processing-labeledin-to-extract-indications/46#note-75"
    },
    {
      "added": "2015-04-02T21:59:36.572209Z",
      "body_html": "<p>Hey Jesse, I was making a stub because one project member may be interested in posting and I thought this would simplify the process.</p>",
      "body_md": "Hey Jesse, I was making a stub because one project member may be interested in posting and I thought this would simplify the process.",
      "comment": 97,
      "profile": 17,
      "url": "/discussion/processing-labeledin-to-extract-indications/46#note-76"
    },
    {
      "added": "2015-04-02T22:05:48.541871Z",
      "body_html": "<p>Oh alright, carry on then :)</p>",
      "body_md": "Oh alright, carry on then :)",
      "comment": 97,
      "profile": 2,
      "url": "/discussion/processing-labeledin-to-extract-indications/46#note-77"
    },
    {
      "added": "2015-04-03T05:28:22.756037Z",
      "body_html": "<p>Ah.. guessing this is the reason behind <a href=\"http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21\" target=\"_blank\">http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21</a> </p>",
      "body_md": "Ah.. guessing this is the reason behind http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21 ",
      "comment": 99,
      "profile": 48,
      "url": "/discussion/evaluation-framework/47#note-78"
    },
    {
      "added": "2015-04-03T18:01:46.261151Z",
      "body_html": "<span><a href=\"/u/ritukhare\" class=\"username\">@ritukhare</a>, if you have a readily-available and exhaustive mapping of ingredient and disease identifiers to names, I could update my analysis with those names.</span>",
      "body_md": "@ritukhare, if you have a readily-available and exhaustive mapping of ingredient and disease identifiers to names, I could update my analysis with those names.",
      "comment": 103,
      "profile": 17,
      "url": "/discussion/processing-labeledin-to-extract-indications/46#note-79"
    },
    {
      "added": "2015-04-08T02:11:06.726415Z",
      "body_html": "<p>Added the reference to my initial post. Given the quality issues, I do not plan to include this resource in our gold standard set of indications. It could be helpful later as a literature-derived set of <em>potential</em> indications.</p>",
      "body_md": "Added the reference to my initial post. Given the quality issues, I do not plan to include this resource in our gold standard set of indications. It could be helpful later as a literature-derived set of *potential* indications.",
      "comment": 110,
      "profile": 17,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#note-82"
    },
    {
      "added": "2015-04-09T01:30:54.293110Z",
      "body_html": "<p>+1 for grant agencies to fund this type of activity</p>",
      "body_md": "+1 for grant agencies to fund this type of activity",
      "comment": 115,
      "profile": 17,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#note-83"
    },
    {
      "added": "2015-04-09T02:27:27.578532Z",
      "body_html": "<p>Thanks for the clarification. We also plan to perform some indication propagation on the Disease Ontology hierarchy.</p>",
      "body_md": "Thanks for the clarification. We also plan to perform some indication propagation on the Disease Ontology hierarchy.",
      "comment": 128,
      "profile": 17,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#note-84"
    },
    {
      "added": "2015-04-21T20:03:30.075789Z",
      "body_html": "<p>The 'notebook' link is broken here</p>",
      "body_md": "The 'notebook' link is broken here",
      "comment": 148,
      "profile": 2,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#note-87"
    },
    {
      "added": "2015-04-21T20:50:45.888240Z",
      "body_html": "<p>Fixed, thanks</p>",
      "body_md": "Fixed, thanks",
      "comment": 148,
      "profile": 17,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#note-88"
    },
    {
      "added": "2015-05-01T22:06:14.259531Z",
      "body_html": "<p>Looks like this is all term type definitions across the entire UMLS.</p>",
      "body_md": "Looks like this is all term type definitions across the entire UMLS.",
      "comment": 183,
      "profile": 17,
      "url": "/discussion/retrieving-the-ingredients-in-rxnorm-concepts/61#note-96"
    },
    {
      "added": "2015-05-01T22:27:00.760636Z",
      "body_html": "<p>Awesome, you have set the train in motion for us to include ehrlink indications. Let's move discussion to <a href=\"http://thinklab.com/discussion/extracting-indications-from-the-ehrlink-resource/62\">this new thread specifically for ehrlink analysis</a>.</p>",
      "body_md": "Awesome, you have set the train in motion for us to include ehrlink indications. Let's move discussion to [this new thread specifically for ehrlink analysis](http://thinklab.com/discussion/extracting-indications-from-the-ehrlink-resource/62).",
      "comment": 169,
      "profile": 17,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#note-97"
    },
    {
      "added": "2015-05-02T20:23:23.972725Z",
      "body_html": "<p>Correct - that is why I thought it could be a useful ressource to share.</p>",
      "body_md": "Correct - that is why I thought it could be a useful ressource to share.",
      "comment": 183,
      "profile": 23,
      "url": "/discussion/retrieving-the-ingredients-in-rxnorm-concepts/61#note-98"
    },
    {
      "added": "2015-05-03T20:54:14.123111Z",
      "body_html": "<p>As a side-note, I removed all the row name columns in the csvs, because it was annoying for display in github. You might have to change slightly your code to understand the new format.</p>",
      "body_md": "As a side-note, I removed all the row name columns in the csvs, because it was annoying for display in github. You might have to change slightly your code to understand the new format.",
      "comment": 185,
      "profile": 23,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#note-99"
    },
    {
      "added": "2015-05-03T20:57:42.872674Z",
      "body_html": "<p>You might want to update #7 with my latest results from <a href=\"http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#185\">#6</a></p>",
      "body_md": "You might want to update #7 with my latest results from [#6](http://thinklab.com/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#185)",
      "comment": 181,
      "profile": 23,
      "url": "/discussion/extracting-indications-from-the-ehrlink-resource/62#note-100"
    },
    {
      "added": "2015-05-04T19:19:49.666061Z",
      "body_html": "<p>For reference, <a href=\"/u/alizee\" class=\"username\">@alizee</a> <a href=\"https://github.com/antoine-lizee/RRxNorm/blob/master/RxNorm2.R#L67\">used</a> the following term type priority list (high to low):</p>\n\n<p><code>SCD</code>, <code>SBD</code>, <code>SCDF</code>, <code>SBDF</code>, <code>BN</code>, <code>SCDC</code>, <code>SBDC</code>, <code>IN</code>, <code>MIN</code>, <code>PIN</code>, <code>GPCK</code>, <code>BPCK</code>, <code>SCDG</code>, <code>SBDG</code>, <code>DF</code>, <code>DFG</code></p>",
      "body_md": "For reference, @alizee [used](https://github.com/antoine-lizee/RRxNorm/blob/master/RxNorm2.R#L67) the following term type priority list (high to low):\n\n`SCD`, `SBD`, `SCDF`, `SBDF`, `BN`, `SCDC`, `SBDC`, `IN`, `MIN`, `PIN`, `GPCK`, `BPCK`, `SCDG`, `SBDG`, `DF`, `DFG`",
      "comment": 185,
      "profile": 17,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#note-103"
    },
    {
      "added": "2015-05-08T19:45:18.976113Z",
      "body_html": "<p>I should add that the full text search is basically a last ditch effort to find genes w/o any matching identifiers in the structured databases. We use this on the description and alias field, as well as all of the cross reference IDs. We use elasticsearch via django-haystack to power this. It's pretty easy to set this up on AWS or locally. We previously used solr, but the configuration headache didn't outweigh the benefits.</p>",
      "body_md": "I should add that the full text search is basically a last ditch effort to find genes w/o any matching identifiers in the structured databases. We use this on the description and alias field, as well as all of the cross reference IDs. We use elasticsearch via django-haystack to power this. It's pretty easy to set this up on AWS or locally. We previously used solr, but the configuration headache didn't outweigh the benefits.",
      "comment": 208,
      "profile": 22,
      "url": "/discussion/using-entrez-gene-as-our-gene-vocabulary/34#note-113"
    },
    {
      "added": "2015-05-10T18:49:08.949801Z",
      "body_html": "<p>A common source of detailed variation between compounds is stereochemistry. See <a href=\"https://youtu.be/457xnJv80O0\">this video</a> for an introduction and <a href=\"//www.ncbi.nlm.nih.gov/pmc/articles/PMC353039/\">this paper</a> <span class=\"citation\">[<a href=\"/doi/10.4088/pcc.v05n0202\" class=\"citation\" data-key=\"10.4088/pcc.v05n0202\">1</a>]</span> for the relevance of stereochemistry in pharmacology.</p>",
      "body_md": "A common source of detailed variation between compounds is stereochemistry. See [this video](https://youtu.be/457xnJv80O0) for an introduction and [this paper](//www.ncbi.nlm.nih.gov/pmc/articles/PMC353039/) [@10.4088/pcc.v05n0202] for the relevance of stereochemistry in pharmacology.",
      "comment": 78,
      "profile": 17,
      "url": "/discussion/unifying-drug-vocabularies/40#note-114"
    },
    {
      "added": "2015-05-13T20:19:22.125336Z",
      "body_html": "<p>Elvira Mitraka <a href=\"http://sourceforge.net/p/diseaseontology/feature-requests/75/#5304\">added these cross-references</a> to revision 2815.</p>",
      "body_md": "Elvira Mitraka [added these cross-references](http://sourceforge.net/p/diseaseontology/feature-requests/75/#5304) to revision 2815.",
      "comment": 219,
      "profile": 17,
      "url": "/discussion/disease-ontology-feature-requests/68#note-115"
    },
    {
      "added": "2015-05-13T20:20:56.580678Z",
      "body_html": "<p>Elvira Mitraka <a href=\"http://sourceforge.net/p/diseaseontology/feature-requests/77/#a1e0\">fixed these typos and inconsistencies</a> in revision 2815.</p>",
      "body_md": "Elvira Mitraka [fixed these typos and inconsistencies](http://sourceforge.net/p/diseaseontology/feature-requests/77/#a1e0) in revision 2815.",
      "comment": 220,
      "profile": 17,
      "url": "/discussion/disease-ontology-feature-requests/68#note-116"
    },
    {
      "added": "2015-05-13T20:21:42.950370Z",
      "body_html": "<p>Elvira Mitraka <a href=\"//sourceforge.net/p/diseaseontology/feature-requests/76/#04c8\">added these cross-references</a> to revision 2815.</p>",
      "body_md": "Elvira Mitraka [added these cross-references](//sourceforge.net/p/diseaseontology/feature-requests/76/#04c8) to revision 2815.",
      "comment": 221,
      "profile": 17,
      "url": "/discussion/disease-ontology-feature-requests/68#note-117"
    },
    {
      "added": "2015-05-19T02:40:56.552657Z",
      "body_html": "<p>See this <a href=\"https://raw.githubusercontent.com/dhimmel/medline/2a427d37c4174e492873e2387c9b1d51236a4f7b/data/disease-symptom-cooccurrence.tsv\">newer symptom–disease pair tsv file</a> which has <a href=\"https://github.com/dhimmel/medline/commit/2a427d37c4174e492873e2387c9b1d51236a4f7b?diff=unified#diff-3417fe5acc9338308981e0e58eca0f11\">additional</a> DO slim diseases with MeSH mappings. </p>",
      "body_md": "See this [newer symptom–disease pair tsv file](https://raw.githubusercontent.com/dhimmel/medline/2a427d37c4174e492873e2387c9b1d51236a4f7b/data/disease-symptom-cooccurrence.tsv) which has [additional](https://github.com/dhimmel/medline/commit/2a427d37c4174e492873e2387c9b1d51236a4f7b?diff=unified#diff-3417fe5acc9338308981e0e58eca0f11) DO slim diseases with MeSH mappings. ",
      "comment": 224,
      "profile": 17,
      "url": "/discussion/mining-knowledge-from-medline-articles-and-their-indexed-mesh-terms/67#note-123"
    },
    {
      "added": "2015-05-28T00:07:51.486503Z",
      "body_html": "<a href=\"/u/allisonmccoy\" class=\"username\">@allisonmccoy</a>, thanks for following up and congratulations on your paper <span class=\"citation\">[<a href=\"/doi/10.4338/ACI-2015-01-RA-0010\" class=\"citation\" data-key=\"10.4338/ACI-2015-01-RA-0010\">1</a>]</span>. I read the abstract, but could you <a href=\"mailto:daniel.himmelstein@gmail.com\">email me</a> the pdf?\n\n<p>Enjoy the travels, we would definitely appreciate the data!</p>",
      "body_md": "@allisonmccoy, thanks for following up and congratulations on your paper [@10.4338/ACI-2015-01-RA-0010]. I read the abstract, but could you [email me](mailto:daniel.himmelstein@gmail.com) the pdf?\n\nEnjoy the travels, we would definitely appreciate the data!",
      "comment": 236,
      "profile": 17,
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21#note-124"
    },
    {
      "added": "2015-06-12T17:22:48.072707Z",
      "body_html": "<p>2015-06-12: I reassessed the implications of the centimorgan versus kilobase window span correlation. My current conclusion is that a single centimorgan threshold cannot be chosen that produces similar windows to the <em>r</em>-squared method.</p>",
      "body_md": "2015-06-12: I reassessed the implications of the centimorgan versus kilobase window span correlation. My current conclusion is that a single centimorgan threshold cannot be chosen that produces similar windows to the *r*-squared method.",
      "comment": 254,
      "profile": 17,
      "url": "/discussion/calculating-genomic-windows-for-gwas-lead-snps/71#note-125"
    },
    {
      "added": "2015-06-19T15:13:07.277483Z",
      "body_html": "<p>What does SRA stand for?</p>",
      "body_md": "What does SRA stand for?",
      "comment": 283,
      "profile": 17,
      "url": "/discussion/processing-gtex-for-tissue-specific-gene-expression/82#note-130"
    },
    {
      "added": "2015-06-19T15:24:56.910197Z",
      "body_html": "<p>Sequence Read Archive (http://www.ncbi.nlm.nih.gov/sra). This is where we usually download the raw data from. But I think GTEx keep them private, for their \"on demand\" data sharing policy...</p>",
      "body_md": "Sequence Read Archive (http://www.ncbi.nlm.nih.gov/sra). This is where we usually download the raw data from. But I think GTEx keep them private, for their \"on demand\" data sharing policy...",
      "comment": 283,
      "profile": 111,
      "url": "/discussion/processing-gtex-for-tissue-specific-gene-expression/82#note-131"
    },
    {
      "added": "2015-06-19T23:32:57.683419Z",
      "body_html": "<p>My mistake regarding \"transcript abundance\" — we actually only care about gene abundance.</p>",
      "body_md": "My mistake regarding \"transcript abundance\" -- we actually only care about gene abundance.",
      "comment": 284,
      "profile": 17,
      "url": "/discussion/tissue-specific-gene-expression-resources/81#note-132"
    },
    {
      "added": "2015-06-20T01:06:56.926835Z",
      "body_html": "<span><a href=\"/u/dhimmel\" class=\"username\">@dhimmel</a>: OK, it should fit your needs then. Let me know if you have any other question.</span>",
      "body_md": "@dhimmel: OK, it should fit your needs then. Let me know if you have any other question.",
      "comment": 284,
      "profile": 111,
      "url": "/discussion/tissue-specific-gene-expression-resources/81#note-133"
    },
    {
      "added": "2015-06-20T22:30:37.369980Z",
      "body_html": "<p>Links are broken</p>",
      "body_md": "Links are broken",
      "comment": 287,
      "profile": 17,
      "url": "/discussion/tissue-node/41#note-134"
    },
    {
      "added": "2015-06-26T23:25:24.504790Z",
      "body_html": "<p>Thanks <a href=\"/u/akolow\" class=\"username\">@akolow</a>! Let us know if you experience any problems or have suggestions.</p>",
      "body_md": "Thanks @akolow! Let us know if you experience any problems or have suggestions.",
      "comment": 301,
      "profile": 17,
      "url": "/discussion/python-for-the-modern-biodata-scientist/84#note-135"
    },
    {
      "added": "2015-06-29T04:07:20.652862Z",
      "body_html": "<p>Much appreciated. Thanks!</p>",
      "body_md": "Much appreciated. Thanks!",
      "comment": 304,
      "profile": 17,
      "url": "/discussion/adding-pathway-resources-to-your-network/72#note-136"
    },
    {
      "added": "2015-08-09T04:06:47.324939Z",
      "body_html": "<p>No, the right person to contact is Michael Kuhn.</p>",
      "body_md": "No, the right person to contact is Michael Kuhn.",
      "comment": 368,
      "profile": 125,
      "url": "/discussion/extracting-side-effects-from-sider-4/97#note-140"
    },
    {
      "added": "2015-08-09T04:12:25.999935Z",
      "body_html": "<p>Please note that using only the experiments channel will not eliminate knowledge biases. In particular, the immunohistochemical staining data from the Human Protein Atlas are heavily biased, since it depends strongly on the number and quality of antibodies available for each protein. </p>",
      "body_md": "Please note that using only the experiments channel will not eliminate knowledge biases. In particular, the immunohistochemical staining data from the Human Protein Atlas are heavily biased, since it depends strongly on the number and quality of antibodies available for each protein. ",
      "comment": 366,
      "profile": 125,
      "url": "/discussion/the-tissues-resource-for-the-tissue-specificity-of-genes/91#note-141"
    },
    {
      "added": "2015-08-09T20:18:38.535913Z",
      "body_html": "<p>In <code>human_tissue_integrated_full.tsv</code> I treated <code>HPA</code> as referring to HPA-IHC. Is this correct?</p>",
      "body_md": "In `human_tissue_integrated_full.tsv` I treated `HPA` as referring to HPA-IHC. Is this correct?",
      "comment": 372,
      "profile": 17,
      "url": "/discussion/the-tissues-resource-for-the-tissue-specificity-of-genes/91#note-142"
    },
    {
      "added": "2015-08-10T04:11:26.832146Z",
      "body_html": "<p>If you meant experiments instead of integrated in that filename, then yes.</p>",
      "body_md": "If you meant experiments instead of integrated in that filename, then yes.",
      "comment": 372,
      "profile": 125,
      "url": "/discussion/the-tissues-resource-for-the-tissue-specificity-of-genes/91#note-143"
    },
    {
      "added": "2015-08-11T05:55:36.726672Z",
      "body_html": "<p>I contacted Michael Kuhn. <code>label_mapping.tsv.gz</code> was not essential and was removed. Documentation was added to the README for <code>meddra_all_indications.tsv.gz</code>.</p>",
      "body_md": "I contacted Michael Kuhn. `label_mapping.tsv.gz` was not essential and was removed. Documentation was added to the README for `meddra_all_indications.tsv.gz`.",
      "comment": 368,
      "profile": 17,
      "url": "/discussion/extracting-side-effects-from-sider-4/97#note-147"
    },
    {
      "added": "2015-08-12T16:48:08.890975Z",
      "body_html": "<p>Yes, I meant <code>human_tissue_experiments_full.tsv.gz</code>.</p>",
      "body_md": "Yes, I meant `human_tissue_experiments_full.tsv.gz`.",
      "comment": 372,
      "profile": 17,
      "url": "/discussion/the-tissues-resource-for-the-tissue-specificity-of-genes/91#note-150"
    },
    {
      "added": "2015-08-12T18:12:56.111600Z",
      "body_html": "<p>Your study alludes that it includes <a href=\"https://dx.doi.org/10.1038/ncomms5074#ref51\">citation 51</a> <span class=\"citation\">[<a href=\"/doi/10.1038/nature01156\" class=\"citation\" data-key=\"10.1038/nature01156\">1</a>]</span> as a human interactome dataset. I don't think we'll include this study as it's old and doesn't seem to be a PPI database.</p>",
      "body_md": "Your study alludes that it includes [citation 51](https://dx.doi.org/10.1038/ncomms5074#ref51) [@10.1038/nature01156] as a human interactome dataset. I don't think we'll include this study as it's old and doesn't seem to be a PPI database.",
      "comment": 336,
      "profile": 17,
      "url": "/discussion/creating-a-catalog-of-protein-interactions/85#note-151"
    },
    {
      "added": "2015-08-16T07:39:18.514235Z",
      "body_html": "<p>I forgot to mention explicitly that this obviously means that you cannot distribute your network file on GitHub under the CC0 waiver as you currently do.</p>",
      "body_md": "I forgot to mention explicitly that this obviously means that you cannot distribute your network file on GitHub under the CC0 waiver as you currently do.",
      "comment": 397,
      "profile": 125,
      "url": "/discussion/one-network-to-rule-them-all/102#note-152"
    },
    {
      "added": "2015-08-16T07:48:48.752425Z",
      "body_html": "<p>Redistribution of LINCS data is also problematic, unless you contacted them and got permission to do so (http://www.lincscloud.org/license/).</p>",
      "body_md": "Redistribution of LINCS data is also problematic, unless you contacted them and got permission to do so (http://www.lincscloud.org/license/).",
      "comment": 397,
      "profile": 125,
      "url": "/discussion/one-network-to-rule-them-all/102#note-153"
    },
    {
      "added": "2015-08-16T08:21:11.712548Z",
      "body_html": "<p>Okay, I will look into the copyright issues and make any necessary modifications. I like the idea of including source and license fields for each node and edge. My understanding is that CC licenses prior to 4.0 <a href=\"https://wiki.creativecommons.org/wiki/Data\">do not restrict</a> the underlying data when used in the United States. However, I will need to do more reading and solicit the advice of copyright experts.</p>",
      "body_md": "Okay, I will look into the copyright issues and make any necessary modifications. I like the idea of including source and license fields for each node and edge. My understanding is that CC licenses prior to 4.0 [do not restrict](https://wiki.creativecommons.org/wiki/Data) the underlying data when used in the United States. However, I will need to do more reading and solicit the advice of copyright experts.",
      "comment": 397,
      "profile": 17,
      "url": "/discussion/one-network-to-rule-them-all/102#note-154"
    },
    {
      "added": "2015-08-16T08:27:17.366674Z",
      "body_html": "<p>I updated the <a href=\"https://github.com/dhimmel/integrate/blob/950f26ddff83e17fa6e398e7ae66179d8b2638e3/LICENSE.md\">repository license</a> until we clarify these issues.</p>",
      "body_md": "I updated the [repository license](https://github.com/dhimmel/integrate/blob/950f26ddff83e17fa6e398e7ae66179d8b2638e3/LICENSE.md) until we clarify these issues.",
      "comment": 397,
      "profile": 17,
      "url": "/discussion/one-network-to-rule-them-all/102#note-155"
    },
    {
      "added": "2015-08-16T08:27:28.794088Z",
      "body_html": "<p>CC licenses prior to 4.0 were not very well suited for data. I only just noticed that the new SIDER still uses the old license, which I think should be changed. Not to make it more restrictive, but simply because the 4.0 license addresses a problem in the earlier license (i.e. the attribution stacking problem).</p>",
      "body_md": "CC licenses prior to 4.0 were not very well suited for data. I only just noticed that the new SIDER still uses the old license, which I think should be changed. Not to make it more restrictive, but simply because the 4.0 license addresses a problem in the earlier license (i.e. the attribution stacking problem).",
      "comment": 397,
      "profile": 125,
      "url": "/discussion/one-network-to-rule-them-all/102#note-156"
    },
    {
      "added": "2015-08-16T08:39:27.607342Z",
      "body_html": "<p>Unfortunately, as far as I know, it is not sufficient to consider US law when you distribute data to the whole world. Although the US does not have sui generis database right, other parts of the world do, including the EU. This means that databases created in the EU are protected by such laws. (Caveat: I am not a lawyer, this does not constitute legal advice, yada yada yada.)</p>",
      "body_md": "Unfortunately, as far as I know, it is not sufficient to consider US law when you distribute data to the whole world. Although the US does not have sui generis database right, other parts of the world do, including the EU. This means that databases created in the EU are protected by such laws. (Caveat: I am not a lawyer, this does not constitute legal advice, yada yada yada.)",
      "comment": 397,
      "profile": 125,
      "url": "/discussion/one-network-to-rule-them-all/102#note-157"
    },
    {
      "added": "2015-08-17T07:33:53.127174Z",
      "body_html": "<p>I think that for gene-disease associations you should check DisGeNET (www.disgenet.org). I would start by checking the disease list that you already have and query DisGeNET with it. Currently, DisGeNET may be searched using MeSH, OMIMs, and UMLS CUIs. You can download the data in tab files.</p>",
      "body_md": "I think that for gene-disease associations you should check DisGeNET (www.disgenet.org). I would start by checking the disease list that you already have and query DisGeNET with it. Currently, DisGeNET may be searched using MeSH, OMIMs, and UMLS CUIs. You can download the data in tab files.",
      "comment": 346,
      "profile": 129,
      "url": "/discussion/suggestions-for-additional-information-types/22#note-159"
    },
    {
      "added": "2015-08-18T03:56:41.965372Z",
      "body_html": "<p><a href=\"/u/janispi\" class=\"username\">@janispi</a>, thanks for the suggestion. We've <a href=\"http://thinklab.com/discussion/processing-disgenet-for-disease-gene-relationships/105\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d105\">begun</a> adding DisGeNET.</p>",
      "body_md": "@janispi, thanks for the suggestion. We've [begun](http://thinklab.com/discussion/processing-disgenet-for-disease-gene-relationships/105) adding DisGeNET.",
      "comment": 346,
      "profile": 17,
      "url": "/discussion/suggestions-for-additional-information-types/22#note-160"
    },
    {
      "added": "2015-08-19T09:03:54.261042Z",
      "body_html": "<p>you are absolutely right, I have removed the tar the files, and now I just gzip them. </p>",
      "body_md": "you are absolutely right, I have removed the tar the files, and now I just gzip them. ",
      "comment": 404,
      "profile": 129,
      "url": "/discussion/processing-disgenet-for-disease-gene-relationships/105#note-161"
    },
    {
      "added": "2015-08-19T09:19:03.563143Z",
      "body_html": "<p>I was waiting for this question. All GDAs have score &gt; 0. <br>If you choose score &gt;= 0.06, then you will be including associations reporting by curated sources, or having animal models supporting them, or being reported by several papers (20 -200). It will not be permissive, though (less than 10% of GDAs satisfies this criteria). MAybe you could start with this score, and see how it goes. </p>",
      "body_md": "I was waiting for this question. All GDAs have score > 0. \nIf you choose score >= 0.06, then you will be including associations reporting by curated sources, or having animal models supporting them, or being reported by several papers (20 -200). It will not be permissive, though (less than 10% of GDAs satisfies this criteria). MAybe you could start with this score, and see how it goes. ",
      "comment": 405,
      "profile": 129,
      "url": "/discussion/processing-disgenet-for-disease-gene-relationships/105#note-162"
    },
    {
      "added": "2015-08-19T18:26:50.742635Z",
      "body_html": "<p>I am not able to resolve <a href=\"http://dev.stargeo.org/\">http://dev.stargeo.org/</a></p>",
      "body_md": "I am not able to resolve http://dev.stargeo.org/",
      "comment": 410,
      "profile": 17,
      "url": "/discussion/stargeo-expression-signatures-for-disease-using-crowdsourced-geo-annotation/96#note-163"
    },
    {
      "added": "2015-08-19T18:33:23.335413Z",
      "body_html": "<p>Make that: <a href=\"http://dev.stargeo.io\">http://dev.stargeo.io</a>. But not live quite yet.  I'll get an update tonight on the UI.  Current only working from command line, but at least its working :)</p>",
      "body_md": "Make that: http://dev.stargeo.io. But not live quite yet.  I'll get an update tonight on the UI.  Current only working from command line, but at least its working :)",
      "comment": 410,
      "profile": 121,
      "url": "/discussion/stargeo-expression-signatures-for-disease-using-crowdsourced-geo-annotation/96#note-164"
    },
    {
      "added": "2015-08-19T18:55:47.985376Z",
      "body_html": "<p>Okay, I'll switch the hyperlinks on Thinklab over to <a href=\"http://dev.stargeo.io/\">http://dev.stargeo.io/</a></p>",
      "body_md": "Okay, I'll switch the hyperlinks on Thinklab over to http://dev.stargeo.io/",
      "comment": 410,
      "profile": 17,
      "url": "/discussion/stargeo-expression-signatures-for-disease-using-crowdsourced-geo-annotation/96#note-165"
    },
    {
      "added": "2015-08-20T00:30:41.694617Z",
      "body_html": "<p>I'm wondering about your relation ontology here.  Can you really distinguish a causal relation from co-occurrence information?  Do the names on these edges actually matter based on how you are using the network ?  If the meaning of the relations is really important, I think there are other text-mining approaches that you should look into.  If not, the co-occurrence stuff should be fine.  Really curious about this experiment..  Would also like to see how the result of other text-ming approaches would influence the outcome.  e.g. would it change things if you swapped in the relations from semmedDB ?</p>",
      "body_md": "I'm wondering about your relation ontology here.  Can you really distinguish a causal relation from co-occurrence information?  Do the names on these edges actually matter based on how you are using the network ?  If the meaning of the relations is really important, I think there are other text-mining approaches that you should look into.  If not, the co-occurrence stuff should be fine.  Really curious about this experiment..  Would also like to see how the result of other text-ming approaches would influence the outcome.  e.g. would it change things if you swapped in the relations from semmedDB ?",
      "comment": 365,
      "profile": 48,
      "url": "/discussion/text-as-a-resource-for-network-population/48#note-166"
    },
    {
      "added": "2015-08-20T17:05:36.232991Z",
      "body_html": "<p>I think whatever we do is ultimately arbitrary and will become our 'protocol'.  We can assume that tags reference the sample itself which probably is some type of tissue and applies to the individual.  Like diabetic pancreas tissue is from a diabetic individual.  But cancer is a \"mosaic\" disease which puts tissue vs individual out of sync.  I think eventually for every set of annotations made on a GSE, we can allow for qualifiers from EFO to be set.</p>",
      "body_md": "I think whatever we do is ultimately arbitrary and will become our 'protocol'.  We can assume that tags reference the sample itself which probably is some type of tissue and applies to the individual.  Like diabetic pancreas tissue is from a diabetic individual.  But cancer is a \"mosaic\" disease which puts tissue vs individual out of sync.  I think eventually for every set of annotations made on a GSE, we can allow for qualifiers from EFO to be set.",
      "comment": 411,
      "profile": 121,
      "url": "/discussion/stargeo-expression-signatures-for-disease-using-crowdsourced-geo-annotation/96#note-167"
    },
    {
      "added": "2015-08-20T17:07:20.448562Z",
      "body_html": "<p>I guess.  This is mainly for testing.  By the end of the month or soon thereafter it should revert back to .org (public marketed site) and .io will remain for testing among a chosen few (and relatively private)</p>",
      "body_md": "I guess.  This is mainly for testing.  By the end of the month or soon thereafter it should revert back to .org (public marketed site) and .io will remain for testing among a chosen few (and relatively private)",
      "comment": 410,
      "profile": 121,
      "url": "/discussion/stargeo-expression-signatures-for-disease-using-crowdsourced-geo-annotation/96#note-168"
    },
    {
      "added": "2015-08-20T17:12:12.642262Z",
      "body_html": "<p>I took a look at that but it doesn't look like it provides hooks to the underlying annotations. Are those going to be available? For the types of analyses that we work on, those are much more valuable than the profiles.</p>",
      "body_md": "I took a look at that but it doesn't look like it provides hooks to the underlying annotations. Are those going to be available? For the types of analyses that we work on, those are much more valuable than the profiles.",
      "comment": 414,
      "profile": 22,
      "url": "/discussion/stargeo-expression-signatures-for-disease-using-crowdsourced-geo-annotation/96#note-169"
    },
    {
      "added": "2015-08-20T17:12:47.779058Z",
      "body_html": "<p>1) Yes multi species are on the books.  But for now we are focusing on humans to get the site launched.<br>2) Lets get you an account which for now is through me.  I'm idrdex at both google hangouts and Skype. </p>\n\n<p>3) Everything for now is accessed through the site.  We are working on an API to serve the data for the computational folks.  Basically allow retrival of annotations and served matrices with matching gene ids.</p>",
      "body_md": "1) Yes multi species are on the books.  But for now we are focusing on humans to get the site launched.\n2) Lets get you an account which for now is through me.  I'm idrdex at both google hangouts and Skype. \n\n\n3) Everything for now is accessed through the site.  We are working on an API to serve the data for the computational folks.  Basically allow retrival of annotations and served matrices with matching gene ids.",
      "comment": 413,
      "profile": 121,
      "url": "/discussion/stargeo-expression-signatures-for-disease-using-crowdsourced-geo-annotation/96#note-170"
    },
    {
      "added": "2015-08-20T17:16:09.000238Z",
      "body_html": "<p>The \"annotations\" interface is not ready yet.  But imagine a searchable list where you could put GSEs in for instance and get all our annotations across GSMs.  For now I could probably generate a .csv for you.   We currently have 400K+ annotations  over 100+ tags.  Some GSEs have been done up to quadruplicate and ones with multiple annotations we have kappa inter-related sats calculated.</p>",
      "body_md": "The \"annotations\" interface is not ready yet.  But imagine a searchable list where you could put GSEs in for instance and get all our annotations across GSMs.  For now I could probably generate a .csv for you.   We currently have 400K+ annotations  over 100+ tags.  Some GSEs have been done up to quadruplicate and ones with multiple annotations we have kappa inter-related sats calculated.",
      "comment": 414,
      "profile": 121,
      "url": "/discussion/stargeo-expression-signatures-for-disease-using-crowdsourced-geo-annotation/96#note-171"
    },
    {
      "added": "2015-08-20T17:21:53.596924Z",
      "body_html": "<p>The annotations interface sounds very helpful! I'm looking forward to it! The analysis that we're doing right now isn't for human datasets, so if your curations are generally there we can't check the overlap with our own curations. We do a lot of human work though, so it'll be very helpful to us to have these annotations available.</p>",
      "body_md": "The annotations interface sounds very helpful! I'm looking forward to it! The analysis that we're doing right now isn't for human datasets, so if your curations are generally there we can't check the overlap with our own curations. We do a lot of human work though, so it'll be very helpful to us to have these annotations available.",
      "comment": 414,
      "profile": 22,
      "url": "/discussion/stargeo-expression-signatures-for-disease-using-crowdsourced-geo-annotation/96#note-172"
    },
    {
      "added": "2015-08-20T17:23:01.622176Z",
      "body_html": "<p>I've got a postgres table that you can query if you like.  Let's hangout and discuss :)</p>",
      "body_md": "I've got a postgres table that you can query if you like.  Let's hangout and discuss :)",
      "comment": 414,
      "profile": 121,
      "url": "/discussion/stargeo-expression-signatures-for-disease-using-crowdsourced-geo-annotation/96#note-173"
    },
    {
      "added": "2015-08-20T17:27:50.566684Z",
      "body_html": "<p>It would be useful to compare and contrast STARGEO with this ADEPTUS list.  To be clear the 400+ annotations cover about 200+ distinct tags made on samples —  i.e samples can be redundantly annotated to check for validity of the original annotations.  This validation on demand allows us to weed out problematic studies that poorly describe their samples and individual sample annotations whose semantics are imprecise.</p>",
      "body_md": "It would be useful to compare and contrast STARGEO with this ADEPTUS list.  To be clear the 400+ annotations cover about 200+ distinct tags made on samples --  i.e samples can be redundantly annotated to check for validity of the original annotations.  This validation on demand allows us to weed out problematic studies that poorly describe their samples and individual sample annotations whose semantics are imprecise.",
      "comment": 387,
      "profile": 121,
      "url": "/discussion/the-adeptus-resource-for-expression-signatures-of-disease/101#note-174"
    },
    {
      "added": "2015-08-20T22:32:29.292522Z",
      "body_html": "<p>Okay, we can change links to <a href=\"http://stargeo.org\">http://stargeo.org</a> now or once the public site goes live. Your call.</p>",
      "body_md": "Okay, we can change links to http://stargeo.org now or once the public site goes live. Your call.",
      "comment": 410,
      "profile": 17,
      "url": "/discussion/stargeo-expression-signatures-for-disease-using-crowdsourced-geo-annotation/96#note-175"
    },
    {
      "added": "2015-08-21T20:54:30.263301Z",
      "body_html": "<p>We don't include COSMIC anywhere else, so I would like to include it. <a href=\"/u/larsjuhljensen\" class=\"username\">@larsjuhljensen</a>, which literature corpus was used for text mining?</p>",
      "body_md": "We don't include COSMIC anywhere else, so I would like to include it. @larsjuhljensen, which literature corpus was used for text mining?",
      "comment": 418,
      "profile": 17,
      "url": "/discussion/processing-the-diseases-resource-for-diseasegene-relationships/106#note-176"
    },
    {
      "added": "2015-08-21T20:59:14.336769Z",
      "body_html": "<p>Just Medline so far.</p>",
      "body_md": "Just Medline so far.",
      "comment": 418,
      "profile": 125,
      "url": "/discussion/processing-the-diseases-resource-for-diseasegene-relationships/106#note-177"
    },
    {
      "added": "2015-08-28T20:38:08.534127Z",
      "body_html": "<p>Great suggestion. This way users can avoid having to subset the network and reconcile various licenses.</p>",
      "body_md": "Great suggestion. This way users can avoid having to subset the network and reconcile various licenses.",
      "comment": 420,
      "profile": 17,
      "url": "/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#note-178"
    },
    {
      "added": "2015-09-07T16:07:27.482234Z",
      "body_html": "<p>Exciting, thanks for the update. On another note, I wasn't able to find any licensing information on the Bgee website, which technically means <a href=\"http://choosealicense.com/no-license/\">all rights reserved</a>. We're <a href=\"http://thinklab.com/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#3\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d107\">tying to compile</a> licenses for each resource we use. It would be great if you could add a license.</p>",
      "body_md": "Exciting, thanks for the update. On another note, I wasn't able to find any licensing information on the Bgee website, which technically means [all rights reserved](http://choosealicense.com/no-license/). We're [tying to compile](http://thinklab.com/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#3) licenses for each resource we use. It would be great if you could add a license.",
      "comment": 426,
      "profile": 17,
      "url": "/discussion/processing-gtex-for-tissue-specific-gene-expression/82#note-179"
    },
    {
      "added": "2015-09-07T16:38:45.337493Z",
      "body_html": "<p>Yep, I wanted to reply to you after checking all our datasources. I think we are going to use a cc-by, but I'll contact you when I can confirm that for sure.</p>",
      "body_md": "Yep, I wanted to reply to you after checking all our datasources. I think we are going to use a cc-by, but I'll contact you when I can confirm that for sure.",
      "comment": 426,
      "profile": 111,
      "url": "/discussion/processing-gtex-for-tissue-specific-gene-expression/82#note-180"
    },
    {
      "added": "2015-09-10T10:52:06.035130Z",
      "body_html": "<p>The big problem I see here is the term \"results\". If the results are actually new results, then I agree that they can be released whichever way you like. But if your \"results\" are in fact other people's databases mapped to different identifiers, bundled, and reformatted in JSON format, then claiming fair use is in my opinion a very risky proposition.</p>",
      "body_md": "The big problem I see here is the term \"results\". If the results are actually new results, then I agree that they can be released whichever way you like. But if your \"results\" are in fact other people's databases mapped to different identifiers, bundled, and reformatted in JSON format, then claiming fair use is in my opinion a very risky proposition.",
      "comment": 431,
      "profile": 125,
      "url": "/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#note-181"
    },
    {
      "added": "2015-09-10T19:26:59.238966Z",
      "body_html": "<p><a href=\"/u/larsjuhljensen\" class=\"username\">@larsjuhljensen</a> and <a href=\"/u/mackenziesmith\" class=\"username\">@mackenziesmith</a>, I think we may all be on the same page. Results from <em>network analyses</em> are truly transformative and thus eligible for CC0 licensing due to fair use. However, the <em>resource processing</em> and <em>integrative network</em> <a href=\"#4\">steps</a>, which distribute unmodified downloads as well as network-coerced versions of databases, should generally transmit the source licensing.</p>",
      "body_md": "@larsjuhljensen and @mackenziesmith, I think we may all be on the same page. Results from *network analyses* are truly transformative and thus eligible for CC0 licensing due to fair use. However, the *resource processing* and *integrative network* [steps](#4), which distribute unmodified downloads as well as network-coerced versions of databases, should generally transmit the source licensing.",
      "comment": 431,
      "profile": 17,
      "url": "/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#note-182"
    },
    {
      "added": "2015-09-10T19:47:47.420275Z",
      "body_html": "<p><a href=\"/u/dhimmel\" class=\"username\">@dhimmel</a>, we almost agree. The one point where I disagree is that, in my opinion, fair use has nothing to do with it. If you do a network analysis that truly produces new results (e.g. predicting new edges based on the imported ones), then those edges are yours. You are free to do with them whatever you want, not because of fair use, but simply because you are the original creator :-)</p>",
      "body_md": "@dhimmel, we almost agree. The one point where I disagree is that, in my opinion, fair use has nothing to do with it. If you do a network analysis that truly produces new results (e.g. predicting new edges based on the imported ones), then those edges are yours. You are free to do with them whatever you want, not because of fair use, but simply because you are the original creator :-)",
      "comment": 431,
      "profile": 125,
      "url": "/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#note-183"
    },
    {
      "added": "2015-09-10T20:17:01.002824Z",
      "body_html": "<p>I am interested in subnetworks for user convenience — if a user wants only CC-BY-SA content, then the subnetwork saves them time. However, I disagree that subnetworks are a substitute for a complete network with mixed licensing. Merging networks will be burdensome to my hypothetical user who is interested only in analyzing rather than redistributing the network. Additionally, I am not sure the entire network can be represented by copyright uniform subnetworks. For example, DrugBank <a href=\"https://github.com/dhimmel/integrate/blob/0e343d8745a98757c86e73f645b41353f52b82b5/licenses/custom/DrugBank.md\">forbids</a> commercial reuse. However, our drug–gene binding edges derive from the CC-BY-SA ChEMBL resource. Thus the CC-BY-SA subnetwork would contain edges whose nodes cannot be included.</p>",
      "body_md": "I am interested in subnetworks for user convenience -- if a user wants only CC-BY-SA content, then the subnetwork saves them time. However, I disagree that subnetworks are a substitute for a complete network with mixed licensing. Merging networks will be burdensome to my hypothetical user who is interested only in analyzing rather than redistributing the network. Additionally, I am not sure the entire network can be represented by copyright uniform subnetworks. For example, DrugBank [forbids](https://github.com/dhimmel/integrate/blob/0e343d8745a98757c86e73f645b41353f52b82b5/licenses/custom/DrugBank.md) commercial reuse. However, our drug--gene binding edges derive from the CC-BY-SA ChEMBL resource. Thus the CC-BY-SA subnetwork would contain edges whose nodes cannot be included.",
      "comment": 429,
      "profile": 17,
      "url": "/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#note-184"
    },
    {
      "added": "2015-09-14T22:38:05.507384Z",
      "body_html": "<p>Continuum <a href=\"http://continuum.io/blog/conda-jupyter-irkernel\">has created</a> an <code>r-essentials</code> bundle containing many of the most common R packages. Installing, <code>r-essentials</code> should also add the R kernel to your jupyter notebook.</p>",
      "body_md": "Continuum [has created](http://continuum.io/blog/conda-jupyter-irkernel) an `r-essentials` bundle containing many of the most common R packages. Installing, `r-essentials` should also add the R kernel to your jupyter notebook.",
      "comment": 422,
      "profile": 17,
      "url": "/discussion/r-best-practices/83#note-185"
    },
    {
      "added": "2015-09-23T05:43:28.403404Z",
      "body_html": "<p>Thanks a lot for also pointing out the politics aspect. In my opinion, the risk of actually getting sued in academia is probably fairly low. However, if you were to systematically take resources with restrictive licenses, integrate them, and redistribute the complete data under CC0, you would almost for sure be burning bridges.</p>",
      "body_md": "Thanks a lot for also pointing out the politics aspect. In my opinion, the risk of actually getting sued in academia is probably fairly low. However, if you were to systematically take resources with restrictive licenses, integrate them, and redistribute the complete data under CC0, you would almost for sure be burning bridges.",
      "comment": 438,
      "profile": 125,
      "url": "/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#note-186"
    },
    {
      "added": "2015-10-02T19:29:30.472824Z",
      "body_html": "<p>Yesterday, we <a href=\"http://thinklab.com/discussion/incomplete-interactome-licensing/111\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d111\">sent out</a> our first permission request for a publication <span class=\"citation\">[<a href=\"/doi/10.1126/science.1257601\" class=\"citation\" data-key=\"10.1126/science.1257601\">1</a>]</span> supplement.</p>",
      "body_md": "Yesterday, we [sent out](http://thinklab.com/discussion/incomplete-interactome-licensing/111) our first permission request for a publication [@10.1126/science.1257601] supplement.",
      "comment": 445,
      "profile": 17,
      "url": "/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#note-187"
    },
    {
      "added": "2015-10-05T02:33:53.056678Z",
      "body_html": "<p>Just a small correction — Thinklab is tracking views independently of Google Analytics. Our figure attempts to track the total number of <em>humans</em> that have viewed the page. Our figure will be less than what Google Analytics has because for them a unique page view is really the number of <em>sessions</em> with at least one page view.</p>",
      "body_md": "Just a small correction -- Thinklab is tracking views independently of Google Analytics. Our figure attempts to track the total number of *humans* that have viewed the page. Our figure will be less than what Google Analytics has because for them a unique page view is really the number of *sessions* with at least one page view.",
      "comment": 451,
      "profile": 2,
      "url": "/discussion/tracking-project-reuse-citation-and-publicity/113#note-188"
    },
    {
      "added": "2015-10-05T02:44:07.397470Z",
      "body_html": "<p>I've emailed them and will report back</p>",
      "body_md": "I've emailed them and will report back",
      "comment": 454,
      "profile": 2,
      "url": "/discussion/tracking-project-reuse-citation-and-publicity/113#note-189"
    },
    {
      "added": "2015-10-06T15:55:53.335805Z",
      "body_html": "<p>2015-09-06: I updated this post to correct edge counts. Previously, I wrote the network contains \"5,998,711 edges (2,977,167 of which are unbiased)\". The issue was caused by counting single edges multiple times.</p>",
      "body_md": "2015-09-06: I updated this post to correct edge counts. Previously, I wrote the network contains \"5,998,711 edges (2,977,167 of which are unbiased)\". The issue was caused by counting single edges multiple times.",
      "comment": 396,
      "profile": 17,
      "url": "/discussion/one-network-to-rule-them-all/102#note-190"
    },
    {
      "added": "2015-10-09T16:11:40.411972Z",
      "body_html": "<p>I've <a href=\"https://github.com/dhimmel/integrate/blob/7ef64533f0822fb5728ab4c1d88dc39f3345dcc8/licenses/README.md\">added institutional affiliations</a>. I did not specify affiliations for community driven projects or multi-affiliated projects. In general, is funding information available? Where can I find it?</p>",
      "body_md": "I've [added institutional affiliations](https://github.com/dhimmel/integrate/blob/7ef64533f0822fb5728ab4c1d88dc39f3345dcc8/licenses/README.md). I did not specify affiliations for community driven projects or multi-affiliated projects. In general, is funding information available? Where can I find it?",
      "comment": 462,
      "profile": 17,
      "url": "/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#note-191"
    },
    {
      "added": "2015-10-13T18:05:23.266060Z",
      "body_html": "<p>Funding is available on the project site or the citation.  For example <a href=\"http://disease-ontology.org/about/\">DO</a>, their last <a href=\"http://nar.oxfordjournals.org/content/early/2014/10/27/nar.gku1011.full.pdf?keytype=ref&amp;ijkey=Ul8AlMyerFSf0rP\">publication</a>, the funding are from several grants from NIH, EMBL and the department of energy. </p>\n\n<p>When sources are looking into licensing or terms of use, do they bind to the affiliation, funding or make the best educated guess?</p>",
      "body_md": "Funding is available on the project site or the citation.  For example [DO] (http://disease-ontology.org/about/), their last [publication]\n(http://nar.oxfordjournals.org/content/early/2014/10/27/nar.gku1011.full.pdf?keytype=ref&ijkey=Ul8AlMyerFSf0rP), the funding are from several grants from NIH, EMBL and the department of energy. \n\nWhen sources are looking into licensing or terms of use, do they bind to the affiliation, funding or make the best educated guess?",
      "comment": 462,
      "profile": 79,
      "url": "/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#note-192"
    },
    {
      "added": "2015-10-14T23:09:31.462736Z",
      "body_html": "<p>Generally with a CC license you should say how the author wants to be credited - if not specific as to manner, than at least with a name of person or organization. That's all so far. I'll keep an eye out now that I'm back in the office...</p>",
      "body_md": "Generally with a CC license you should say how the author wants to be credited - if not specific as to manner, than at least with a name of person or organization. That's all so far. I'll keep an eye out now that I'm back in the office...",
      "comment": 440,
      "profile": 137,
      "url": "/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#note-193"
    },
    {
      "added": "2015-10-22T03:37:18.366074Z",
      "body_html": "<p>Neo4j 3.0 should be interesting. So far, what has mainly held me back was that I do not want to develop my bioinformatics software in Java and doing everything through Cypher was not sufficiently efficient to warrant migration from a PostgreSQL database.</p>",
      "body_md": "Neo4j 3.0 should be interesting. So far, what has mainly held me back was that I do not want to develop my bioinformatics software in Java and doing everything through Cypher was not sufficiently efficient to warrant migration from a PostgreSQL database.",
      "comment": 484,
      "profile": 125,
      "url": "/discussion/using-the-neo4j-graph-database-for-hetnets/112#note-194"
    },
    {
      "added": "2015-12-04T21:52:50.278510Z",
      "body_html": "<p>Milestone 1 of version 3.0 was <a href=\"http://neo4j.com/blog/neo4j-3-0-milestone-1-release/\">released today</a> with a <a href=\"https://github.com/neo4j/neo4j-python-driver\">python driver</a>. The update promises fast access from outside of java. However, you will still need to use Cypher (which I find easier and more powerful than SQL). <a href=\"/u/larsjuhljensen\" class=\"username\">@larsjuhljensen</a>, I would wait till the final 3.0 release for production use but wanted to give you a heads up of what lies ahead.</p>",
      "body_md": "Milestone 1 of version 3.0 was [released today](http://neo4j.com/blog/neo4j-3-0-milestone-1-release/) with a [python driver](https://github.com/neo4j/neo4j-python-driver). The update promises fast access from outside of java. However, you will still need to use Cypher (which I find easier and more powerful than SQL). @larsjuhljensen, I would wait till the final 3.0 release for production use but wanted to give you a heads up of what lies ahead.",
      "comment": 484,
      "profile": 17,
      "url": "/discussion/using-the-neo4j-graph-database-for-hetnets/112#note-195"
    },
    {
      "added": "2015-12-04T22:16:40.803100Z",
      "body_html": "<p>Thanks - my problem was, though, that my network queries could not be expressed efficiently in Cypher. Whereas shortest path could be done very efficiently, something as simple as shortest path in a weighted graph could not. Maybe Cypher has become more powerful since?</p>",
      "body_md": "Thanks - my problem was, though, that my network queries could not be expressed efficiently in Cypher. Whereas shortest path could be done very efficiently, something as simple as shortest path in a weighted graph could not. Maybe Cypher has become more powerful since?",
      "comment": 484,
      "profile": 125,
      "url": "/discussion/using-the-neo4j-graph-database-for-hetnets/112#note-196"
    },
    {
      "added": "2015-12-04T23:34:50.299760Z",
      "body_html": "<p>Cypher doesn't have great algorithm coverage. Shortest weighted path <a href=\"http://www.khalidabuhakmeh.com/finding-the-shortest-path-using-neo4j-graph-database\">isn't too hard</a> to implement, albeit inefficiently. However, if you're encoding anything that resembles a hetnet, cypher will beat SQL for data interactions.</p>",
      "body_md": "Cypher doesn't have great algorithm coverage. Shortest weighted path [isn't too hard](http://www.khalidabuhakmeh.com/finding-the-shortest-path-using-neo4j-graph-database) to implement, albeit inefficiently. However, if you're encoding anything that resembles a hetnet, cypher will beat SQL for data interactions.",
      "comment": 484,
      "profile": 17,
      "url": "/discussion/using-the-neo4j-graph-database-for-hetnets/112#note-197"
    },
    {
      "added": "2015-12-14T00:55:48.202064Z",
      "body_html": "<p>We've <a href=\"https://github.com/dhimmel/hetio/commit/0683f5fd43ff0ca51c0bd1de1217f589a8891274\">implemented</a> the <strong>labeled</strong> method, which can be used for our path traversal, since the metapath is known <em>a priori</em>.</p>",
      "body_md": "We've [implemented](https://github.com/dhimmel/hetio/commit/0683f5fd43ff0ca51c0bd1de1217f589a8891274) the **labeled** method, which can be used for our path traversal, since the metapath is known *a priori*.",
      "comment": 572,
      "profile": 17,
      "url": "/discussion/path-exclusion-conditions/134#note-198"
    },
    {
      "added": "2016-02-05T22:03:55.034805Z",
      "body_html": "<p>Noting another disease–phenotype approach <span class=\"citation\">[<a href=\"/doi/10.1038/srep10888\" class=\"citation\" data-key=\"10.1038/srep10888\">1</a>]</span> titled \"Analysis of the human diseasome using phenotype similarity between common, genetic, and infectious diseases.\"</p>",
      "body_md": "Noting another disease--phenotype approach [@10.1038/srep10888] titled \"Analysis of the human diseasome using phenotype similarity between common, genetic, and infectious diseases.\"",
      "comment": 345,
      "profile": 17,
      "url": "/discussion/suggestions-for-additional-information-types/22#note-204"
    },
    {
      "added": "2016-02-20T06:23:56.287611Z",
      "body_html": "<p>Pouya's curations are available as a <a href=\"https://github.com/dhimmel/indications/blob/d3d57b07abcf8d7af55f8e42ef7c4c99acd87ca8/curation/pk/template-pk%20final.xlsx\" title=\"GitHub · `template-pk final.xlsx`\">spreadsheet</a> or <a href=\"https://github.com/dhimmel/indications/blob/49cf8e3858f124e552051fc1e8c1c229a43dadf6/curation/pk/curation-PK.tsv\" title=\"GitHub · `curation-PK.tsv`\">TSV file</a>.</p>",
      "body_md": "Pouya's curations are available as a [spreadsheet](https://github.com/dhimmel/indications/blob/d3d57b07abcf8d7af55f8e42ef7c4c99acd87ca8/curation/pk/template-pk%20final.xlsx \"GitHub · `template-pk final.xlsx`\") or [TSV file](https://github.com/dhimmel/indications/blob/49cf8e3858f124e552051fc1e8c1c229a43dadf6/curation/pk/curation-PK.tsv \"GitHub · `curation-PK.tsv`\").",
      "comment": 900,
      "profile": 17,
      "url": "/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95#note-205"
    },
    {
      "added": "2016-02-26T17:37:11.558424Z",
      "body_html": "<p>Exactly - we think alike. What I refer to as \"not a happy cell\" is usually some mix of stress response and reduced growth rate, which also results in a change in the distribution of cells across cell-cycle phases.</p>",
      "body_md": "Exactly - we think alike. What I refer to as \"not a happy cell\" is usually some mix of stress response and reduced growth rate, which also results in a change in the distribution of cells across cell-cycle phases.",
      "comment": 1137,
      "profile": 125,
      "url": "/discussion/positive-correlations-between-knockdown-and-overexpression-profiles-from-lincs-l1000/171#note-207"
    },
    {
      "added": "2016-02-26T17:42:00.179885Z",
      "body_html": "<p><a href=\"/u/larsjuhljensen\" class=\"username\">@larsjuhljensen</a> Indeed! My guess right now is that or no gene-wise normalization (e.g. the most highly expressed genes are still the most highly expressed).</p>",
      "body_md": "@larsjuhljensen Indeed! My guess right now is that or no gene-wise normalization (e.g. the most highly expressed genes are still the most highly expressed).",
      "comment": 1137,
      "profile": 22,
      "url": "/discussion/positive-correlations-between-knockdown-and-overexpression-profiles-from-lincs-l1000/171#note-208"
    },
    {
      "added": "2016-02-26T17:46:19.124138Z",
      "body_html": "<p>I assume that the expression values here are already ratios between perturbed and non-perturbed. Otherwise, I would put my money on it being a normalization artifact, but in that case I would expect a much stronger correlation than what is observed.</p>",
      "body_md": "I assume that the expression values here are already ratios between perturbed and non-perturbed. Otherwise, I would put my money on it being a normalization artifact, but in that case I would expect a much stronger correlation than what is observed.",
      "comment": 1137,
      "profile": 125,
      "url": "/discussion/positive-correlations-between-knockdown-and-overexpression-profiles-from-lincs-l1000/171#note-209"
    },
    {
      "added": "2016-03-08T06:25:39.678908Z",
      "body_html": "<p>I removed the following sentence, since it is not true:</p>\n\n<blockquote><p>Unfortunately, none of the perturbed genes were in the landmark set, so I can't detect whether the perturbations are actually affecting their target genes in the desired direction.</p></blockquote>",
      "body_md": "I removed the following sentence, since it is not true:\n\n> Unfortunately, none of the perturbed genes were in the landmark set, so I can't detect whether the perturbations are actually affecting their target genes in the desired direction.",
      "comment": 1051,
      "profile": 17,
      "url": "/discussion/positive-correlations-between-knockdown-and-overexpression-profiles-from-lincs-l1000/171#note-210"
    },
    {
      "added": "2016-03-09T06:46:22.029813Z",
      "body_html": "<p>Reassuring to see that things behave the way I would expect. This should make it fairly easy to derive a scoring scheme that extracts only associations that are specifically associated with a small number of perturbations, as opposed to associated with any perturbation.</p>",
      "body_md": "Reassuring to see that things behave the way I would expect. This should make it fairly easy to derive a scoring scheme that extracts only associations that are specifically associated with a small number of perturbations, as opposed to associated with any perturbation.",
      "comment": 1166,
      "profile": 125,
      "url": "/discussion/workshop-to-analyze-lincs-data-for-the-systems-pharmacology-course-at-ucsf/181#note-211"
    },
    {
      "added": "2016-03-09T06:47:25.413092Z",
      "body_html": "<p>The part about broad downregulation occurring in tandem with broad upregulation is almost a given. Even if it is not the case biologically, this will be the case after most normalization methods.</p>",
      "body_md": "The part about broad downregulation occurring in tandem with broad upregulation is almost a given. Even if it is not the case biologically, this will be the case after most normalization methods.",
      "comment": 1166,
      "profile": 125,
      "url": "/discussion/workshop-to-analyze-lincs-data-for-the-systems-pharmacology-course-at-ucsf/181#note-212"
    },
    {
      "added": "2016-03-15T03:00:45.439522Z",
      "body_html": "<p>I <a href=\"https://github.com/dhimmel/integrate/blob/145810796f0729d1ed5255bcb83c658490a198f9/licenses/README.md\">updated the table</a> to include funding information. Without seeing the specific contractual arrangements between funders and Universities and between Universities and their researchers, it's difficult to know whether there are any binding obligations regarding data licensing.</p>",
      "body_md": "I [updated the table](https://github.com/dhimmel/integrate/blob/145810796f0729d1ed5255bcb83c658490a198f9/licenses/README.md) to include funding information. Without seeing the specific contractual arrangements between funders and Universities and between Universities and their researchers, it's difficult to know whether there are any binding obligations regarding data licensing.",
      "comment": 462,
      "profile": 17,
      "url": "/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#note-214"
    },
    {
      "added": "2016-03-15T06:31:19.499600Z",
      "body_html": "<p>I get an error when trying to follow the figshare link. Looks like the DOI is either wrong or not registered correctly (yet).</p>",
      "body_md": "I get an error when trying to follow the figshare link. Looks like the DOI is either wrong or not registered correctly (yet).",
      "comment": 1170,
      "profile": 125,
      "url": "/discussion/announcing-pharmacotherapydb-the-open-catalog-of-drug-therapies-for-disease/182#note-215"
    },
    {
      "added": "2016-03-15T08:09:10.808485Z",
      "body_html": "<p>If I were you, I would just delete Human Interactome Database and ADEPTUS from the network, unless they are crucial. Considering that over half a year has gone by without them responding, it seems likely that you will get permission, so I would remove them to not have legally questionable data in my resource.</p>",
      "body_md": "If I were you, I would just delete Human Interactome Database and ADEPTUS from the network, unless they are crucial. Considering that over half a year has gone by without them responding, it seems likely that you will get permission, so I would remove them to not have legally questionable data in my resource.",
      "comment": 1176,
      "profile": 125,
      "url": "/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#note-216"
    },
    {
      "added": "2016-03-15T15:02:14.154216Z",
      "body_html": "<p>The figshare DOI link is <a href=\"https://doi.org/10.6084/m9.figshare.3103054\">now active</a>. The citation metadata <span class=\"citation\">[<a href=\"/doi/10.6084/m9.figshare.3103054\" class=\"citation\" data-key=\"10.6084/m9.figshare.3103054\">1</a>]</span> should resolve with time.</p>",
      "body_md": "The figshare DOI link is [now active](https://doi.org/10.6084/m9.figshare.3103054). The citation metadata [@10.6084/m9.figshare.3103054] should resolve with time.",
      "comment": 1170,
      "profile": 17,
      "url": "/discussion/announcing-pharmacotherapydb-the-open-catalog-of-drug-therapies-for-disease/182#note-217"
    },
    {
      "added": "2016-03-15T16:36:03.799865Z",
      "body_html": "<p>ADEPTUS is no longer included (indicated by ‡ <a href=\"https://github.com/dhimmel/integrate/blob/145810796f0729d1ed5255bcb83c658490a198f9/licenses/README.md\">here</a>). The Human Interactome Database (<a href=\"http://interactome.dfci.harvard.edu/H_sapiens/index.php?page=download\">HID</a>) is important because it's the only <a href=\"http://thinklab.com/discussion/creating-a-catalog-of-protein-interactions/85#9\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d85\">resource we include</a> of systematic (unbiased by knowledge) interactions. The HID — having received many millions in NIH funding (see <a href=\"http://grantome.com/grant/NIH/U01-HG001715-16\">a</a>, <a href=\"http://grantome.com/grant/NIH/U41-HG001715-17\">b</a>, &amp; <a href=\"http://grantome.com/grant/NIH/R01-HG001715-13\">c</a>) to create its dataset — provides a unique resource that is commonly reused despite it's lack of a license. I will follow up on my initial emails to the HID.</p>",
      "body_md": "ADEPTUS is no longer included (indicated by ‡ [here](https://github.com/dhimmel/integrate/blob/145810796f0729d1ed5255bcb83c658490a198f9/licenses/README.md)). The Human Interactome Database ([HID](http://interactome.dfci.harvard.edu/H_sapiens/index.php?page=download)) is important because it's the only [resource we include](http://thinklab.com/discussion/creating-a-catalog-of-protein-interactions/85#9) of systematic (unbiased by knowledge) interactions. The HID -- having received many millions in NIH funding (see [a](http://grantome.com/grant/NIH/U01-HG001715-16), [b](http://grantome.com/grant/NIH/U41-HG001715-17), & [c](http://grantome.com/grant/NIH/R01-HG001715-13)) to create its dataset -- provides a unique resource that is commonly reused despite it's lack of a license. I will follow up on my initial emails to the HID.",
      "comment": 1176,
      "profile": 17,
      "url": "/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#note-218"
    },
    {
      "added": "2016-03-17T04:22:20.379990Z",
      "body_html": "<p>Lars, you should have mentioned your article on whether graph databases are ready for bioinformatics <span class=\"citation\">[<a href=\"/doi/10.1093/bioinformatics/btt549\" class=\"citation\" data-key=\"10.1093/bioinformatics/btt549\">1</a>]</span>! This is a citation that belongs in this discussion.</p>",
      "body_md": "Lars, you should have mentioned your article on whether graph databases are ready for bioinformatics [@10.1093/bioinformatics/btt549]! This is a citation that belongs in this discussion.",
      "comment": 484,
      "profile": 17,
      "url": "/discussion/using-the-neo4j-graph-database-for-hetnets/112#note-219"
    },
    {
      "added": "2016-03-21T13:29:59.475518Z",
      "body_html": "<p>I'm up for it. I should have some time either late this week or early next week. </p>",
      "body_md": "I'm up for it. I should have some time either late this week or early next week. ",
      "comment": 1184,
      "profile": 188,
      "url": "/discussion/incorporating-drugcentral-data-in-our-network/186#note-220"
    },
    {
      "added": "2016-03-31T05:00:22.399653Z",
      "body_html": "<p>We <a href=\"http://thinklab.com/discussion/the-adeptus-resource-for-expression-signatures-of-disease/101#2\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d101\">received permission</a> to use ADEPTUS. The ADEPTUS row in the permission request table now becomes PERM after 213 days.</p>",
      "body_md": "We [received permission](http://thinklab.com/discussion/the-adeptus-resource-for-expression-signatures-of-disease/101#2) to use ADEPTUS. The ADEPTUS row in the permission request table now becomes PERM after 213 days.",
      "comment": 1176,
      "profile": 17,
      "url": "/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#note-221"
    },
    {
      "added": "2016-03-31T05:10:48.632581Z",
      "body_html": "<p>I know it's dangerous to make pre-morning coffee comments on a mathematical proof, but didn't you get N_START and N_END swapped in the last formula? In the formula just above, I thought N_A was the number of start nodes and N_D the number of end nodes.</p>\n\n<p>It would also make more sense intuitively, since it implies that N_START⋅FC = N_END⋅BC. In other words, it doesn't matter whether you start from every start node and do forward traversal or start from every end node and do backward traversal. Both involve that you traverse all possible paths between all start nodes and all end nodes and have the same complexity.</p>",
      "body_md": "I know it's dangerous to make pre-morning coffee comments on a mathematical proof, but didn't you get N_START and N_END swapped in the last formula? In the formula just above, I thought N_A was the number of start nodes and N_D the number of end nodes.\n\nIt would also make more sense intuitively, since it implies that N_START⋅FC = N_END⋅BC. In other words, it doesn't matter whether you start from every start node and do forward traversal or start from every end node and do backward traversal. Both involve that you traverse all possible paths between all start nodes and all end nodes and have the same complexity.",
      "comment": 1191,
      "profile": 125,
      "url": "/discussion/estimating-the-complexity-of-hetnet-traversal/187#note-222"
    },
    {
      "added": "2016-04-01T01:47:55.441382Z",
      "body_html": "<p>Just came across this publication <span class=\"citation\">[<a href=\"/doi/10.1002/psp4.12009\" class=\"citation\" data-key=\"10.1002/psp4.12009\">1</a>]</span>, which analyzed how chemical similarity correlated with transcriptional similarity in LINCS L1000.</p>",
      "body_md": "Just came across this publication [@10.1002/psp4.12009], which analyzed how chemical similarity correlated with transcriptional similarity in LINCS L1000.",
      "comment": 350,
      "profile": 17,
      "url": "/discussion/calculating-molecular-similarities-between-drugbank-compounds/70#note-223"
    },
    {
      "added": "2016-04-02T20:22:56.180964Z",
      "body_html": "<p>Noting an updated WikiPathways citation <span class=\"citation\">[<a href=\"/doi/10.1093/nar/gkv1024\" class=\"citation\" data-key=\"10.1093/nar/gkv1024\">1</a>]</span> in 2016 <em>NAR</em> Database Issue.</p>",
      "body_md": "Noting an updated WikiPathways citation [@10.1093/nar/gkv1024] in 2016 _NAR_ Database Issue.",
      "comment": 244,
      "profile": 17,
      "url": "/discussion/adding-pathway-resources-to-your-network/72#note-224"
    },
    {
      "added": "2016-04-03T04:21:04.803273Z",
      "body_html": "<p>MSigDB — the only remaining category 3 resource — has now <a href=\"http://thinklab.com/discussion/msigdb-licensing/108#3\" class=\"citation thread-citation\" data-key=\"10.15363/thinklab.d108\">been removed</a>.</p>",
      "body_md": "MSigDB -- the only remaining category 3 resource -- has now [been removed](http://thinklab.com/discussion/msigdb-licensing/108#3).",
      "comment": 1176,
      "profile": 17,
      "url": "/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107#note-225"
    },
    {
      "added": "2016-04-03T20:18:20.176280Z",
      "body_html": "<p><a href=\"/u/larsjuhljensen\" class=\"username\">@larsjuhljensen</a>: I wish I could say I let this one slip to test readership's attention, but it just seems that your pre-morning coffee brain is more awake than my post-lunch one. Well spotted - thank you.</p>",
      "body_md": "@larsjuhljensen: I wish I could say I let this one slip to test readership's attention, but it just seems that your pre-morning coffee brain is more awake than my post-lunch one. Well spotted - thank you.",
      "comment": 1191,
      "profile": 23,
      "url": "/discussion/estimating-the-complexity-of-hetnet-traversal/187#note-226"
    },
    {
      "added": "2016-04-03T20:19:09.802940Z",
      "body_html": "<p>Correct</p>",
      "body_md": "Correct",
      "comment": 1203,
      "profile": 23,
      "url": "/discussion/estimating-the-complexity-of-hetnet-traversal/187#note-227"
    },
    {
      "added": "2016-04-04T17:33:36.794753Z",
      "body_html": "<p><a href=\"/u/pouyakhankhanian\" class=\"username\">@pouyakhankhanian</a> I find your last remark interesting, but unfortunately not quite accurate. I'll try to show it below (notes are not good for math...)</p>",
      "body_md": "@pouyakhankhanian I find your last remark interesting, but unfortunately not quite accurate. I'll try to show it below (notes are not good for math...)",
      "comment": 1211,
      "profile": 23,
      "url": "/discussion/transforming-dwpcs-for-hetnet-edge-prediction/193#note-228"
    },
    {
      "added": "2016-04-04T18:19:03.130302Z",
      "body_html": "<p>To be clear, I didn't mean \"derivative\" in the strict mathematical definition (i.e. d/dx f(x)) .  I supposed a better word would be a \"relative\" or \"derivation\", so please excuse the language. And I do agree that both functions are very similar, that was the point I was trying to make. I think Daniel understood the spirit of my comment and has addressed it appropriately.</p>",
      "body_md": "To be clear, I didn't mean \"derivative\" in the strict mathematical definition (i.e. d/dx f(x)) .  I supposed a better word would be a \"relative\" or \"derivation\", so please excuse the language. And I do agree that both functions are very similar, that was the point I was trying to make. I think Daniel understood the spirit of my comment and has addressed it appropriately.",
      "comment": 1215,
      "profile": 188,
      "url": "/discussion/transforming-dwpcs-for-hetnet-edge-prediction/193#note-229"
    },
    {
      "added": "2016-04-04T18:29:09.339041Z",
      "body_html": "<p>I thought it was maybe the case, but also that clarification might be useful in general.</p>",
      "body_md": "I thought it was maybe the case, but also that clarification might be useful in general.",
      "comment": 1215,
      "profile": 23,
      "url": "/discussion/transforming-dwpcs-for-hetnet-edge-prediction/193#note-230"
    },
    {
      "added": "2016-04-04T18:37:42.089421Z",
      "body_html": "<p>RStudio is getting better with every iteration and has improved significantly since this post. Many features (smart completion, tight git integration, ...) make it a better IDE today than the Jupyter notebooks in my opinion.</p>",
      "body_md": "RStudio is getting better with every iteration and has improved significantly since this post. Many features (smart completion, tight git integration, ...) make it a better IDE today than the Jupyter notebooks in my opinion.",
      "comment": 277,
      "profile": 23,
      "url": "/discussion/r-best-practices/83#note-231"
    },
    {
      "added": "2016-04-05T17:39:22.111050Z",
      "body_html": "<p>I would love to see how sequential complexity predicts midpoint runtime when these results will be ready.</p>",
      "body_md": "I would love to see how sequential complexity predicts midpoint runtime when these results will be ready.",
      "comment": 1206,
      "profile": 23,
      "url": "/discussion/estimating-the-complexity-of-hetnet-traversal/187#note-233"
    },
    {
      "added": "2016-04-12T16:36:09.961963Z",
      "body_html": "<p>CHANGELOG: <br>- better counting of total number (relied on the cumulative density before)</p>",
      "body_md": "CHANGELOG: \n- better counting of total number (relied on the cumulative density before)",
      "comment": 1248,
      "profile": 23,
      "url": "/discussion/measuring-user-contribution-and-content-creation/200#note-240"
    },
    {
      "added": "2016-04-14T19:34:24.792264Z",
      "body_html": "<blockquote><p>Ultimately the predictions will be unaffected by whether we adopt a formal testing scheme.</p></blockquote>\n\n<p>I don't really agree with that, as the self-testing here also applies when fitting the model. Correctly incorporating the prior should nevertheless mitigate this issue.</p>",
      "body_md": "> Ultimately the predictions will be unaffected by whether we adopt a formal testing scheme.\n\nI don't really agree with that, as the self-testing here also applies when fitting the model. Correctly incorporating the prior should nevertheless mitigate this issue.\n",
      "comment": 1221,
      "profile": 23,
      "url": "/discussion/network-edge-prediction-how-to-deal-with-self-testing/194#note-241"
    },
    {
      "added": "2016-04-14T23:00:59.751005Z",
      "body_html": "<p>I was assuming an approach where you partition for assessing performance, but then refit on the entire dataset for your final predictions.</p>",
      "body_md": "I was assuming an approach where you partition for assessing performance, but then refit on the entire dataset for your final predictions.",
      "comment": 1221,
      "profile": 17,
      "url": "/discussion/network-edge-prediction-how-to-deal-with-self-testing/194#note-242"
    },
    {
      "added": "2016-04-18T17:37:29.490854Z",
      "body_html": "<p>We knew there would be a great deal of disparity, at least in the number of different values (hence the high auroc) - the surprise to me is the high value of the highest probability. Could we get the updated AUROC from this empiric prior? <br>Also, </p>\n\n<blockquote><p>The pseudo-linear relationship we see ... suggests an analytic solution may exist</p></blockquote>\n\n<p>I agree only approximately :-) The intricate relation that we see suggest on the contrary that there is no easy exact solution. We should focus on getting a good approximation.</p>",
      "body_md": "We knew there would be a great deal of disparity, at least in the number of different values (hence the high auroc) - the surprise to me is the high value of the highest probability. Could we get the updated AUROC from this empiric prior? \nAlso, \n\n>The pseudo-linear relationship we see ... suggests an analytic solution may exist\n\nI agree only approximately :-) The intricate relation that we see suggest on the contrary that there is no easy exact solution. We should focus on getting a good approximation.",
      "comment": 1267,
      "profile": 23,
      "url": "/discussion/network-edge-prediction-estimating-the-prior/201#note-244"
    },
    {
      "added": "2016-04-18T17:41:43.533656Z",
      "body_html": "<p>Beautiful - why not the complete square? I find it easier to read and the remaining space is left blank here anyway.</p>",
      "body_md": "Beautiful - why not the complete square? I find it easier to read and the remaining space is left blank here anyway.",
      "comment": 1276,
      "profile": 23,
      "url": "/discussion/describing-hetionet-v10-through-visualization-and-statistics/202#note-245"
    },
    {
      "added": "2016-04-18T17:47:20.553457Z",
      "body_html": "<p>Thanks for the reference - great read. It seems hard to implement without easy-to-use tools. Will you give it a try?</p>",
      "body_md": "Thanks for the reference - great read. It seems hard to implement without easy-to-use tools. Will you give it a try?",
      "comment": 1278,
      "profile": 23,
      "url": "/discussion/describing-hetionet-v10-through-visualization-and-statistics/202#note-246"
    },
    {
      "added": "2016-04-18T18:00:49.040466Z",
      "body_html": "<p>EDIT: removed because unnecessarily complicated while wrong.</p>",
      "body_md": "EDIT: removed because unnecessarily complicated while wrong.",
      "comment": 1269,
      "profile": 23,
      "url": "/discussion/estimating-the-complexity-of-hetnet-traversal/187#note-247"
    },
    {
      "added": "2016-04-18T18:55:45.079216Z",
      "body_html": "<p>By sequential complexity, I am referring to forward sequential complexity. My choice between forward versus backward complexity was arbitrary but motivated by our application where we have more compounds than diseases. The way I have approached complexity estimation is for a single source–target node pair. Therefore the current averaging allows comparison to join index complexities. Can you explain how the multipliers differ between metapaths, when they all start on compounds and end on diseases?</p>",
      "body_md": "By sequential complexity, I am referring to forward sequential complexity. My choice between forward versus backward complexity was arbitrary but motivated by our application where we have more compounds than diseases. The way I have approached complexity estimation is for a single source--target node pair. Therefore the current averaging allows comparison to join index complexities. Can you explain how the multipliers differ between metapaths, when they all start on compounds and end on diseases?",
      "comment": 1269,
      "profile": 17,
      "url": "/discussion/estimating-the-complexity-of-hetnet-traversal/187#note-248"
    }
  ],
  "profiles": [
    {
      "first_name": "Jesse",
      "last_name": "Spaulding",
      "url": "/u/jspauld",
      "username": "jspauld"
    },
    {
      "first_name": "Daniel",
      "last_name": "Himmelstein",
      "url": "/u/dhimmel",
      "username": "dhimmel"
    },
    {
      "first_name": "Sergio",
      "last_name": "Baranzini",
      "url": "/u/sergiobaranzini",
      "username": "sergiobaranzini"
    },
    {
      "first_name": "Leo",
      "last_name": "Brueggeman",
      "url": "/u/leobrueggeman",
      "username": "leobrueggeman"
    },
    {
      "first_name": "Casey",
      "last_name": "Greene",
      "url": "/u/caseygreene",
      "username": "caseygreene"
    },
    {
      "first_name": "Antoine",
      "last_name": "Lizee",
      "url": "/u/alizee",
      "username": "alizee"
    },
    {
      "first_name": "Venkat",
      "last_name": "Malladi",
      "url": "/u/vsmalladi",
      "username": "vsmalladi"
    },
    {
      "first_name": "Benjamin",
      "last_name": "Good",
      "url": "/u/b_good",
      "username": "b_good"
    },
    {
      "first_name": "Ritu ",
      "last_name": "Khare",
      "url": "/u/ritukhare",
      "username": "ritukhare"
    },
    {
      "first_name": "Tudor",
      "last_name": "Oprea",
      "url": "/u/TIOprea",
      "username": "TIOprea"
    },
    {
      "first_name": "Allison",
      "last_name": "McCoy",
      "url": "/u/allisonmccoy",
      "username": "allisonmccoy"
    },
    {
      "first_name": "Caty",
      "last_name": "Chung",
      "url": "/u/cchung",
      "username": "cchung"
    },
    {
      "first_name": "Mike",
      "last_name": "Gilson",
      "url": "/u/mkgilson",
      "username": "mkgilson"
    },
    {
      "first_name": "Alessandro",
      "last_name": "Didonna",
      "url": "/u/alessandrodidonna",
      "username": "alessandrodidonna"
    },
    {
      "first_name": "Alex",
      "last_name": "Pankov",
      "url": "/u/apankov",
      "username": "apankov"
    },
    {
      "first_name": "Marina",
      "last_name": "Sirota",
      "url": "/u/marinasirota",
      "username": "marinasirota"
    },
    {
      "first_name": "Alexander",
      "last_name": "Pico",
      "url": "/u/alexanderpico",
      "username": "alexanderpico"
    },
    {
      "first_name": "Raghavendran",
      "last_name": "Partha",
      "url": "/u/raghavpartha",
      "username": "raghavpartha"
    },
    {
      "first_name": "Chris",
      "last_name": "Mungall",
      "url": "/u/chrismungall",
      "username": "chrismungall"
    },
    {
      "first_name": "Frederic",
      "last_name": "Bastian",
      "url": "/u/fbastian",
      "username": "fbastian"
    },
    {
      "first_name": "Sabrina",
      "last_name": "Chen",
      "url": "/u/sabrinachen",
      "username": "sabrinachen"
    },
    {
      "first_name": "Ola",
      "last_name": "O",
      "url": "/u/akolow",
      "username": "akolow"
    },
    {
      "first_name": "Dexter",
      "last_name": "Hadley",
      "url": "/u/idrdex",
      "username": "idrdex"
    },
    {
      "first_name": "Lars Juhl",
      "last_name": "Jensen",
      "url": "/u/larsjuhljensen",
      "username": "larsjuhljensen"
    },
    {
      "first_name": "janet",
      "last_name": "piñero",
      "url": "/u/janispi",
      "username": "janispi"
    },
    {
      "first_name": "MacKenzie",
      "last_name": "Smith",
      "url": "/u/mackenziesmith",
      "username": "mackenziesmith"
    },
    {
      "first_name": "Katie",
      "last_name": "Fortney",
      "url": "/u/katiefortney",
      "username": "katiefortney"
    },
    {
      "first_name": "Chrissy",
      "last_name": "Hessler",
      "url": "/u/chrissyhessler",
      "username": "chrissyhessler"
    },
    {
      "first_name": "Pouya",
      "last_name": "Khankhanian",
      "url": "/u/pouyakhankhanian",
      "username": "pouyakhankhanian"
    },
    {
      "first_name": "Kathleen",
      "last_name": "Keough",
      "url": "/u/kathleenk",
      "username": "kathleenk"
    },
    {
      "first_name": "Beau",
      "last_name": "Norgeot",
      "url": "/u/beaunorgeot",
      "username": "beaunorgeot"
    },
    {
      "first_name": "Misha",
      "last_name": "Vysotskiy",
      "url": "/u/mishavysotskiy",
      "username": "mishavysotskiy"
    },
    {
      "first_name": "Jeffrey",
      "last_name": "Kim",
      "url": "/u/jeffreykim",
      "username": "jeffreykim"
    },
    {
      "first_name": "Julia",
      "last_name": "Cluceru",
      "url": "/u/juliacluceru",
      "username": "juliacluceru"
    },
    {
      "first_name": "Marjorie",
      "last_name": "Imperial",
      "url": "/u/marjorieimperial",
      "username": "marjorieimperial"
    },
    {
      "first_name": "Emmalyn",
      "last_name": "Chen",
      "url": "/u/emmalynchen",
      "username": "emmalynchen"
    },
    {
      "first_name": "Jasleen",
      "last_name": "Sodhi",
      "url": "/u/jasleensodhi",
      "username": "jasleensodhi"
    },
    {
      "first_name": "Elizabeth",
      "last_name": "Levy",
      "url": "/u/elizabethlevy1",
      "username": "elizabethlevy1"
    },
    {
      "first_name": "Greg",
      "last_name": "Way",
      "url": "/u/gregway",
      "username": "gregway"
    },
    {
      "first_name": "Alexey",
      "last_name": "Strokach",
      "url": "/u/ostrokach",
      "username": "ostrokach"
    }
  ],
  "retrieved": "2016-04-19T14:18:14.280907Z",
  "threads": [
    {
      "document": null,
      "doi": "10.15363/thinklab.d21",
      "doi_field": null,
      "profile": 17,
      "published": "2015-01-14T05:55:24.808111Z",
      "subject": "How should we construct a catalog of drug indications?",
      "topic_field": "Bioinformatics,Natural Language Processing,Indications,Pharmacology,Text Mining,Pubchem,Informatics,Disease Ontology",
      "url": "/discussion/how-should-we-construct-a-catalog-of-drug-indications/21",
      "views": 300
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d22",
      "doi_field": null,
      "profile": 17,
      "published": "2015-01-16T00:46:28.770398Z",
      "subject": "Suggestions for additional information types?",
      "topic_field": "Bioinformatics,Chemoinformatics,Databases",
      "url": "/discussion/suggestions-for-additional-information-types/22",
      "views": 74
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d23",
      "doi_field": null,
      "profile": 2,
      "published": "2015-01-16T10:18:57.231835Z",
      "subject": "Enabling reproducibility and reuse",
      "topic_field": "Reproducibility",
      "url": "/discussion/enabling-reproducibility-and-reuse/23",
      "views": 74
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d25",
      "doi_field": null,
      "profile": 17,
      "published": "2015-01-22T18:14:09.267876Z",
      "subject": "Seeking an open source implementation of the Gerstein-Sonnhammer-Chothia Algorithm",
      "topic_field": "Python,R,Algorithms,Computer Science,SIDER,GSC Weighting,Gerstein-Sonnhammer-Chothia,Julia,Open Source Software",
      "url": "/discussion/seeking-an-open-source-implementation-of-the-gerstein-sonnhammer-chothia-algorithm/25",
      "views": 93
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d30",
      "doi_field": "10.1038/msb.2009.98",
      "profile": 17,
      "published": "2015-02-13T02:25:05.517557Z",
      "subject": "Assessing the quality and applicability of the SIDER 2 resource",
      "topic_field": "SIDER,Indications,NLP,Drugs,Side Effects,Drug Labels",
      "url": "/discussion/assessing-the-quality-and-applicability-of-the-sider-2-resource/30",
      "views": 85
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d31",
      "doi_field": "10.1136/amiajnl-2012-001431",
      "profile": 17,
      "published": "2015-02-17T02:21:47.729396Z",
      "subject": "MEDI indications data — discrepancy in resource-specific counts",
      "topic_field": "SIDER,Indications,NLP,MEDI,Wikipedia,MedlinePlus,ICD9,UMLS,RxNorm",
      "url": "/discussion/medi-indications-data-discrepancy-in-resource-specific-counts/31",
      "views": 40
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d34",
      "doi_field": null,
      "profile": 17,
      "published": "2015-02-27T19:35:36.720301Z",
      "subject": "Using Entrez Gene as our gene vocabulary",
      "topic_field": "Databases,HGNC,Genes,Entrez Gene",
      "url": "/discussion/using-entrez-gene-as-our-gene-vocabulary/34",
      "views": 102
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d39",
      "doi_field": "10.1038/75556",
      "profile": 17,
      "published": "2015-03-12T16:26:39.575818Z",
      "subject": "Compiling Gene Ontology annotations into an easy-to-use format",
      "topic_field": "GO,Propagation,Gene Ontology",
      "url": "/discussion/compiling-gene-ontology-annotations-into-an-easy-to-use-format/39",
      "views": 126
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d40",
      "doi_field": null,
      "profile": 17,
      "published": "2015-03-16T23:22:11.497921Z",
      "subject": "Unifying drug vocabularies",
      "topic_field": "Drugs,Small Molecules,Compounds,Terminologies,Standards,Mapping,DrugBank,UniChem,InChI",
      "url": "/discussion/unifying-drug-vocabularies/40",
      "views": 164
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d41",
      "doi_field": "10.1186/gb-2012-13-1-r5",
      "profile": 35,
      "published": "2015-03-19T19:15:53.241554Z",
      "subject": "Tissue Node",
      "topic_field": "Ontologies",
      "url": "/discussion/tissue-node/41",
      "views": 86
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d42",
      "doi_field": "10.1126/science.1257601",
      "profile": 21,
      "published": "2015-03-25T21:20:49.503543Z",
      "subject": "Mapping Incomplete Interactome disease names to MeSH",
      "topic_field": "Terminologies,Mapping,Incomplete Interactome,MeSH",
      "url": "/discussion/mapping-incomplete-interactome-disease-names-to-mesh/42",
      "views": 45
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d43",
      "doi_field": "",
      "profile": 17,
      "published": "2015-03-27T02:43:44.010612Z",
      "subject": "Computing consensus transcriptional profiles for LINCS L1000 perturbations",
      "topic_field": "LINCS,Expression Signatures,Transcriptional Profiles",
      "url": "/discussion/computing-consensus-transcriptional-profiles-for-lincs-l1000-perturbations/43",
      "views": 218
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d44",
      "doi_field": null,
      "profile": 17,
      "published": "2015-03-31T00:16:47.505964Z",
      "subject": "Unifying disease vocabularies",
      "topic_field": "UMLS,Terminologies,Mapping,Disease Ontology,Vocabularies,Diseases,MeSH,DO",
      "url": "/discussion/unifying-disease-vocabularies/44",
      "views": 112
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d46",
      "doi_field": "10.1016/j.jbi.2014.08.004",
      "profile": 17,
      "published": "2015-04-02T17:16:17.459590Z",
      "subject": "Processing LabeledIn to extract indications",
      "topic_field": "Indications,LabeledIn,Data Processing,Crowdsourcing",
      "url": "/discussion/processing-labeledin-to-extract-indications/46",
      "views": 53
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d47",
      "doi_field": null,
      "profile": 48,
      "published": "2015-04-03T04:43:56.722211Z",
      "subject": "Evaluation framework",
      "topic_field": "Validation,Evalauation",
      "url": "/discussion/evaluation-framework/47",
      "views": 29
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d48",
      "doi_field": null,
      "profile": 48,
      "published": "2015-04-03T04:53:35.463502Z",
      "subject": "Text as a resource for network population?",
      "topic_field": "Natural Language Processing,Relation Extraction,Bias,Prior Knowledge",
      "url": "/discussion/text-as-a-resource-for-network-population/48",
      "views": 45
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d51",
      "doi_field": null,
      "profile": 21,
      "published": "2015-04-09T00:53:23.860871Z",
      "subject": "UniChem Mapping to LINCS Small Molecules",
      "topic_field": "Mapping,UniChem,LINCS",
      "url": "/discussion/unichem-mapping-to-lincs-small-molecules/51",
      "views": 146
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d52",
      "doi_field": "10.1038/ncomms5212",
      "profile": 21,
      "published": "2015-04-09T17:39:42.860225Z",
      "subject": "Human Symptom Disease Network-MeSH ID Matching",
      "topic_field": "HSDN",
      "url": "/discussion/human-symptom-disease-network-mesh-id-matching/52",
      "views": 174
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d53",
      "doi_field": "10.1093/nar/gkl999",
      "profile": 17,
      "published": "2015-04-13T20:32:22.752831Z",
      "subject": "Integrating drug target information from BindingDB",
      "topic_field": "Databases,Compounds,BindingDB,PubChem BioAssay,Affinity,Binding Affinity,Drug Targets,ChEMBL",
      "url": "/discussion/integrating-drug-target-information-from-bindingdb/53",
      "views": 134
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d57",
      "doi_field": "10.1371/journal.pgen.1004967",
      "profile": 17,
      "published": "2015-04-23T03:38:31.895356Z",
      "subject": "Selecting informative ERC (evolutionary rate covariation) values between genes",
      "topic_field": "Evolution,Entrez Gene,ERC,UCSC Genome Browser,Evolutionary Rate Covariation",
      "url": "/discussion/selecting-informative-erc-evolutionary-rate-covariation-values-between-genes/57",
      "views": 49
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d61",
      "doi_field": null,
      "profile": 17,
      "published": "2015-04-30T22:03:36.172813Z",
      "subject": "Retrieving the ingredients in RxNorm concepts",
      "topic_field": "Drugs,RxNorm,Compounds,Terminologies,Ontologies,Mapping",
      "url": "/discussion/retrieving-the-ingredients-in-rxnorm-concepts/61",
      "views": 221
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d62",
      "doi_field": "10.1136/amiajnl-2012-000852",
      "profile": 17,
      "published": "2015-05-01T14:51:24.521647Z",
      "subject": "Extracting indications from the ehrlink resource",
      "topic_field": "Databases,Indications,Clinical Informatics,EHR,Ehrlink,Medications,Health Records",
      "url": "/discussion/extracting-indications-from-the-ehrlink-resource/62",
      "views": 46
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d65",
      "doi_field": "10.1093/nar/gkt1068",
      "profile": 17,
      "published": "2015-05-09T06:26:06.835470Z",
      "subject": "Protein (target, carrier, transporter, and enzyme) interactions in DrugBank",
      "topic_field": "Databases,DrugBank,Drug Targets",
      "url": "/discussion/protein-target-carrier-transporter-and-enzyme-interactions-in-drugbank/65",
      "views": 181
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d66",
      "doi_field": "10.1186/s12859-015-0559-3",
      "profile": 22,
      "published": "2015-05-09T13:55:34.587997Z",
      "subject": "KaBOB Knowledgebase",
      "topic_field": "",
      "url": "/discussion/kabob-knowledgebase/66",
      "views": 61
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d67",
      "doi_field": null,
      "profile": 17,
      "published": "2015-05-10T21:10:43.786973Z",
      "subject": "Mining knowledge from MEDLINE articles and their indexed MeSH terms",
      "topic_field": "Text Mining,MEDLINE,Pubmed,Literature Mining,MeSH",
      "url": "/discussion/mining-knowledge-from-medline-articles-and-their-indexed-mesh-terms/67",
      "views": 91
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d68",
      "doi_field": null,
      "profile": 17,
      "published": "2015-05-12T02:41:44.048927Z",
      "subject": "Disease Ontology feature requests",
      "topic_field": "Open Source,Ontologies,Disease Ontology",
      "url": "/discussion/disease-ontology-feature-requests/68",
      "views": 38
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d70",
      "doi_field": "",
      "profile": 17,
      "published": "2015-05-19T02:56:31.048772Z",
      "subject": "Calculating molecular similarities between DrugBank compounds",
      "topic_field": "DrugBank,Structural Similarity,Molecular Similarity,Fingerprint,Dice coefficient,Similarity,Chemical Similarity",
      "url": "/discussion/calculating-molecular-similarities-between-drugbank-compounds/70",
      "views": 57
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d71",
      "doi_field": null,
      "profile": 17,
      "published": "2015-06-08T18:54:53.831578Z",
      "subject": "Calculating genomic windows for GWAS lead SNPs",
      "topic_field": "Genomics,Genetics,LD,Loci,HapMap,Linkage Disequilibrium,Recombination Hotspots,GWAS Catalog,1000 Genomes",
      "url": "/discussion/calculating-genomic-windows-for-gwas-lead-snps/71",
      "views": 209
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d72",
      "doi_field": null,
      "profile": 104,
      "published": "2015-06-09T01:15:52.882392Z",
      "subject": "Adding pathway resources to your network",
      "topic_field": "Databases,Networks,Pathways",
      "url": "/discussion/adding-pathway-resources-to-your-network/72",
      "views": 68
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d80",
      "doi_field": null,
      "profile": 17,
      "published": "2015-06-16T20:43:50.470993Z",
      "subject": "Extracting disease-gene associations from the GWAS Catalog",
      "topic_field": "DO,Disease Etiology,EFO,Association Studies,EBI,GWAS,GWAS Catalog,NHGRI,Associations",
      "url": "/discussion/extracting-disease-gene-associations-from-the-gwas-catalog/80",
      "views": 104
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d81",
      "doi_field": null,
      "profile": 17,
      "published": "2015-06-17T18:56:26.645203Z",
      "subject": "Tissue-specific gene expression resources",
      "topic_field": "Transcription,RNASeq,Gene Expression,Tissue Specificity,Microarrays",
      "url": "/discussion/tissue-specific-gene-expression-resources/81",
      "views": 219
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d82",
      "doi_field": "10.1126/science.aaa0355",
      "profile": 17,
      "published": "2015-06-17T20:51:35.121449Z",
      "subject": "Processing GTEx for tissue-specific gene expression",
      "topic_field": "Transcription,Gene Expression,GTEx,RNA-Seq",
      "url": "/discussion/processing-gtex-for-tissue-specific-gene-expression/82",
      "views": 196
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d83",
      "doi_field": "",
      "profile": 17,
      "published": "2015-06-18T04:18:56.839367Z",
      "subject": "R best practices",
      "topic_field": "R,Rstats,Programming,Coding,Hadley Wickham,Style,Hadleyverse",
      "url": "/discussion/r-best-practices/83",
      "views": 373
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d84",
      "doi_field": null,
      "profile": 17,
      "published": "2015-06-24T05:08:18.194553Z",
      "subject": "Python for the modern biodata scientist",
      "topic_field": "Python,Computer Science,Programming,Jupyter,Coding,IPython,Notebooks",
      "url": "/discussion/python-for-the-modern-biodata-scientist/84",
      "views": 467
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d85",
      "doi_field": null,
      "profile": 17,
      "published": "2015-07-02T00:44:39.833696Z",
      "subject": "Creating a catalog of protein interactions",
      "topic_field": "Protein Interactions,Interactions,PPI",
      "url": "/discussion/creating-a-catalog-of-protein-interactions/85",
      "views": 84
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d91",
      "doi_field": "",
      "profile": 17,
      "published": "2015-07-10T18:15:03.705673Z",
      "subject": "The TISSUES resource for the tissue-specificity of genes",
      "topic_field": "TISSUES,Proteome,UniProtKB,Tissue Specificity,Transcriptome",
      "url": "/discussion/the-tissues-resource-for-the-tissue-specificity-of-genes/91",
      "views": 65
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d93",
      "doi_field": "",
      "profile": 17,
      "published": "2015-07-14T19:30:52.136138Z",
      "subject": "Disease similarity from MEDLINE topic cooccurrence",
      "topic_field": "MEDLINE,Literature Mining,MeSH,Cooccurrence,Disease Similarity",
      "url": "/discussion/disease-similarity-from-medline-topic-cooccurrence/93",
      "views": 28
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d94",
      "doi_field": "10.1371/journal.pone.0049686",
      "profile": 17,
      "published": "2015-07-14T19:55:24.432032Z",
      "subject": "Functional disease annotations for genes using DOAF",
      "topic_field": "Entrez Gene,Disease Ontology,DO,DOAF,GeneRIF,Gene Annotations",
      "url": "/discussion/functional-disease-annotations-for-genes-using-doaf/94",
      "views": 34
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d95",
      "doi_field": null,
      "profile": 17,
      "published": "2015-07-14T21:45:53.200462Z",
      "subject": "Expert curation of our indication catalog for disease-modifying treatments",
      "topic_field": "Indications,Symptomatic Treatment,Disease-Modifying Therapy,Expert Curation",
      "url": "/discussion/expert-curation-of-our-indication-catalog-for-disease-modifying-treatments/95",
      "views": 127
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d96",
      "doi_field": "",
      "profile": 17,
      "published": "2015-07-28T21:14:07.053888Z",
      "subject": "STARGEO: expression signatures for disease using crowdsourced GEO annotation",
      "topic_field": "Gene Expression,Gene Expression Omnibus,STARGEO,GEO,STAR-GEO,Transcriptome",
      "url": "/discussion/stargeo-expression-signatures-for-disease-using-crowdsourced-geo-annotation/96",
      "views": 139
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d97",
      "doi_field": "10.1093/nar/gkv1075",
      "profile": 17,
      "published": "2015-08-08T23:36:57.800091Z",
      "subject": "Extracting side effects from SIDER 4",
      "topic_field": "SIDER,NLP,Drugs,Side Effects,Drug Labels,Text Mining,SIDER4",
      "url": "/discussion/extracting-side-effects-from-sider-4/97",
      "views": 74
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d101",
      "doi_field": "10.1093/nar/gkv810",
      "profile": 17,
      "published": "2015-08-12T23:40:14.796814Z",
      "subject": "The ADEPTUS resource for expression signatures of disease",
      "topic_field": "Transcription,Expression Signatures,STARGEO,ADEPTUS,Transcriptome",
      "url": "/discussion/the-adeptus-resource-for-expression-signatures-of-disease/101",
      "views": 61
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d102",
      "doi_field": null,
      "profile": 17,
      "published": "2015-08-14T22:24:08.006792Z",
      "subject": "One network to rule them all",
      "topic_field": "Networks,Drug Repurposing,Network Biology,Dataset,Heterogeneous Network",
      "url": "/discussion/one-network-to-rule-them-all/102",
      "views": 160
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d104",
      "doi_field": null,
      "profile": 17,
      "published": "2015-08-17T02:56:46.304997Z",
      "subject": "Renaming ‘heterogeneous networks’ to a more concise and catchy term",
      "topic_field": "Heterogeneous Networks,Heterogeneous Information Networks,Nomenclature",
      "url": "/discussion/renaming-heterogeneous-networks-to-a-more-concise-and-catchy-term/104",
      "views": 86
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d105",
      "doi_field": "10.1093/database/bav028",
      "profile": 17,
      "published": "2015-08-18T03:46:32.961745Z",
      "subject": "Processing DisGeNET for disease-gene relationships",
      "topic_field": "Genes,Diseases,DisGeNET,Associations",
      "url": "/discussion/processing-disgenet-for-disease-gene-relationships/105",
      "views": 45
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d106",
      "doi_field": "10.1016/j.ymeth.2014.11.020",
      "profile": 17,
      "published": "2015-08-20T21:45:22.187428Z",
      "subject": "Processing the DISEASES resource for disease–gene relationships",
      "topic_field": "Databases,Diseases,Associations",
      "url": "/discussion/processing-the-diseases-resource-for-diseasegene-relationships/106",
      "views": 32
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d107",
      "doi_field": null,
      "profile": 17,
      "published": "2015-08-28T19:10:38.805491Z",
      "subject": "Integrating resources with disparate licensing into an open network",
      "topic_field": "Open science,Open Data,Copyright,License,Permission,Licensing,Intellectual Property,Creative Commons",
      "url": "/discussion/integrating-resources-with-disparate-licensing-into-an-open-network/107",
      "views": 155
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d108",
      "doi_field": null,
      "profile": 17,
      "published": "2015-09-28T18:33:52.774845Z",
      "subject": "MSigDB licensing",
      "topic_field": "Copyright,Permission,Licensing,MSigDB,MIT",
      "url": "/discussion/msigdb-licensing/108",
      "views": 66
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d110",
      "doi_field": null,
      "profile": 17,
      "published": "2015-09-28T22:59:26.327280Z",
      "subject": "LINCS L1000 licensing",
      "topic_field": "LINCS,Copyright,Licensing,MIT,L1000,Permissions",
      "url": "/discussion/lincs-l1000-licensing/110",
      "views": 126
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d111",
      "doi_field": "10.1126/science.1257601",
      "profile": 17,
      "published": "2015-10-02T01:58:42.504714Z",
      "subject": "Incomplete Interactome licensing",
      "topic_field": "Incomplete Interactome,Permission,Licensing,Copyright Transfer,Supplement,AAAS",
      "url": "/discussion/incomplete-interactome-licensing/111",
      "views": 62
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d112",
      "doi_field": null,
      "profile": 17,
      "published": "2015-10-02T22:20:16.807633Z",
      "subject": "Using the neo4j graph database for hetnets",
      "topic_field": "Database,Neo4J,Graphs,Graph Database,NoSQL",
      "url": "/discussion/using-the-neo4j-graph-database-for-hetnets/112",
      "views": 210
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d113",
      "doi_field": null,
      "profile": 17,
      "published": "2015-10-03T20:51:26.506341Z",
      "subject": "Tracking project reuse, citation, and publicity",
      "topic_field": "Citations,Publicity,Media Coverage,Reuse,Attention",
      "url": "/discussion/tracking-project-reuse-citation-and-publicity/113",
      "views": 37
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d115",
      "doi_field": "",
      "profile": 17,
      "published": "2015-10-04T18:59:31.943591Z",
      "subject": "Assessing the informativeness of features",
      "topic_field": "HNEP,Hetnets,Features,Metapaths",
      "url": "/discussion/assessing-the-informativeness-of-features/115",
      "views": 22
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d124",
      "doi_field": "10.1007/978-3-540-69828-9_12",
      "profile": 17,
      "published": "2015-11-04T02:25:25.390587Z",
      "subject": "Processing Bgee for tissue-specific gene presence and over/under-expression",
      "topic_field": "Gene Expression,Over-Expression,Under-Expression,Bgee,Transcriptome",
      "url": "/discussion/processing-bgee-for-tissue-specific-gene-presence-and-overunder-expression/124",
      "views": 50
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d130",
      "doi_field": null,
      "profile": 17,
      "published": "2015-11-30T20:04:57.637787Z",
      "subject": "Licensing neo4j",
      "topic_field": "Open Source,Licensing,Neo4J,GPLv3,AGPLv3",
      "url": "/discussion/licensing-neo4j/130",
      "views": 47
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d134",
      "doi_field": null,
      "profile": 17,
      "published": "2015-12-09T02:51:40.846273Z",
      "subject": "Path exclusion conditions",
      "topic_field": "HNEP,Network Theory,Graph Theory,Hetnets,Paths",
      "url": "/discussion/path-exclusion-conditions/134",
      "views": 54
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d136",
      "doi_field": null,
      "profile": 17,
      "published": "2015-12-22T01:25:50.912906Z",
      "subject": "Permuting hetnets and implementing randomized edge swaps in cypher",
      "topic_field": "Neo4J,Edge Swaps,Cypher,XSwap,Permutation,Hetnets",
      "url": "/discussion/permuting-hetnets-and-implementing-randomized-edge-swaps-in-cypher/136",
      "views": 32
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d162",
      "doi_field": "",
      "profile": 17,
      "published": "2016-02-17T19:43:44.509494Z",
      "subject": "Data nomenclature: naming and abbreviating our network types",
      "topic_field": "Drug Repurposing,Nomenclature,Hetnets",
      "url": "/discussion/data-nomenclature-naming-and-abbreviating-our-network-types/162",
      "views": 35
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d171",
      "doi_field": "",
      "profile": 17,
      "published": "2016-02-26T17:17:30.782382Z",
      "subject": "Positive correlations between knockdown and overexpression profiles from LINCS L1000",
      "topic_field": "LINCS,L1000,shRNAs,overexpression,knockdown,upregulation,downregulation",
      "url": "/discussion/positive-correlations-between-knockdown-and-overexpression-profiles-from-lincs-l1000/171",
      "views": 77
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d178",
      "doi_field": "",
      "profile": 17,
      "published": "2016-02-25T22:56:18.276962Z",
      "subject": "Assessing the effectiveness of our hetnet permutations",
      "topic_field": "permutation,XSwap,network randomization,hetnets,hetio",
      "url": "/discussion/assessing-the-effectiveness-of-our-hetnet-permutations/178",
      "views": 18
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d181",
      "doi_field": "",
      "profile": 17,
      "published": "2016-03-08T20:48:53.483423Z",
      "subject": "Workshop to analyze LINCS data for the Systems Pharmacology course at UCSF",
      "topic_field": "LINCS,L1000,Systems Pharmacology,UCSF,open education",
      "url": "/discussion/workshop-to-analyze-lincs-data-for-the-systems-pharmacology-course-at-ucsf/181",
      "views": 68
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d182",
      "doi_field": "",
      "profile": 17,
      "published": "2016-03-15T05:24:35.059545Z",
      "subject": "Announcing PharmacotherapyDB: the Open Catalog of Drug Therapies for Disease",
      "topic_field": "Pharmacotherapy,Indications,PharmacotherapyDB,curation",
      "url": "/discussion/announcing-pharmacotherapydb-the-open-catalog-of-drug-therapies-for-disease/182",
      "views": 96
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d185",
      "doi_field": "",
      "profile": 17,
      "published": "2016-03-11T22:41:36.229906Z",
      "subject": "Assessing the imputation quality of gene expression in LINCS L1000",
      "topic_field": "LINCS,L1000,gene expression,imputation,genetic perturbation,knockdown,BING,overepression,downregulation,upregulation",
      "url": "/discussion/assessing-the-imputation-quality-of-gene-expression-in-lincs-l1000/185",
      "views": 23
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d186",
      "doi_field": "",
      "profile": 17,
      "published": "2016-03-20T16:41:10.657363Z",
      "subject": "Incorporating DrugCentral data in our network",
      "topic_field": "DrugCentral,pharmacology,Database",
      "url": "/discussion/incorporating-drugcentral-data-in-our-network/186",
      "views": 28
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d187",
      "doi_field": "",
      "profile": 17,
      "published": "2016-03-22T17:03:53.346144Z",
      "subject": "Estimating the complexity of hetnet traversal",
      "topic_field": "complexity,runtime,optimization,Neo4J,join hints",
      "url": "/discussion/estimating-the-complexity-of-hetnet-traversal/187",
      "views": 22
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d193",
      "doi_field": "",
      "profile": 17,
      "published": "2016-04-01T21:13:21.069496Z",
      "subject": "Transforming DWPCs for hetnet edge prediction",
      "topic_field": "IHS,transformation,DWPC,regression",
      "url": "/discussion/transforming-dwpcs-for-hetnet-edge-prediction/193",
      "views": 19
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d194",
      "doi_field": "",
      "profile": 23,
      "published": "2016-04-05T22:49:42.877538Z",
      "subject": "Network Edge Prediction: how to deal with self-testing",
      "topic_field": "HNEP,Hetnets,Machine learning,self-testing",
      "url": "/discussion/network-edge-prediction-how-to-deal-with-self-testing/194",
      "views": 23
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d200",
      "doi_field": "",
      "profile": 17,
      "published": "2016-04-11T22:43:07.306129Z",
      "subject": "Measuring user contribution and content creation",
      "topic_field": "analytics,contribution,content",
      "url": "/discussion/measuring-user-contribution-and-content-creation/200",
      "views": 9
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d201",
      "doi_field": "",
      "profile": 23,
      "published": "2016-04-15T01:44:45.372095Z",
      "subject": "Network Edge Prediction: Estimating the prior",
      "topic_field": "Machine learning,Heterogeneous Networks,Hetnets,HNEP,Prior Knowledge",
      "url": "/discussion/network-edge-prediction-estimating-the-prior/201",
      "views": 14
    },
    {
      "document": null,
      "doi": "10.15363/thinklab.d202",
      "doi_field": "",
      "profile": 17,
      "published": "2016-04-16T21:39:16.671437Z",
      "subject": "Describing Hetionet v1.0 through visualization and statistics",
      "topic_field": "hetnets,Data Visualization,Hetionet",
      "url": "/discussion/describing-hetionet-v10-through-visualization-and-statistics/202",
      "views": 13
    }
  ]
}